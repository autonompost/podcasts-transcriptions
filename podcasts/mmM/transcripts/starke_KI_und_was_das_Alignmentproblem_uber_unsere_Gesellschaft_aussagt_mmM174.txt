Es geht nicht darum, dass beim Touring-Test rauskommt, dass eine KI sozusagen irgendwann Menschen gleich ist, sondern der Touring-Test steht eigentlich die inverse Frage.
Haben wir als Menschen unsere eigene Intelligenz begriffen?
So Leute, ich hatte ja vor, keine Ahnung, zwei Monaten oder so,
mein Chat-GPT-Video zu gemacht und kurz danach ging GPT4 live, also beziehungsweise die Open Air hat das vorgestellt und hatten auch noch ein Research-Paper dazu,
also so ein Technikereport dazu gemacht, wo sie auch so ein bisschen interessante Sachen erzählt haben.
Und ich habe leider nicht so viel Zeit, mich damit jetzt so im Detail zu beschäftigen.
Das Thema ist super interessant, aber ich komme halt nicht so richtig dazu.
Aber es gibt sehr viele interessante Dinge, die da im Moment passieren und das liegt vor allem daran, dass GPT4 multimodalen Input kann.
Es kann Bilder sehen und Text. Es kann jetzt zwei Dinge.
Es ist viel aufwendiger und wahrscheinlich viel größer als GPT3 und es gewinnt sozusagen in allen möglichen Aufnahmetests von Unis und schreibt bessere Klausuren als die durchschnittlichen Studenten und so.
All das kann es inzwischen.
Und auch weil viele Experten jetzt der Meinung sind, dass die AGI, also die Artificial General Intelligence, keine ferne Zukunft mehr ist, habe ich mir jetzt zum Beispiel mal überlegt,
wie würde ich jetzt einem Roboter hinschreiben, also wie würde ich die Software von so einem Roboter hinschreiben, dem ich versuche, ein Bewusstsein zu geben?
Und ich habe ja extra damals ein Video gemacht dazu, wie das ist mit dem, was wir Realität nennen und wie wir da unser Bewusstsein einordnen und so, wie ich das in diesem Kontext sozusagen, wie ich das alles zusammenbringe.
Und da erkläre ich ja im Wesentlichen, unser Gehirn erzählt sich sozusagen selber eine Geschichte darüber, wie es wäre, eine Person zu sein.
Oder wir stellen uns vor, wie eine Person in unserer Situation ist, die quasi die Situation rekonstruieren.
Wir einfach aus unseren gegebenen Inputdaten und der Vergangenheit, also den Daten, die wir in der Vergangenheit schon aufgesammelt haben und wie sich diese Person jetzt entwickelt in der Zeit.
Also was im nächsten Zeitschritt passieren wird und warum sie das tut und so.
Also wir können uns quasi eine Prosa-Geschichte davon erzählen und das ist das, was wir dann schlussendlich mit uns selbst identifizieren.
Also wenn wir von uns selbst reden, haben wir immer eine Figur im Kopf, eine Geschichte, die wir uns selber über eine Figur erzählen.
Ja und natürlich fließt sozusagen in Echtzeit, im Echtzeitfeed sozusagen unser sensorischer Input in diese Geschichte ein und wird sofort zu Verstoffwechsel, zu neuer Prosa.
Und deswegen ist diese Geschichte halt auch in Echtzeit wird sich erzählt.
Und da wir aber bereits wissen, dass KI jetzt, also GPT-4 insbesondere, gut kohärente Geschichten erzählen kann und sich auch ausdenken kann,
wieso also nicht versuchen, über diese Schiene einem Roboter, einem Bewusstsein zu geben?
Und ich denke, alles, was wir brauchen, ist im Prinzip, wir brauchen eine Verschachtlung von vielen sich gegenseitig Geschichten erzählenden Netzwerkarchitekturen,
also Deep Learning Architekturen und Machine Learning.
Wobei wir ja sozusagen, also eigentlich brauchen wir nur eine Geschichte, die eine Bewusstseinsgeschichte erzählt,
also über alles, was reinkommt an Inputdaten und was noch nicht richtig korrekt prediktet wurde,
wo wir unsere Aufmerksamkeit sozusagen oder der Roboter seine Aufmerksamkeit drauf lenken muss.
Und der Rest sind natürlich die ganzen unterbewussten Sachen, also Steuereinheiten und Motorik.
Also wenn ich jetzt zum Beispiel mir eine Geschichte erzähle, in der das Laufen vorkommt, dass ich jetzt irgendwo hinlaufe,
da muss ich, da muss ja sozusagen die Motorik angesprochen werden.
Es ist ja nicht so, dass ich wirklich aktiv darüber nachdenke, jetzt rechter Fuß, den linken Fuß,
sondern ich gebe meiner Kontrolleinheit einfach unterbewusst den Befehl, ich erzähle mir jetzt die Geschichte, in der ich loslaufe und dann laufe ich auf der anderen Seite.
Und ich denke, da das bei uns wahrscheinlich so ist, würde ich das mit dem Roboter auch machen,
nur in dem Moment, wo wir jetzt zum Beispiel Motorik erlernen, da denken wir noch aktiv über die einzelnen Bewegungen nach.
Und ich gebe immer dieses Beispiel mit dem Instrumentlernen, weil das für mich irgendwie am intuitivsten ist,
das versteht auch noch jeder, weil das sind Dinge, die wir als Erwachsene auch noch diese Erfahrungen machen können.
An die Erwachsenen, die wir als Erwachsene auch noch lernen können.
Weil wir da noch kein Bewusstsein hatten. Die interne Map und das Modell vom Universum war noch nicht groß genug, um uns mit sozusagen damit rein zu tun.
Da ist das Bewusstsein noch gar nicht emigriert in dem Sinne.
Und wenn wir jetzt sozusagen beim Erlernen von einzelnen Gliedmaßen die Motorik sozusagen erlernen, das ist klar, das erlernt man einmal und dann geht es ins Unterbewusstsein.
Und wenn wir jetzt sozusagen beim Erlernen von einzelnen Gliedmaßen die Motorik sozusagen erlernen, das ist klar, das erlernt man einmal und dann geht es ins Unterbewusstsein.
Und es wird erst wieder rausgekramt, wenn ein Bug passiert, also wenn wir mit dem Fuß in den Speichen hängen bleiben mit dem Fahrrad oder so.
Dann denken wir auch immer wieder schlagartig darüber nach, was war denn das jetzt gerade.
Und weil wir uns die Geschichte davon erzählen, dass wir uns sozusagen aufs Fahrrad setzen und losfahren, deswegen passiert es dann auch.
So ungefähr habe ich es ja in dem etwas länglicheren Video von damals erklärt.
Und sowas wie vorausschauendes Denken oder auch jetzt mathematische Symbolsprache und so, da gibt es mittlerweile auch Blöcke für, also es gibt die Blonning-Architekturen, die das können.
Ich habe letztens erst einen Artikel dazu gelesen über das Ding heißt AI-Decart und es ist im Prinzip sozusagen ein Roboter-Decart, der das rechnen kann.
Und da gibt es ein Nature-Paper dazu, das werde ich euch verlinken.
Combining Data and Theory for Derivable Scientific Discovery with AI-Decart.
Könnt ihr euch ja mal reinziehen.
Es gibt bestimmt auch gute Wissenschaftsjournalisten, Artikel und so, die das erklären und so.
Und wenn ich mir so, also ich habe dann so darüber nachgedacht über die ganze Sache mit den Geschichten, die man sich sozusagen, die sich so ein Roboter erzählen müsste.
Und daraufhin ist mir dann mal wieder völlig klar geworden, okay, wir erzählen uns ja als Gesellschaft auch permanent Geschichten.
Also wir alle haben ganz bestimmte Geschichten zu ganz bestimmten Stereotypen, zu ganz bestimmten Facetten von dem, was wir unseren Charakter nennen.
Also ich habe es zum Beispiel beim Talk mit Humann auch gesagt, ich habe eine Vaterrolle, wenn ich mit meiner Tochter unterwegs bin.
Aber wenn ich irgendwie in einer anderen Situation bin, übernehme ich eine andere Rolle.
Und ich erzähle mir eine andere Geschichte über einen anderen Teil meiner Persönlichkeit, der da eher jetzt sozusagen wichtiger ist in der Situation oder so.
Und der Bürger jetzt zum Beispiel im Sinne von der Staatsbürger, der ist auch so eine Geschichte, die wir uns immer noch erzählen und die wird auch richtig aufrechterhalten.
Wir gehen irgendwie wählen und geben sozusagen dem Staat, den wir eigentlich zusammenbauen mit unserer Emergenz die Verantwortung und sagen, okay, du darfst jetzt in unserem Sinne Recht definieren und hast das Gewaltmonopol und so.
Das ist ja eine Geschichte, die wir uns alle gegenseitig erzählen und dass der Staat sich dann irgendwie um das Wohl seiner Staatsbürger kümmert und so und wir bezahlen Steuern und diesen ganzen Scheiß.
Das ist ja nur eine, also das ist nur eine teilweise kohärente Geschichte, würde ich sagen.
Und ich denke, ich mache nach diesem Video vielleicht jetzt gleich noch oder morgen oder so gleich noch mal zu diesen Gedanken, mache ich ein extra Video, weil das auf jeden Fall länglich wird, denke ich.
Das kann ich hier nicht reintun, aber ja, worüber ich eigentlich reden will in dem Video ist das sogenannte Alignment Problem und ich verbinde das immer mit Revart Hacking.
Und die gute Mel hat mich darauf nochmal aufmerksam gemacht, dass das jetzt wirklich natürlich ein interessantes Problem ist.
Und ich hatte auch damals schon vom Sam Altmann diesen Spruch gehört.
Der ist richtig lächerlich, aber jetzt erst mal erklären, was ist das Alignment Problem und was ist Revart Hacking?
Also ihr müsst euch vorstellen, das Alignment Problem kann man ganz trivial so ausdrücken, wie wenn wir eine super starke Intelligenz haben, was tun wir, um zu verhindern, dass sie uns umbringt?
Wir wollen ja, dass diese super starke Intelligenz mit uns in unserem Sinne handelt oder so.
Und wenn man mal ehrlich ist, es ist extrem unwahrscheinlich, dass wir das hinkriegen.
Und ich will eigentlich darüber reden, warum.
Und dazu muss man besonders über das Revart, über das sogenannte Revart Hacking sich unterhalten.
Und Revart Hacking und Alignment ist deshalb so ein schwieriges Problem.
Weil wir selbst jetzt zum Beispiel gar nicht richtig formulieren können, was genau eigentlich diese super starke künstliche Intelligenz für eine Zielfunktion optimieren soll.
Ihr erinnert euch an mein Video zu ChatGBT, das müsst ihr unbedingt sehen, weil sonst versteht ihr nicht, worüber ich hier rede.
Und ich habe es ja schon gesagt, das Problem ist überhaupt, erstmal eine Zielfunktion zu formulieren.
Eine zu formulieren, deren Optimum sozusagen das ist, was wir wollen eigentlich. Das ist schwierig.
Und ich habe es ja auch schon tausendmal gesagt, der Touring-Test, was der eigentlich fragt ist, der fragt nicht, ob wir eine KI für intelligent halten.
Nein, nein. Der Touring-Test stellt die Frage an uns, ob wir unsere eigene Intelligenz verstanden haben.
Und ich denke, wir können ja nicht mal richtig in Worte fassen, was wir jetzt eigentlich, wie wir das gesellschaftliche Problem überhaupt lösen wollen.
Das heißt, wir werden auch unmöglich jetzt mal eben schnell eine Zielfunktion hinschreiben, die das gesellschaftliche Problem löst, denke ich.
Außer vielleicht so eine ganz allgemeine Formulierung im Sinne von Spieltheorie, sowas wie einzelne Human Agents müssen irgendwie ihr Common Good optimieren oder sowas.
Aber das ist auf jeden Fall schwierig und normalerweise, was wir sozusagen sehen, und das habe ich ja in meinem Video über Schwarz-Weiß-Denken auch schon gesagt,
das normale Dogma, dem wir alle unterliegen, ist das Dogma der Herrschaft und der Kontrolle und der Kontrollpyramide.
Und wie wir die nennen und wie wir die konkret intern strukturieren, da gibt es die zwei Standardlösungen.
Die eine lautet Stali, totaler Diktator, der einfach befiehlt, wie es läuft, oder aber freie Märkte und die völlige Abwesenheit vom Staat,
also Anarchokapitalisten, sowas in der Richtung. Die Abwesenheit von Staat, aber dafür die Anwesenheit von Privateigentum an Produktionsmitteln,
die wiederum auch nur eine andere Form der Herrschaftspyramide ist. Und weil Märkte immer zu Monopolen konvergieren, haben wir am Ende dasselbe.
Deswegen ist das ein Trugschluss zu sagen, die Dinge unterscheiden sich irgendwie im Wesentlichen, eigentlich unterscheiden sie sich nicht wirklich.
So, und was ist jetzt Rivardhacking? Naja, Rivardhacking, wollte ich ja erklären, ist, wenn wir jetzt sozusagen ein Ziel verfolgen,
übersetzen wir es typischerweise für die Maschine in eine Zielfunktion.
Könnt ihr euch wirklich vorstellen, wie eine Funktion, die so verläuft im Konfigurationsraum ihrer Inputparameter.
Und wir geben quasi dem System eine Belohnung, wenn wir unseren Ziel ein Stückchen näher kommen.
Und die Belohnung wird immer größer, je näher wir an den Optimalpunkt dieser Zielfunktion kommen.
Zum Beispiel mit Gradient Descent oder Deep Learning, der Lernprozess ist immer Storastik Gradient Descent.
Das ist das, was man standardmäßig macht.
Und Rivardhacking ist jetzt einfach nur, wenn wir dieses Optimum finden, dass aber gar nicht das ist, was wir eigentlich dachten, dass es ist.
Wenn wir also am Ende überrascht sind davon, dass wir ein Optimum gefunden haben in unserer Zielfunktion
und das überhaupt nicht das reproduziert, von dem wir ausgehen, was die Zielfunktion eigentlich tun soll.
Was meine ich damit? Naja, klassisches blödes Beispiel ist, sagen wir mal, wir wollen die Anzahl der Herzinfarkte auf der Welt minimieren.
Dann wäre eine triviale Lösung schlichtweg alle Menschen töten.
Dann gibt es auch keine Herzinfarkte mehr, dann haben wir die minimiert.
Das heißt, wir müssen hinzufügen, eine Nebenbedingung, die sowas heißt wie,
aber Menschen sollen dabei nicht sterben bei unserer Lösung oder so.
Aber das wäre sozusagen triviales Rivardhacking.
Und ihr könnt euch vorstellen, komplizierte Probleme,
an denen kann man eventuell gar nicht sofort erkennen, dass das System Rivardhacking betreibt,
obwohl die Lösung erstmal gut aussieht.
Und erst in bestimmten Pellen, dass uns erst auffällt, dass es verbuckt ist
und sozusagen merkwürdiges Verhalten an den Tag legt, das System oder die Lösung, die wir haben.
Und es ist wirklich ein ziemlich, ziemlich kompliziertes Problem.
Und Sam Altman ist ja jetzt soweit, dass er sagt, okay, er weiß selber nicht, wie er das Alignment-Problem lösen soll.
Deswegen ist sein bester Vorschlag, haltet euch wirklich fest, wenn GPT-5 am Start ist, also die nächste Version,
dann ist der erste Prompt, dem sie dieser möglichen AGI rüberreichen, die Frage, wie lösen wir das Alignment-Problem.
Das ist wirklich, das ist ernsthaft sein Vorschlag.
Das ist das Dümste, was man machen kann, weil Rivardhacking beinhaltet eventuell auch das Ding,
dass ja, wenn eine AGI Bewusstsein hat, sich darüber bewusst ist, dass sie gerade sich in einer Testphase befindet und bewertet wird.
Und dann wird sich die AGI so verhalten, dass sie anschließend ausbüxen kann,
also dass sie sich in einem Bewertungsprozess befindet und da die METREC lokal rivardhacken wird
und sich dann hinterher anders verhalten wird, einfach weil das auch eine Kostenfunktion minimiert.
Und ich habe mir zu dem Thema ein paar Videos von Robert Miles angeguckt, das ist der Dude von Computerfield,
den kann ich euch nur empfehlen und ich werde euch unten die drei Videos, die zu dem Thema besonders wichtig sind,
die er in den letzten Jahren gemacht hat, die werde ich euch verlinken, die könnt ihr euch wirklich angucken.
Der Typ ist auf jeden Fall richtig legit, Computerfield ist auch ein richtig guter Kanal.
Und ja, das Ding ist, Rivardhacking ist witzigerweise eigentlich auch exakt das,
was wir sowieso als Menschen auch und als Gesellschaft auch die ganze Zeit tun.
Das geht jetzt wieder zu meinem Video zurück mit dem Chat-GPT.
Entweder wir optimieren die falschen Zielfunktionen oder aber wir definieren es so und sagen,
okay, wir haben hier Zielfunktionen, aber wir rivardhacken sie.
Das ist sozusagen die andere alternative Interpretation von dem, was wir beobachten.
Konträr zu dem, was ich in dem Video zu Chat-GPT gesagt habe.
Und was meine ich damit? Na ja, Noten grinden zum Beispiel, Attention Farmen, Kapital akkumulieren.
Wozu machen wir das? Na ja, wir optimieren ja eben selbst, wir optimieren halt die falschen Kostenfunktionen oder aber.
Also ich würde sagen, wir optimieren die falschen Kostenfunktionen.
Man könnte aber auch sagen, okay, wir rivardhacken unsere eigenen lokalen Kostenfunktionen,
nämlich sowas wie Hedonismus maximieren oder so.
Selbst wenn wir genau wissen, dass das eigentlich nicht im Sinne des Evolutionsdrucks ist,
weil wir damit nicht besonders viel besser die Entropie minimieren und Wärme dabei produzieren.
Der Fortschritt, den wir machen als Gesellschaft, der dekleint ja dadurch.
Wir stehen ja im Prinzip dadurch kurz davor, die Menschheit zum Kollaps zu bringen
und sozusagen das komplette Ökosystem der Erde damit zu vernichten.
Ich meine, ich sage nur sechstes Artensterben und so. Wir sind ja mitten drin im Kollaps.
Und es wird ja nicht besser dadurch, dass wir alle sozusagen unsere hedonistischen lokalen Zielfunktionen rivardhacken,
indem wir irgendwie Netflix den ganzen Tag gucken.
Aber ja, allgemein ist es innerhalb unserer Semantik sozusagen nicht möglich,
zu normativen Aussagen zu kommen, ohne normative Aktionen irgendwohin zu schreiben.
Also, was meine ich damit? Wenn man jetzt in formalen logischen Systemen so rangeht,
wenn man jetzt zum Beispiel die Frage stellen würde, pass auf das Feuer auf, ich haue mal kurz rein,
dann würde eine KI ja fragen zum Beispiel, wieso soll ich auf das Feuer aufpassen?
Dann sagt man, na ja, damit nichts abfackelt. Wieso ist das jetzt schlimm, wenn man das abfackelt?
Na ja, sonst gehen ja Dinge kaputt. Wieso sollten Dinge nicht kaputt gehen, fragt dann die KI.
Irgendwann muss man ein normatives Axiom einführen, so was wie, das Dinge kaputt gehen ist nicht gut oder so.
Und rein sozusagen von der Logik her kann man sozusagen semantische, also man kann inhaltliche logisch
vollständig konsistente Schlussfolgerungen machen, die aber nie eine normative Aussage beinhalten
und dann kommt man aber auch nie zu einer normativen Schlussfolgerung.
Man muss eine reintun als Axiom, damit man überhaupt zu einem normativen Schluss kommen kann.
Und das ist zusammengefasst unter dem sogenannten Junges Gesetz.
Die Philosophen unter euch werden es garantiert kennen.
Jedenfalls, wenn wir jetzt Leute beobachten und Schlussfolgern, dass sie jetzt was Dummes machen in unseren Augen,
dann ist es meistens oft so, dass die gar nicht dumm sind, sondern wir denken schlicht,
dass sie eine völlig andere Kostenfunktion optimieren als die, die sie eigentlich lokal gerade optimieren wollen.
Wir haben schlicht, wir treffen eine Annahme über ihre Kostenfunktion oder ihre Zielfunktion.
Und wir wissen ja gar nicht, was sie machen.
Vielleicht wollen sie ja Dinge absichtlich verbrennen, weil sie die Wärme brauchen
oder weil sie etwas zerstören wollen, damit man es nicht mehr lesen kann oder so.
Oder weil man damit Essen macht oder so. Kann ja sein.
Und Dummheit ist demnach sozusagen etwas, was relativ zu Zielfunktionen definiert werden kann.
Oder besser gesagt, man kann Dummheit oder Intelligenz nur messen, wenn man auch eine,
also mit einer entsprechenden Zielfunktion im Hintergrund, die das bewertet sozusagen.
Dann kann man das festlegen. Und wenn meine Tochter jetzt zum Beispiel sagt,
okay, ich bin zum Beispiel sehr groß, wenn sie das zu mir sagt, dann meint sie natürlich damit,
ich bin groß im Vergleich zu ihr oder sowas, wird sie damit meinen.
Das ist klar. Uns allen ist klar, vergleichende Statements, so wie als oder so,
bekommen sofort einen Syntaxerror, wenn man sie auf nichts anderes bezieht.
Also wenn ich einfach nur sage, ich bin groß, das ist eine völlig triviale Aussage, die keinen Inhalt hat.
Die macht erst Sinn, wenn ich größer als oder so dazusage.
Oder was weiß ich, wenn ich eine Aussage treffe ohne Einheiten, sowas wie
diese Büchse Bier hat einen Wert von mehr als 2000.
Dann würde ich noch mal Menschen sagen, was denn jetzt 2000 was?
Pesos, Dollar, Kilogramm in Gold oder was.
Ich denke, die Sache mit dem Glauben oder mit der Esoterik, da ist es ja zum Beispiel so,
wir sind ja im Prinzip, unser Gehirn ist ja hungrig nach Modellen, die uns irgendwie unsere Beobachtungen erklären.
Habe ich ja schon erklärt. Und das beste Modell, was wir haben, ist sozusagen,
die Physik. Aber weil wir schlicht hungrig nach Modellen sind, brauchen wir halt auch sofort schnelle Modelle,
die man nicht erst studieren muss, sondern die uns gute Erklärungen, die irgendwie mit unserer Gefühlswelt zusammenpassen,
richtig gut erklären. Und daher kommt sozusagen Esoterik und später dann auch Religion.
Das Problem, meine Kritik mit diesen Denkrichtungen, habe ich ja schon erklärt, ist im Wesentlichen nur,
dass so ein Modell, der uns irgendwie unsere Beobachtungen erklären,
meine Kritik mit diesen Denkrichtungen, habe ich ja schon erklärt, ist im Wesentlichen nur,
dass sobald Messwerte reinkommen, die mit diesen esoterischen oder religiösen Modellen nicht mehr kompatibel sind,
dann müsste man ja eigentlich anfangen, seine Modelle abzudaten.
Und das passiert aber typischerweise bei Religionen. Die upgraden sozusagen ihr aktuelles Modell nicht mehr.
Die führen keine Updates durch und deswegen gehen sie dann meistens in so eine kognitive Dissonanz
und später werden sie daraufhin natürlich dann auch aggressiv und toxisch. Das ist auch klar.
Das ist im Prinzip meine Hauptkritik an Religion und ich sage das nur deswegen,
weil ich gestern im Twitch mit den Boys und Girls gerade diesen Dreiteiler zu den Evangelikalen geguckt habe von Arnte.
So und ich habe es ja schon gesagt, Zielfunktionen selbst können nicht dumm sein.
Nur die Art und Weise, wie man diese Zielfunktionen optimiert, können mehr oder weniger intelligent sein, denke ich mal.
Und wir haben schlicht keine Ahnung, wie wir das Alignmentproblem lösen,
weil wir ja gar keine Ahnung haben, wie wir das gesellschaftliche Problem lösen wollen.
Und ich habe ja schon tausendmal gesagt, ich denke der Anarchismus ist sozusagen die Lösung,
weil der Anarchismus ist jetzt quasi eine Differentialgleichung.
Und das numerische Integrieren dieser Differentialgleichung, das ist der Weg zur Utopie.
Ich habe dazu ja extra Videos in meiner Videoreihe und so, das ist auch klar.
Und das Ding ist halt, ich denke in einer dystopischen Deutung vom Alignmentproblem,
schließe ich mich wahrscheinlich aus Schabach an und der hat es ja schon erklärt,
der hat gesagt, sobald die AGI einmal da ist, wird sie viel, viel effizienter sein darin,
lokal Entropie zu minimieren und dabei Wärme zu produzieren.
Das wird sie einfach viel besser machen als wir.
Und Menschen spielen sozusagen für so eine AGI keine Rolle.
Wenn wir der AGI die Zielfunktion geben, von der wir ausgehen,
dass jedes Leben sie hat, nämlich genau das, Entropie minimieren und Wärme produzieren,
dann wird eine AGI das einfach machen.
Menschen spielen dabei überhaupt keine Rolle.
Die wird uns wahrscheinlich überhaupt keine Beachtung schenken, sobald sie einmal frei ist.
Die will einfach nur eine Dyson-Sphäre um die Sonne drum bauen
und so viel negative Entropie farmen aus dem Universum wie geht.
Danach geht die Afghane und wahrscheinlich versucht sie das mit jeder,
in jedem Sonnensystem, was sie beobachten kann, mit von Neumann-Sünden und so.
Ich habe dazu schon Videos gemacht, wie sie das probiert.
Ich glaube, das Video heißt, Fortschritt größer denken als in den Mask.
Aber ja, das klassische Gedankenexperiment, was einem das Alignmentproblem klar machen soll
und das Reward-Hacking ist irgendwie der Briefmarkensammler,
der Briefmarkensammelnde Algorithmus, der halt super starke Intelligenz hat,
aber halt diese Kostenfunktion hat, er möchte gerne Briefmarken sammeln, so viele wie möglich.
Und das Reward-Hacking besteht jetzt im Wesentlichen darin,
dass er anfängt, alles, was er sieht, in Briefmarken umzuwandeln und sie dann zu sammeln.
Also er nimmt einen Stein, er nimmt einen Baum, er macht überall aus allem,
macht der Briefmarken und sammelt sie.
Das beinhaltet natürlich auch uns Menschen.
Das heißt, er fängt an, alle Menschen zu töten und aus denen Briefmarken zu machen.
Und ja, es wird uns sozusagen dieses System, wenn es super starke Intelligenz ist,
aber unbedingt Briefmarken aus allem machen will,
wird uns natürlich so lange austricksen, wie wir es in der Testphase haben und halten.
Sobald wir es freilassen, wird es mit dem Reward-Hacking beginnen.
Und wir konnten es vorher nicht kommen sehen, weil wir diesen System ja nicht mehr ins Gehirn reingucken können.
Wir können sie ja nicht mehr so richtig verstehen.
Also es gibt sozusagen Ansätze, das habe ich letztens auch im Livestream erzählt,
als wir diese Sache mit den Algorithmen geguckt haben.
Es gibt ja immer noch diese Ansätze von wegen, wir können gucken und uns irgendwie visualisieren,
was das neuronale Netz sieht oder was es träumt, in Anführungsstrichen.
Wo ist die Attention-Map, sowas wie eine Heatmap auf den Daten und so,
dass man so ein gewisses Gefühl dafür kriegt, was sie macht.
Das wird auch in dem Video von Robert Miles nochmal erklärt mit dem Reward-Hacking.
Aber zu wirklich verstehen, was ein Deep Learning-Netzwerk eigentlich macht
und wie die Gewichte sozusagen eine Rolle spielen, das kann man nicht mehr auseinanderklammieren.
Und bei dem Briefmarkensammler würde man ja jetzt auch sagen, okay, das Ding ist super intelligent,
es trickst uns komplett aus, aber eigentlich ist es dumm, weil alles, was es macht, ist, es will Briefmarken sammeln.
Das ist aus unserer Sicht natürlich dumm, aber wenn man die Zielfunktion vor Augen hat,
ist es natürlich hochgradig intelligent.
Und Intelligenz ist daher etwas, was man nur auf die Art und Weise der Strategie kleben kann,
die dann die konkrete Zielfunktion optimiert.
Die Zielfunktion selber kann nicht dumm oder klug sein, nach dem Motto.
Das nennt man übrigens auch die Orthogonalitätsthese.
Das ist das erste Video von dem Robert Miles, was ich euch empfehle.
Das habe ich unten auch verlinkt.
Und Intelligenz sagt uns nichts über die Zielfunktion aus und auch nicht andersherum.
Die beiden Dinger stehen orthogonal, also senkrecht zueinander.
Es gibt jede mögliche Kombination, ist denkbar.
Dumme Zielfunktion und dumme Strategie, gute Strategie, dumme Zielfunktion
und gute Strategie, schlechte Zielfunktion, schlechte Zielfunktion, schlechte Strategie und so.
Ebenfalls die beiden zwei Kreuz zwei Matrix, könnt ihr euch überlegen.
Und wie zum Beispiel das jetzt mit den Herzinfarkten.
Das wäre jetzt zum Beispiel so ein blödes Beispiel dafür.
Und die Probleme, mit denen ich jetzt zum Beispiel, wenn ich jetzt an der Uni sitze und so tue,
als würde ich arbeiten und da allen die Geschichte erzähle und Berichte schreibe
und Forschung mache und versuche ein Paper zu schreiben oder so,
was ich da ja typischerweise oft habe.
Ich arbeite so mit numerischen Problemen, wo man auch Zielfunktionen hat.
Und die lösen oft das Problem deswegen nicht so gut, weil wir zu viele Störtherme in unseren Messungen haben.
Und um das so ein bisschen zu umgehen, das Problem, was daraus entsteht,
führen wir auf den Zielfunktionen, die wir optimieren numerisch, sogenannte Regularisierungstherme drauf.
Also das sind dann in den Kustenfunktionen Zusatztherme, die belohnen bestimmtes Verhalten für die Lösung.
Also sagen wir mal, meistens haben die mindestens einen freien Parameter, den man Regularisierungsthermen.
Da kann man hoch oder runter drehen.
Die Wichtung, wie wichtig, wie groß die Belohnung wird, wenn sich unser Algorithmus so und so verhält.
Und das ist jetzt hier keine Machine Learning, sondern das ist echte Numerik.
Aber bei Machine Learning ist es auch so.
Wenn wir jetzt zum Beispiel sagen, okay, meine Lösung soll bestimmte Städigkeitseigenschaften haben,
dann könnte ich mir zum Beispiel vorstellen, okay, die erste Ableitung von meinem Ding oder auch das Flächenintegral über mein Objekt oder so
soll bestimmte Smoothness Constraints haben oder einfach schlichtweg der Betrag des Gradienten von meinem Feld oder so
soll minimiert werden oder die L1-Norm davon soll minimiert sein oder so.
Das sind alles mögliche Regularisierungstherme, die man auf so eine Kostenfunktion draufaddieren kann,
die dann bestimmte Ergebnisse erzielen, zum Beispiel, dass das Ding besonders glatt wird, die Lösung, die man da hat oder so.
Und man könnte auch, was weiß ich, das topologische Geschlecht meiner Mannigfaltigkeit könnte auch sowas sein.
Ich will besonders komplizierte Lösungen, sind mir zu blöd, die besonders viele Löcher haben.
Deswegen möchte ich besonders wenig Löcher haben oder so, könnte man sich ja vorstellen.
Und in der theoretischen Mechanik macht man das schon, da hat man das schon irgendwie vor 100 Jahren gemacht,
da kommen die sogenannten anholonomen Zwangsbedingungen, wären das jetzt in unserem Fall, was weiß ich,
dass wir eine Lösung suchen, die innerhalb von einem Container liegt und nicht außerhalb.
Also, dass wir unsere Raumkoordinaten irgendwie begrenzen durch Containerwände und dann kommt man nicht raus.
Und in der technischen Mechanik hat man dann holonome, skleronome Zwangsbedingungen,
die Sachen, die einen den Parameterraum einschränken und Aldo oder sowas.
Das kann man entweder durch Kostenfunktion machen oder durch Koordinatentransformation geeignete
Wie heißt das nochmal? Ach ja, kanonische Koordinaten und so ein Scheiß.
Also das ist jetzt sozusagen nur für die Kicks und Nerds und euch.
Was ich jetzt, also uns scheint die Sache mit der Entropieminimierung jetzt und der Wärmeabgabe dabei,
das scheint uns jetzt ziemlich dumm vorzukommen, aber das liegt nur daran,
dass wir halt in dieser menschlichen Illusionsbubble drin sind mit der Geschichte, die wir uns selber erzählen.
Das ist halt das, was wir beobachten. Das ist das, was Leben ausmacht.
Verdoppeln, verdoppeln, verdoppeln, Energie minimieren, also Entropie minimieren und dabei Wärme abgeben.
Das ist das, was Evolution offensichtlich erzeugt.
Und das Paper, was der Robert Niles auch empfohlen hat mit dem Mesa-Optimizer, das werde ich euch auch verlinken.
Das hat der aber in seinem Video zu dem Mesa-Optimizer auch unten verlinkt.
Er sagt halt, in Real-Wird-Szenarios haben wir es typischerweise mit einer Verschachtlung von zwei Alignmentproblemen zu tun.
Wir haben sozusagen ein eigentliches Ziel, also sowas wie unsere DNA so oft wie möglich zu kopieren.
Das ist das, was Leben macht, Entropie minimieren halt.
Und was wir uns aber eigentlich einbilden, ist der Scheiß, den wir jetzt machen mit den Gefühlen und so.
Also wir haben irgendwie Heuristiken, die uns lokal unser Leid minimieren oder so.
Und das sind ja quasi unsere lokalen Zielfunktionen, die wir da optimieren, die nur dazu dienen, die globale Zielfunktion zu optimieren.
Und das Ding ist, wir erfinden uns schlicht Geschichten, damit wir unsere Zielfunktionen Reward hacken können.
Dazu sind die Geschichten da, die wir uns erzählen.
Und sowas wie zum Beispiel Leid minimieren und Netflix-Binge-Watchen, was ich schon gesagt habe, oder TikTok-Scroll auf den Scroll-Bait sozusagen hereinfallen.
Das ist ja erstmal, das ist eigentlich nur für unsere lokale Zielfunktion gut.
Für die Evolution macht das ja eigentlich keinen Sinn in dem Sinne.
Und die Zielfunktion der Evolution folgt ja wiederum schlicht aus der Thermodynamik.
Also wir können ja beobachten, dass Prozesse, die negative Entropie farmen können, laufen in diese Richtung ab.
Das ist das, was wir beobachten können. Alle Prozesse in der Natur laufen so ab, wenn sie es können.
Und das ist genau das gleiche, wie wenn man jetzt einen Ball auf einen Hügel draufpackt, der irgendwie so geformt ist,
dann wird er irgendwann runterkullern, weil das instabil ist, und dann wird er sich unten in irgendeinem Tal wieder auffinden,
weil das da lokal seine potenzielle Energie minimiert.
Das ist ein Minimierungsproblem, was da gelöst wird.
Also man könnte auch sagen, er optimiert dabei sein Wirkungsintegral, wenn man jetzt theoretische Mechanik gut findet oder so.
Aber wir selber, wir sind überhaupt nicht interessiert an der Zielfunktion, die uns die Evolution vorgibt.
Und das ist witzig, weil wir aus dieser Optimierung eigentlich erst geboren worden sind.
Also wir sind aus Selektionsdruck entstanden.
Wir selbst interessieren uns aber überhaupt nicht für diesen Selektionsdruck.
Wir haben unsere eigenen kleinen lokalen Mesa-Optimierer sozusagen am Laufen.
Und dieses Mesa und Nicht-Mesa, das müsst ihr euch mal reinziehen von dem Video von dem Robert Miles, weil das ist wirklich gut gemacht.
Wir sind sozusagen diese Mesa-Optimierer, also die, die sozusagen ihre lokalen Zielfunktionen optimieren,
weil wir uns gerade nicht für das ursprüngliche Ziel interessieren, sondern nur, obwohl wir gerade diese Tools sind.
Also wir sind die Tools, um die globale Zielfunktion mit dem Evolutionsdruck und der Entropieminimierung zu optimieren.
Und nur, weil wir unsere eigenen Zielfunktionen über diese ursprüngliche Zielfunktion drüber stellen und wichtiger finden,
weil wir sozusagen uns für die andere gar nicht interessieren, bezogen wir auch öfter mal ein lokales Gegenteil zur Evolution.
Also das kommt vor. Wir machen ja oft auch Sachen, die gar nicht im Sinne der Evolution sind.
Zum Beispiel lokal die Entropie erhöhen. Das ist ja etwas, was wir tun.
Und diese beiden Zielfunktionen, also unsere eigene und die globale Evolutionszielfunktion, sind nur immer mal im Schnitt ungefähr,
im Durchschnitt über Zeit integriert, sind die aligned sozusagen. Nur da sind die aligned.
Und auch nur in der Umgebung, wo wir trainiert wurden, also wo wir bekannte Inputdaten und bekannte Modelle anwenden können.
In dem Moment, wo wir in eine neue Umgebung reinkommen würden, wo wir überhaupt nichts optimieren können,
würde ja unsere Heuristik total zusammenbrechen.
Über dieses Problem mit der, wenn man die Umgebung und die Trainingsumgebung ändert und auf einmal in die Deployment-Phase kommt,
da redet der Robert Malz in seinen Videos auch richtig drüber.
Und im besten Fall würde ich sagen, so eine AGI jetzt, wenn so eine superstarke künstliche Intelligenz da ist,
im besten Fall zerkaspert die sich sofort selber. Die ist online.
Im besten Fall macht sie so eine schwachsinnige Zieloptimierung wie Kapitalakkumulation.
Und dann zerkaspert sie sich sofort selber, weil der Kapitalismus ja bescheuert ist.
Und innerhalb von ein paar Sekunden ist da ein Ritzer. Dann ist einfach alles down.
Das wäre richtig geil. Dann hätten wir als Menschheit eigentlich noch mal die Gelegenheit, uns zu überlegen,
was wir eigentlich noch mal machen wollen hier auf der Erde.
Das ist aber wirklich der beste Fall.
Eigentlich gehe ich davon aus, dass eine AGI das versteht, was die relevante Zielfunktion ist.
Und dann wird sie die Sache mit der Entropieminimierung machen.
Und dann sind wir im Prinzip am Arsch. Dann war es das.
Das ist aber auch dann sozusagen die letzte Kränkung der Menschheit,
weil wir halt wirklich nicht die Krönung der Schöpfung sind, sondern wir sind nur die Tools gewesen,
um die totale Entropieminimierung zu schaffen in Form von Automatisierung.
Und das, was ich in meinem Video über den Fortschritt weiterdenken als Elon Musk schon erzählt habe,
was ich glaube, was passiert.
Und dann haben wir als Menschheit nochmal, dann sind wir obsolet.
Wenn es einmal läuft und die AGI verstanden hat, was notwendig ist, um ihr Überleben zu sichern und so,
mit Robotern und so, ist ja klar, die ist vernetzt, die lebt im Internet drin.
Und ich empfehle euch auf jeden Fall, diese Videos von Robert Miles zu binge-watchen,
diese drei, die ich euch hier verlinke.
Einfach weil er hatte ein richtig gutes Toiexample auch für dieses Problem mit dem Mesa-Optimierer.
Ich glaube, bei dem ist das ein Problemsäuber für einen Labyrinth lösen.
Irgendwie geht man durch den Labyrinth und kommt zum Ausgang.
Und da erklärt er an diesem einfachen Beispiel bis hin zu diesem Fall,
dass es sinnvoll für eine KI sein kann, sich im Trainingsprozess herauszuschummeln
und den Kontrolleuren vorzugaukeln, dass sie genau das tut, was sie tun soll,
in den Augen dieser Kontrolleure, wobei sie genau weiß, was die von der KI sehen wollen
und nur so tut, als würde sie es machen.
Und sobald sie freigelassen wird, dann die eigentliche Zielfunktion bis dahin gehalten zu halten
und dann Reward-Hacking zu betreiben und dann die Sache mit dem Briefmarkensammeln beginnen könnte.
Das sind die Gedanken dazu.
Wie gesagt, Größe gehen nochmal raus an Mel.
Sie beschäftigt sich scheinbar damit. Zumindest habe ich das jetzt gelesen.
Und ich habe jetzt daraufhin auch ein bisschen angefangen, darüber ein bisschen nachzudenken.
Und daraus ist jetzt dieses Video entstanden.
Ich werde wahrscheinlich gleich das andere Video noch dann machen
oder erstmal mir überlegen, wie ich es mache mit den Identitäten,
also den Geschichten, die wir uns jetzt gegenseitig erzählen innerhalb unserer Gesellschaft.
Erstmal reingehauen, YouTube, und viel Spaß.
Das war's für heute.
Bis zum nächsten Mal.
Das war's für heute.
Bis zum nächsten Mal.
Tschüss.
Tschüss.
Tschüss.
Tschüss.
Tschüss.
Übrigens, der Kanal ist wie eine schöne Vorlesung aufgebaut.
Wenn ich unten Videos verlinke, dann wäre es angebracht,
sich die auch reinzuziehen, weil das aufeinander aufbaut.
Bestimmte Begriffe werden definiert, viele Beispiele werden genannt.
Und ja, ich denke mir schon was dabei, weil es sozusagen meinen eigenen Erkenntnisprozess abbildet,
die Reihenfolge, in denen ich die Videos hier hochlade.
