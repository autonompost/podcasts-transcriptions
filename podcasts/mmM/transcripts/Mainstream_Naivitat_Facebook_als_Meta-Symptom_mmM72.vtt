WEBVTT

00:00.000 --> 00:17.720
Es ist absolut symptomatisch, dass Zuckerberg den Missbrauch beim User verortet, aber nie

00:17.720 --> 00:18.720
wirklich im Unternehmen.

00:18.720 --> 00:23.840
Und selbst wenn das Unternehmen, was ja jetzt häufiger wieder passiert, dann doch verantworten

00:23.840 --> 00:24.840
die Zugehörungen.

00:24.840 --> 00:25.840
Ein Literaturwissenschaftler.

00:25.840 --> 00:29.040
Dass dann eher gesagt wird, ja wir haben es falsch, wir haben die Technologie falsch

00:29.040 --> 00:32.880
bedient oder wir haben uns die falschen Fragen gestellt und so weiter und so weiter.

00:32.880 --> 00:34.840
Das System an sich ist nie daran schuld.

00:34.840 --> 00:38.800
Stanford im Herzen des Silicon Valley.

00:38.800 --> 00:42.600
Hier bekommen Tech-Pioniere ihr intellektuelles Rüstzeug.

00:42.600 --> 00:45.880
Hier lehrt Adrian Daub Literaturwissenschaft.

00:45.880 --> 00:50.760
In seinem Buch Was das Valley Denken nennt, nimmt er DenkerInnen in den Blick, derer

00:50.760 --> 00:52.440
sich die Tech-Branche bedient.

00:53.400 --> 00:59.160
Ich glaube, dass sich in den letzten 50 bis 60 Jahren in Nordkalifornien eine Art Denke

00:59.160 --> 01:09.440
etabliert hat, die ganz stark Profit und technologische Innovationen über Sicherheit und über Werte

01:09.440 --> 01:10.440
gestellt hat.

01:10.440 --> 01:17.920
Die sozusagen die Kommunikation und den Individualismus als Wert an sich entdeckt hat.

01:17.920 --> 01:21.200
Das Medium ist die Botschaft.

01:21.640 --> 01:27.080
Der kanadische Medientheoretiker Marshall McLuhan wurde durch diesen Satz in den 1960er

01:27.080 --> 01:28.080
Jahren berühmt.

01:28.080 --> 01:32.040
Für ihn formen Medien und nicht Inhalte unsere Wahrnehmung.

01:32.040 --> 01:39.160
So greift das Medium Facebook heute tief in die Psyche seiner User ein, indem es bestimmt,

01:39.160 --> 01:40.480
wie wir Informationen teilen.

01:40.480 --> 01:45.040
Die Inhalte werden überwunden.

01:45.040 --> 01:46.520
Inhalte Nebensache.

01:46.520 --> 01:47.520
Kannte.

01:47.520 --> 01:52.480
Ein solches Denken ermögliche Mark Zuckerberg einen skrupellosen Expansionskurs.

01:52.480 --> 01:58.360
Die Tatsache, dass möglicherweise da ein Kommunikationsbegriff dahinter steckt, der

01:58.360 --> 02:06.320
nicht ganz durchdacht ist und dass es wirklich schwierig ist zu sagen, wir bringen Menschen

02:06.320 --> 02:10.880
miteinander in Verbindung und wir stellen aber keine ethischen Maßstäbe bereit, wie dort

02:10.880 --> 02:14.640
zu kommunizieren wäre, das ist eine Frage, die man sich bei Facebook sehr wenig zu stellen

02:14.640 --> 02:15.640
scheint.

02:16.400 --> 02:22.160
Welche Folgen ein solches Denken haben kann, zeigen Ken und Frankl in ihrem Buch.

02:22.160 --> 02:28.920
2017 startete das Militär in Myanmar auf Facebook eine Desinformationskampagne.

02:28.920 --> 02:34.040
Facebooks Algorithmus verbreitete Hass gegen die muslimische Rohingya-Minderheit.

02:34.040 --> 02:35.520
Die Gewalt eskalierte.

02:35.520 --> 02:39.560
25.000 Menschen starben, hunderttausende flohen.

02:39.560 --> 02:44.320
Die Zentrale in Kalifornien habe jahrelang Warnungen von Menschenrechtsorganisationen

02:44.320 --> 02:45.600
ignoriert.

02:45.640 --> 02:51.960
Bis 2018 gab es nur fünf Burmesisch sprechende Mitarbeiterinnen für einen Markt von 18 Millionen

02:51.960 --> 02:52.960
Usern.

02:52.960 --> 02:58.080
Das Unternehmen habe ein Streichholz in einen ethnischen Konflikt geworfen und sich dann

02:58.080 --> 03:01.960
abgewandt, so die Autorinnen.

03:01.960 --> 03:06.760
Mark Zuckerberg setzte auch im Wachstumsmarkt Myanmar auf sein zentrales Geschäftsmodell.

03:10.760 --> 03:13.720
Das ist eine der hässlichen Wahrheiten.

03:13.840 --> 03:18.840
Facebook benötigt unbedingt die Beteiligung seiner User, um zu wachsen.

03:18.840 --> 03:21.840
Wachstum ist das Wichtigste für das Unternehmen.

03:21.840 --> 03:27.480
Der Newsfeed-Algorithmus zielt darauf ab, User zu binden, spült Aufsehen Erregendes nach

03:27.480 --> 03:28.480
oben.

03:28.480 --> 03:32.000
Er priorisiert Inhalte, die Emotionen erzeugen.

03:32.000 --> 03:35.680
Egal ob Wut, Traurigkeit, Leid oder Empöhrung.

03:35.680 --> 03:42.080
Oftmals Dinge, die einen in die Irre führen oder desinformieren.

03:44.080 --> 03:46.800
Die algorithmische Verstärkung von Emotionen.

03:46.800 --> 03:49.400
Die zentrale DNA von Facebook.

03:49.400 --> 03:54.240
Für Daub steckt hinter diesem Geschäftsmodell ein zutiefst pessimistisches Menschenbild.

03:54.240 --> 04:03.480
So findet das Denken des französischen Kulturanthropologen und Stanford-Professors René Girard großen

04:03.480 --> 04:08.360
Anklang bei Gründern und Investoren im Silicon Valley.

04:08.360 --> 04:12.680
Laut Girard begehren Menschen immer das, was andere auch wollen.

04:12.680 --> 04:15.520
Mimetisches Begehren, so nennt er das.

04:15.520 --> 04:17.760
Und so funktioniert auch Facebook.

04:17.760 --> 04:23.680
Das Begehren wird durch den ständigen Vergleich mit anderen und die Jagd nach Likes geschürt.

04:23.680 --> 04:27.160
Konflikte und Gewalt sind laut Girard Folge dieser Dynamik.

04:27.160 --> 04:32.160
Und doch wird uns dieses Geschäftsmodell als geniale Idee verkauft, die die Welt näher

04:32.160 --> 04:33.160
zusammenbringt.

04:33.160 --> 04:39.160
Am 6.

04:39.160 --> 04:44.840
Januar entlud sich der Hass, der in Facebook-Gruppen über Monate hinweg den Ton angegeben hatte.

04:44.840 --> 04:52.240
Vor der Attacke auf das Kapitol empfahl Facebooks Algorithmus Trump-Followern hunderttausendfach

04:52.240 --> 04:55.680
Gruppen, in denen sich gewaltbereite Demonstranten organisierten.

04:55.680 --> 05:00.600
Trump selbst befeuerte den Mythos der gestohlenen Wahl.

05:00.600 --> 05:01.600
Klar.

05:02.600 --> 05:09.400
Zu seinen 28 Millionen Followern hatte ihn Zuckerberg noch im Herbst 2019 gratuliert.

05:09.400 --> 05:12.600
Erst nach dem Sturm aufs Kapitol ließ ihn Facebook sperren.

05:12.600 --> 05:15.600
Die Autorinnen kommen zu einem erstaunlichen Ergebnis.

05:17.600 --> 05:19.800
Facebook wusste bereits vor dem 6.

05:19.800 --> 05:21.200
Januar, was sie planten.

05:21.200 --> 05:25.400
Sie waren alarmiert, bemerkten, dass Personen davon sprachen, Waffen mitzubringen.

05:25.400 --> 05:31.400
Fotos posteten und in Gruppen schrieben, ich bringe dieses Sturmgewehr mit nach Washington.

05:32.400 --> 05:35.400
Trump solle sich für seine Worte verantwortlich fühlen.

05:35.400 --> 05:38.400
Die Menschen, die das Gesetz brachen für ihre Taten.

05:38.400 --> 05:41.400
So wie Zuckerberg jegliche Verantwortung von sich.

05:41.400 --> 05:45.400
Einige Leute behaupten, dass soziale Netzwerke polarisieren.

05:45.400 --> 05:49.400
Unseren Forschungen zufolge ist das alles andere als klar.

05:49.400 --> 05:53.400
Immer wieder betont er, dass er an einer Gemeinschaft für alle arbeite.

05:53.400 --> 05:56.400
Genau darin liegt das Dilemma von Facebook.

05:57.200 --> 06:00.200
Die Wirklichkeit eines Algorithmus, der Hass und Gewalt befördert.

06:00.200 --> 06:04.200
Und der Anspruch, die Welt zu einem besseren Ort zu machen.

06:04.200 --> 06:07.200
Das ist glaube ich unglaublich wichtig für diese Unternehmen.

06:07.200 --> 06:10.200
Während sie eben diese enorme Macht anhäufen.

06:10.200 --> 06:14.200
Sozusagen vor dieser Macht auch zu einem gewissen Grad die Augen verschließen zu können.

06:14.200 --> 06:17.200
Und das ist wichtig für die Unternehmenschefs.

06:17.200 --> 06:19.200
Das ist wichtig für diesen Expansionskurs.

06:19.200 --> 06:22.200
Es ist aber auch einfach eine Art, in der man sich selber belügen kann.

06:22.200 --> 06:24.200
Sondern kognitive Dissonanz abbauen kann.

06:25.000 --> 06:27.000
Er belügt nur dich.

06:27.000 --> 06:30.000
Der Unternehmer als Held, der sich von niemanden befreundet.

06:30.000 --> 06:32.000
Er weiß ganz genau, was er da macht.

06:32.000 --> 06:35.000
Diese weit verbreitete Denkart in der Technologiebranche.

06:37.000 --> 06:40.000
Also alle, die jetzt sich hier an meinen Channel reinziehen.

06:40.000 --> 06:44.000
Haben sich wahrscheinlich das Video zu Facebook schon reingezogen.

06:44.000 --> 06:48.000
Aus der Reihe Digitalisierung verstehen lernen.

06:48.800 --> 06:55.800
Und ihr dürft euch alle sehr erhaben fühlen über den durchschnittlichen GEZ-Medienkonsumenten.

06:55.800 --> 07:04.800
Weil da wurde jetzt offensichtlich ein Literaturwissenschaftler nach seiner Meinung befragt zu Facebook.

07:04.800 --> 07:08.800
Und natürlich macht er das, was alle machen.

07:08.800 --> 07:11.800
Er identifiziert ein Symptom.

07:11.800 --> 07:15.800
Und jetzt haltet euch fest, sie werfen Facebook vor.

07:16.600 --> 07:23.600
Dass sie das pure Böse sind und vor allem unwissentlich das falsche tun.

07:23.600 --> 07:25.600
Das ist natürlich totaler Spaß.

07:25.600 --> 07:27.600
Erstens wissen sie ganz genau, was sie tun.

07:27.600 --> 07:29.600
Sie tun es absichtlich.

07:29.600 --> 07:31.600
Und ja, der Sturm aufs Kapitol ist für Facebook super gewesen.

07:31.600 --> 07:36.600
Natürlich wäre es noch viel geiler, wenn noch mehr hinterher Facebook-Traffic entstanden wäre.

07:36.600 --> 07:38.600
Aber wahrscheinlich war es schon unendlich viel Traffic.

07:38.600 --> 07:40.600
Und sie haben es immer noch nicht verstanden.

07:40.600 --> 07:41.600
Natürlich.

07:41.600 --> 07:44.600
Auch so ein Gemetzel in Myanmar ist super für Facebook.

07:44.600 --> 07:48.600
Alles ist super für Facebook, solange es Traffic im Facebook erzeugt.

07:48.600 --> 07:52.600
Facebook braucht Daten und Facebook braucht mehr Traffic.

07:52.600 --> 07:54.600
Das ist ihr Geschäftsmodell.

07:54.600 --> 07:59.600
Und ihnen Wachstum, Wachstumszwang vorzuwerfen, das ist Schwachsinn.

07:59.600 --> 08:02.600
Weil das ist immanent, das ist im System drin.

08:02.600 --> 08:07.600
Und ich habe ja schon erklärt, Facebook und naja, eigentlich auch Fake News.

08:07.600 --> 08:09.600
Fake News entstehen aus Wettbewerb.

08:09.600 --> 08:17.600
Aus der ganz simplen Tatsache, dass da um die Aufmerksamkeit der User gekämpft wird.

08:17.600 --> 08:20.600
Aber da sieht man, was dabei rumkommt.

08:20.600 --> 08:25.600
Da gibt es jetzt hier so einen 10-Minuten-Beitrag mit dem super Enthüllungsbuch zu Facebook.

08:25.600 --> 08:29.600
Und da steht mit Sicherheit nichts drin, was nicht schon längst bekannt ist.

08:29.600 --> 08:31.600
Garantiert.

08:31.600 --> 08:33.600
Ich will es gar nicht lesen. Will ich auch nicht.

08:33.600 --> 08:37.600
Aber offensichtlich macht Kulturzeit einfach nur Werbung für irgendwelche Bücher.

08:37.600 --> 08:41.600
Für irgendwelche Boomer, die keine Ahnung haben von dem Neuland Internet.

08:41.600 --> 08:45.600
Und deswegen mal ein Buch lesen müssen darüber und sich dann aber nur noch über Symptome aufregen.

08:45.600 --> 08:47.600
Herrlich. Find ich richtig geil.

08:47.600 --> 08:49.600
Kann man eigentlich nichts mehr zu sagen.

08:50.600 --> 08:51.600
Super Beitrag.

08:51.600 --> 08:53.600
Herrliches Ding.

09:07.600 --> 09:10.600
Super Beitrag.

09:37.600 --> 09:39.600
Super Beitrag.

10:07.600 --> 10:09.600
Super Beitrag.

10:16.600 --> 10:20.600
Übrigens, der Kanal ist wie eine schöne Vorlesung aufgebaut.

10:20.600 --> 10:27.600
Wenn ich unten Videos verlinke, dann wäre es angebracht, sich die auch reinzuziehen, weil das aufeinander aufbaut.

10:27.600 --> 10:30.600
Bestimmte Begriffe werden definiert, viele Beispiele werden genannt.

10:30.600 --> 10:38.600
Und ja, ich denke mir schon was dabei, weil es sozusagen meinen eigenen Erkenntnisprozess abbildet, die Reihenfolge, in denen ich die Videos hier hochlade.

