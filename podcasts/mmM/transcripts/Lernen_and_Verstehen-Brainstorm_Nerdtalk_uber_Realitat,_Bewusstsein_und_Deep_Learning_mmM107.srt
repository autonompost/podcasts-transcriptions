1
00:00:00,000 --> 00:00:06,680
Das Gehirn erzählt sich ja selber eine Geschichte darüber, wie es wäre, wenn es eine Person gäbe,

2
00:00:06,680 --> 00:00:12,880
von die so ist wie wir und die in einer Situation ist, die wir Gegenwart nennen.

3
00:00:12,880 --> 00:00:18,120
Und wir konstruieren, während wir den sensorischen Input über die Gegenwart bekommen,

4
00:00:18,120 --> 00:00:22,720
diese Person und stellen uns vor, wie es wäre, so eine Person zu sein.

5
00:00:22,720 --> 00:00:25,120
Wir simulieren das.

6
00:00:25,120 --> 00:00:26,760
Das ist das, was das Gehirn macht.

7
00:00:26,760 --> 00:00:30,920
Deswegen sagt er, nur eine Simulation kann überhaupt Bewusstsein haben.

8
00:00:30,920 --> 00:00:35,600
Riesen Sorry schonmal an alle, die sich dieses Video jetzt hier rein gönnen.

9
00:00:35,600 --> 00:00:40,640
Ich glaube, es ist absolut unmöglich zu verstehen, was ich jetzt hier in diesem Video quatsche,

10
00:00:40,640 --> 00:00:44,800
wenn ihr mein Video zum Bewusstsein nicht zuerst gesehen habt.

11
00:00:44,800 --> 00:00:50,640
Das baut er halt darauf auf und ich benutze viele komische Fremdworte.

12
00:00:50,640 --> 00:00:56,040
Ich glaube, die ergeben alle überhaupt keinen Sinn, wenn ihr das Video mit dem Bewusstsein nicht vorher geguckt habt.

13
00:00:56,040 --> 00:00:58,560
Das ist nur so schon mal als Warnung.

14
00:00:58,560 --> 00:01:00,680
Das ist ein absolutes Nerd-Video.

15
00:01:00,680 --> 00:01:04,440
Ich habe das gestern aufgenommen und da hatte ich auch schon einen im Tee.

16
00:01:04,440 --> 00:01:07,720
Und jetzt habe ich mir halt angeguckt und dachte mir so, ja, okay, schon klar.

17
00:01:07,720 --> 00:01:11,960
Aber das kann glaube ich keiner verstehen, wenn man nicht irgendwie das mindset dafür hat.

18
00:01:11,960 --> 00:01:15,120
Und dafür ist das andere Video auch gut geeignet.

19
00:01:15,120 --> 00:01:17,320
Also guckt euch vorher an, bevor ihr jetzt hier weiter guckt.

20
00:01:26,040 --> 00:01:47,200
So Leute, jetzt mal wieder eins von diesen Deep Philosophical U-Bow-Videos des Todes und der Vorläufer zu diesem Video, würde ich sagen, ist das Video zum Bewusstsein.

21
00:01:47,200 --> 00:01:53,240
Und ich möchte hier erst mal mit euch über Emergenz reden, weil das braucht man irgendwie ständig.

22
00:01:54,200 --> 00:01:58,760
Und dann so ein bisschen mehr über, naja, was ist die Realität?

23
00:01:58,760 --> 00:02:00,840
Was ist das Bewusstsein?

24
00:02:00,840 --> 00:02:03,240
Und was soll das alles?

25
00:02:03,240 --> 00:02:17,000
Einfach mal ein bisschen euch mal ein bisschen mal was rein knallen an krasser Erkenntnis, die ich so in den letzten Jahren, ich würde sagen im letzten Jahr so ungefähr hatte.

26
00:02:17,000 --> 00:02:19,160
Und ja, also was ist jetzt Emergenz?

27
00:02:19,160 --> 00:02:22,040
Ich habe schon ein paar Mal verwendet das Wort.

28
00:02:22,040 --> 00:02:32,600
Emergenz ist immer dann etwas, wenn ihr etwas beobachtet, was erstmal total kompliziert wirkt, wenn ihr da reingucken wollt.

29
00:02:32,600 --> 00:02:41,880
Von außen sieht es irgendwie krass aus und ihr denkt, okay, das hat irgendwie einen Mechanismus, das ist heftig, aber wenn man reinguckt, steht man fest, es besteht aus ganz vielen simplen Teilen.

30
00:02:41,880 --> 00:02:54,520
Und irgendwie ist die Summe dieser Teile mehr als einfach nur, also die Summe der Teile ergibt mehr als einfach nur das, was man sich vorstellen würde, wenn man die Summe der Teile tatsächlich addieren würde.

31
00:02:54,520 --> 00:02:55,320
Was meine ich damit?

32
00:02:55,320 --> 00:02:56,600
Na ja, zum Beispiel ein Ameisenhaufen.

33
00:02:56,600 --> 00:02:59,800
Die kleinen Ameisen, die sind alle sehr einfach, wie die funktionieren.

34
00:02:59,800 --> 00:03:10,040
Trotzdem ist die Gesamtheit der Ameisen, das Konglomerat, so eine Art komplizierter Lebens, ein komplizierter Metabolismus, der sich verhält wie ein Lebewesen.

35
00:03:10,120 --> 00:03:21,080
Und wenn das Objekt, was ihr seht, nur aus Verbindungen besteht, die irgendwie auch rein virtuell dadurch sind, die sind ja nicht anfassbar.

36
00:03:21,080 --> 00:03:26,280
Man kann ja nicht sagen, ach, diese Wechselwirkung zwischen den Ameisen, die macht das erst so interessant.

37
00:03:26,280 --> 00:03:27,160
Aber genau darum geht es.

38
00:03:27,160 --> 00:03:28,040
Das ist die Emergenz.

39
00:03:28,040 --> 00:03:38,440
Das ist etwas, was man nicht so greifen kann, sondern es entsteht als ein Prozess höherer Ordnung zwischen Wechselwirkungen von einfacheren Mechanismen, die man sehr wohl verstehen kann.

40
00:03:38,440 --> 00:03:41,000
Und darum ist Emergenz nicht greifbar.

41
00:03:41,000 --> 00:03:42,360
Und das ist immer das Problem.

42
00:03:42,360 --> 00:03:43,240
Das seht ihr ganz oft.

43
00:03:43,240 --> 00:03:45,880
Das ist immer, wenn es um gesellschaftliche Themen geht.

44
00:03:45,880 --> 00:03:48,200
Gesellschaft ist zum Beispiel eine emergente Struktur.

45
00:03:48,200 --> 00:03:53,000
Sie entsteht aus den ganz vielen Interaktionen zwischen Menschen.

46
00:03:53,000 --> 00:03:55,480
Wenn wir die alle addieren, kriegen wir die Gesellschaft raus.

47
00:03:55,480 --> 00:03:57,960
Und Evolution ist auch so was.

48
00:03:57,960 --> 00:04:07,640
Evolution entsteht ja nur dadurch, dass wir einen Selektionsdruck haben, der nur dadurch entsteht, dass es sozusagen eine äußere Umgebung gibt, die sozusagen dazu führt,

49
00:04:07,720 --> 00:04:21,880
dass bestimmte Lebensformen, die besser, ich würde mal sagen, negative Entropie farmen können oder besser den Gradienten in die Entropie runtergehen können als andere und dadurch sozusagen einen Wettbewerbsvorteil gegeneinander haben und sich dann durchsetzen.

50
00:04:21,880 --> 00:04:26,600
Und Schwarmintelligenz ist auch so was wie Emergenz.

51
00:04:26,600 --> 00:04:31,480
Und der Markt, das ist das, was die FDP-Menschen nicht verstehen.

52
00:04:31,480 --> 00:04:32,760
Die raffen es ja nicht.

53
00:04:32,760 --> 00:04:34,440
Aber der Markt ist eine emergente Struktur.

54
00:04:34,440 --> 00:04:43,160
Und weil das so unbegreifbar ist, dachte damals Hayek auch, und das ist sozusagen so eine Art Gott für ihn und versteht mich da nicht falsch.

55
00:04:43,160 --> 00:04:48,360
Es ist natürlich eine emergente Struktur und es ist natürlich interessant, darüber nachzudenken.

56
00:04:48,360 --> 00:04:56,920
Aber nur weil es kompliziert ist oder irgendwie geil aussieht oder man ein bisschen Mathematik damit machen kann, heißt es ja nicht, dass das sozusagen die Lösung aller Probleme ist.

57
00:04:57,000 --> 00:04:58,520
Aber genau so geht Hayek daran.

58
00:04:58,520 --> 00:05:04,120
Das habe ich auch schon mal in einem anderen Video erklärt, von wegen welches Problem lösen die Märkte eigentlich.

59
00:05:07,000 --> 00:05:17,240
So und was soll ich sagen, wenn es etwas in den letzten Jahren gab, was ich mir sehr gerne reingezogen habe, dann sind das Videos von Joscha Bach.

60
00:05:17,240 --> 00:05:21,800
Und ich kann euch wirklich nur empfehlen, schmeißt alles in die Ecke.

61
00:05:22,120 --> 00:05:28,920
Jo Roggen Podcasts oder alle möglichen Lex Friedman Interviews oder sonst welche Sachen, die ihr im englischen YouTube drin gesehen habt.

62
00:05:28,920 --> 00:05:34,680
Das nächste, was ihr bei euch auf die Watchlist drauf tut, ist der erste Talk von Joscha Bach.

63
00:05:34,680 --> 00:05:36,840
Und der zweite, den könnt ihr gleich noch hinten dran hängen.

64
00:05:36,840 --> 00:05:41,880
Dann habt ihr sechs Stunden lang komprimierteste Todesweisheit des Todes.

65
00:05:41,880 --> 00:05:44,360
Der Typ ist der lebende Sheldon Cooper.

66
00:05:44,360 --> 00:05:48,600
Das ist der krasseste Wichser, den ich je irgendwo gehört habe.

67
00:05:48,600 --> 00:05:51,960
Ich glaube, das ist so ein verdammtes Genie.

68
00:05:51,960 --> 00:05:58,760
Und der lebt heutzutage und ich habe ihn schon auf Twitter vollgequatscht, er soll mal mit mir reden, aber natürlich, warum sollte der das tun?

69
00:05:58,760 --> 00:06:02,200
Der hat einfach eine volle Inbox, der hat keine Zeit mit einem scheiß Ossi zu reden.

70
00:06:02,200 --> 00:06:04,440
By the way, Joscha Bach ist selber ein Ossi.

71
00:06:04,440 --> 00:06:06,680
Aber ihr müsst euch den Typen reinziehen.

72
00:06:06,680 --> 00:06:11,400
Der alles, was er jemals in seinem Leben gelesen hat, hat er hier oben drin und kann sofort abrufen.

73
00:06:11,400 --> 00:06:12,920
Der ist krass, der Typ.

74
00:06:13,880 --> 00:06:21,880
Und ich bin übrigens nur auf den gestoßen, weil jemand mir den empfohlen hat unter einem englischen Matrix-Video.

75
00:06:21,880 --> 00:06:23,400
Hat mir jemand Joscha Bach empfohlen.

76
00:06:23,400 --> 00:06:25,560
Dafür hat sich der englische Kanal schon gelohnt.

77
00:06:25,560 --> 00:06:27,240
Das ist super.

78
00:06:27,240 --> 00:06:32,600
Und jetzt wollte ich ein bisschen, jetzt haben wir das geklärt mit der Emergenz, ich habe jetzt dieses Wort etabliert.

79
00:06:32,600 --> 00:06:41,080
Jetzt will ich ein anderes wichtiges Wort für euch etablieren, weil das ist natürlich auch so ein Ding, wo die Leute immer rumschlavieren.

80
00:06:41,080 --> 00:06:44,280
Und irgendwie so so eine Krämer-Laber-Attitüde haben.

81
00:06:44,280 --> 00:06:46,520
Was ist jetzt eigentlich Intelligenz?

82
00:06:46,520 --> 00:06:52,920
Also guter Freund von mir hat damals zu mir gesagt, Intelligenz ist die Fähigkeit, Analogien zu bilden.

83
00:06:52,920 --> 00:06:59,000
Und das ist eine ziemlich gute Definition, wenn man da mal drüber nachdenkt, weil damit kann man eine ganze Menge erschlagen.

84
00:06:59,000 --> 00:07:02,360
Quasi ich habe hier, sagen wir mal, verstanden, wie das eine Ding funktioniert.

85
00:07:02,360 --> 00:07:05,160
Und jetzt habe ich hier ein anderes Ding, das heißt anders.

86
00:07:05,160 --> 00:07:10,680
Aber der Mechanismus, der hier abläuft, ist genau derselbe Mechanismus wie in dem Ding, was ich schon verstanden habe.

87
00:07:10,680 --> 00:07:17,480
Wenn ich also die Analogie bilden kann und feststellen kann, dass abgesehen von den Bezeichnungen, dass da was da passiert, dasselbe oder das gleiche ist,

88
00:07:17,480 --> 00:07:20,280
dann habe ich eine Analogie gebildet und dann bin ich krass.

89
00:07:20,280 --> 00:07:23,080
Und der King der Analogien, meiner Meinung nach, ist Slavoj G. Jack.

90
00:07:23,080 --> 00:07:25,240
Der hat die witzigsten Analogien.

91
00:07:25,240 --> 00:07:30,200
So und Joscha Bach jetzt, der sagt, Intelligenz ist die Fähigkeit, Modelle zu bauen.

92
00:07:30,200 --> 00:07:32,920
Also Modelle über das, was man sieht.

93
00:07:32,920 --> 00:07:34,600
Und was meint man jetzt damit?

94
00:07:34,680 --> 00:07:41,640
Ein mathematisches Modell typischerweise hat was damit zu tun, dass man eine Komprimierung macht.

95
00:07:41,640 --> 00:07:49,880
Man kompressiert, man komprimiert irgendwie etwas, was man sieht, auf das Wesentliche herunter.

96
00:07:49,880 --> 00:07:52,040
Das ist das, was beim Komprimieren passiert.

97
00:07:52,040 --> 00:07:59,800
Und da muss man natürlich sich genau fragen, in welcher Sprache habe ich es auf das Wesentliche, auf das Minimum runtergekocht?

98
00:07:59,800 --> 00:08:03,400
Und das ist genau die Fragestellung, die man beim Archivieren von Dateien auch hat.

99
00:08:03,400 --> 00:08:08,680
Kann man sozusagen ein Muster finden, auf die man es runterbrechen kann?

100
00:08:08,680 --> 00:08:14,680
Kann ich das Muster sozusagen einmal abspeichern und dann nur noch sagen, jetzt kommt das Muster und dann spare ich mir 500 Bits oder so?

101
00:08:14,680 --> 00:08:16,680
So eine Idee ist das.

102
00:08:16,680 --> 00:08:20,360
Und darauf basieren dann sozusagen die ganzen modernen Komprimierungsverfahren auch.

103
00:08:20,360 --> 00:08:27,640
Im Wesentlichen, in der Mathematik würde man sagen, man findet eine Vektorbasis oder im Kompress-Censing würde man sagen, man findet eine Basis,

104
00:08:27,720 --> 00:08:39,960
die irgendwie im Sinne der L2-Norm oder beziehungsweise in der L1-Norm, eigentlich in der L0-Norm, ich glaube es ist tatsächlich die L0-Norm oder die L1-Norm, da wo das sozusagen ein Minimum annimmt.

105
00:08:39,960 --> 00:08:44,040
Das ist quasi Kompress-Censing, aber nur für die Geeks und Nerds, die sich damit auskennen.

106
00:08:44,040 --> 00:08:50,600
Ansonsten, ihr könnt euch einfach vorstellen, wann kann ich sozusagen etwas aufs Wesentliche so weit runterbrechen, bis es nicht mehr weiter runtergeht?

107
00:08:50,600 --> 00:08:53,880
Bis ich es sozusagen auf die Einsen und Nullen runtergebrochen habe.

108
00:08:53,960 --> 00:09:04,760
Und Sprache ist sozusagen in diesem Kontext sozusagen eine High-Level-Abstraktion, weil sie ist sehr schwammig und Mathematik ist quasi Low-Level.

109
00:09:04,760 --> 00:09:10,040
Man kann sozusagen das eine in das andere überführen, aber es ist sehr langwierig.

110
00:09:10,040 --> 00:09:18,360
Bestimmte Sachverhalte lassen sich mit Sprache in ein, zwei Sätzen erklären und man müsste einen riesen Apparat Mathematik drauf werfen, um dasselbe in Mathematik auszudrücken.

111
00:09:18,680 --> 00:09:24,440
Joscha Bach sagte, Mathematik ist die Domäne aller Sprachen und mittlerweile glaube ich, da hat er natürlich recht.

112
00:09:24,440 --> 00:09:34,840
Das ist ziemlich krass, denn es ist so, Sprache in diesem Sinne, das Kompress-Censing zum Beispiel, ist eine Projektion von Inhalt.

113
00:09:36,680 --> 00:09:43,720
Wir haben hier dieses Ding, wir haben diese echten Inhalte, die sind kompliziert und mit Sprache vereinfachen wir die.

114
00:09:43,720 --> 00:09:51,640
Wir machen schon so eine Art Komprimierung, wir komprimieren schon den Inhalt, sodass wir ihn rüberreichen können und natürlich ist dann sozusagen Informationsverlust.

115
00:09:51,640 --> 00:09:57,960
Und das ist bei jeder Projektion, jeder der lineare Algebra schon mal gehört hat, weiß, Projektionen verlieren Informationen und das ist da auch der Fall.

116
00:09:57,960 --> 00:10:06,840
Und das ist ja klar, Sprache kann beim Empfänger anders ankommen, als wir es eigentlich als der Empfänger, als der Aussender der Sprache es eigentlich sagen wollte.

117
00:10:06,840 --> 00:10:13,800
Und ja, Wittgenstein zum Beispiel dachte, dass man alles mit Sprache aufbauen kann und das war sozusagen sein Fail.

118
00:10:13,800 --> 00:10:20,280
Er hat sozusagen nicht runterbrechen können auf Mathematik, sondern er dachte Sprache ist sozusagen der Shit.

119
00:10:20,280 --> 00:10:27,160
Und deswegen sage ich ja auch, Wittgenstein hätte die Sache mit dem Word Embeddings in Deep Learning auch richtig gut gefallen.

120
00:10:27,160 --> 00:10:32,280
Ja, aber so Sachen wie Bilder oder Geometrie mit Sprache zu beschreiben, ist halt völlig Fail.

121
00:10:32,520 --> 00:10:37,640
Da ist völlig klar, dass die etabliert, also die bessere Variante irgendwie Mathematik sein muss.

122
00:10:37,640 --> 00:10:45,560
Und jetzt wollte ich mal erklären, was nennen wir eigentlich Realität bzw. was nennen wir unser Universum?

123
00:10:45,560 --> 00:10:57,080
Na ja, wir bezeichnen mit Realität typischerweise das, was unser Modell als Ursache angibt für den sensorischen Input, den wir erfahren.

124
00:10:57,080 --> 00:11:03,240
Das einzige, was unser Gehirn bekommt, ist der sensorische Input über unsere Augen, unsere Sinnesvornehmungen und so.

125
00:11:03,240 --> 00:11:10,840
Wir wissen nichts über die Welt da draußen, außer wir sind nur beschränkt darauf auf den sensorischen Input, den wir haben und daraus schlussfolgern wir alles.

126
00:11:10,840 --> 00:11:19,320
Und das Gehirn, das habe ich ja in meinem Bewusstheitsvideo schon erklärt, versucht sozusagen das mathematische schlechtgestellte inverse Problem zu lösen.

127
00:11:19,320 --> 00:11:24,520
Was hat die Inputdaten verursacht? Wir wissen ja nicht, was die verursacht haben.

128
00:11:24,600 --> 00:11:31,720
Unser Gehirn, quasi unser Verstand, unser was auch immer, unser Gehirn versucht zu beschreiben und zu erklären, was diese Inputdaten verursacht hat.

129
00:11:31,720 --> 00:11:35,400
Und dabei konstruieren wir das, was wir Realität nennen.

130
00:11:35,400 --> 00:11:47,960
Und das beste, was Urscha Bachter immer sagt, ist, es gibt in dieser, also die beste Sprache, die wir im Moment dafür haben, ist sozusagen die Physik, die physikalischen Gesetze.

131
00:11:47,960 --> 00:11:51,960
Und in dieser Sprache gibt es keine Geräusche und es gibt auch keine Farben.

132
00:11:52,600 --> 00:12:01,320
Das sind einfach nur höhere Ordnung von Strukturen, denen wir diese Bedeutung zuweisen im Sinne von einer Komprimierung.

133
00:12:06,680 --> 00:12:09,000
Und jetzt ist es so...

134
00:12:11,000 --> 00:12:17,720
Übrigens, das ist der Grund, wenn man jetzt irgendwie, sagen wir mal, ich nehme mir jetzt das Lineal, tue das hier auf den Tisch und knall hiergegen, dann hör ich so ein Brrrrrrr.

135
00:12:18,280 --> 00:12:25,240
Und diese höheren Schwingungsmoden und so, diese ganzen Sachen mit den Schwingungen, Sounds sind auch fraktale.

136
00:12:25,240 --> 00:12:28,760
Da gibt es ein ziemlich geiles Video von Adam Neely, falls ihr den kennt.

137
00:12:28,760 --> 00:12:30,760
Das ist ein sehr witziges Video, das werde ich verlinken.

138
00:12:30,760 --> 00:12:38,840
Dass sozusagen sich auch da durch die höheren Schwingungsmoden, Sounds wie, also Geräusche und Töne wie fraktale verhalten.

139
00:12:38,840 --> 00:12:43,400
Also man kann da immer weiter reinzoomen und die werden dann immer wieder selbst ähnlich.

140
00:12:43,400 --> 00:12:50,040
Und das ist super geil, denn das bedeutet im Prinzip, dass man alles auf eine Ringstruktur abbilden kann.

141
00:12:50,040 --> 00:12:55,800
Deswegen kann man Farben auch in einem Ring anordnen und bei Tönen kann man die sozusagen auf diese Oktaven abbilden.

142
00:12:55,800 --> 00:12:59,160
Und das hat damit zu tun, dass sie sozusagen fraktale darstellen.

143
00:12:59,160 --> 00:13:01,000
Und in echt existiert das nicht.

144
00:13:01,000 --> 00:13:08,360
Das ist was, was unser Gehirn konstruiert, genauso wie, was weiß ich, wenn wir irgendwie rumgucken mit unseren Augen,

145
00:13:08,360 --> 00:13:14,120
dann wird der Hauptinput, der wird interpoliert und auch extrapoliert hier am Rand und so.

146
00:13:14,120 --> 00:13:22,120
Unser Gehirn kann sozusagen die Daten, die eigentlich aufgenommen werden müssten, um das, was wir hier sehen, zu konstruieren, kann es gar nicht verarbeiten.

147
00:13:22,120 --> 00:13:23,640
Das braucht das Gehirn nicht.

148
00:13:23,640 --> 00:13:27,880
Unser Gehirn ist so krass, dass es übelst Rechenleistung investiert, um das alles zu interpolieren.

149
00:13:27,880 --> 00:13:30,600
Und in dem Sinne ist es halt so,

150
00:13:30,600 --> 00:13:43,560
Unser Gehirn leistet diese Komprimierungsarbeit schon für uns und diese periodischen Eigenschaften von zum Beispiel Geräuschen und Farben machen, dass man die auf einem Ring anordnen kann.

151
00:13:43,560 --> 00:13:56,920
Und wir existieren jetzt, also das, was wir als wir bezeichnen, wir existieren in dieser virtuellen Realität, die unser Gehirn konstruiert, um zu erklären, was die Messdaten verursacht hat.

152
00:13:56,920 --> 00:14:00,040
Und das habe ich ja in dem Video über das Bewusstsein schon erklärt.

153
00:14:00,120 --> 00:14:09,640
Also unser Gehirn konstruiert diese virtuelle Realität und auf diesem, weil das Ding ist, auf diesem Level von diesem äußerst merkwürdigen Quantengraf,

154
00:14:09,640 --> 00:14:16,120
den wir jetzt, von dem wir annehmen, dass es, dass der existiert durch unsere mathematischen Modelle und unsere Physik und so,

155
00:14:16,120 --> 00:14:19,000
da existiert wirklich nur kalte Algebra.

156
00:14:19,000 --> 00:14:29,960
Von einem Zustand des Universums zum nächsten transformieren die physikalischen Gesetze diesen Quantengraf und wir können nur einen ganz kleinen Auschnitt davon messen.

157
00:14:30,920 --> 00:14:32,920
Durch unseren sensorischen Input.

158
00:14:32,920 --> 00:14:40,040
Und das, was die Philosophen dann sozusagen als Qualia bezeichnen, das ist genauso eine Illusion wie der Freie Wille.

159
00:14:40,040 --> 00:14:45,640
Das ist einfach nur eine emergente Struktur, die auf unserem Gehirn entsteht, Qualia.

160
00:14:45,640 --> 00:14:53,240
Und Joscha Bach nimmt jetzt als Beispiel für Realität, ziemlich geiles Beispiel, wie ich finde, das Mandelbrot-Fraktal.

161
00:14:53,240 --> 00:14:58,520
Vielleicht binde ich das jetzt hier auch in das Video ein, da gibt es bestimmt ein cooles Gift oder so.

162
00:14:58,520 --> 00:15:05,560
Das ist so ein Fraktal, das entsteht im Wesentlichen, wenn man eine iterative Abbildung lösen will.

163
00:15:05,560 --> 00:15:15,000
Ja genau, man hat eine quadratische iterative Abbildung in einer komplexen Ebene und guckt für welche Werte von einer konstanten komplexen Zahl die konvergiert oder nicht.

164
00:15:15,000 --> 00:15:24,280
Und wenn man das sozusagen macht für die komplexe Zahlenebene und alles was divergiert mit schwarz abbildet, dann kriegt man dieses Fraktal und so baut man das.

165
00:15:24,360 --> 00:15:28,840
Das ist relativ einfach das zu bauen tatsächlich, aber das Muster was rauskommt, das sieht halt super krass aus.

166
00:15:28,840 --> 00:15:43,480
Und wenn man sich jetzt vorstellt, wir leben auf so einer fraktalen Oberfläche von dem Mandelbrot-Fraktal, dann würden wir sehen, okay, da gibt es so eine rechtsdrehende Spirale.

167
00:15:43,480 --> 00:15:48,120
Wenn man die jetzt reingeht, würde man irgendwann an einer Singularität ankommen.

168
00:15:48,200 --> 00:15:58,120
Und wenn man die sozusagen, wenn man darüber hinweg guckt, gibt es quasi Periodenverdopplung oder ich weiß gar nicht mehr wie dieser Effekt heißt, auf jeden Fall, dann beginnt diese Spirale von vorne.

169
00:15:58,120 --> 00:16:04,760
Dann beginnt sozusagen die Oberfläche von vorne zu wachsen, dann kommt diese Selbstähnlichkeit, die kickt rein bei dem Fraktal und dann sieht es wieder von vorne so aus.

170
00:16:04,760 --> 00:16:10,760
Und man müsste wieder die Spirale weiter reingehen, bis man wieder zur Singularität kommt und dann wieder reingehen und so weiter.

171
00:16:10,760 --> 00:16:19,080
Und wenn man so mehrere Layers bauen würde von seiner eigenen Realität, weil wir leben ja auf diesem Fraktal, dann wäre das eine gute Approximation für die Realität.

172
00:16:19,080 --> 00:16:24,360
Was wir niemals sehen würden, ist sozusagen das komplette Picture, weil das Fraktal ist.

173
00:16:24,360 --> 00:16:28,760
Es ist ein unendlich großes und selbstähnliches periodisches Muster.

174
00:16:28,760 --> 00:16:31,080
So und so ist es mit unserem Universum auch.

175
00:16:31,080 --> 00:16:36,920
Wir haben halt diese physikalischen Gesetze, sie sind nicht perfekt, sie sind aber eine gute Beschreibung für das, was wir haben, was wir sehen und so.

176
00:16:36,920 --> 00:16:41,080
Und je näher wir uns ran tasten an die Realität, desto besser werden wir.

177
00:16:41,080 --> 00:16:51,000
Irgendwann würden wir, wenn wir auf diesem Fraktal leben, in der Mandelbrotmenge, irgendwann würden wir diese paar Zeilen Code, die das braucht, um das zu generieren, vielleicht bauen können.

178
00:16:51,000 --> 00:16:54,280
Und dann hätten wir es gekickt, dann hätten wir die Realität vollständig beschrieben.

179
00:16:54,280 --> 00:17:00,040
Das heißt, was man machen muss, um die Realität zu beschreiben, ist man muss sich aus First Principles Schluss folgern.

180
00:17:00,040 --> 00:17:04,280
Und das geht natürlich nicht so als Schlussfolgerung, sondern das ist im Prinzip zwar ein Error.

181
00:17:04,280 --> 00:17:18,520
Also man muss Mathematik machen, betreiben und man muss es solange mit 3 in Error machen, bis man irgendwie auf das kommt, was sozusagen die minimale Information beinhaltet, die alles beschreibt, was wir haben.

182
00:17:18,520 --> 00:17:21,240
Das ist das Prinzip von Okams Racer auch.

183
00:17:21,240 --> 00:17:25,720
Also wir wollen so wenig wie möglich voraussetzen oder so wenig wie möglich annehmen.

184
00:17:25,720 --> 00:17:28,920
Und das soll maximal viel von dem, was wir beobachten, beschreiben.

185
00:17:28,920 --> 00:17:33,120
Und wenn wir etwas haben, was alles mit einem Formalismus beschreibt, dann sind wir fertig.

186
00:17:33,120 --> 00:17:38,000
Das ist die Idee. Das ist sozusagen das Ideal, an das ich sozusagen die Wissenschaft da ran tasten möchte.

187
00:17:43,840 --> 00:18:01,440
So und wenn wir also, Joshua Bach gibt auch noch so einen anderen coolen Spruch, nämlich er sagt, die physikalische Realität hat kein Bewusstsein und da gibt es auch kein Bewusstsein.

188
00:18:01,440 --> 00:18:05,200
Nur eine Simulation kann Bewusstsein haben.

189
00:18:05,200 --> 00:18:17,160
Das ist das, was er sagt und das ist ein ziemlich cooler Spruch, weil was er damit meint ist, wenn wir jetzt ein Buch lesen und da ist ein Protagonist drin und der fühlt irgendwas oder dem geht es irgendwie scheiße.

190
00:18:17,160 --> 00:18:19,080
Dann empfinden wir das, während wir das lesen.

191
00:18:19,080 --> 00:18:30,040
Das liegt daran, dass wir in unserem Kopf uns ein Bild davon, ein Abbild dieser Person in dem Buch, die da beschrieben wird durch Sprache übrigens, bauen und dadurch wird es real.

192
00:18:30,040 --> 00:18:37,320
Und der Witz ist, zwischen dieser Person in dem Buch und dem, was wir uns selbst nennen, gibt es eigentlich keinen Unterschied.

193
00:18:37,320 --> 00:18:50,480
Denn das Gehirn erzählt sich ja selber eine Geschichte darüber, wie es wäre, wenn es eine Person gäbe, die so ist wie wir und die in einer Situation ist, die wir Gegenwart nennen.

194
00:18:50,480 --> 00:18:59,000
Und wir konstruieren, während wir den sensorischen Input über die Gegenwart bekommen, diese Person und stellen uns vor, wie es wäre, so eine Person zu sein.

195
00:19:00,280 --> 00:19:01,480
Wir simulieren das.

196
00:19:02,680 --> 00:19:03,960
Das ist das, was das Gehirn macht.

197
00:19:04,320 --> 00:19:07,520
Deswegen sagt er, nur eine Simulation kann überhaupt Bewusstsein haben.

198
00:19:07,920 --> 00:19:17,640
Nur eine Simulation ist fähig, diese emergente Struktur zu haben, die dann wieder, also die emergente Struktur, die sozusagen das Bewusstsein als Nebenprodukt hat.

199
00:19:18,400 --> 00:19:19,160
Und das ist krass.

200
00:19:19,360 --> 00:19:20,360
Das ist ziemlich heftig.

201
00:19:20,360 --> 00:19:25,600
Also wenn ihr, wenn ihr das nicht kennt, diese Joscha Bach Talks, ihr müsst euch die reinziehen.

202
00:19:25,600 --> 00:19:27,200
Ich verlinke die beiden von Lex Friedman.

203
00:19:27,320 --> 00:19:32,240
Es gibt aber noch viele mehr, die sozusagen ins Detail gehen, wenn Joscha Bach seine Meinung zu GPT-3 erzählt.

204
00:19:32,240 --> 00:19:34,280
Das ist super krass oder generell.

205
00:19:34,280 --> 00:19:39,960
Es gibt, glaube ich, auch zwei, drei deutsche Talks, von denen einer mindestens schon mal ziemlich gut ist für Einsteiger.

206
00:19:39,960 --> 00:19:42,360
Die anderen sind zu abgefahren, absolute Nord-Talks.

207
00:19:42,880 --> 00:19:47,360
Aber wie gesagt, der Typ ist ein Ostdeutscher, der kann also auch fließend Deutsch, ist gar kein Problem.

208
00:19:47,360 --> 00:19:49,040
Man findet bloß nicht so viel von dem auf Deutsch.

209
00:19:50,440 --> 00:19:51,240
Aber hammermäßig.

210
00:19:52,320 --> 00:19:58,960
Also wirklich alles andere, alle anderen Talks, alle anderen Podcasts, die es auf dieser Welt gibt, sind, meiner Meinung nach, Zeitverschwendung,

211
00:19:58,960 --> 00:20:04,520
bevor man nicht vollständig durchdrungen hat, was Joscha Bach einen in drei Stunden reinquetscht an Wissen.

212
00:20:04,600 --> 00:20:05,440
Das ist so viel.

213
00:20:05,480 --> 00:20:16,040
Ich habe den Talk jetzt bestimmt vier, fünf Mal schon durchgeknallt und ich komme immer wieder auf neue Erkenntnisse, weil der Typ in einem Satz so viel Informationen und so viel Verständnis

214
00:20:16,040 --> 00:20:20,800
reinsteckt, dass man das wirklich erst nach einem Jahr nochmal dann checkt oder so.

215
00:20:21,960 --> 00:20:25,640
So und in Deep Learning ist es jetzt nur so.

216
00:20:26,200 --> 00:20:30,240
Man muss dazu wissen, das habe ich auch noch nie so erwähnt, aber das ist quasi bekannt.

217
00:20:30,240 --> 00:20:33,560
Das ist wahrscheinlich erste Vorlesung, Deep Learning im Informatikstudium.

218
00:20:34,240 --> 00:20:37,880
Der sogenannte universelle Funktions-Approximator, das ist das Relevante.

219
00:20:38,360 --> 00:20:52,720
Der sagt Folgendes aus, wenn ihr ein neuronales Netz macht mit N-Input, N-dimensionalen Input und ihr wollt nur ein Output haben, also sozusagen eine Funktion mit beliebigem Input und die soll was ausgeben.

220
00:20:53,160 --> 00:20:58,000
Und ihr habt ein Layer dazwischen, der hat beliebig viele, also auch beliebig viele Knoten.

221
00:20:59,200 --> 00:21:02,560
Dann könnt ihr jede mathematische Funktion damit approximieren.

222
00:21:03,120 --> 00:21:07,680
Das ist erstmal irgendwie klar, aber es wurde sozusagen bewiesen, dass das so ist.

223
00:21:08,560 --> 00:21:09,920
Und der Witz ist jetzt Folgendes.

224
00:21:10,360 --> 00:21:20,240
In der Informatik geht es jetzt darum, wir nehmen an, dass unsere Welt determiniert ist und deswegen kann man alles mit einer Funktion quasi hinschreiben.

225
00:21:21,320 --> 00:21:25,400
Die kann beliebig kompliziert sein, aber es muss durch eine Funktion approximierbar sein.

226
00:21:26,040 --> 00:21:31,480
Das bedeutet, dass alles mit einem Deep Learning Netz, also mit einem neuronalen Netz beschreibbar ist.

227
00:21:31,960 --> 00:21:33,000
Das ist die Voraussetzung.

228
00:21:33,400 --> 00:21:36,840
Und jetzt ist nur die Frage, wenn wir beliebig viele Knoten haben, dann ist das klar.

229
00:21:37,200 --> 00:21:38,800
Was man dann einfach macht, ist Overfitting.

230
00:21:38,960 --> 00:21:46,160
Also ihr könnt euch quasi vorstellen, ich habe eine Aufgabenstellung und ich verstehe die Aufgabenstellung nicht, sondern ich lerne sie einfach auswendig.

231
00:21:46,480 --> 00:21:56,600
Sagen wir mal, mein Konfigurationsraum hat 10 hoch 32 viele Möglichkeiten und 10 hoch 32 viele Outputs zu einem gegebenen Problem, eins aus diesen 10 hoch 32.

232
00:21:56,920 --> 00:22:02,200
Dann kann ich doch, wenn ich genug Knoten, also Speicherpunkte habe, kann ich die einfach auswendig lernen.

233
00:22:02,200 --> 00:22:04,880
Ich kann alle Antworten auf alle Fragen auswendig lernen.

234
00:22:06,760 --> 00:22:12,280
Also stellt euch einfach nur vor, ihr wollt eine Software schreiben, die Bilder erkennt und ihr gebt der Software vor.

235
00:22:12,960 --> 00:22:16,520
Man gibt ein Bild rein, das ist 256 mal 256 Pixel groß.

236
00:22:17,720 --> 00:22:27,160
Und jetzt macht ihr eine feine Abschätzung und steckt sozusagen einfach alle möglichen Bilder, die überhaupt existieren können im Konfigurationsraum.

237
00:22:27,360 --> 00:22:39,120
Also alle Kombinationen von Pixelwerten, die 256 mal 256 Bilder haben können überhaupt, die steckt ihr rein und lernt sie auswendig mit dem Satz darunter, was sieht man auf dem Bild.

238
00:22:39,520 --> 00:22:40,520
Das lernt ihr auswendig.

239
00:22:41,360 --> 00:22:46,080
Wenn ihr unendlich viel Speicherplatz habt und unendlich groß sozusagen das Lehr machen könnt, ist das natürlich kein Problem.

240
00:22:46,360 --> 00:22:51,720
Dabei habt ihr aber sozusagen, ihr habt das Problem nur gelernt, aber ihr habt es nicht verstanden.

241
00:22:52,440 --> 00:23:02,280
Und die Idee ist sozusagen, kann man das runter kochen auf irgendetwas, was weniger ist als Overfitting und sozusagen die Datenmenge besser abbildet,

242
00:23:02,480 --> 00:23:10,440
sozusagen eine sehr große Datenmenge, zum Beispiel dieser riesige Konfigurationsraum von allen möglichen Bildern, die 256 Quadratviele Pixel haben,

243
00:23:11,400 --> 00:23:18,040
irgendwie abzubilden auf irgendwas, wo wir trotzdem das Problem lösen können, nämlich eine Software, die uns am Ende sagt, was sieht man auf dem Bild oder so.

244
00:23:18,360 --> 00:23:24,960
Und das ist quasi das, was ich hier sozusagen jetzt mit Overfitting auch erklären wollte, aber wie gesagt, das Video ist ein bisschen heftig, ich weiß schon.

245
00:23:25,960 --> 00:23:27,040
Das nennt man Overfitting.

246
00:23:27,480 --> 00:23:33,800
Viel besser wäre es doch, wenn ich runter gehe mit meiner Komplexität in meinem Deep Learning Netzwerk, also in diesem Zwischenlayer,

247
00:23:35,120 --> 00:23:42,960
auf viel weniger Dimension und trotzdem noch eine sehr hohe Approximationsgüte kriege, also 99,999 Prozent oder so.

248
00:23:43,760 --> 00:23:45,280
Denn dann habe ich ja Folgendes geschafft.

249
00:23:45,680 --> 00:23:52,120
Ich habe mein Problem komprimiert, was konkret, ich habe aus dem Auswendiglernen Verstehen gemacht.

250
00:23:52,640 --> 00:23:59,720
Ich habe sozusagen, und das ist ja das, was grundsätzlich irgendwie nie jemand mal rafft, was ist jetzt der Unterschied zwischen lehnen und lernen und verstehen?

251
00:24:00,160 --> 00:24:11,560
Verstehen bedeutet, mit dem minimalen Satz an Handlungsanweisungen das Problem vollständig zu lösen und aus verschiedensten Wissenssachen, aus Kombinationen von Wissen, neues Wissen zu erzeugen.

252
00:24:12,360 --> 00:24:19,000
Ja, eine Anwendung zu finden, die sozusagen nur darauf basiert, dass ich hier das eine schon mal verstanden habe und ich direkt auf das andere anwenden kann.

253
00:24:20,080 --> 00:24:21,600
So, und das ist auch da der Fall.

254
00:24:21,920 --> 00:24:26,480
Wenn man also den minimalen Satz quasi die beste Architektur von Layern erzeugen kann,

255
00:24:27,720 --> 00:24:32,800
um zum Beispiel Energie zu sparen, Speicher zu sparen, dann komprimiert das sozusagen. Ich mache eine Kompression dabei.

256
00:24:33,640 --> 00:24:42,840
Und da kommen dann sozusagen die mehreren Layer dann typischerweise im Deep Learning dazu. Es ist nicht einer, sondern es sind viele verschachtelte Layer, die runter und hoch gehen in ihrer Dimension.

257
00:24:42,840 --> 00:24:48,960
Da gibt es sozusagen Konvolutiones und was auch immer. Es gibt übelst viele krasse Architekturen mittlerweile, die alle sehr tricky sind.

258
00:24:49,760 --> 00:25:01,920
Aber das ist sozusagen erstmal die Idee. Es gibt diesen universellen Funktions-Approximator und man kann mit einem Neuronalen jetzt jede beliebige Funktion, insbesondere alles, was wir in unserer Realität beobachten, wenn es dem geht.

259
00:25:03,760 --> 00:25:14,320
Das ist die Annahme. Und warum sollte man also nicht das Gehirn damit beschreiben können? Und warum sollte man nicht die gesamte Realität und das gesamte Universum damit beschreiben können?

260
00:25:21,680 --> 00:25:25,240
Ich habe jetzt schon erklärt in meinem anderen Video, was das Bewusstsein ist.

261
00:25:26,120 --> 00:25:34,680
Und Joshua Bach hat es ziemlich kompakt nochmal ausgedrückt und hat gesagt, das Bewusstsein ist ein Modell für den Inhalt unseres Aufmerksamkeitsmodells.

262
00:25:34,680 --> 00:25:41,840
Und was meint ihr damit? Na ja, es ist so eine Art Meta-Aufmerksamkeit. Ihr müsst euch vorstellen, es ist so eine Art Meta-Learning.

263
00:25:41,840 --> 00:25:47,320
Was meint jetzt eigentlich Aufmerksamkeit? Na ja, also erstmal gibt es ja mittlerweile schon Transformatoren.

264
00:25:47,320 --> 00:25:58,960
Also es gibt schon Attention-Networks im Deep Learning. Diese Sprache wurde schon etabliert und das macht auch ziemlich genau das, was man sich eigentlich unter dem Konzept von Aufmerksamkeit vorstellt.

265
00:25:58,960 --> 00:26:06,600
Also erstmal Aufmerksamkeit und Bewusstheit, das hängt schon miteinander zusammen, auch vom Wort, sozusagen von der wörtlichen Bedeutung.

266
00:26:07,080 --> 00:26:17,520
Und im Moment ist es so, normale Deep Learning-Netzwerke machen Folgendes. Die haben einen Fehler-Signal, also sagen wir mal, wir haben eine Beobachtung und eine Prediktion von unserem Modell und das ist falsch.

267
00:26:17,520 --> 00:26:24,520
Und dieses Fehler-Signal, die Differenz davon, die versuchen wir zurückzuführen, in unser Modell wegzutrecken. Wo kommt sozusagen der Fehler her?

268
00:26:24,520 --> 00:26:30,880
Und dabei sozusagen bei vielen Daten entstehen natürlich eine ganze Menge in diesen statistischen Gewichten, wo irgendwas sich ändert.

269
00:26:30,880 --> 00:26:43,440
Wenn man jetzt mit vielen Daten lernt, mitteln sich sozusagen die unwichtigen Gewichte raus und was überbleibt ist das, wo der wirkliche Fehler herkommt, also das sind dann meistens nur noch weniger Gewichte.

270
00:26:43,440 --> 00:26:53,160
So, das ist aber ein Prozess, der super ineffizient ist und langsam, der ist super langsam, weil man immer wieder den kompletten Backpropagating-Step durchmachen muss.

271
00:26:54,040 --> 00:27:09,520
Viel geiler wäre, wenn man ein Netzwerk hat, was oben da bei diesem Lernprozess draufguckt und feststellt, wo sind die größten Gewichte im Gradienten, also quasi in dem Backpropagating und kann ich das beobachten, wenn ich das sozusagen öfter mache.

272
00:27:09,520 --> 00:27:19,280
Und dann kann man Statistiken betreiben und sozusagen das abkürzen. Man kann diesen Lernprozess abkürzen, weil man sozusagen den Shortcut geht.

273
00:27:19,440 --> 00:27:28,400
Das hat so ein bisschen was von, wenn ihr euch mit iterativen Lösungsverfahren von algebraischen Gleichungen auskennt, das hat ein bisschen was von Order-Zapsets, finde ich.

274
00:27:28,400 --> 00:27:34,960
Aber das ist wirklich nur meine Physiker-Intubation, da bin ich sozusagen mit gefährlichem Halbwissen hier unterwegs.

275
00:27:34,960 --> 00:28:00,840
Jedenfalls, das kürzt den Lernprozess ab und das erklärt natürlich auch, warum sozusagen heutzutage so ein Alpha Star, also die KI, die StarCraft gewinnt, die hat ja so was wie umgerechnet 200 Jahre StarCraft gespielt, bevor sie auf einem Level ist, wie ein normaler Master-Spieler im High MMR Blizzard Elo.

276
00:28:00,840 --> 00:28:10,680
Und ein Mensch braucht natürlich viel weniger Daten. Warum? Na ja, weil unsere Hirnarchitektur krasser ist. Wir leben schon auf dem nächsten Level von Deep Learning.

277
00:28:10,680 --> 00:28:22,800
Was sozusagen die State of the Art ist im Moment, das erste Level war quasi, wir bringen ein Programm bei Schach zu spielen und das kriegen wir einfach hin.

278
00:28:22,800 --> 00:28:30,680
Das nächste Level war, wir schreiben eine Architektur, zum Beispiel bei Go, da geht das nicht mehr, das Spiel ist zu kompliziert, man kann nicht in einem Programm einfach schreiben, was das kann.

279
00:28:30,760 --> 00:28:36,120
Sondern wir schreiben ein Netzwerk, was selber lernt, sich Go beizubringen.

280
00:28:37,120 --> 00:28:43,040
Und Joshua Bach sagt jetzt, das ist aber immer noch nicht das, was wir können, wir Menschen sind noch ein Level krasser.

281
00:28:43,520 --> 00:28:51,920
Wir können Meta-Learning machen, wir können also ein Programm schreiben, was ein Programm findet, um ein Problem zu lernen.

282
00:28:52,600 --> 00:29:00,400
Das ist sozusagen das, von dem er sagt, was wir als nächsten Schritt brauchen. Und ich nehme an, das passiert erst nach dem nächsten KI-Winter.

283
00:29:00,400 --> 00:29:07,120
Also wenn sozusagen nur Slow reinkickt und wir noch mehr Rechnerkapazitäten haben, um das zu machen.

284
00:29:07,120 --> 00:29:12,680
Weil ihr dürft mich vergessen, wie unfassbar viele Neuronen wir in unserem Kopf drin haben.

285
00:29:12,680 --> 00:29:15,360
Also die Komplexität unseres Gehirns ist schon ziemlich krass.

286
00:29:16,320 --> 00:29:22,720
Und man kann sozusagen sagen, da ist eine ganze Menge krasser, universeller Funktions-Approximator mit drin.

287
00:29:22,720 --> 00:29:29,560
Und das Gehirn hat halt wirklich jetzt, das beobachtet halt, wo die meiste Performance herkommt beim Lernen.

288
00:29:29,560 --> 00:29:36,960
Und speichert quasi diese Aufmerksamkeit, das speichert es zusammen mit dem Kontext, in dem es gelernt hat, ab.

289
00:29:37,400 --> 00:29:41,720
Und das ist sozusagen der Inhalt der Aufmerksamkeit.

290
00:29:41,720 --> 00:29:48,600
Also das ist der Inhalt der Aufmerksamkeit, wo haben wir wann, was, wie am besten gelernt.

291
00:29:48,600 --> 00:29:57,000
Was sozusagen hat die meisten Performance Boost in unserer Performance gebracht, wenn wir an dem einen statistischen Gewicht oder an dem einen Neuronen wackeln.

292
00:29:57,840 --> 00:30:07,600
Und der Witz ist, das ist so eine Art Meta-Learning, weil es sozusagen eine Aufmerksamkeit über die Aufmerksamkeit macht.

293
00:30:07,600 --> 00:30:12,040
Es speichert sozusagen den Kontext und wo die Aufmerksamkeit war.

294
00:30:12,040 --> 00:30:15,720
Und das muss es beobachten, das Gehirn. Und das tut es.

295
00:30:15,720 --> 00:30:22,200
Das ist sozusagen Meta-Learning, weil Aufmerksamkeit über die Aufmerksamkeit quasi oder der Inhalt der Aufmerksamkeit zusammen mit dem Kontext.

296
00:30:22,200 --> 00:30:24,080
So ungefähr muss man sich das vorstellen.

297
00:30:24,080 --> 00:30:29,400
Und ich finde, an diesem Punkt ist sozusagen diese maximale Selbstbezüglichkeit auch klar.

298
00:30:29,400 --> 00:30:34,400
Und das ist auch das, was die Leute so schizophren finden an der Frage, habe ich einen freien Willen oder nicht.

299
00:30:34,400 --> 00:30:35,800
Oder was ist das Bewusstsein?

300
00:30:35,800 --> 00:30:40,040
Das ist halt sozusagen, das ist das, was Douglas Hofstadter wahrscheinlich meinte, mit einem Estrange Loop.

301
00:30:40,040 --> 00:30:44,600
Und das Buch habe ich bis jetzt noch nicht gelesen, aber das steht auch auf meinem Pile of Shame.

302
00:30:44,600 --> 00:30:57,800
Und das Hören muss sozusagen auch genau darauf achten, also es muss ich selbst beobachten, worauf ich jetzt genau Aufmerksamkeit lege.

303
00:30:57,800 --> 00:30:59,040
Also was gucke ich mir gerade an?

304
00:30:59,040 --> 00:31:03,360
Und der Punkt ist irgendwie diese maximale Selbstbezüglichkeit, die macht es halt.

305
00:31:03,360 --> 00:31:11,640
Und falls ihr das kennt, also bei mir war es so, wenn ich mal, ich hatte mal so einen Moment, ich war auf einer LAN-Party und war wirklich komplett im Tunnel.

306
00:31:11,640 --> 00:31:18,240
Also ich hatte das nicht oft in meinem Leben, aber manchmal war ich vollständig im Tunnel, zum Beispiel beim Zocken oder auch mal beim Schlagzeugspielen.

307
00:31:18,240 --> 00:31:25,960
Und wenn man einmal so vollständig in diesem Modus drin ist, dass man seine eigene Aufmerksamkeit overfittet,

308
00:31:25,960 --> 00:31:33,840
wenn man irgendwie so krass sich selber dabei beobachtet, wie heftig man unterwegs ist, dann ist man wie bei Matrix, wie bei Neo und sieht alles in Slow-Motion.

309
00:31:33,840 --> 00:31:35,760
Man ist auf einmal superschnell.

310
00:31:35,760 --> 00:31:40,720
Und ich hatte das, ich war normalerweise so ein Spieler, ich hatte so 250 APM vielleicht, wenn es hochkommt.

311
00:31:40,760 --> 00:31:44,240
Und ich hatte mal auf einer LAN-Party so 320 bis 350.

312
00:31:44,240 --> 00:31:47,400
Und ich kann mich auch im Nachhinein noch erinnern, das ging runter wie Butter.

313
00:31:47,400 --> 00:31:53,840
Meine Finger, das hat sich angefühlt wie das geschmeidigste, konstante Tippen der Welt.

314
00:31:53,840 --> 00:31:58,120
Ich konnte nie wieder diese Performance erlangen wie in diesem einen Game, was ich mal hatte in meinem Leben.

315
00:31:58,120 --> 00:32:02,800
Und ich glaube, das war so ein Zustand, der hat sich schon fast so ein bisschen transcendent angefühlt.

316
00:32:02,800 --> 00:32:06,560
Und ich glaube, das war, wo meine Aufmerksamkeit diesen Zustand erreicht hat,

317
00:32:06,560 --> 00:32:13,320
dass sie sich sozusagen selber vollständig beobachtet und jederzeit eingreifen kann, völlig problemlos.

318
00:32:13,320 --> 00:32:18,080
Ich glaube, so einen Zustand kann man auch durch Meditation erreichen, wenn man irgendwie, wenn man das lang genug macht.

319
00:32:18,080 --> 00:32:23,080
Jedenfalls, das ist irgendwie krass und das merkt ihr, wenn ihr irgendwann mal, wenn ihr irgendwas habt, wo ihr euch krass konzentrieren müsst.

320
00:32:23,080 --> 00:32:27,960
Und das genaue Gegenteil ist sowas wie mit dem Fahrrad irgendwo langfahren, zur Arbeit oder so.

321
00:32:27,960 --> 00:32:32,160
Da denkt man gar nicht mehr drüber nach, muss ich jetzt hier abbiegen, muss ich hier meinen rechten Fuß nach unten drücken,

322
00:32:32,160 --> 00:32:34,880
muss ich jetzt hier die Bremse drücken, muss ich hier irgendwie lenken.

323
00:32:34,880 --> 00:32:36,920
Das ist alles Autopilot, ne?

324
00:32:36,920 --> 00:32:39,840
Da ist genau das Gegenteil, keine Aufmerksamkeit mehr quasi.

325
00:32:39,840 --> 00:32:46,960
Und übrigens das wichtige, meilensteinartige Paper dazu zu den Transformers heißt Attention is all you need.

326
00:32:46,960 --> 00:32:48,040
Das ist schon so ein geiler Titel.

327
00:32:48,040 --> 00:32:52,080
Ich glaube, das war auch von so Boys, die bei Google in der AI-Abteilung sitzen.

328
00:32:52,080 --> 00:32:54,200
Das ist auch cool geschrieben, das kann man sich wirklich mal reinziehen.

329
00:32:54,200 --> 00:32:58,760
Also wenn ihr euch damit auskennt, wenn ihr euch damit auskennt, kennt ihr das Paper eh, was erzähle ich hier.

330
00:32:58,800 --> 00:33:06,680
Ich bin ja sozusagen der Laie, weil ich hab mir nur mal so einen wöchentlichen Crashkurs bei uns in der Informatikfakultät mal reingegönnt dazu.

331
00:33:06,680 --> 00:33:11,720
Ja und da wird dann sozusagen auch klar, dass diese Sache mit dem Attention so geil ist,

332
00:33:11,720 --> 00:33:18,720
weil wenn man jetzt zum Beispiel Textübersetzung macht und man geht Brutforce von damals vor und übersetzt Wort für Wort oder so,

333
00:33:18,720 --> 00:33:21,400
dann geht das immer krachen, was man braucht, das Kontext.

334
00:33:21,400 --> 00:33:25,800
Und Attention ist richtig gut dafür, weil Attention kann sozusagen den Finger auf ein Wort legen und sagen,

335
00:33:25,800 --> 00:33:33,040
also dieses Wort jetzt hier, zum Beispiel der Arzt oder der Künstler, zum Beispiel Cain West.

336
00:33:33,040 --> 00:33:38,560
Später in einem anderen Satz steht dann nur noch R und dieses R muss bezüglich auf Cain West sein.

337
00:33:38,560 --> 00:33:42,360
Dieser Kontext, das kann Attention richtig gut lernen.

338
00:33:42,360 --> 00:33:47,720
Und das konnten sozusagen die Learning-Strukturen vor den Transformern nicht so gut.

339
00:33:47,720 --> 00:33:53,960
Und das ist sozusagen der erste Schritt in Sachen jetzt mal Attention machen und das ist halt krass.

340
00:33:53,960 --> 00:33:57,880
Der nächste Schritt, also das ist jetzt sozusagen State of the Art, das ist das, was alle mittlerweile können

341
00:33:57,880 --> 00:34:00,280
und das ist auch das, was GPT-3 maximal krass macht.

342
00:34:00,280 --> 00:34:08,440
Und man, also Ben Gürtel hat den Tag auch so was gesagt wie, naja, GPT-3 hat halt auch so unendlich viele Knoten,

343
00:34:08,440 --> 00:34:10,800
dass wahrscheinlich auch Overfitting schon dabei ist.

344
00:34:10,800 --> 00:34:16,000
Das ist einfach nur ein völlig übertrieben großes neuronales Netzwerk, was das komplette Internet gelernt hat einmal

345
00:34:16,000 --> 00:34:19,120
und deswegen mit dir reden kann wie ein Mensch, kurz mal.

346
00:34:19,160 --> 00:34:25,000
Das habe ich in meinem Live-Talk dann über Künstliche Intelligenz auch schon mal gezeigt, so ein Beispiel.

347
00:34:25,000 --> 00:34:31,080
Und jetzt ist es halt so, die normalen Transformer-Networks, die haben halt einen begrenzten Arbeitsspeicher,

348
00:34:31,080 --> 00:34:35,120
das heißt, sie kriegen auch nur einen begrenzten Text in sich rein.

349
00:34:35,120 --> 00:34:40,800
Was wir aber machen ist, wir haben sozusagen eine besseren Komprimierungsalgorithmus

350
00:34:40,800 --> 00:34:46,600
und können deswegen viel mehr Daten in Attention, also in Kontext zueinandersetzen.

351
00:34:46,640 --> 00:34:52,080
Und das, was jetzt sozusagen Cain West und er und sozusagen der Kontext für so ein Deep Learning Netzwerk ist,

352
00:34:52,080 --> 00:34:54,280
ist für uns das gesamte Universum.

353
00:34:54,280 --> 00:34:58,520
Das heißt erst, wenn wir auf diesem Level sind, quasi ein Level höher in der Attention,

354
00:34:58,520 --> 00:35:03,000
dann können wir auch so was wie künstliches Bewusstsein konstruieren.

355
00:35:03,000 --> 00:35:06,280
Also das ist zumindest das, was Joshua Bach sagt und was soll ich sagen,

356
00:35:06,280 --> 00:35:11,240
alles, was Joshua Bach sagt, klingt extrem plausibel und hier muss ich auf jeden Fall eingestehen,

357
00:35:11,240 --> 00:35:17,240
dass ich nicht so krass bin wie Stanislav Lemb und ich finde im Prinzip fast alles, was Joshua Bach sagt,

358
00:35:17,240 --> 00:35:20,200
ist des Todes legit des Todes.

359
00:35:20,200 --> 00:35:22,760
Der Typ ist einfach nur zu geil.

360
00:35:22,760 --> 00:35:28,520
Und auf der Ebene der Neuronen übrigens kann man das Bewusstsein auch noch mal genauso erklären,

361
00:35:28,520 --> 00:35:33,880
nämlich mit Emergenz, da kann man einfach sagen, naja, so ein Neuron muss jetzt sozusagen eine Funktion lernen,

362
00:35:33,880 --> 00:35:35,120
wann es feuert.

363
00:35:35,120 --> 00:35:38,840
Ein Neuron hat sozusagen, kann eine nicht-linearer Funktion approximieren.

364
00:35:38,840 --> 00:35:43,000
Es hat ja ein multidimensionales Input, die ganzen Daten, die reinkommen.

365
00:35:43,000 --> 00:35:45,160
Wann feuert es? Das ist der Output.

366
00:35:45,160 --> 00:35:49,760
Und das Neuron muss jetzt sozusagen das Output an das Universum liefern.

367
00:35:49,760 --> 00:35:53,360
Also es hat ja nur diesen Ausgang, es weiß ja nichts von der äußeren Welt.

368
00:35:53,360 --> 00:35:57,280
Irgendwann kommt wieder Input rein und dieser Input füttert das Neuron ja.

369
00:35:57,280 --> 00:36:00,560
Wenn kein Input mehr kommt, dann stirbt das Neuron ja ab.

370
00:36:00,560 --> 00:36:05,320
Das heißt, Neuronen haben ein Interesse daran, diese Funktion sehr gut zu approximieren,

371
00:36:05,320 --> 00:36:08,000
um lange zu leben und fetter zu werden.

372
00:36:08,000 --> 00:36:09,760
Und das ist genau das, was wir haben.

373
00:36:09,760 --> 00:36:12,400
Und in unserem Hirn gibt es sozusagen diese goldene Regel,

374
00:36:12,400 --> 00:36:14,840
Wires Together, Fires Together oder so.

375
00:36:14,840 --> 00:36:19,560
Das ist so eine Grundregel, die wird um Deep Learning sozusagen, diese Intuition wird da auch verwendet,

376
00:36:19,560 --> 00:36:26,360
weil sozusagen die ganze Struktur der neuronalen Netze auf dieser Analogie funktionieren,

377
00:36:26,360 --> 00:36:30,160
von den Axionen und den Neuronen und so.

378
00:36:30,160 --> 00:36:36,720
Dem Scheiß, den wir sozusagen auf der biologischen Ebene schon sehen können,

379
00:36:36,720 --> 00:36:38,920
wie unser Gehirn im Prinzip funktionieren muss.

380
00:36:38,920 --> 00:36:43,600
Wir sehen sozusagen nur diese unendlich komplizierten Netzwerke und checken nicht, wie die gehen, so ungefähr.

381
00:36:48,680 --> 00:36:54,000
So und nur die Neuronen, die das richtig machen, die überleben und die anderen sterben halt aus,

382
00:36:54,000 --> 00:36:58,200
die das nicht machen, also da ist sozusagen wieder evolutionärer Selektionsdruck.

383
00:36:58,200 --> 00:37:01,720
Und Joshua Bach sagt halt jetzt, und das finde ich auch schon wieder so geil,

384
00:37:01,720 --> 00:37:07,600
ein Neuron ist halt einfach nur ein normaler kleiner Reinforcement Learning Agent.

385
00:37:07,600 --> 00:37:16,320
Das ist ein kleines Reinforcement Agent Konzept, der sozusagen sich selbst immer verbessert.

386
00:37:16,320 --> 00:37:20,160
Und ja, es sendet halt einfach diese Signale in das Universum, also nach draußen.

387
00:37:20,160 --> 00:37:22,200
Das ist zum Beispiel das, was wir hier machen.

388
00:37:22,200 --> 00:37:26,280
Das beeinflusst unsere Handlung und den Willen und den freien Willen und diesen ganzen Scheiß,

389
00:37:26,280 --> 00:37:28,360
nur um eine positive Antwort zu bekommen.

390
00:37:28,360 --> 00:37:33,120
Und das Konglomerat von den ganzen Neuronen, die wir halt im Kopf haben, ist halt so angelegt,

391
00:37:33,120 --> 00:37:37,600
von der Architektur her auch, aber auch von räumlich getrennten Kostenfunktionen,

392
00:37:37,600 --> 00:37:44,640
also bestimmte Hirnareale machen bestimmte Sachen, dass es robust wächst in der meisten,

393
00:37:44,640 --> 00:37:46,640
bei fast allen Menschen, die geboren werden.

394
00:37:46,640 --> 00:37:49,360
Und die Gesamtheit davon nennen wir halt Gehirn.

395
00:37:49,360 --> 00:37:50,760
Das ist das.

396
00:37:50,760 --> 00:37:52,600
Und das ist krass.

397
00:37:52,600 --> 00:37:56,200
Das ist wirklich erstaunlich, dass die Evolution das herausgebracht hat.

398
00:37:56,240 --> 00:38:00,040
Aber der Gehirn, habe ich ja auch glaube ich schon gesagt in dem anderen Video,

399
00:38:00,040 --> 00:38:02,960
was für krasse Sprünge da dazu gehören.

400
00:38:02,960 --> 00:38:08,320
Weil es ein super schlechter Trade-off war, für so eine Spezies wie den Menschenaffen

401
00:38:08,320 --> 00:38:13,560
mehr in seinen Gehirn zu investieren und dafür mehr Ressourcen irgendwo anders einzubüßen.

402
00:38:13,560 --> 00:38:16,640
Zum Beispiel war er nicht mehr so stark.

403
00:38:16,640 --> 00:38:21,560
Das musste sozusagen in einer Zeit der Erbgeschichte auch entstehen, wo das gerade so ging,

404
00:38:21,560 --> 00:38:25,360
damit wir über diesen Potenzialwald rüberkommen

405
00:38:25,360 --> 00:38:29,080
und dann uns als Menschen überhaupt als Homo sapiens etablieren konnten.

406
00:38:29,080 --> 00:38:32,640
Das war ein super Heft, da sind mehrere Sachen zusammengefallen,

407
00:38:32,640 --> 00:38:37,240
dass das durch Selektionsdruck dann passieren konnte und der Mensch daraus entstehen konnte.

408
00:38:37,240 --> 00:38:42,240
Weil sowas wie Augen zum Beispiel, das wurde glaube ich drei, viermal separat voneinander

409
00:38:42,240 --> 00:38:45,840
in der Evolution festgestellt, dass sich Augen gebildet haben.

410
00:38:45,840 --> 00:38:47,000
Zufällig.

411
00:38:47,000 --> 00:38:48,800
Durch Evolutionsdruck.

412
00:38:48,800 --> 00:38:53,200
Das ist zum Beispiel was, was man öfter beobachtet, aber so ein Shit wie krasses Gehirn

413
00:38:53,200 --> 00:38:59,440
und dafür natürlich weniger Robustheit, körperliche, physische Robustheit, das ist selten.

414
00:38:59,440 --> 00:39:02,680
Und ja, was soll ich dazu sagen?

415
00:39:02,680 --> 00:39:07,400
Die emergente Struktur von dem, was ich sozusagen gerade als Gehirn beschrieben habe,

416
00:39:07,400 --> 00:39:08,960
das ist das Bewusstsein.

417
00:39:08,960 --> 00:39:10,400
So kann man es auch artikulieren.

418
00:39:10,400 --> 00:39:16,320
Und ich wollte jetzt wirklich nur mal dieses Video machen, weil ich finde das so krass.

419
00:39:16,320 --> 00:39:20,200
Immer wenn ich mir wieder was von Erscha Bach einziehe, denke ich mir so, ja,

420
00:39:20,240 --> 00:39:23,480
Dieser Typ, den jeder muss wissen, wer das ist.

421
00:39:23,480 --> 00:39:26,240
Ich finde, das ist der krasseste Dude, den es gibt.

422
00:39:26,240 --> 00:39:29,400
Der basht halt auch die anderen Leute, also er basht die natürlich nicht weg,

423
00:39:29,400 --> 00:39:32,200
macht kein Reaction-Video oder ratet irgendwie auf irgendwen.

424
00:39:32,200 --> 00:39:36,120
Aber er macht sie alle platt, er macht, also meiner Meinung nach ist er völlig überlegen

425
00:39:36,120 --> 00:39:41,640
gegenüber zum Beispiel Roger Penrose oder so anderen Leuten, die sich natürlich auch den ganzen Tag,

426
00:39:41,640 --> 00:39:46,040
auch Douglas Hofstadter, die sich den ganzen Tag mit Bewusstsein und diesen Fragen beschäftigen.

427
00:39:46,040 --> 00:39:53,960
Ich finde, Erscha Bach hat die perfekte, sozusagen die bestmögliche verfügbare Komprimierung

428
00:39:53,960 --> 00:39:56,120
schon da in seiner Sprache.

429
00:39:56,120 --> 00:40:01,560
Seine Sprache ist so kompakt, da steckt so viel drin, weil er sozusagen diese ganze Terminologie

430
00:40:01,560 --> 00:40:06,440
von den Deep Learning Netzwerken so krass gefressen hat und diese ganzen Konzepte auch

431
00:40:06,440 --> 00:40:10,120
problemlos auf alles Mögliche anwenden kann, weil er die Intelligenz da hat,

432
00:40:10,120 --> 00:40:12,920
um sozusagen die Analogien zu bilden, um das zu tun.

433
00:40:13,000 --> 00:40:17,640
Es ist einfach nur ein Genuss, dem Typen zuzuhören, auch wenn er extrem schnell redet,

434
00:40:17,640 --> 00:40:19,160
so wie Sheldon Cooper übrigens auch.

435
00:40:19,160 --> 00:40:23,320
Aber ist kein Problem, man kann es einfach mehrmals hören und super.

436
00:40:23,320 --> 00:40:27,160
Und Lex Friedman macht auch immer einen guten Job, wenn er nochmal nachfragt und so.

437
00:40:27,160 --> 00:40:31,320
Also, ich wollte einfach ein Video darüber machen.

438
00:40:31,320 --> 00:40:33,320
Reingehauen, YouTube.

439
00:43:42,920 --> 00:43:44,920
Das war's für heute.

440
00:43:44,920 --> 00:43:46,920
Vielen Dank für's Zuschauen.

441
00:43:46,920 --> 00:43:48,920
Bis zum nächsten Mal.

442
00:44:12,920 --> 00:44:14,920
Der Kanal ist wie eine schöne Vorlesung aufgebaut.

443
00:44:14,920 --> 00:44:20,920
Wenn ich unten Videos verlinke, dann wäre es angebracht, sich die auch reinzuziehen,

444
00:44:20,920 --> 00:44:22,440
weil das aufeinander aufbaut.

445
00:44:22,440 --> 00:44:24,920
Bestimmte Begriffe werden definiert, viele Beispiele werden genannt.

446
00:44:24,920 --> 00:44:30,200
Und ja, ich denke mir schon was dabei, weil es sozusagen meinen eigenen Erkenntnisprozess

447
00:44:30,200 --> 00:44:32,920
abbildet, die Reihenfolge, in denen ich die Videos hier hochlade.

