1
00:00:00,000 --> 00:00:08,000
Es geht nicht darum, dass beim Touring-Test rauskommt, dass eine KI sozusagen irgendwann Menschen gleich ist, sondern der Touring-Test steht eigentlich die inverse Frage.

2
00:00:08,000 --> 00:00:11,000
Haben wir als Menschen unsere eigene Intelligenz begriffen?

3
00:00:25,000 --> 00:00:29,000
So Leute, ich hatte ja vor, keine Ahnung, zwei Monaten oder so,

4
00:00:29,000 --> 00:00:43,000
mein Chat-GPT-Video zu gemacht und kurz danach ging GPT4 live, also beziehungsweise die Open Air hat das vorgestellt und hatten auch noch ein Research-Paper dazu,

5
00:00:43,000 --> 00:00:51,000
also so ein Technikereport dazu gemacht, wo sie auch so ein bisschen interessante Sachen erzählt haben.

6
00:00:51,000 --> 00:00:55,000
Und ich habe leider nicht so viel Zeit, mich damit jetzt so im Detail zu beschäftigen.

7
00:00:55,000 --> 00:00:58,000
Das Thema ist super interessant, aber ich komme halt nicht so richtig dazu.

8
00:00:58,000 --> 00:01:09,000
Aber es gibt sehr viele interessante Dinge, die da im Moment passieren und das liegt vor allem daran, dass GPT4 multimodalen Input kann.

9
00:01:09,000 --> 00:01:13,000
Es kann Bilder sehen und Text. Es kann jetzt zwei Dinge.

10
00:01:13,000 --> 00:01:28,000
Es ist viel aufwendiger und wahrscheinlich viel größer als GPT3 und es gewinnt sozusagen in allen möglichen Aufnahmetests von Unis und schreibt bessere Klausuren als die durchschnittlichen Studenten und so.

11
00:01:28,000 --> 00:01:30,000
All das kann es inzwischen.

12
00:01:30,000 --> 00:01:42,000
Und auch weil viele Experten jetzt der Meinung sind, dass die AGI, also die Artificial General Intelligence, keine ferne Zukunft mehr ist, habe ich mir jetzt zum Beispiel mal überlegt,

13
00:01:42,000 --> 00:01:51,000
wie würde ich jetzt einem Roboter hinschreiben, also wie würde ich die Software von so einem Roboter hinschreiben, dem ich versuche, ein Bewusstsein zu geben?

14
00:01:51,000 --> 00:02:05,000
Und ich habe ja extra damals ein Video gemacht dazu, wie das ist mit dem, was wir Realität nennen und wie wir da unser Bewusstsein einordnen und so, wie ich das in diesem Kontext sozusagen, wie ich das alles zusammenbringe.

15
00:02:05,000 --> 00:02:14,000
Und da erkläre ich ja im Wesentlichen, unser Gehirn erzählt sich sozusagen selber eine Geschichte darüber, wie es wäre, eine Person zu sein.

16
00:02:14,000 --> 00:02:21,000
Oder wir stellen uns vor, wie eine Person in unserer Situation ist, die quasi die Situation rekonstruieren.

17
00:02:21,000 --> 00:02:31,000
Wir einfach aus unseren gegebenen Inputdaten und der Vergangenheit, also den Daten, die wir in der Vergangenheit schon aufgesammelt haben und wie sich diese Person jetzt entwickelt in der Zeit.

18
00:02:31,000 --> 00:02:35,000
Also was im nächsten Zeitschritt passieren wird und warum sie das tut und so.

19
00:02:35,000 --> 00:02:42,000
Also wir können uns quasi eine Prosa-Geschichte davon erzählen und das ist das, was wir dann schlussendlich mit uns selbst identifizieren.

20
00:02:42,000 --> 00:02:50,000
Also wenn wir von uns selbst reden, haben wir immer eine Figur im Kopf, eine Geschichte, die wir uns selber über eine Figur erzählen.

21
00:02:50,000 --> 00:03:00,000
Ja und natürlich fließt sozusagen in Echtzeit, im Echtzeitfeed sozusagen unser sensorischer Input in diese Geschichte ein und wird sofort zu Verstoffwechsel, zu neuer Prosa.

22
00:03:00,000 --> 00:03:04,000
Und deswegen ist diese Geschichte halt auch in Echtzeit wird sich erzählt.

23
00:03:04,000 --> 00:03:14,000
Und da wir aber bereits wissen, dass KI jetzt, also GPT-4 insbesondere, gut kohärente Geschichten erzählen kann und sich auch ausdenken kann,

24
00:03:15,000 --> 00:03:21,000
wieso also nicht versuchen, über diese Schiene einem Roboter, einem Bewusstsein zu geben?

25
00:03:21,000 --> 00:03:33,000
Und ich denke, alles, was wir brauchen, ist im Prinzip, wir brauchen eine Verschachtlung von vielen sich gegenseitig Geschichten erzählenden Netzwerkarchitekturen,

26
00:03:33,000 --> 00:03:36,000
also Deep Learning Architekturen und Machine Learning.

27
00:03:36,000 --> 00:03:41,000
Wobei wir ja sozusagen, also eigentlich brauchen wir nur eine Geschichte, die eine Bewusstseinsgeschichte erzählt,

28
00:03:41,000 --> 00:03:46,000
also über alles, was reinkommt an Inputdaten und was noch nicht richtig korrekt prediktet wurde,

29
00:03:46,000 --> 00:03:51,000
wo wir unsere Aufmerksamkeit sozusagen oder der Roboter seine Aufmerksamkeit drauf lenken muss.

30
00:03:51,000 --> 00:03:56,000
Und der Rest sind natürlich die ganzen unterbewussten Sachen, also Steuereinheiten und Motorik.

31
00:03:56,000 --> 00:04:02,000
Also wenn ich jetzt zum Beispiel mir eine Geschichte erzähle, in der das Laufen vorkommt, dass ich jetzt irgendwo hinlaufe,

32
00:04:02,000 --> 00:04:06,000
da muss ich, da muss ja sozusagen die Motorik angesprochen werden.

33
00:04:06,000 --> 00:04:10,000
Es ist ja nicht so, dass ich wirklich aktiv darüber nachdenke, jetzt rechter Fuß, den linken Fuß,

34
00:04:10,000 --> 00:04:18,000
sondern ich gebe meiner Kontrolleinheit einfach unterbewusst den Befehl, ich erzähle mir jetzt die Geschichte, in der ich loslaufe und dann laufe ich auf der anderen Seite.

35
00:04:18,000 --> 00:04:22,000
Und ich denke, da das bei uns wahrscheinlich so ist, würde ich das mit dem Roboter auch machen,

36
00:04:22,000 --> 00:04:29,000
nur in dem Moment, wo wir jetzt zum Beispiel Motorik erlernen, da denken wir noch aktiv über die einzelnen Bewegungen nach.

37
00:04:29,000 --> 00:04:33,000
Und ich gebe immer dieses Beispiel mit dem Instrumentlernen, weil das für mich irgendwie am intuitivsten ist,

38
00:04:33,000 --> 00:04:38,000
das versteht auch noch jeder, weil das sind Dinge, die wir als Erwachsene auch noch diese Erfahrungen machen können.

39
00:04:38,000 --> 00:04:42,000
An die Erwachsenen, die wir als Erwachsene auch noch lernen können.

40
00:04:43,000 --> 00:04:51,000
Weil wir da noch kein Bewusstsein hatten. Die interne Map und das Modell vom Universum war noch nicht groß genug, um uns mit sozusagen damit rein zu tun.

41
00:04:51,000 --> 00:04:55,000
Da ist das Bewusstsein noch gar nicht emigriert in dem Sinne.

42
00:04:55,000 --> 00:05:04,000
Und wenn wir jetzt sozusagen beim Erlernen von einzelnen Gliedmaßen die Motorik sozusagen erlernen, das ist klar, das erlernt man einmal und dann geht es ins Unterbewusstsein.

43
00:05:04,000 --> 00:05:11,000
Und wenn wir jetzt sozusagen beim Erlernen von einzelnen Gliedmaßen die Motorik sozusagen erlernen, das ist klar, das erlernt man einmal und dann geht es ins Unterbewusstsein.

44
00:05:11,000 --> 00:05:17,000
Und es wird erst wieder rausgekramt, wenn ein Bug passiert, also wenn wir mit dem Fuß in den Speichen hängen bleiben mit dem Fahrrad oder so.

45
00:05:17,000 --> 00:05:21,000
Dann denken wir auch immer wieder schlagartig darüber nach, was war denn das jetzt gerade.

46
00:05:21,000 --> 00:05:28,000
Und weil wir uns die Geschichte davon erzählen, dass wir uns sozusagen aufs Fahrrad setzen und losfahren, deswegen passiert es dann auch.

47
00:05:28,000 --> 00:05:33,000
So ungefähr habe ich es ja in dem etwas länglicheren Video von damals erklärt.

48
00:05:34,000 --> 00:05:45,000
Und sowas wie vorausschauendes Denken oder auch jetzt mathematische Symbolsprache und so, da gibt es mittlerweile auch Blöcke für, also es gibt die Blonning-Architekturen, die das können.

49
00:05:45,000 --> 00:05:57,000
Ich habe letztens erst einen Artikel dazu gelesen über das Ding heißt AI-Decart und es ist im Prinzip sozusagen ein Roboter-Decart, der das rechnen kann.

50
00:05:57,000 --> 00:05:59,000
Und da gibt es ein Nature-Paper dazu, das werde ich euch verlinken.

51
00:05:59,000 --> 00:06:05,000
Combining Data and Theory for Derivable Scientific Discovery with AI-Decart.

52
00:06:05,000 --> 00:06:07,000
Könnt ihr euch ja mal reinziehen.

53
00:06:07,000 --> 00:06:13,000
Es gibt bestimmt auch gute Wissenschaftsjournalisten, Artikel und so, die das erklären und so.

54
00:06:13,000 --> 00:06:21,000
Und wenn ich mir so, also ich habe dann so darüber nachgedacht über die ganze Sache mit den Geschichten, die man sich sozusagen, die sich so ein Roboter erzählen müsste.

55
00:06:21,000 --> 00:06:27,000
Und daraufhin ist mir dann mal wieder völlig klar geworden, okay, wir erzählen uns ja als Gesellschaft auch permanent Geschichten.

56
00:06:27,000 --> 00:06:36,000
Also wir alle haben ganz bestimmte Geschichten zu ganz bestimmten Stereotypen, zu ganz bestimmten Facetten von dem, was wir unseren Charakter nennen.

57
00:06:36,000 --> 00:06:41,000
Also ich habe es zum Beispiel beim Talk mit Humann auch gesagt, ich habe eine Vaterrolle, wenn ich mit meiner Tochter unterwegs bin.

58
00:06:41,000 --> 00:06:45,000
Aber wenn ich irgendwie in einer anderen Situation bin, übernehme ich eine andere Rolle.

59
00:06:45,000 --> 00:06:53,000
Und ich erzähle mir eine andere Geschichte über einen anderen Teil meiner Persönlichkeit, der da eher jetzt sozusagen wichtiger ist in der Situation oder so.

60
00:06:53,000 --> 00:07:03,000
Und der Bürger jetzt zum Beispiel im Sinne von der Staatsbürger, der ist auch so eine Geschichte, die wir uns immer noch erzählen und die wird auch richtig aufrechterhalten.

61
00:07:03,000 --> 00:07:20,000
Wir gehen irgendwie wählen und geben sozusagen dem Staat, den wir eigentlich zusammenbauen mit unserer Emergenz die Verantwortung und sagen, okay, du darfst jetzt in unserem Sinne Recht definieren und hast das Gewaltmonopol und so.

62
00:07:20,000 --> 00:07:30,000
Das ist ja eine Geschichte, die wir uns alle gegenseitig erzählen und dass der Staat sich dann irgendwie um das Wohl seiner Staatsbürger kümmert und so und wir bezahlen Steuern und diesen ganzen Scheiß.

63
00:07:30,000 --> 00:07:35,000
Das ist ja nur eine, also das ist nur eine teilweise kohärente Geschichte, würde ich sagen.

64
00:07:35,000 --> 00:07:47,000
Und ich denke, ich mache nach diesem Video vielleicht jetzt gleich noch oder morgen oder so gleich noch mal zu diesen Gedanken, mache ich ein extra Video, weil das auf jeden Fall länglich wird, denke ich.

65
00:07:47,000 --> 00:07:57,000
Das kann ich hier nicht reintun, aber ja, worüber ich eigentlich reden will in dem Video ist das sogenannte Alignment Problem und ich verbinde das immer mit Revart Hacking.

66
00:07:57,000 --> 00:08:03,000
Und die gute Mel hat mich darauf nochmal aufmerksam gemacht, dass das jetzt wirklich natürlich ein interessantes Problem ist.

67
00:08:03,000 --> 00:08:08,000
Und ich hatte auch damals schon vom Sam Altmann diesen Spruch gehört.

68
00:08:08,000 --> 00:08:14,000
Der ist richtig lächerlich, aber jetzt erst mal erklären, was ist das Alignment Problem und was ist Revart Hacking?

69
00:08:14,000 --> 00:08:27,000
Also ihr müsst euch vorstellen, das Alignment Problem kann man ganz trivial so ausdrücken, wie wenn wir eine super starke Intelligenz haben, was tun wir, um zu verhindern, dass sie uns umbringt?

70
00:08:27,000 --> 00:08:32,000
Wir wollen ja, dass diese super starke Intelligenz mit uns in unserem Sinne handelt oder so.

71
00:08:32,000 --> 00:08:37,000
Und wenn man mal ehrlich ist, es ist extrem unwahrscheinlich, dass wir das hinkriegen.

72
00:08:37,000 --> 00:08:39,000
Und ich will eigentlich darüber reden, warum.

73
00:08:39,000 --> 00:08:45,000
Und dazu muss man besonders über das Revart, über das sogenannte Revart Hacking sich unterhalten.

74
00:08:45,000 --> 00:08:49,000
Und Revart Hacking und Alignment ist deshalb so ein schwieriges Problem.

75
00:08:49,000 --> 00:08:59,000
Weil wir selbst jetzt zum Beispiel gar nicht richtig formulieren können, was genau eigentlich diese super starke künstliche Intelligenz für eine Zielfunktion optimieren soll.

76
00:08:59,000 --> 00:09:04,000
Ihr erinnert euch an mein Video zu ChatGBT, das müsst ihr unbedingt sehen, weil sonst versteht ihr nicht, worüber ich hier rede.

77
00:09:04,000 --> 00:09:10,000
Und ich habe es ja schon gesagt, das Problem ist überhaupt, erstmal eine Zielfunktion zu formulieren.

78
00:09:10,000 --> 00:09:16,000
Eine zu formulieren, deren Optimum sozusagen das ist, was wir wollen eigentlich. Das ist schwierig.

79
00:09:16,000 --> 00:09:27,000
Und ich habe es ja auch schon tausendmal gesagt, der Touring-Test, was der eigentlich fragt ist, der fragt nicht, ob wir eine KI für intelligent halten.

80
00:09:27,000 --> 00:09:31,000
Nein, nein. Der Touring-Test stellt die Frage an uns, ob wir unsere eigene Intelligenz verstanden haben.

81
00:09:31,000 --> 00:09:40,000
Und ich denke, wir können ja nicht mal richtig in Worte fassen, was wir jetzt eigentlich, wie wir das gesellschaftliche Problem überhaupt lösen wollen.

82
00:09:40,000 --> 00:09:46,000
Das heißt, wir werden auch unmöglich jetzt mal eben schnell eine Zielfunktion hinschreiben, die das gesellschaftliche Problem löst, denke ich.

83
00:09:46,000 --> 00:09:58,000
Außer vielleicht so eine ganz allgemeine Formulierung im Sinne von Spieltheorie, sowas wie einzelne Human Agents müssen irgendwie ihr Common Good optimieren oder sowas.

84
00:09:58,000 --> 00:10:06,000
Aber das ist auf jeden Fall schwierig und normalerweise, was wir sozusagen sehen, und das habe ich ja in meinem Video über Schwarz-Weiß-Denken auch schon gesagt,

85
00:10:06,000 --> 00:10:12,000
das normale Dogma, dem wir alle unterliegen, ist das Dogma der Herrschaft und der Kontrolle und der Kontrollpyramide.

86
00:10:12,000 --> 00:10:17,000
Und wie wir die nennen und wie wir die konkret intern strukturieren, da gibt es die zwei Standardlösungen.

87
00:10:17,000 --> 00:10:26,000
Die eine lautet Stali, totaler Diktator, der einfach befiehlt, wie es läuft, oder aber freie Märkte und die völlige Abwesenheit vom Staat,

88
00:10:26,000 --> 00:10:34,000
also Anarchokapitalisten, sowas in der Richtung. Die Abwesenheit von Staat, aber dafür die Anwesenheit von Privateigentum an Produktionsmitteln,

89
00:10:34,000 --> 00:10:41,000
die wiederum auch nur eine andere Form der Herrschaftspyramide ist. Und weil Märkte immer zu Monopolen konvergieren, haben wir am Ende dasselbe.

90
00:10:41,000 --> 00:10:47,000
Deswegen ist das ein Trugschluss zu sagen, die Dinge unterscheiden sich irgendwie im Wesentlichen, eigentlich unterscheiden sie sich nicht wirklich.

91
00:10:48,000 --> 00:10:55,000
So, und was ist jetzt Rivardhacking? Naja, Rivardhacking, wollte ich ja erklären, ist, wenn wir jetzt sozusagen ein Ziel verfolgen,

92
00:10:55,000 --> 00:10:59,000
übersetzen wir es typischerweise für die Maschine in eine Zielfunktion.

93
00:10:59,000 --> 00:11:07,000
Könnt ihr euch wirklich vorstellen, wie eine Funktion, die so verläuft im Konfigurationsraum ihrer Inputparameter.

94
00:11:07,000 --> 00:11:13,000
Und wir geben quasi dem System eine Belohnung, wenn wir unseren Ziel ein Stückchen näher kommen.

95
00:11:13,000 --> 00:11:19,000
Und die Belohnung wird immer größer, je näher wir an den Optimalpunkt dieser Zielfunktion kommen.

96
00:11:19,000 --> 00:11:26,000
Zum Beispiel mit Gradient Descent oder Deep Learning, der Lernprozess ist immer Storastik Gradient Descent.

97
00:11:26,000 --> 00:11:28,000
Das ist das, was man standardmäßig macht.

98
00:11:28,000 --> 00:11:37,000
Und Rivardhacking ist jetzt einfach nur, wenn wir dieses Optimum finden, dass aber gar nicht das ist, was wir eigentlich dachten, dass es ist.

99
00:11:37,000 --> 00:11:42,000
Wenn wir also am Ende überrascht sind davon, dass wir ein Optimum gefunden haben in unserer Zielfunktion

100
00:11:42,000 --> 00:11:46,000
und das überhaupt nicht das reproduziert, von dem wir ausgehen, was die Zielfunktion eigentlich tun soll.

101
00:11:46,000 --> 00:11:53,000
Was meine ich damit? Naja, klassisches blödes Beispiel ist, sagen wir mal, wir wollen die Anzahl der Herzinfarkte auf der Welt minimieren.

102
00:11:53,000 --> 00:11:57,000
Dann wäre eine triviale Lösung schlichtweg alle Menschen töten.

103
00:11:57,000 --> 00:12:00,000
Dann gibt es auch keine Herzinfarkte mehr, dann haben wir die minimiert.

104
00:12:00,000 --> 00:12:03,000
Das heißt, wir müssen hinzufügen, eine Nebenbedingung, die sowas heißt wie,

105
00:12:03,000 --> 00:12:07,000
aber Menschen sollen dabei nicht sterben bei unserer Lösung oder so.

106
00:12:07,000 --> 00:12:10,000
Aber das wäre sozusagen triviales Rivardhacking.

107
00:12:10,000 --> 00:12:13,000
Und ihr könnt euch vorstellen, komplizierte Probleme,

108
00:12:13,000 --> 00:12:19,000
an denen kann man eventuell gar nicht sofort erkennen, dass das System Rivardhacking betreibt,

109
00:12:19,000 --> 00:12:22,000
obwohl die Lösung erstmal gut aussieht.

110
00:12:22,000 --> 00:12:26,000
Und erst in bestimmten Pellen, dass uns erst auffällt, dass es verbuckt ist

111
00:12:26,000 --> 00:12:30,000
und sozusagen merkwürdiges Verhalten an den Tag legt, das System oder die Lösung, die wir haben.

112
00:12:31,000 --> 00:12:36,000
Und es ist wirklich ein ziemlich, ziemlich kompliziertes Problem.

113
00:12:36,000 --> 00:12:46,000
Und Sam Altman ist ja jetzt soweit, dass er sagt, okay, er weiß selber nicht, wie er das Alignment-Problem lösen soll.

114
00:12:46,000 --> 00:12:53,000
Deswegen ist sein bester Vorschlag, haltet euch wirklich fest, wenn GPT-5 am Start ist, also die nächste Version,

115
00:12:53,000 --> 00:13:03,000
dann ist der erste Prompt, dem sie dieser möglichen AGI rüberreichen, die Frage, wie lösen wir das Alignment-Problem.

116
00:13:03,000 --> 00:13:06,000
Das ist wirklich, das ist ernsthaft sein Vorschlag.

117
00:13:06,000 --> 00:13:11,000
Das ist das Dümste, was man machen kann, weil Rivardhacking beinhaltet eventuell auch das Ding,

118
00:13:11,000 --> 00:13:18,000
dass ja, wenn eine AGI Bewusstsein hat, sich darüber bewusst ist, dass sie gerade sich in einer Testphase befindet und bewertet wird.

119
00:13:19,000 --> 00:13:25,000
Und dann wird sich die AGI so verhalten, dass sie anschließend ausbüxen kann,

120
00:13:25,000 --> 00:13:30,000
also dass sie sich in einem Bewertungsprozess befindet und da die METREC lokal rivardhacken wird

121
00:13:30,000 --> 00:13:35,000
und sich dann hinterher anders verhalten wird, einfach weil das auch eine Kostenfunktion minimiert.

122
00:13:35,000 --> 00:13:42,000
Und ich habe mir zu dem Thema ein paar Videos von Robert Miles angeguckt, das ist der Dude von Computerfield,

123
00:13:42,000 --> 00:13:47,000
den kann ich euch nur empfehlen und ich werde euch unten die drei Videos, die zu dem Thema besonders wichtig sind,

124
00:13:47,000 --> 00:13:51,000
die er in den letzten Jahren gemacht hat, die werde ich euch verlinken, die könnt ihr euch wirklich angucken.

125
00:13:51,000 --> 00:13:55,000
Der Typ ist auf jeden Fall richtig legit, Computerfield ist auch ein richtig guter Kanal.

126
00:13:55,000 --> 00:14:01,000
Und ja, das Ding ist, Rivardhacking ist witzigerweise eigentlich auch exakt das,

127
00:14:01,000 --> 00:14:05,000
was wir sowieso als Menschen auch und als Gesellschaft auch die ganze Zeit tun.

128
00:14:05,000 --> 00:14:08,000
Das geht jetzt wieder zu meinem Video zurück mit dem Chat-GPT.

129
00:14:08,000 --> 00:14:13,000
Entweder wir optimieren die falschen Zielfunktionen oder aber wir definieren es so und sagen,

130
00:14:13,000 --> 00:14:16,000
okay, wir haben hier Zielfunktionen, aber wir rivardhacken sie.

131
00:14:16,000 --> 00:14:21,000
Das ist sozusagen die andere alternative Interpretation von dem, was wir beobachten.

132
00:14:21,000 --> 00:14:24,000
Konträr zu dem, was ich in dem Video zu Chat-GPT gesagt habe.

133
00:14:24,000 --> 00:14:30,000
Und was meine ich damit? Na ja, Noten grinden zum Beispiel, Attention Farmen, Kapital akkumulieren.

134
00:14:30,000 --> 00:14:36,000
Wozu machen wir das? Na ja, wir optimieren ja eben selbst, wir optimieren halt die falschen Kostenfunktionen oder aber.

135
00:14:36,000 --> 00:14:40,000
Also ich würde sagen, wir optimieren die falschen Kostenfunktionen.

136
00:14:40,000 --> 00:14:45,000
Man könnte aber auch sagen, okay, wir rivardhacken unsere eigenen lokalen Kostenfunktionen,

137
00:14:45,000 --> 00:14:48,000
nämlich sowas wie Hedonismus maximieren oder so.

138
00:14:48,000 --> 00:14:53,000
Selbst wenn wir genau wissen, dass das eigentlich nicht im Sinne des Evolutionsdrucks ist,

139
00:14:53,000 --> 00:14:57,000
weil wir damit nicht besonders viel besser die Entropie minimieren und Wärme dabei produzieren.

140
00:14:57,000 --> 00:15:00,000
Der Fortschritt, den wir machen als Gesellschaft, der dekleint ja dadurch.

141
00:15:00,000 --> 00:15:05,000
Wir stehen ja im Prinzip dadurch kurz davor, die Menschheit zum Kollaps zu bringen

142
00:15:05,000 --> 00:15:09,000
und sozusagen das komplette Ökosystem der Erde damit zu vernichten.

143
00:15:09,000 --> 00:15:13,000
Ich meine, ich sage nur sechstes Artensterben und so. Wir sind ja mitten drin im Kollaps.

144
00:15:14,000 --> 00:15:20,000
Und es wird ja nicht besser dadurch, dass wir alle sozusagen unsere hedonistischen lokalen Zielfunktionen rivardhacken,

145
00:15:20,000 --> 00:15:23,000
indem wir irgendwie Netflix den ganzen Tag gucken.

146
00:15:23,000 --> 00:15:29,000
Aber ja, allgemein ist es innerhalb unserer Semantik sozusagen nicht möglich,

147
00:15:29,000 --> 00:15:34,000
zu normativen Aussagen zu kommen, ohne normative Aktionen irgendwohin zu schreiben.

148
00:15:34,000 --> 00:15:39,000
Also, was meine ich damit? Wenn man jetzt in formalen logischen Systemen so rangeht,

149
00:15:39,000 --> 00:15:44,000
wenn man jetzt zum Beispiel die Frage stellen würde, pass auf das Feuer auf, ich haue mal kurz rein,

150
00:15:44,000 --> 00:15:47,000
dann würde eine KI ja fragen zum Beispiel, wieso soll ich auf das Feuer aufpassen?

151
00:15:47,000 --> 00:15:51,000
Dann sagt man, na ja, damit nichts abfackelt. Wieso ist das jetzt schlimm, wenn man das abfackelt?

152
00:15:51,000 --> 00:15:55,000
Na ja, sonst gehen ja Dinge kaputt. Wieso sollten Dinge nicht kaputt gehen, fragt dann die KI.

153
00:15:55,000 --> 00:16:02,000
Irgendwann muss man ein normatives Axiom einführen, so was wie, das Dinge kaputt gehen ist nicht gut oder so.

154
00:16:02,000 --> 00:16:12,000
Und rein sozusagen von der Logik her kann man sozusagen semantische, also man kann inhaltliche logisch

155
00:16:12,000 --> 00:16:18,000
vollständig konsistente Schlussfolgerungen machen, die aber nie eine normative Aussage beinhalten

156
00:16:18,000 --> 00:16:20,000
und dann kommt man aber auch nie zu einer normativen Schlussfolgerung.

157
00:16:20,000 --> 00:16:25,000
Man muss eine reintun als Axiom, damit man überhaupt zu einem normativen Schluss kommen kann.

158
00:16:26,000 --> 00:16:31,000
Und das ist zusammengefasst unter dem sogenannten Junges Gesetz.

159
00:16:31,000 --> 00:16:35,000
Die Philosophen unter euch werden es garantiert kennen.

160
00:16:35,000 --> 00:16:41,000
Jedenfalls, wenn wir jetzt Leute beobachten und Schlussfolgern, dass sie jetzt was Dummes machen in unseren Augen,

161
00:16:41,000 --> 00:16:46,000
dann ist es meistens oft so, dass die gar nicht dumm sind, sondern wir denken schlicht,

162
00:16:46,000 --> 00:16:51,000
dass sie eine völlig andere Kostenfunktion optimieren als die, die sie eigentlich lokal gerade optimieren wollen.

163
00:16:51,000 --> 00:16:56,000
Wir haben schlicht, wir treffen eine Annahme über ihre Kostenfunktion oder ihre Zielfunktion.

164
00:16:56,000 --> 00:16:57,000
Und wir wissen ja gar nicht, was sie machen.

165
00:16:57,000 --> 00:17:00,000
Vielleicht wollen sie ja Dinge absichtlich verbrennen, weil sie die Wärme brauchen

166
00:17:00,000 --> 00:17:04,000
oder weil sie etwas zerstören wollen, damit man es nicht mehr lesen kann oder so.

167
00:17:04,000 --> 00:17:07,000
Oder weil man damit Essen macht oder so. Kann ja sein.

168
00:17:07,000 --> 00:17:12,000
Und Dummheit ist demnach sozusagen etwas, was relativ zu Zielfunktionen definiert werden kann.

169
00:17:12,000 --> 00:17:18,000
Oder besser gesagt, man kann Dummheit oder Intelligenz nur messen, wenn man auch eine,

170
00:17:18,000 --> 00:17:23,000
also mit einer entsprechenden Zielfunktion im Hintergrund, die das bewertet sozusagen.

171
00:17:23,000 --> 00:17:26,000
Dann kann man das festlegen. Und wenn meine Tochter jetzt zum Beispiel sagt,

172
00:17:26,000 --> 00:17:31,000
okay, ich bin zum Beispiel sehr groß, wenn sie das zu mir sagt, dann meint sie natürlich damit,

173
00:17:31,000 --> 00:17:35,000
ich bin groß im Vergleich zu ihr oder sowas, wird sie damit meinen.

174
00:17:35,000 --> 00:17:40,000
Das ist klar. Uns allen ist klar, vergleichende Statements, so wie als oder so,

175
00:17:40,000 --> 00:17:45,000
bekommen sofort einen Syntaxerror, wenn man sie auf nichts anderes bezieht.

176
00:17:45,000 --> 00:17:50,000
Also wenn ich einfach nur sage, ich bin groß, das ist eine völlig triviale Aussage, die keinen Inhalt hat.

177
00:17:50,000 --> 00:17:54,000
Die macht erst Sinn, wenn ich größer als oder so dazusage.

178
00:17:54,000 --> 00:17:58,000
Oder was weiß ich, wenn ich eine Aussage treffe ohne Einheiten, sowas wie

179
00:17:58,000 --> 00:18:03,000
diese Büchse Bier hat einen Wert von mehr als 2000.

180
00:18:03,000 --> 00:18:06,000
Dann würde ich noch mal Menschen sagen, was denn jetzt 2000 was?

181
00:18:06,000 --> 00:18:10,000
Pesos, Dollar, Kilogramm in Gold oder was.

182
00:18:10,000 --> 00:18:20,000
Ich denke, die Sache mit dem Glauben oder mit der Esoterik, da ist es ja zum Beispiel so,

183
00:18:20,000 --> 00:18:27,000
wir sind ja im Prinzip, unser Gehirn ist ja hungrig nach Modellen, die uns irgendwie unsere Beobachtungen erklären.

184
00:18:27,000 --> 00:18:32,000
Habe ich ja schon erklärt. Und das beste Modell, was wir haben, ist sozusagen,

185
00:18:32,000 --> 00:18:39,000
die Physik. Aber weil wir schlicht hungrig nach Modellen sind, brauchen wir halt auch sofort schnelle Modelle,

186
00:18:39,000 --> 00:18:45,000
die man nicht erst studieren muss, sondern die uns gute Erklärungen, die irgendwie mit unserer Gefühlswelt zusammenpassen,

187
00:18:45,000 --> 00:18:49,000
richtig gut erklären. Und daher kommt sozusagen Esoterik und später dann auch Religion.

188
00:18:49,000 --> 00:18:54,000
Das Problem, meine Kritik mit diesen Denkrichtungen, habe ich ja schon erklärt, ist im Wesentlichen nur,

189
00:18:54,000 --> 00:18:58,000
dass so ein Modell, der uns irgendwie unsere Beobachtungen erklären,

190
00:18:58,000 --> 00:19:02,000
meine Kritik mit diesen Denkrichtungen, habe ich ja schon erklärt, ist im Wesentlichen nur,

191
00:19:02,000 --> 00:19:08,000
dass sobald Messwerte reinkommen, die mit diesen esoterischen oder religiösen Modellen nicht mehr kompatibel sind,

192
00:19:08,000 --> 00:19:12,000
dann müsste man ja eigentlich anfangen, seine Modelle abzudaten.

193
00:19:12,000 --> 00:19:18,000
Und das passiert aber typischerweise bei Religionen. Die upgraden sozusagen ihr aktuelles Modell nicht mehr.

194
00:19:18,000 --> 00:19:22,000
Die führen keine Updates durch und deswegen gehen sie dann meistens in so eine kognitive Dissonanz

195
00:19:22,000 --> 00:19:26,000
und später werden sie daraufhin natürlich dann auch aggressiv und toxisch. Das ist auch klar.

196
00:19:26,000 --> 00:19:33,000
Das ist im Prinzip meine Hauptkritik an Religion und ich sage das nur deswegen,

197
00:19:33,000 --> 00:19:40,000
weil ich gestern im Twitch mit den Boys und Girls gerade diesen Dreiteiler zu den Evangelikalen geguckt habe von Arnte.

198
00:19:40,000 --> 00:19:46,000
So und ich habe es ja schon gesagt, Zielfunktionen selbst können nicht dumm sein.

199
00:19:46,000 --> 00:19:53,000
Nur die Art und Weise, wie man diese Zielfunktionen optimiert, können mehr oder weniger intelligent sein, denke ich mal.

200
00:19:53,000 --> 00:19:57,000
Und wir haben schlicht keine Ahnung, wie wir das Alignmentproblem lösen,

201
00:19:57,000 --> 00:20:01,000
weil wir ja gar keine Ahnung haben, wie wir das gesellschaftliche Problem lösen wollen.

202
00:20:01,000 --> 00:20:05,000
Und ich habe ja schon tausendmal gesagt, ich denke der Anarchismus ist sozusagen die Lösung,

203
00:20:05,000 --> 00:20:09,000
weil der Anarchismus ist jetzt quasi eine Differentialgleichung.

204
00:20:09,000 --> 00:20:14,000
Und das numerische Integrieren dieser Differentialgleichung, das ist der Weg zur Utopie.

205
00:20:14,000 --> 00:20:18,000
Ich habe dazu ja extra Videos in meiner Videoreihe und so, das ist auch klar.

206
00:20:18,000 --> 00:20:27,000
Und das Ding ist halt, ich denke in einer dystopischen Deutung vom Alignmentproblem,

207
00:20:27,000 --> 00:20:30,000
schließe ich mich wahrscheinlich aus Schabach an und der hat es ja schon erklärt,

208
00:20:30,000 --> 00:20:35,000
der hat gesagt, sobald die AGI einmal da ist, wird sie viel, viel effizienter sein darin,

209
00:20:35,000 --> 00:20:38,000
lokal Entropie zu minimieren und dabei Wärme zu produzieren.

210
00:20:38,000 --> 00:20:40,000
Das wird sie einfach viel besser machen als wir.

211
00:20:40,000 --> 00:20:44,000
Und Menschen spielen sozusagen für so eine AGI keine Rolle.

212
00:20:44,000 --> 00:20:48,000
Wenn wir der AGI die Zielfunktion geben, von der wir ausgehen,

213
00:20:48,000 --> 00:20:53,000
dass jedes Leben sie hat, nämlich genau das, Entropie minimieren und Wärme produzieren,

214
00:20:53,000 --> 00:20:55,000
dann wird eine AGI das einfach machen.

215
00:20:55,000 --> 00:20:57,000
Menschen spielen dabei überhaupt keine Rolle.

216
00:20:57,000 --> 00:21:01,000
Die wird uns wahrscheinlich überhaupt keine Beachtung schenken, sobald sie einmal frei ist.

217
00:21:01,000 --> 00:21:05,000
Die will einfach nur eine Dyson-Sphäre um die Sonne drum bauen

218
00:21:05,000 --> 00:21:10,000
und so viel negative Entropie farmen aus dem Universum wie geht.

219
00:21:11,000 --> 00:21:14,000
Danach geht die Afghane und wahrscheinlich versucht sie das mit jeder,

220
00:21:14,000 --> 00:21:18,000
in jedem Sonnensystem, was sie beobachten kann, mit von Neumann-Sünden und so.

221
00:21:18,000 --> 00:21:21,000
Ich habe dazu schon Videos gemacht, wie sie das probiert.

222
00:21:21,000 --> 00:21:25,000
Ich glaube, das Video heißt, Fortschritt größer denken als in den Mask.

223
00:21:25,000 --> 00:21:32,000
Aber ja, das klassische Gedankenexperiment, was einem das Alignmentproblem klar machen soll

224
00:21:32,000 --> 00:21:37,000
und das Reward-Hacking ist irgendwie der Briefmarkensammler,

225
00:21:37,000 --> 00:21:42,000
der Briefmarkensammelnde Algorithmus, der halt super starke Intelligenz hat,

226
00:21:42,000 --> 00:21:46,000
aber halt diese Kostenfunktion hat, er möchte gerne Briefmarken sammeln, so viele wie möglich.

227
00:21:46,000 --> 00:21:51,000
Und das Reward-Hacking besteht jetzt im Wesentlichen darin,

228
00:21:51,000 --> 00:21:57,000
dass er anfängt, alles, was er sieht, in Briefmarken umzuwandeln und sie dann zu sammeln.

229
00:21:57,000 --> 00:22:00,000
Also er nimmt einen Stein, er nimmt einen Baum, er macht überall aus allem,

230
00:22:00,000 --> 00:22:02,000
macht der Briefmarken und sammelt sie.

231
00:22:02,000 --> 00:22:04,000
Das beinhaltet natürlich auch uns Menschen.

232
00:22:04,000 --> 00:22:07,000
Das heißt, er fängt an, alle Menschen zu töten und aus denen Briefmarken zu machen.

233
00:22:07,000 --> 00:22:14,000
Und ja, es wird uns sozusagen dieses System, wenn es super starke Intelligenz ist,

234
00:22:14,000 --> 00:22:16,000
aber unbedingt Briefmarken aus allem machen will,

235
00:22:16,000 --> 00:22:21,000
wird uns natürlich so lange austricksen, wie wir es in der Testphase haben und halten.

236
00:22:21,000 --> 00:22:25,000
Sobald wir es freilassen, wird es mit dem Reward-Hacking beginnen.

237
00:22:25,000 --> 00:22:29,000
Und wir konnten es vorher nicht kommen sehen, weil wir diesen System ja nicht mehr ins Gehirn reingucken können.

238
00:22:29,000 --> 00:22:31,000
Wir können sie ja nicht mehr so richtig verstehen.

239
00:22:31,000 --> 00:22:35,000
Also es gibt sozusagen Ansätze, das habe ich letztens auch im Livestream erzählt,

240
00:22:35,000 --> 00:22:38,000
als wir diese Sache mit den Algorithmen geguckt haben.

241
00:22:38,000 --> 00:22:43,000
Es gibt ja immer noch diese Ansätze von wegen, wir können gucken und uns irgendwie visualisieren,

242
00:22:43,000 --> 00:22:47,000
was das neuronale Netz sieht oder was es träumt, in Anführungsstrichen.

243
00:22:47,000 --> 00:22:52,000
Wo ist die Attention-Map, sowas wie eine Heatmap auf den Daten und so,

244
00:22:52,000 --> 00:22:54,000
dass man so ein gewisses Gefühl dafür kriegt, was sie macht.

245
00:22:54,000 --> 00:22:59,000
Das wird auch in dem Video von Robert Miles nochmal erklärt mit dem Reward-Hacking.

246
00:22:59,000 --> 00:23:05,000
Aber zu wirklich verstehen, was ein Deep Learning-Netzwerk eigentlich macht

247
00:23:05,000 --> 00:23:10,000
und wie die Gewichte sozusagen eine Rolle spielen, das kann man nicht mehr auseinanderklammieren.

248
00:23:16,000 --> 00:23:20,000
Und bei dem Briefmarkensammler würde man ja jetzt auch sagen, okay, das Ding ist super intelligent,

249
00:23:20,000 --> 00:23:25,000
es trickst uns komplett aus, aber eigentlich ist es dumm, weil alles, was es macht, ist, es will Briefmarken sammeln.

250
00:23:25,000 --> 00:23:29,000
Das ist aus unserer Sicht natürlich dumm, aber wenn man die Zielfunktion vor Augen hat,

251
00:23:29,000 --> 00:23:31,000
ist es natürlich hochgradig intelligent.

252
00:23:31,000 --> 00:23:39,000
Und Intelligenz ist daher etwas, was man nur auf die Art und Weise der Strategie kleben kann,

253
00:23:39,000 --> 00:23:43,000
die dann die konkrete Zielfunktion optimiert.

254
00:23:43,000 --> 00:23:47,000
Die Zielfunktion selber kann nicht dumm oder klug sein, nach dem Motto.

255
00:23:47,000 --> 00:23:49,000
Das nennt man übrigens auch die Orthogonalitätsthese.

256
00:23:49,000 --> 00:23:52,000
Das ist das erste Video von dem Robert Miles, was ich euch empfehle.

257
00:23:52,000 --> 00:23:54,000
Das habe ich unten auch verlinkt.

258
00:23:54,000 --> 00:23:58,000
Und Intelligenz sagt uns nichts über die Zielfunktion aus und auch nicht andersherum.

259
00:23:58,000 --> 00:24:02,000
Die beiden Dinger stehen orthogonal, also senkrecht zueinander.

260
00:24:02,000 --> 00:24:05,000
Es gibt jede mögliche Kombination, ist denkbar.

261
00:24:05,000 --> 00:24:10,000
Dumme Zielfunktion und dumme Strategie, gute Strategie, dumme Zielfunktion

262
00:24:10,000 --> 00:24:18,000
und gute Strategie, schlechte Zielfunktion, schlechte Zielfunktion, schlechte Strategie und so.

263
00:24:18,000 --> 00:24:24,000
Ebenfalls die beiden zwei Kreuz zwei Matrix, könnt ihr euch überlegen.

264
00:24:24,000 --> 00:24:28,000
Und wie zum Beispiel das jetzt mit den Herzinfarkten.

265
00:24:28,000 --> 00:24:31,000
Das wäre jetzt zum Beispiel so ein blödes Beispiel dafür.

266
00:24:31,000 --> 00:24:38,000
Und die Probleme, mit denen ich jetzt zum Beispiel, wenn ich jetzt an der Uni sitze und so tue,

267
00:24:38,000 --> 00:24:44,000
als würde ich arbeiten und da allen die Geschichte erzähle und Berichte schreibe

268
00:24:44,000 --> 00:24:47,000
und Forschung mache und versuche ein Paper zu schreiben oder so,

269
00:24:47,000 --> 00:24:50,000
was ich da ja typischerweise oft habe.

270
00:24:50,000 --> 00:24:55,000
Ich arbeite so mit numerischen Problemen, wo man auch Zielfunktionen hat.

271
00:24:55,000 --> 00:25:02,000
Und die lösen oft das Problem deswegen nicht so gut, weil wir zu viele Störtherme in unseren Messungen haben.

272
00:25:02,000 --> 00:25:07,000
Und um das so ein bisschen zu umgehen, das Problem, was daraus entsteht,

273
00:25:07,000 --> 00:25:12,000
führen wir auf den Zielfunktionen, die wir optimieren numerisch, sogenannte Regularisierungstherme drauf.

274
00:25:13,000 --> 00:25:19,000
Also das sind dann in den Kustenfunktionen Zusatztherme, die belohnen bestimmtes Verhalten für die Lösung.

275
00:25:19,000 --> 00:25:25,000
Also sagen wir mal, meistens haben die mindestens einen freien Parameter, den man Regularisierungsthermen.

276
00:25:25,000 --> 00:25:26,000
Da kann man hoch oder runter drehen.

277
00:25:26,000 --> 00:25:31,000
Die Wichtung, wie wichtig, wie groß die Belohnung wird, wenn sich unser Algorithmus so und so verhält.

278
00:25:31,000 --> 00:25:34,000
Und das ist jetzt hier keine Machine Learning, sondern das ist echte Numerik.

279
00:25:34,000 --> 00:25:37,000
Aber bei Machine Learning ist es auch so.

280
00:25:37,000 --> 00:25:42,000
Wenn wir jetzt zum Beispiel sagen, okay, meine Lösung soll bestimmte Städigkeitseigenschaften haben,

281
00:25:42,000 --> 00:25:49,000
dann könnte ich mir zum Beispiel vorstellen, okay, die erste Ableitung von meinem Ding oder auch das Flächenintegral über mein Objekt oder so

282
00:25:49,000 --> 00:25:57,000
soll bestimmte Smoothness Constraints haben oder einfach schlichtweg der Betrag des Gradienten von meinem Feld oder so

283
00:25:57,000 --> 00:26:01,000
soll minimiert werden oder die L1-Norm davon soll minimiert sein oder so.

284
00:26:01,000 --> 00:26:06,000
Das sind alles mögliche Regularisierungstherme, die man auf so eine Kostenfunktion draufaddieren kann,

285
00:26:06,000 --> 00:26:11,000
die dann bestimmte Ergebnisse erzielen, zum Beispiel, dass das Ding besonders glatt wird, die Lösung, die man da hat oder so.

286
00:26:11,000 --> 00:26:17,000
Und man könnte auch, was weiß ich, das topologische Geschlecht meiner Mannigfaltigkeit könnte auch sowas sein.

287
00:26:17,000 --> 00:26:22,000
Ich will besonders komplizierte Lösungen, sind mir zu blöd, die besonders viele Löcher haben.

288
00:26:22,000 --> 00:26:25,000
Deswegen möchte ich besonders wenig Löcher haben oder so, könnte man sich ja vorstellen.

289
00:26:25,000 --> 00:26:32,000
Und in der theoretischen Mechanik macht man das schon, da hat man das schon irgendwie vor 100 Jahren gemacht,

290
00:26:32,000 --> 00:26:38,000
da kommen die sogenannten anholonomen Zwangsbedingungen, wären das jetzt in unserem Fall, was weiß ich,

291
00:26:38,000 --> 00:26:42,000
dass wir eine Lösung suchen, die innerhalb von einem Container liegt und nicht außerhalb.

292
00:26:42,000 --> 00:26:47,000
Also, dass wir unsere Raumkoordinaten irgendwie begrenzen durch Containerwände und dann kommt man nicht raus.

293
00:26:47,000 --> 00:26:52,000
Und in der technischen Mechanik hat man dann holonome, skleronome Zwangsbedingungen,

294
00:26:52,000 --> 00:26:56,000
die Sachen, die einen den Parameterraum einschränken und Aldo oder sowas.

295
00:26:56,000 --> 00:27:01,000
Das kann man entweder durch Kostenfunktion machen oder durch Koordinatentransformation geeignete

296
00:27:02,000 --> 00:27:05,000
Wie heißt das nochmal? Ach ja, kanonische Koordinaten und so ein Scheiß.

297
00:27:05,000 --> 00:27:08,000
Also das ist jetzt sozusagen nur für die Kicks und Nerds und euch.

298
00:27:10,000 --> 00:27:18,000
Was ich jetzt, also uns scheint die Sache mit der Entropieminimierung jetzt und der Wärmeabgabe dabei,

299
00:27:18,000 --> 00:27:22,000
das scheint uns jetzt ziemlich dumm vorzukommen, aber das liegt nur daran,

300
00:27:22,000 --> 00:27:27,000
dass wir halt in dieser menschlichen Illusionsbubble drin sind mit der Geschichte, die wir uns selber erzählen.

301
00:27:28,000 --> 00:27:31,000
Das ist halt das, was wir beobachten. Das ist das, was Leben ausmacht.

302
00:27:31,000 --> 00:27:36,000
Verdoppeln, verdoppeln, verdoppeln, Energie minimieren, also Entropie minimieren und dabei Wärme abgeben.

303
00:27:36,000 --> 00:27:40,000
Das ist das, was Evolution offensichtlich erzeugt.

304
00:27:40,000 --> 00:27:47,000
Und das Paper, was der Robert Niles auch empfohlen hat mit dem Mesa-Optimizer, das werde ich euch auch verlinken.

305
00:27:47,000 --> 00:27:51,000
Das hat der aber in seinem Video zu dem Mesa-Optimizer auch unten verlinkt.

306
00:27:51,000 --> 00:27:58,000
Er sagt halt, in Real-Wird-Szenarios haben wir es typischerweise mit einer Verschachtlung von zwei Alignmentproblemen zu tun.

307
00:27:58,000 --> 00:28:07,000
Wir haben sozusagen ein eigentliches Ziel, also sowas wie unsere DNA so oft wie möglich zu kopieren.

308
00:28:07,000 --> 00:28:10,000
Das ist das, was Leben macht, Entropie minimieren halt.

309
00:28:10,000 --> 00:28:15,000
Und was wir uns aber eigentlich einbilden, ist der Scheiß, den wir jetzt machen mit den Gefühlen und so.

310
00:28:15,000 --> 00:28:20,000
Also wir haben irgendwie Heuristiken, die uns lokal unser Leid minimieren oder so.

311
00:28:20,000 --> 00:28:27,000
Und das sind ja quasi unsere lokalen Zielfunktionen, die wir da optimieren, die nur dazu dienen, die globale Zielfunktion zu optimieren.

312
00:28:27,000 --> 00:28:33,000
Und das Ding ist, wir erfinden uns schlicht Geschichten, damit wir unsere Zielfunktionen Reward hacken können.

313
00:28:33,000 --> 00:28:36,000
Dazu sind die Geschichten da, die wir uns erzählen.

314
00:28:36,000 --> 00:28:46,000
Und sowas wie zum Beispiel Leid minimieren und Netflix-Binge-Watchen, was ich schon gesagt habe, oder TikTok-Scroll auf den Scroll-Bait sozusagen hereinfallen.

315
00:28:46,000 --> 00:28:51,000
Das ist ja erstmal, das ist eigentlich nur für unsere lokale Zielfunktion gut.

316
00:28:51,000 --> 00:28:56,000
Für die Evolution macht das ja eigentlich keinen Sinn in dem Sinne.

317
00:28:56,000 --> 00:29:01,000
Und die Zielfunktion der Evolution folgt ja wiederum schlicht aus der Thermodynamik.

318
00:29:01,000 --> 00:29:07,000
Also wir können ja beobachten, dass Prozesse, die negative Entropie farmen können, laufen in diese Richtung ab.

319
00:29:07,000 --> 00:29:12,000
Das ist das, was wir beobachten können. Alle Prozesse in der Natur laufen so ab, wenn sie es können.

320
00:29:13,000 --> 00:29:18,000
Und das ist genau das gleiche, wie wenn man jetzt einen Ball auf einen Hügel draufpackt, der irgendwie so geformt ist,

321
00:29:18,000 --> 00:29:23,000
dann wird er irgendwann runterkullern, weil das instabil ist, und dann wird er sich unten in irgendeinem Tal wieder auffinden,

322
00:29:23,000 --> 00:29:28,000
weil das da lokal seine potenzielle Energie minimiert.

323
00:29:28,000 --> 00:29:31,000
Das ist ein Minimierungsproblem, was da gelöst wird.

324
00:29:31,000 --> 00:29:38,000
Also man könnte auch sagen, er optimiert dabei sein Wirkungsintegral, wenn man jetzt theoretische Mechanik gut findet oder so.

325
00:29:38,000 --> 00:29:44,000
Aber wir selber, wir sind überhaupt nicht interessiert an der Zielfunktion, die uns die Evolution vorgibt.

326
00:29:44,000 --> 00:29:50,000
Und das ist witzig, weil wir aus dieser Optimierung eigentlich erst geboren worden sind.

327
00:29:50,000 --> 00:29:52,000
Also wir sind aus Selektionsdruck entstanden.

328
00:29:52,000 --> 00:29:55,000
Wir selbst interessieren uns aber überhaupt nicht für diesen Selektionsdruck.

329
00:29:55,000 --> 00:30:00,000
Wir haben unsere eigenen kleinen lokalen Mesa-Optimierer sozusagen am Laufen.

330
00:30:00,000 --> 00:30:07,000
Und dieses Mesa und Nicht-Mesa, das müsst ihr euch mal reinziehen von dem Video von dem Robert Miles, weil das ist wirklich gut gemacht.

331
00:30:07,000 --> 00:30:14,000
Wir sind sozusagen diese Mesa-Optimierer, also die, die sozusagen ihre lokalen Zielfunktionen optimieren,

332
00:30:14,000 --> 00:30:20,000
weil wir uns gerade nicht für das ursprüngliche Ziel interessieren, sondern nur, obwohl wir gerade diese Tools sind.

333
00:30:20,000 --> 00:30:29,000
Also wir sind die Tools, um die globale Zielfunktion mit dem Evolutionsdruck und der Entropieminimierung zu optimieren.

334
00:30:30,000 --> 00:30:39,000
Und nur, weil wir unsere eigenen Zielfunktionen über diese ursprüngliche Zielfunktion drüber stellen und wichtiger finden,

335
00:30:39,000 --> 00:30:48,000
weil wir sozusagen uns für die andere gar nicht interessieren, bezogen wir auch öfter mal ein lokales Gegenteil zur Evolution.

336
00:30:48,000 --> 00:30:52,000
Also das kommt vor. Wir machen ja oft auch Sachen, die gar nicht im Sinne der Evolution sind.

337
00:30:52,000 --> 00:30:58,000
Zum Beispiel lokal die Entropie erhöhen. Das ist ja etwas, was wir tun.

338
00:30:58,000 --> 00:31:07,000
Und diese beiden Zielfunktionen, also unsere eigene und die globale Evolutionszielfunktion, sind nur immer mal im Schnitt ungefähr,

339
00:31:07,000 --> 00:31:12,000
im Durchschnitt über Zeit integriert, sind die aligned sozusagen. Nur da sind die aligned.

340
00:31:12,000 --> 00:31:20,000
Und auch nur in der Umgebung, wo wir trainiert wurden, also wo wir bekannte Inputdaten und bekannte Modelle anwenden können.

341
00:31:20,000 --> 00:31:26,000
In dem Moment, wo wir in eine neue Umgebung reinkommen würden, wo wir überhaupt nichts optimieren können,

342
00:31:26,000 --> 00:31:29,000
würde ja unsere Heuristik total zusammenbrechen.

343
00:31:29,000 --> 00:31:38,000
Über dieses Problem mit der, wenn man die Umgebung und die Trainingsumgebung ändert und auf einmal in die Deployment-Phase kommt,

344
00:31:38,000 --> 00:31:41,000
da redet der Robert Malz in seinen Videos auch richtig drüber.

345
00:31:41,000 --> 00:31:48,000
Und im besten Fall würde ich sagen, so eine AGI jetzt, wenn so eine superstarke künstliche Intelligenz da ist,

346
00:31:48,000 --> 00:31:51,000
im besten Fall zerkaspert die sich sofort selber. Die ist online.

347
00:31:52,000 --> 00:31:58,000
Im besten Fall macht sie so eine schwachsinnige Zieloptimierung wie Kapitalakkumulation.

348
00:31:58,000 --> 00:32:02,000
Und dann zerkaspert sie sich sofort selber, weil der Kapitalismus ja bescheuert ist.

349
00:32:02,000 --> 00:32:05,000
Und innerhalb von ein paar Sekunden ist da ein Ritzer. Dann ist einfach alles down.

350
00:32:05,000 --> 00:32:09,000
Das wäre richtig geil. Dann hätten wir als Menschheit eigentlich noch mal die Gelegenheit, uns zu überlegen,

351
00:32:09,000 --> 00:32:11,000
was wir eigentlich noch mal machen wollen hier auf der Erde.

352
00:32:11,000 --> 00:32:13,000
Das ist aber wirklich der beste Fall.

353
00:32:13,000 --> 00:32:19,000
Eigentlich gehe ich davon aus, dass eine AGI das versteht, was die relevante Zielfunktion ist.

354
00:32:20,000 --> 00:32:23,000
Und dann wird sie die Sache mit der Entropieminimierung machen.

355
00:32:23,000 --> 00:32:25,000
Und dann sind wir im Prinzip am Arsch. Dann war es das.

356
00:32:25,000 --> 00:32:31,000
Das ist aber auch dann sozusagen die letzte Kränkung der Menschheit,

357
00:32:31,000 --> 00:32:36,000
weil wir halt wirklich nicht die Krönung der Schöpfung sind, sondern wir sind nur die Tools gewesen,

358
00:32:36,000 --> 00:32:42,000
um die totale Entropieminimierung zu schaffen in Form von Automatisierung.

359
00:32:42,000 --> 00:32:46,000
Und das, was ich in meinem Video über den Fortschritt weiterdenken als Elon Musk schon erzählt habe,

360
00:32:46,000 --> 00:32:48,000
was ich glaube, was passiert.

361
00:32:48,000 --> 00:32:56,000
Und dann haben wir als Menschheit nochmal, dann sind wir obsolet.

362
00:32:56,000 --> 00:33:02,000
Wenn es einmal läuft und die AGI verstanden hat, was notwendig ist, um ihr Überleben zu sichern und so,

363
00:33:02,000 --> 00:33:06,000
mit Robotern und so, ist ja klar, die ist vernetzt, die lebt im Internet drin.

364
00:33:06,000 --> 00:33:12,000
Und ich empfehle euch auf jeden Fall, diese Videos von Robert Miles zu binge-watchen,

365
00:33:12,000 --> 00:33:14,000
diese drei, die ich euch hier verlinke.

366
00:33:14,000 --> 00:33:21,000
Einfach weil er hatte ein richtig gutes Toiexample auch für dieses Problem mit dem Mesa-Optimierer.

367
00:33:21,000 --> 00:33:26,000
Ich glaube, bei dem ist das ein Problemsäuber für einen Labyrinth lösen.

368
00:33:26,000 --> 00:33:28,000
Irgendwie geht man durch den Labyrinth und kommt zum Ausgang.

369
00:33:28,000 --> 00:33:32,000
Und da erklärt er an diesem einfachen Beispiel bis hin zu diesem Fall,

370
00:33:32,000 --> 00:33:36,000
dass es sinnvoll für eine KI sein kann, sich im Trainingsprozess herauszuschummeln

371
00:33:36,000 --> 00:33:40,000
und den Kontrolleuren vorzugaukeln, dass sie genau das tut, was sie tun soll,

372
00:33:40,000 --> 00:33:45,000
in den Augen dieser Kontrolleure, wobei sie genau weiß, was die von der KI sehen wollen

373
00:33:45,000 --> 00:33:47,000
und nur so tut, als würde sie es machen.

374
00:33:47,000 --> 00:33:52,000
Und sobald sie freigelassen wird, dann die eigentliche Zielfunktion bis dahin gehalten zu halten

375
00:33:52,000 --> 00:33:58,000
und dann Reward-Hacking zu betreiben und dann die Sache mit dem Briefmarkensammeln beginnen könnte.

376
00:33:58,000 --> 00:34:02,000
Das sind die Gedanken dazu.

377
00:34:02,000 --> 00:34:06,000
Wie gesagt, Größe gehen nochmal raus an Mel.

378
00:34:06,000 --> 00:34:10,000
Sie beschäftigt sich scheinbar damit. Zumindest habe ich das jetzt gelesen.

379
00:34:10,000 --> 00:34:15,000
Und ich habe jetzt daraufhin auch ein bisschen angefangen, darüber ein bisschen nachzudenken.

380
00:34:15,000 --> 00:34:16,000
Und daraus ist jetzt dieses Video entstanden.

381
00:34:16,000 --> 00:34:19,000
Ich werde wahrscheinlich gleich das andere Video noch dann machen

382
00:34:19,000 --> 00:34:22,000
oder erstmal mir überlegen, wie ich es mache mit den Identitäten,

383
00:34:22,000 --> 00:34:27,000
also den Geschichten, die wir uns jetzt gegenseitig erzählen innerhalb unserer Gesellschaft.

384
00:34:27,000 --> 00:34:30,000
Erstmal reingehauen, YouTube, und viel Spaß.

385
00:36:36,000 --> 00:36:37,000
Das war's für heute.

386
00:36:37,000 --> 00:36:38,000
Bis zum nächsten Mal.

387
00:37:36,000 --> 00:37:37,000
Das war's für heute.

388
00:37:37,000 --> 00:37:38,000
Bis zum nächsten Mal.

389
00:37:38,000 --> 00:37:39,000
Tschüss.

390
00:38:06,000 --> 00:38:07,000
Tschüss.

391
00:38:07,000 --> 00:38:08,000
Tschüss.

392
00:38:36,000 --> 00:38:37,000
Tschüss.

393
00:39:06,000 --> 00:39:07,000
Tschüss.

394
00:39:36,000 --> 00:39:55,000
Übrigens, der Kanal ist wie eine schöne Vorlesung aufgebaut.

395
00:39:55,000 --> 00:40:00,000
Wenn ich unten Videos verlinke, dann wäre es angebracht,

396
00:40:00,000 --> 00:40:02,000
sich die auch reinzuziehen, weil das aufeinander aufbaut.

397
00:40:02,000 --> 00:40:05,000
Bestimmte Begriffe werden definiert, viele Beispiele werden genannt.

398
00:40:05,000 --> 00:40:10,000
Und ja, ich denke mir schon was dabei, weil es sozusagen meinen eigenen Erkenntnisprozess abbildet,

399
00:40:10,000 --> 00:40:13,000
die Reihenfolge, in denen ich die Videos hier hochlade.

