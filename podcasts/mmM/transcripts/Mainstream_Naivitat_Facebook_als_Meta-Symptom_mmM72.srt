1
00:00:00,000 --> 00:00:17,720
Es ist absolut symptomatisch, dass Zuckerberg den Missbrauch beim User verortet, aber nie

2
00:00:17,720 --> 00:00:18,720
wirklich im Unternehmen.

3
00:00:18,720 --> 00:00:23,840
Und selbst wenn das Unternehmen, was ja jetzt häufiger wieder passiert, dann doch verantworten

4
00:00:23,840 --> 00:00:24,840
die Zugehörungen.

5
00:00:24,840 --> 00:00:25,840
Ein Literaturwissenschaftler.

6
00:00:25,840 --> 00:00:29,040
Dass dann eher gesagt wird, ja wir haben es falsch, wir haben die Technologie falsch

7
00:00:29,040 --> 00:00:32,880
bedient oder wir haben uns die falschen Fragen gestellt und so weiter und so weiter.

8
00:00:32,880 --> 00:00:34,840
Das System an sich ist nie daran schuld.

9
00:00:34,840 --> 00:00:38,800
Stanford im Herzen des Silicon Valley.

10
00:00:38,800 --> 00:00:42,600
Hier bekommen Tech-Pioniere ihr intellektuelles Rüstzeug.

11
00:00:42,600 --> 00:00:45,880
Hier lehrt Adrian Daub Literaturwissenschaft.

12
00:00:45,880 --> 00:00:50,760
In seinem Buch Was das Valley Denken nennt, nimmt er DenkerInnen in den Blick, derer

13
00:00:50,760 --> 00:00:52,440
sich die Tech-Branche bedient.

14
00:00:53,400 --> 00:00:59,160
Ich glaube, dass sich in den letzten 50 bis 60 Jahren in Nordkalifornien eine Art Denke

15
00:00:59,160 --> 00:01:09,440
etabliert hat, die ganz stark Profit und technologische Innovationen über Sicherheit und über Werte

16
00:01:09,440 --> 00:01:10,440
gestellt hat.

17
00:01:10,440 --> 00:01:17,920
Die sozusagen die Kommunikation und den Individualismus als Wert an sich entdeckt hat.

18
00:01:17,920 --> 00:01:21,200
Das Medium ist die Botschaft.

19
00:01:21,640 --> 00:01:27,080
Der kanadische Medientheoretiker Marshall McLuhan wurde durch diesen Satz in den 1960er

20
00:01:27,080 --> 00:01:28,080
Jahren berühmt.

21
00:01:28,080 --> 00:01:32,040
Für ihn formen Medien und nicht Inhalte unsere Wahrnehmung.

22
00:01:32,040 --> 00:01:39,160
So greift das Medium Facebook heute tief in die Psyche seiner User ein, indem es bestimmt,

23
00:01:39,160 --> 00:01:40,480
wie wir Informationen teilen.

24
00:01:40,480 --> 00:01:45,040
Die Inhalte werden überwunden.

25
00:01:45,040 --> 00:01:46,520
Inhalte Nebensache.

26
00:01:46,520 --> 00:01:47,520
Kannte.

27
00:01:47,520 --> 00:01:52,480
Ein solches Denken ermögliche Mark Zuckerberg einen skrupellosen Expansionskurs.

28
00:01:52,480 --> 00:01:58,360
Die Tatsache, dass möglicherweise da ein Kommunikationsbegriff dahinter steckt, der

29
00:01:58,360 --> 00:02:06,320
nicht ganz durchdacht ist und dass es wirklich schwierig ist zu sagen, wir bringen Menschen

30
00:02:06,320 --> 00:02:10,880
miteinander in Verbindung und wir stellen aber keine ethischen Maßstäbe bereit, wie dort

31
00:02:10,880 --> 00:02:14,640
zu kommunizieren wäre, das ist eine Frage, die man sich bei Facebook sehr wenig zu stellen

32
00:02:14,640 --> 00:02:15,640
scheint.

33
00:02:16,400 --> 00:02:22,160
Welche Folgen ein solches Denken haben kann, zeigen Ken und Frankl in ihrem Buch.

34
00:02:22,160 --> 00:02:28,920
2017 startete das Militär in Myanmar auf Facebook eine Desinformationskampagne.

35
00:02:28,920 --> 00:02:34,040
Facebooks Algorithmus verbreitete Hass gegen die muslimische Rohingya-Minderheit.

36
00:02:34,040 --> 00:02:35,520
Die Gewalt eskalierte.

37
00:02:35,520 --> 00:02:39,560
25.000 Menschen starben, hunderttausende flohen.

38
00:02:39,560 --> 00:02:44,320
Die Zentrale in Kalifornien habe jahrelang Warnungen von Menschenrechtsorganisationen

39
00:02:44,320 --> 00:02:45,600
ignoriert.

40
00:02:45,640 --> 00:02:51,960
Bis 2018 gab es nur fünf Burmesisch sprechende Mitarbeiterinnen für einen Markt von 18 Millionen

41
00:02:51,960 --> 00:02:52,960
Usern.

42
00:02:52,960 --> 00:02:58,080
Das Unternehmen habe ein Streichholz in einen ethnischen Konflikt geworfen und sich dann

43
00:02:58,080 --> 00:03:01,960
abgewandt, so die Autorinnen.

44
00:03:01,960 --> 00:03:06,760
Mark Zuckerberg setzte auch im Wachstumsmarkt Myanmar auf sein zentrales Geschäftsmodell.

45
00:03:10,760 --> 00:03:13,720
Das ist eine der hässlichen Wahrheiten.

46
00:03:13,840 --> 00:03:18,840
Facebook benötigt unbedingt die Beteiligung seiner User, um zu wachsen.

47
00:03:18,840 --> 00:03:21,840
Wachstum ist das Wichtigste für das Unternehmen.

48
00:03:21,840 --> 00:03:27,480
Der Newsfeed-Algorithmus zielt darauf ab, User zu binden, spült Aufsehen Erregendes nach

49
00:03:27,480 --> 00:03:28,480
oben.

50
00:03:28,480 --> 00:03:32,000
Er priorisiert Inhalte, die Emotionen erzeugen.

51
00:03:32,000 --> 00:03:35,680
Egal ob Wut, Traurigkeit, Leid oder Empöhrung.

52
00:03:35,680 --> 00:03:42,080
Oftmals Dinge, die einen in die Irre führen oder desinformieren.

53
00:03:44,080 --> 00:03:46,800
Die algorithmische Verstärkung von Emotionen.

54
00:03:46,800 --> 00:03:49,400
Die zentrale DNA von Facebook.

55
00:03:49,400 --> 00:03:54,240
Für Daub steckt hinter diesem Geschäftsmodell ein zutiefst pessimistisches Menschenbild.

56
00:03:54,240 --> 00:04:03,480
So findet das Denken des französischen Kulturanthropologen und Stanford-Professors René Girard großen

57
00:04:03,480 --> 00:04:08,360
Anklang bei Gründern und Investoren im Silicon Valley.

58
00:04:08,360 --> 00:04:12,680
Laut Girard begehren Menschen immer das, was andere auch wollen.

59
00:04:12,680 --> 00:04:15,520
Mimetisches Begehren, so nennt er das.

60
00:04:15,520 --> 00:04:17,760
Und so funktioniert auch Facebook.

61
00:04:17,760 --> 00:04:23,680
Das Begehren wird durch den ständigen Vergleich mit anderen und die Jagd nach Likes geschürt.

62
00:04:23,680 --> 00:04:27,160
Konflikte und Gewalt sind laut Girard Folge dieser Dynamik.

63
00:04:27,160 --> 00:04:32,160
Und doch wird uns dieses Geschäftsmodell als geniale Idee verkauft, die die Welt näher

64
00:04:32,160 --> 00:04:33,160
zusammenbringt.

65
00:04:33,160 --> 00:04:39,160
Am 6.

66
00:04:39,160 --> 00:04:44,840
Januar entlud sich der Hass, der in Facebook-Gruppen über Monate hinweg den Ton angegeben hatte.

67
00:04:44,840 --> 00:04:52,240
Vor der Attacke auf das Kapitol empfahl Facebooks Algorithmus Trump-Followern hunderttausendfach

68
00:04:52,240 --> 00:04:55,680
Gruppen, in denen sich gewaltbereite Demonstranten organisierten.

69
00:04:55,680 --> 00:05:00,600
Trump selbst befeuerte den Mythos der gestohlenen Wahl.

70
00:05:00,600 --> 00:05:01,600
Klar.

71
00:05:02,600 --> 00:05:09,400
Zu seinen 28 Millionen Followern hatte ihn Zuckerberg noch im Herbst 2019 gratuliert.

72
00:05:09,400 --> 00:05:12,600
Erst nach dem Sturm aufs Kapitol ließ ihn Facebook sperren.

73
00:05:12,600 --> 00:05:15,600
Die Autorinnen kommen zu einem erstaunlichen Ergebnis.

74
00:05:17,600 --> 00:05:19,800
Facebook wusste bereits vor dem 6.

75
00:05:19,800 --> 00:05:21,200
Januar, was sie planten.

76
00:05:21,200 --> 00:05:25,400
Sie waren alarmiert, bemerkten, dass Personen davon sprachen, Waffen mitzubringen.

77
00:05:25,400 --> 00:05:31,400
Fotos posteten und in Gruppen schrieben, ich bringe dieses Sturmgewehr mit nach Washington.

78
00:05:32,400 --> 00:05:35,400
Trump solle sich für seine Worte verantwortlich fühlen.

79
00:05:35,400 --> 00:05:38,400
Die Menschen, die das Gesetz brachen für ihre Taten.

80
00:05:38,400 --> 00:05:41,400
So wie Zuckerberg jegliche Verantwortung von sich.

81
00:05:41,400 --> 00:05:45,400
Einige Leute behaupten, dass soziale Netzwerke polarisieren.

82
00:05:45,400 --> 00:05:49,400
Unseren Forschungen zufolge ist das alles andere als klar.

83
00:05:49,400 --> 00:05:53,400
Immer wieder betont er, dass er an einer Gemeinschaft für alle arbeite.

84
00:05:53,400 --> 00:05:56,400
Genau darin liegt das Dilemma von Facebook.

85
00:05:57,200 --> 00:06:00,200
Die Wirklichkeit eines Algorithmus, der Hass und Gewalt befördert.

86
00:06:00,200 --> 00:06:04,200
Und der Anspruch, die Welt zu einem besseren Ort zu machen.

87
00:06:04,200 --> 00:06:07,200
Das ist glaube ich unglaublich wichtig für diese Unternehmen.

88
00:06:07,200 --> 00:06:10,200
Während sie eben diese enorme Macht anhäufen.

89
00:06:10,200 --> 00:06:14,200
Sozusagen vor dieser Macht auch zu einem gewissen Grad die Augen verschließen zu können.

90
00:06:14,200 --> 00:06:17,200
Und das ist wichtig für die Unternehmenschefs.

91
00:06:17,200 --> 00:06:19,200
Das ist wichtig für diesen Expansionskurs.

92
00:06:19,200 --> 00:06:22,200
Es ist aber auch einfach eine Art, in der man sich selber belügen kann.

93
00:06:22,200 --> 00:06:24,200
Sondern kognitive Dissonanz abbauen kann.

94
00:06:25,000 --> 00:06:27,000
Er belügt nur dich.

95
00:06:27,000 --> 00:06:30,000
Der Unternehmer als Held, der sich von niemanden befreundet.

96
00:06:30,000 --> 00:06:32,000
Er weiß ganz genau, was er da macht.

97
00:06:32,000 --> 00:06:35,000
Diese weit verbreitete Denkart in der Technologiebranche.

98
00:06:37,000 --> 00:06:40,000
Also alle, die jetzt sich hier an meinen Channel reinziehen.

99
00:06:40,000 --> 00:06:44,000
Haben sich wahrscheinlich das Video zu Facebook schon reingezogen.

100
00:06:44,000 --> 00:06:48,000
Aus der Reihe Digitalisierung verstehen lernen.

101
00:06:48,800 --> 00:06:55,800
Und ihr dürft euch alle sehr erhaben fühlen über den durchschnittlichen GEZ-Medienkonsumenten.

102
00:06:55,800 --> 00:07:04,800
Weil da wurde jetzt offensichtlich ein Literaturwissenschaftler nach seiner Meinung befragt zu Facebook.

103
00:07:04,800 --> 00:07:08,800
Und natürlich macht er das, was alle machen.

104
00:07:08,800 --> 00:07:11,800
Er identifiziert ein Symptom.

105
00:07:11,800 --> 00:07:15,800
Und jetzt haltet euch fest, sie werfen Facebook vor.

106
00:07:16,600 --> 00:07:23,600
Dass sie das pure Böse sind und vor allem unwissentlich das falsche tun.

107
00:07:23,600 --> 00:07:25,600
Das ist natürlich totaler Spaß.

108
00:07:25,600 --> 00:07:27,600
Erstens wissen sie ganz genau, was sie tun.

109
00:07:27,600 --> 00:07:29,600
Sie tun es absichtlich.

110
00:07:29,600 --> 00:07:31,600
Und ja, der Sturm aufs Kapitol ist für Facebook super gewesen.

111
00:07:31,600 --> 00:07:36,600
Natürlich wäre es noch viel geiler, wenn noch mehr hinterher Facebook-Traffic entstanden wäre.

112
00:07:36,600 --> 00:07:38,600
Aber wahrscheinlich war es schon unendlich viel Traffic.

113
00:07:38,600 --> 00:07:40,600
Und sie haben es immer noch nicht verstanden.

114
00:07:40,600 --> 00:07:41,600
Natürlich.

115
00:07:41,600 --> 00:07:44,600
Auch so ein Gemetzel in Myanmar ist super für Facebook.

116
00:07:44,600 --> 00:07:48,600
Alles ist super für Facebook, solange es Traffic im Facebook erzeugt.

117
00:07:48,600 --> 00:07:52,600
Facebook braucht Daten und Facebook braucht mehr Traffic.

118
00:07:52,600 --> 00:07:54,600
Das ist ihr Geschäftsmodell.

119
00:07:54,600 --> 00:07:59,600
Und ihnen Wachstum, Wachstumszwang vorzuwerfen, das ist Schwachsinn.

120
00:07:59,600 --> 00:08:02,600
Weil das ist immanent, das ist im System drin.

121
00:08:02,600 --> 00:08:07,600
Und ich habe ja schon erklärt, Facebook und naja, eigentlich auch Fake News.

122
00:08:07,600 --> 00:08:09,600
Fake News entstehen aus Wettbewerb.

123
00:08:09,600 --> 00:08:17,600
Aus der ganz simplen Tatsache, dass da um die Aufmerksamkeit der User gekämpft wird.

124
00:08:17,600 --> 00:08:20,600
Aber da sieht man, was dabei rumkommt.

125
00:08:20,600 --> 00:08:25,600
Da gibt es jetzt hier so einen 10-Minuten-Beitrag mit dem super Enthüllungsbuch zu Facebook.

126
00:08:25,600 --> 00:08:29,600
Und da steht mit Sicherheit nichts drin, was nicht schon längst bekannt ist.

127
00:08:29,600 --> 00:08:31,600
Garantiert.

128
00:08:31,600 --> 00:08:33,600
Ich will es gar nicht lesen. Will ich auch nicht.

129
00:08:33,600 --> 00:08:37,600
Aber offensichtlich macht Kulturzeit einfach nur Werbung für irgendwelche Bücher.

130
00:08:37,600 --> 00:08:41,600
Für irgendwelche Boomer, die keine Ahnung haben von dem Neuland Internet.

131
00:08:41,600 --> 00:08:45,600
Und deswegen mal ein Buch lesen müssen darüber und sich dann aber nur noch über Symptome aufregen.

132
00:08:45,600 --> 00:08:47,600
Herrlich. Find ich richtig geil.

133
00:08:47,600 --> 00:08:49,600
Kann man eigentlich nichts mehr zu sagen.

134
00:08:50,600 --> 00:08:51,600
Super Beitrag.

135
00:08:51,600 --> 00:08:53,600
Herrliches Ding.

136
00:09:07,600 --> 00:09:10,600
Super Beitrag.

137
00:09:37,600 --> 00:09:39,600
Super Beitrag.

138
00:10:07,600 --> 00:10:09,600
Super Beitrag.

139
00:10:16,600 --> 00:10:20,600
Übrigens, der Kanal ist wie eine schöne Vorlesung aufgebaut.

140
00:10:20,600 --> 00:10:27,600
Wenn ich unten Videos verlinke, dann wäre es angebracht, sich die auch reinzuziehen, weil das aufeinander aufbaut.

141
00:10:27,600 --> 00:10:30,600
Bestimmte Begriffe werden definiert, viele Beispiele werden genannt.

142
00:10:30,600 --> 00:10:38,600
Und ja, ich denke mir schon was dabei, weil es sozusagen meinen eigenen Erkenntnisprozess abbildet, die Reihenfolge, in denen ich die Videos hier hochlade.

