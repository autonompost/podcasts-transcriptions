Niemals kann man da erwarten, dass dahinten ein perfektes gutes Urteil oder ein gutes Weltmodell über die Gesellschaft rauskommt, wenn die Gesellschaft
für die Bürokratie so aussieht, als wäre sie ein Haufen oberflächliche Zahlentupel.
Und das ist ja der Punkt. Man kann also gar nicht erwarten, dass eine Bürokratie sinnvolle Entscheidungen treffen kann, wenn sie so ein unterkomplexes Weltmodell hat.
Insbesondere, weil sich die Außenwelt permanent ändert und in einer Bürokratie, das ja überhaupt nicht so angelegt ist. Die Bürokratien sind ja gar nicht darauf
angelegt, dass da eine Dynamik irgendwo stattfindet. Und eine Dynamik abbilden und Emergenz abbilden schon mal gar nicht.
So Leute, ich mach nochmal so ein Video über Bewusstsein und Nachdenken und so ein Weltmodell, ihr wisst Bescheid.
Was macht jetzt das Weltmodell und was macht das Bewusstsein?
Ich habe ja schon erklärt, unser Hirn löst das inverse Problem, was hat unsere Inputdaten verursacht, mit dem Zweck, die Vorhersagen, um bessere Entscheidungen treffen zu können
und sozusagen die evolutionäre Fitness zu maximieren. Und ich habe ja auch erklärt, wir konstruieren sozusagen uns dabei ein Weltmodell und erzählen uns eine Geschichte darüber.
Und wir sind Protagonisten in dieser Geschichte und mit diesen Protagonisten identifizieren wir uns selber.
Und daraus folgt zum Beispiel auch, wenn wir andere bewerten wollen, dass die energiesparendste Möglichkeit wahrscheinlich die mit der bösen Absicht ist, dazu habe ich schon Videos gemacht.
Und ich habe jetzt hier nochmal folgendes Statement, was man immer hört, wenn man so Experten zuhört, die sich viel mit Deep Learning und Machine Learning und KI und so beschäftigen.
Und zwar die sagen, wir verstehen nicht im Detail, wie KI funktioniert, wie diese Netzwerke funktionieren.
Also was passiert ist, wir geben eine Architektur vor, Trainingsdaten und dann trainiert das Netzwerk, wir verstehen, wieso es das tut.
Aber dann intern, wenn wir fertig trainiert sind und wir wollen die Frage an das Netzwerk stellen, wieso hast du diese und diese Entscheidung getroffen?
Zum Beispiel beim autonomen Fahren oder so.
Können wir natürlich zurückverfolgen, wie das Signal sich da drinnen verausbreitet und im Prinzip besteht es aus einer unfassbaren Menge an Multiplikationen, Additionen und so und Softmax und hin und her und richtig viele Kombinationen von Rechenschritten.
Und wir verstehen halt einfach nur nicht, wie das passiert. Warum? Weil wir im Prinzip es geschafft haben, durch Mathematik den Trick mit der Emergenz abzubilden auf diese Zahlenkolonnen.
Und wenn wir sagen, wir verstehen es nicht, meinen wir damit, wir verstehen diesen Prozess innen drinnen nicht mehr.
Und daraus emergiert aber sozusagen das, was hinterher diese Netzwerke tun, zum Beispiel Entscheidung treffen und klassifizieren und so.
Wobei die Image-Klassifikator noch ungefähr klar sind, weil wir da noch eine Intuition haben.
Und ich hatte ja auch schon ein Video gemacht darüber, dass wir ja durch diese Sprachmodelle jetzt eine relativ gute Schnittstelle zu KI haben, weil wir die KI fragen können oder ein Sprachmodell fragen können, was es gerade tut.
Und wir können einer KI auch Geschichten geben und diese KI kann mit diesen Geschichten weiterarbeiten.
Das heißt, was Geschichten eigentlich sind, ist so eine Art Schnittstelle.
Und das ist interessant, denn ich habe ja auch schon das Video gemacht, dass wir als Gesellschaft Geschichten brauchen, um miteinander zu kommunizieren und dass die Geschichte selbst notwendig ist für den performativen Akt der Gesellschaft.
Und das reiht sich jetzt alles irgendwie ein in diese Idee, die ich jetzt sozusagen hatte, als ich nochmal darüber nachgedacht habe, dass wir ja KI eigentlich gar nicht verstehen.
Und jetzt nochmal zurück auf der Ebene des Zahlensalats verstehen wir nicht, wie diese Entscheidung funktioniert.
Und dann dachte ich kurz danach und habe festgestellt, na ja, wenn ich jetzt zum Beispiel ich programmiere ja viele numerische Verfahren, die so Lösungen finden für so schlecht gestellte Probleme und so.
Und wenn ich mir den Zahlensalat angucken würde, dann würde ich den auch nicht verstehen.
Dann würde ich den auch nicht verstehen, schon allein weil so ein, sagen wir mal, so ein Datensatz aus richtig vielen Zahlen besteht.
Wenn ich auf die Zahlen drauf gucke oder noch schlimmer, auf die Rohdaten, würde ich überhaupt nichts erkennen, wenn ich die Binäre mir angucken würde oder so.
Ich verstehe erst was auf einer höheren Ebene der Emergenz, wo sozusagen die Bits genommen werden, in eine Abbildung gesteckt werden und diese Funktion gibt mir als Output ein Bild oder so oder einen Grafen oder so.
Dann verstehe ich was da passiert, aber auf der untersten Ebene verstehe ich es auch nicht, das heißt, das ist eigentlich kein großer Unterschied.
Was da dazwischen steckt, damit ich es verstehen kann, nennen wir Sprache, zum Beispiel Mathematik.
Oder aber, wenn ich zu jemandem erklären will, warum ich die und die Entscheidung getroffen habe, dann werde ich ihm doch auch nicht erklären, na ja, meine Neurotransmitter, meine Hormone, meine gesamte Vergangenheit, mein Genmaterial und was unsere Zivilisation vor 1000 Jahren gemacht hat, hat dazu geführt, dass ich diese Entscheidung getroffen habe.
Ich weiß es doch selber nicht, das weiß ich doch alles gar nicht, das nehme ich ja gar nicht wahr, das ist ja nicht Teil meines bewussten Messprozesses.
Und genau so gehen ja Maschinen auch vor und wenn ihr euch reinzieht, was die neuesten Anwendungen von GPT-4 sind, dann geht das alles in diese Richtung, dass wir versuchen, durch das Tool der Geschichte, GPT dazu zu bringen, Dinge zu tun für uns.
Und ganz ähnlich, wie ich das in meinem Video gesagt habe, wie ich jetzt so eine KI und einen Roboter schreiben würde, habe ich gesehen, jetzt gibt es das mit Minecraft.
Minecraft ist ja Touring vollständig und es gibt jetzt sozusagen Papiere, die nichts anderes machen, außer in Minecraft durch Sprachmodelle eine KI emergieren zu lassen, die sich selber Code schreibt, um da drinnen Probleme zu lösen.
Und ich glaube, die Zielfunktion, die das Ding optimiert, ist einfach nur, werde der beste Minecraft-Spieler der Welt, das heißt, baue alles, was möglich ist oder so und versuche so viel Tech-Tri wie möglich zu erschaffen und so.
Irgendwie so, ich werde das Paper mal verlinken, wenn ihr Zeit habt, könnt ihr euch das mal angucken, ist auf jeden Fall interessant.
Und das geht so, dass sich sozusagen wirklich das Sprachmodell überprompt, also GPT ist eine Schnittstelle, die aus Code und Anweisungen Geschichten produziert, die dann in ein Interface reingehen, die mit Minecraft kommunizieren, um da drinnen eine KI zu steuern, um da drinnen einen Menschen zu steuern, also einen Player zu steuern, der dann was baut und so.
Und das Ding, das GPT selbst sieht ja keine, sieht ja keinen Screen vor sich, sondern es hat sozusagen eine Syntax, die textbasiert ist, sowas wie du hast so und so viel Zeug in deinem Inventar, die Koordinaten von deiner Schatztruhe sind hier und da und um dahin zu kommen, muss man so und so.
Das ist alles textbasiert und nur über diese Text-Schnittstelle wird sozusagen dieser Roboter gesteuert. Das ist im Prinzip so, wie ich mir das damals vorgestellt habe in meinem alten Video.
Und jetzt nochmal zurück zur Physik. Wenn ich jetzt zum Beispiel in der Physik Differential-Gleichung hinschreibe und die Natur hier beschreiben will, zum Beispiel in der allgemeinen Relativitätstheorie, dann blickt da niemand durch.
Ich finde, selbst die normale Tensor-Schreibweise der Feldgleichung von Einstein sind so ultraheftig, da checkt man nichts.
Und ich habe mir sagen lassen von einem Kumpel, der Mathe studiert hat, der hat Differential-Geometrie als Spezialfach in seinem Master und die haben eine moderne Schreibweise für Differential-Geometrie, die viel intuitiver ist und die auch die einsteinischen Feldgleichungen viel intuitiver hinschreibt und auch das Rechnen mit diesen Gleichungen viel intuitiver macht.
Das ist so ein bisschen wie, als würde man keine lineare Algebra können und trotzdem die ganze Zeit Fourier-Analyse machen. Wenn man das nicht oder sagen wir mal, man probiert Quantenmechanik zu machen ohne Matrizenschreibweise und ohne lineare Algebra-Kennnisse, das würde gehen.
Aber diese kompakte Schreibweise und dieser komplette Satzbau der linearen Algebra, der ist extrem hilfreich fürs Verständnis. Also wieder, da passiert irgendwas mit Zahlenkolonnen und wir können das nicht so richtig begreifen.
Wir müssen es erst auf eine Art und Weise ausdrücken durch am besten eine bijektive Abbildung, also eine, die eindeutig umkehrbar ist in Sprache und dann verstehen wir es. Und das haben wir auch in der normalen klassischen Mathematik und in der klassischen Physik auch.
Das heißt, diese Aussage, wir verstehen nicht genau, was Deep Learning macht, ist meiner Meinung nach jetzt habe ich es erst so richtig durchdrungen. Das ist genau das gleiche, was es auch schon vorher war, genauso wie mit meiner Aussage, dass wir nicht verstehen, wie eine CPU funktioniert von einem Computer.
Wir verstehen, wie die Building Blocks funktionieren, aber auf der binären Ebene checken wir nicht, was da passiert. Ein Elektrotechnik-Mensch, so wie der Michael, mit dem ich gequatscht habe auf meinem Kanal, der baut ja solche Teile.
Man versteht ungefähr, wie ein Building Block funktioniert, aber wie die anderen damit dann verschaltet werden und was es dann in Summe schon macht, ist schon übelst kompliziert. Auf höheren Leveln der Emergenz kann man das verstehen.
Auf dem unteren Level der Bits und Bytes kann man es nicht checken und unterhalb davon, also auf der quantenmechanischen Ebene, kann man es schon mal gar nicht verstehen.
Und so ist es ja im Prinzip, mit unserem Bewusstsein auch. Wir erzählen uns ja auch permanent Geschichten darüber, wieso wir Entscheidungen getroffen haben.
Und wir fangen wirklich an, Overfitting zu betreiben und uns die Vergangenheit schön zu reden mit Geschichten. Dabei ist sozusagen nachgewiesen, ich lese gerade das Buch von Robert Sapulsky und das ist natürlich großartig.
Er ist schon in den ersten Kapiteln, fängt er direkt an, so ein paar geile Experimente zu erklären, von denen ich auch schon mal gehört hatte, zum Beispiel abhängig davon, ob der Raum, in dem man einen Test macht über Verhalten und Entscheidungstreffen und so, ob das da stinkt oder nicht, ist man eher pro-sozial und eher kooperativ oder nicht und alle solche Sachen.
Und ich habe auch schon mal gesagt, abhängig davon, ob man viel Zucker zum Frühstück isst oder eher fettig, ist man mehr risikofreudig oder nicht und so und all diese Sachen.
Wir werden ständig, also unterbewusst durch Hormone und Inputdaten beeinflusst in unseren Entscheidungen und hinterher erfinden wir die Geschichte, warum wir uns so entschieden haben.
Und das ist total interessant. Und das liegt einfach nur daran, dass wir natürlich gar nicht durchdringen, wieso wir Entscheidungen getroffen haben.
Und das liegt auch daran, dass wir natürlich permanent Overfitting über unsere Vergangenheit betreiben. Und das habe ich letztens in irgendeinem Video auch erklärt, wo ich irgendwie über Konservativismus rede oder so.
So, worauf will ich jetzt eigentlich in dem Video raus? Naja, also das war jetzt der erste Gedanke. Und ich wollte auch nochmal diesen Schwenk zum Bewusstseinsmodell machen.
Wir konstruieren uns ja ein Modell der Welt, wie Mono-Kausal am besten einfache lineare Zusammenhänge funktionieren, die uns erklären und vorhersagen können, was als nächstes passiert.
Also wenn ich meine Flasche fallen lasse, dann geht sie wahrscheinlich kaputt, so nach dem Motto.
Und jetzt ist es so, wir sind sozusagen eine zentrale Datenverarbeitung. Wir machen ja, wir kriegen Input-Daten und die werden hier oben in unserem Kopf verstoffwechselt zu weiteren Informationen.
Und die Neuronen feuern, wenn bestimmte Sachen passieren und so. Das ist alles nicht linear, hochgradig vernetzt, emergent, klar.
Und jetzt ist sozusagen das Bild, was ich im Buch zeichne, Machthierarchien, zum Beispiel Verwaltung, machen das ja auch.
Die kriegen ja von uns übelst viel Input-Daten und müssen dann Entscheidungen treffen.
Und das Problem ist, die sind nicht durch Evolutionsdruck gewachsen, also nicht über Jahr, Milliarden von Jahren, so wie wir, sondern die sind gewachsen, weil wir sie am Reißbrett entworfen haben.
Und das ist der Grund, warum die Scheiße ständig fehlt. Die können gar nicht richtig funktionieren, weil sie würden, wenn dann durch Zufall die aus Versehen die richtigen Zahlen messen.
Wir betreiben eine Menge Datenkomprimierung, wenn wir irgendwas sehen. Zum Beispiel das, was wir sehen, da ist eine Menge Interpolation dazwischen.
Wir nehmen nicht alle Photonen, die hier ankommen auf der Retina, sondern da hinten in der Retina findet schon Datenkomprimierung statt.
Daher folgen dann auch so viele Missverständnisse und Illusionen und optische Täuschungen und so, ist ja klar.
Und Bürokratien jetzt haben ja ein vollständig unterkomplexes Weltmodell, weil sie ja wirklich nur ein paar Kennzahlen überhaupt messen.
Also für eine Bürokratie ist es völlig egal, was für eine Person davor steht.
Sie fragt einfach nur so ein paar Daten ab, wie Geburtstag, ein Nettoeinkommen, was auch immer, Geburtstagszeit, so ein Scheiß.
Niemals kann man da erwarten, dass da hinten ein perfektes gutes Urteil oder ein gutes Weltmodell über die Gesellschaft rauskommt,
wenn die Gesellschaft für die Bürokratie so aussieht, als wäre sie ein Haufen oberflächliche Zahlentupel.
Und das ist ja der Punkt. Man kann also gar nicht erwarten, dass eine Bürokratie sinnvolle Entscheidungen treffen kann, wenn sie so ein unterkomplexes Weltmodell hat.
Insbesondere, weil sich die Außenwelt permanent ändert und in einer Bürokratie das ja überhaupt nicht so angelegt ist.
Die Bürokratien sind ja gar nicht darauf angelegt, dass da eine Dynamik irgendwo stattfindet.
Und eine Dynamik abbilden und Emergenz abbilden schon mal gar nicht. Und das ist ja das, was wir permanent beobachten.
Bürokratien sind überhaupt nicht fähig, die Realität überhaupt nur abzubilden.
Deswegen sind sie auch nicht fähig, irgendwas zu machen. Deswegen kollabieren sie auch auf der Scythe-Skala von ein paar Jahrzehnten ständig.
Das ist Schwachsinn. Und wenn wir da noch weiter darüber nachdenken, gibt es doch auch diesen geilen Spruch von dem Daniel Schmachtenberger,
von dem habe ich mir auch mal was reingezogen. Der sagt ja, wir als Gesellschaft sind schon eine AGI.
Stellt euch mal vor, wir sind ja ein Haufen kleine Touring-Maschinen. Wir alle optimieren sozusagen eine lokale Zielfunktion.
Und das, was wir im Moment global machen, also die Gesellschaft, die daraus immergiert, aus unseren Wechselwirkungen untereinander,
ist im Prinzip sowas wie eine Artificial General Intelligence.
Das Problem ist bloß, dass die Zielfunktion dieser Artificial General Intelligence misaligned, und das geht so wie in meinem Video zum Misalignment-Problem zurück,
mit dem Wohl für alle. Das gesellschaftliche Problem, wie ich sehr gerne nenne, lautet ja eigentlich, maximiere das Wohl für alle unter den Nebenbedingungen, Gewalt zu minimieren.
Und was meine ich mit Gewalt? Naja, ich habe meine Gewaltdefinition schon mal gebracht in einem anderen Video.
Was die AGI unserer Gesellschaft im Kapitalismus normalerweise gerade optimiert, ist halt leider die Kapitalakkumulation.
Das habe ich schon erklärt. Und das geht zurück zu meinem Video hier von wegen, nimm dieses Schaubild, damit kannst du eigentlich alles erklären,
weil wir ja alles in Pyramiden sortieren und so. Und das ist eigentlich ziemlich straight forward, diese Vorstellung.
Und während ich also jetzt wieder da saß und wieder über die Geschichten, AGI und Bewusstseinsmodelle, Weltmodelle und so einen Scheiß nachgedacht habe,
ist mir erst klar geworden, wozu Geschichten eigentlich da sind. Man kann Geschichten als solche jetzt nochmal ganz neu interpretieren,
nämlich das ist, wir haben eine interne Datenverarbeitung. Wir sehen nicht, was unsere Datenverarbeitung.
Wir selber verstehen unsere eigene Datenverarbeitung nur wenig bis gar nicht. Und wenn wir unsere Datenverarbeitung mit anderen Datenverarbeitungen,
zum Beispiel anderen Menschen, synchronisieren wollen, benutzen wir dafür Sprache, weil Geschichten das Einzige sind,
was irgendwie so viel Daten komprimiert und auf Wesentliches so runter bricht, weil wir sozusagen zum Beispiel auf konkrete Objekte verweisen können,
denen wir einen Namen gegeben haben, zum Beispiel Bierflasche. Wir schreiben ja nicht die quantenmechanische Konfiguration dieser Bierflasche hin.
Wir gehen ja nicht in den Zahlen-Salat rein, sondern wir sagen, wir haben eine Bierflasche.
Es ist auch gar nicht zweckmäßig genau zu erklären, wie die jetzt aufgebaut ist. Für den meisten Zwecke ist es nur sinnvoll zu wissen, es ist eine Bierflasche.
Und genau so funktionieren dazu sind Geschichten da. Das heißt, Geschichten sind eigentlich ein Interface dieser Datenverarbeitung.
Und die Geschichten, die wir mit Machthierarchien teilen, haben vor allem das Problem, dass es besonders oberflächliche Geschichten sind.
Und insbesondere zwängen uns die Machthierarchien die Syntax dieser Geschichten auf in Form von Zetteln und Formularen und reduzieren unsere Geschichten auf oberflächliche Kennzahlen.
Und das ist das Problem. Das heißt, in Wahrheit liegt da permanent ein permanentes Missverständnis vor mit der AGI, die wir da konstruieren,
wenn wir mit den Bürokratien und den Staaten und den anderen Machthierarchien uns unterhalten.
Wir kommunizieren mit einer minderbewussten Entität, die aus unseren Wechselwirkungen emergiert, zum Beispiel den Sachbearbeitern und dem Staat,
der irgendwie irgendwelche Zielfunktionen vorgibt. Und dann wird irgendwie hinten erwartet, dass hinten aus dieser Datenverarbeitung was sinnvolles rauskommt.
Und nochmal, das ist Schwachsinn. Unsere zentrale Datenverarbeitung ist ja nur deswegen so krass, weil wir so schon so lange gewachsen sind.
Und auch die ist noch lange nicht fertig. Ich habe jetzt im Buch von Robert Sapolsky gelesen, dass wir noch keinen Unterschied machen können,
zwischen wenn wir angeekelt sind aufgrund von evolutionärer Nahr-Fitness, also zum Beispiel wir riechen, dass etwas eklig ist, zum Beispiel verschümmelt oder wir finden jemanden besonders hässlich.
Wir können das, das spricht im Moment bei uns, weil das erst ein paar hunderttausend Jahre da ist, in unserem Gehirn dasselbe Areal an.
Und das ist natürlich der Grund, warum wir sozusagen hässliche Menschen genauso scheiße bewerten wie irgendetwas, was wir eklig finden.
Und das geht auch so weit, dass wir zum Beispiel Verhalten, was wir widerlich finden, genauso einordnen.
Und das Framing, was dann intern sogar passiert ist, wir würden hässlicheren Menschen eher das schlimme Zeug zusprechen.
Und tatsächlich sehen wir, dass dieser Effekt erst rausgenommen werden kann im Experiment mit Menschen, wenn wir noch mehr Zusatzinformationen rüberreichen.
Aber natürlich wissen wir alle, dass diese Zusatzinformationen oft nicht vorliegen und deswegen böse Absicht kommt.
Also ihr seht sozusagen, das ist super geil eigentlich, wenn man darüber nachdenkt, wozu Geschichten eigentlich dienen.
Also erstmal ist es irgendwie intuitiv klar, Geschichten sind zur Kommunikation da, aber wenn man das mal wirklich ernst nimmt und wirklich weiter zu Ende denkt, ist das schon irgendwie geil.
Und so wie ich jetzt sozusagen mir vorstellen würde, dass ein Roboter mit einem Roboterarm intern sich Geschichten baut, damit der Roboterarm der Signale sendet, hier nach da Signale sendet und zum ganzen Körpersignale sendet, so funktioniert es bei uns innen drin auch.
Wir senden Signale in Form von Hormonen, irgendwelchen anderen Signen, Botenstoffen, das wird alles codiert intern zu Geschichten, die sozusagen nur dazu da sind, dass unsere Datenverarbeitung möglichst effizient arbeitet.
Und die Effizienz kommt daher, dass sie lange über Evolutionsdruck gewachsen ist.
Und nochmal, unsere lokalen Datenverarbeitungen, die wir selber emigrieren lassen, unsere Verwaltung und so, sind bei weitem nicht so kompetent, ist doch klar.
Die wurden ja am Reisbrett entworfen.
Und wenn wir jetzt sozusagen die Kapitalakkumulation vorantreiben mit unserer AGI als Gesellschaft, dann benutzen wir tatsächlich sozusagen ausschließlich Geschichten und oberflächliche Kennzahlen, die irgendwie unsere eigenen Geschichten auch irgendwie untermauern sollen.
Zum Beispiel unser Verhalten oder so, wenn wir uns rechtfertigen vor, keine Ahnung, einer zentralen Autorität oder so.
Also dieses Video ist jetzt wirklich schon wieder so ein Brainstorm, ich weiß auch noch nicht, wie ich es nenne und so.
Ich mache auch gleich einen Stream an.
Ich hatte bloß gerade noch diesen Gedanken zu Ende gedacht, mit dieser Sache von wegen, was das eigentlich bedeutet, dass wir die KI auf dieser Zahlenebene nicht verstehen.
Und stellt euch das wirklich nochmal vor.
In der normalen klassischen Mechanik könnte ich mir ein System aus Pendeln und Kugeln, die irgendwo runterrollen und, dass die verkabelt sind, irgendwie hinschreiben mit Zahlensalat und das komplett ausrechnen.
Das ist gar kein Problem.
Ich kann die partielle Differenzhergleichung numerisch lösen.
Auf der Zahlenebene verstehe ich nur nicht, was da rauskommt.
Erst wenn ich diese Zahlen wieder nehme und in Größen verwandele, die für das System charakteristisch und nicht oberflächlich sind, kann ich verstehen, was in diesem dynamischen System vor sich geht.
Also zum Beispiel irgendwelche Winkelkonfiguration von irgendwelchen Pendeln oder so.
Auf der Zahlensalat-Ebene checke ich sowieso nicht.
Ich checke es erst, wenn ich es wieder zurücktransformiere in eine für uns lesbare und verstehbare Sprache.
Und die nennen wir typischerweise Physik und kanonische Variablen und Phasenraum und so.
Aber ja, das erstmal jetzt hierfür und reingehauen, YouTube.
Oh.
Oh.
Oh.
Oh.
Übrigens, der Kanal ist wie eine schöne Vorlesung aufgebaut.
Wenn ich unten Videos verlinke, dann wäre es angebracht, sich die auch reinzuziehen, weil das aufeinander aufbaut.
Bestimmte Begriffe werden definiert, viele Beispiele werden genannt.
Und ja, ich denke mir schon was dabei, weil es sozusagen meinen eigenen Erkenntnisprozess abbildet, die Reihenfolge, in denen ich die Videos hier hochlade.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
Ja.
