start	end	text
0	8000	Es geht nicht darum, dass beim Touring-Test rauskommt, dass eine KI sozusagen irgendwann Menschen gleich ist, sondern der Touring-Test steht eigentlich die inverse Frage.
8000	11000	Haben wir als Menschen unsere eigene Intelligenz begriffen?
25000	29000	So Leute, ich hatte ja vor, keine Ahnung, zwei Monaten oder so,
29000	43000	mein Chat-GPT-Video zu gemacht und kurz danach ging GPT4 live, also beziehungsweise die Open Air hat das vorgestellt und hatten auch noch ein Research-Paper dazu,
43000	51000	also so ein Technikereport dazu gemacht, wo sie auch so ein bisschen interessante Sachen erzählt haben.
51000	55000	Und ich habe leider nicht so viel Zeit, mich damit jetzt so im Detail zu beschäftigen.
55000	58000	Das Thema ist super interessant, aber ich komme halt nicht so richtig dazu.
58000	69000	Aber es gibt sehr viele interessante Dinge, die da im Moment passieren und das liegt vor allem daran, dass GPT4 multimodalen Input kann.
69000	73000	Es kann Bilder sehen und Text. Es kann jetzt zwei Dinge.
73000	88000	Es ist viel aufwendiger und wahrscheinlich viel größer als GPT3 und es gewinnt sozusagen in allen möglichen Aufnahmetests von Unis und schreibt bessere Klausuren als die durchschnittlichen Studenten und so.
88000	90000	All das kann es inzwischen.
90000	102000	Und auch weil viele Experten jetzt der Meinung sind, dass die AGI, also die Artificial General Intelligence, keine ferne Zukunft mehr ist, habe ich mir jetzt zum Beispiel mal überlegt,
102000	111000	wie würde ich jetzt einem Roboter hinschreiben, also wie würde ich die Software von so einem Roboter hinschreiben, dem ich versuche, ein Bewusstsein zu geben?
111000	125000	Und ich habe ja extra damals ein Video gemacht dazu, wie das ist mit dem, was wir Realität nennen und wie wir da unser Bewusstsein einordnen und so, wie ich das in diesem Kontext sozusagen, wie ich das alles zusammenbringe.
125000	134000	Und da erkläre ich ja im Wesentlichen, unser Gehirn erzählt sich sozusagen selber eine Geschichte darüber, wie es wäre, eine Person zu sein.
134000	141000	Oder wir stellen uns vor, wie eine Person in unserer Situation ist, die quasi die Situation rekonstruieren.
141000	151000	Wir einfach aus unseren gegebenen Inputdaten und der Vergangenheit, also den Daten, die wir in der Vergangenheit schon aufgesammelt haben und wie sich diese Person jetzt entwickelt in der Zeit.
151000	155000	Also was im nächsten Zeitschritt passieren wird und warum sie das tut und so.
155000	162000	Also wir können uns quasi eine Prosa-Geschichte davon erzählen und das ist das, was wir dann schlussendlich mit uns selbst identifizieren.
162000	170000	Also wenn wir von uns selbst reden, haben wir immer eine Figur im Kopf, eine Geschichte, die wir uns selber über eine Figur erzählen.
170000	180000	Ja und natürlich fließt sozusagen in Echtzeit, im Echtzeitfeed sozusagen unser sensorischer Input in diese Geschichte ein und wird sofort zu Verstoffwechsel, zu neuer Prosa.
180000	184000	Und deswegen ist diese Geschichte halt auch in Echtzeit wird sich erzählt.
184000	194000	Und da wir aber bereits wissen, dass KI jetzt, also GPT-4 insbesondere, gut kohärente Geschichten erzählen kann und sich auch ausdenken kann,
195000	201000	wieso also nicht versuchen, über diese Schiene einem Roboter, einem Bewusstsein zu geben?
201000	213000	Und ich denke, alles, was wir brauchen, ist im Prinzip, wir brauchen eine Verschachtlung von vielen sich gegenseitig Geschichten erzählenden Netzwerkarchitekturen,
213000	216000	also Deep Learning Architekturen und Machine Learning.
216000	221000	Wobei wir ja sozusagen, also eigentlich brauchen wir nur eine Geschichte, die eine Bewusstseinsgeschichte erzählt,
221000	226000	also über alles, was reinkommt an Inputdaten und was noch nicht richtig korrekt prediktet wurde,
226000	231000	wo wir unsere Aufmerksamkeit sozusagen oder der Roboter seine Aufmerksamkeit drauf lenken muss.
231000	236000	Und der Rest sind natürlich die ganzen unterbewussten Sachen, also Steuereinheiten und Motorik.
236000	242000	Also wenn ich jetzt zum Beispiel mir eine Geschichte erzähle, in der das Laufen vorkommt, dass ich jetzt irgendwo hinlaufe,
242000	246000	da muss ich, da muss ja sozusagen die Motorik angesprochen werden.
246000	250000	Es ist ja nicht so, dass ich wirklich aktiv darüber nachdenke, jetzt rechter Fuß, den linken Fuß,
250000	258000	sondern ich gebe meiner Kontrolleinheit einfach unterbewusst den Befehl, ich erzähle mir jetzt die Geschichte, in der ich loslaufe und dann laufe ich auf der anderen Seite.
258000	262000	Und ich denke, da das bei uns wahrscheinlich so ist, würde ich das mit dem Roboter auch machen,
262000	269000	nur in dem Moment, wo wir jetzt zum Beispiel Motorik erlernen, da denken wir noch aktiv über die einzelnen Bewegungen nach.
269000	273000	Und ich gebe immer dieses Beispiel mit dem Instrumentlernen, weil das für mich irgendwie am intuitivsten ist,
273000	278000	das versteht auch noch jeder, weil das sind Dinge, die wir als Erwachsene auch noch diese Erfahrungen machen können.
278000	282000	An die Erwachsenen, die wir als Erwachsene auch noch lernen können.
283000	291000	Weil wir da noch kein Bewusstsein hatten. Die interne Map und das Modell vom Universum war noch nicht groß genug, um uns mit sozusagen damit rein zu tun.
291000	295000	Da ist das Bewusstsein noch gar nicht emigriert in dem Sinne.
295000	304000	Und wenn wir jetzt sozusagen beim Erlernen von einzelnen Gliedmaßen die Motorik sozusagen erlernen, das ist klar, das erlernt man einmal und dann geht es ins Unterbewusstsein.
304000	311000	Und wenn wir jetzt sozusagen beim Erlernen von einzelnen Gliedmaßen die Motorik sozusagen erlernen, das ist klar, das erlernt man einmal und dann geht es ins Unterbewusstsein.
311000	317000	Und es wird erst wieder rausgekramt, wenn ein Bug passiert, also wenn wir mit dem Fuß in den Speichen hängen bleiben mit dem Fahrrad oder so.
317000	321000	Dann denken wir auch immer wieder schlagartig darüber nach, was war denn das jetzt gerade.
321000	328000	Und weil wir uns die Geschichte davon erzählen, dass wir uns sozusagen aufs Fahrrad setzen und losfahren, deswegen passiert es dann auch.
328000	333000	So ungefähr habe ich es ja in dem etwas länglicheren Video von damals erklärt.
334000	345000	Und sowas wie vorausschauendes Denken oder auch jetzt mathematische Symbolsprache und so, da gibt es mittlerweile auch Blöcke für, also es gibt die Blonning-Architekturen, die das können.
345000	357000	Ich habe letztens erst einen Artikel dazu gelesen über das Ding heißt AI-Decart und es ist im Prinzip sozusagen ein Roboter-Decart, der das rechnen kann.
357000	359000	Und da gibt es ein Nature-Paper dazu, das werde ich euch verlinken.
359000	365000	Combining Data and Theory for Derivable Scientific Discovery with AI-Decart.
365000	367000	Könnt ihr euch ja mal reinziehen.
367000	373000	Es gibt bestimmt auch gute Wissenschaftsjournalisten, Artikel und so, die das erklären und so.
373000	381000	Und wenn ich mir so, also ich habe dann so darüber nachgedacht über die ganze Sache mit den Geschichten, die man sich sozusagen, die sich so ein Roboter erzählen müsste.
381000	387000	Und daraufhin ist mir dann mal wieder völlig klar geworden, okay, wir erzählen uns ja als Gesellschaft auch permanent Geschichten.
387000	396000	Also wir alle haben ganz bestimmte Geschichten zu ganz bestimmten Stereotypen, zu ganz bestimmten Facetten von dem, was wir unseren Charakter nennen.
396000	401000	Also ich habe es zum Beispiel beim Talk mit Humann auch gesagt, ich habe eine Vaterrolle, wenn ich mit meiner Tochter unterwegs bin.
401000	405000	Aber wenn ich irgendwie in einer anderen Situation bin, übernehme ich eine andere Rolle.
405000	413000	Und ich erzähle mir eine andere Geschichte über einen anderen Teil meiner Persönlichkeit, der da eher jetzt sozusagen wichtiger ist in der Situation oder so.
413000	423000	Und der Bürger jetzt zum Beispiel im Sinne von der Staatsbürger, der ist auch so eine Geschichte, die wir uns immer noch erzählen und die wird auch richtig aufrechterhalten.
423000	440000	Wir gehen irgendwie wählen und geben sozusagen dem Staat, den wir eigentlich zusammenbauen mit unserer Emergenz die Verantwortung und sagen, okay, du darfst jetzt in unserem Sinne Recht definieren und hast das Gewaltmonopol und so.
440000	450000	Das ist ja eine Geschichte, die wir uns alle gegenseitig erzählen und dass der Staat sich dann irgendwie um das Wohl seiner Staatsbürger kümmert und so und wir bezahlen Steuern und diesen ganzen Scheiß.
450000	455000	Das ist ja nur eine, also das ist nur eine teilweise kohärente Geschichte, würde ich sagen.
455000	467000	Und ich denke, ich mache nach diesem Video vielleicht jetzt gleich noch oder morgen oder so gleich noch mal zu diesen Gedanken, mache ich ein extra Video, weil das auf jeden Fall länglich wird, denke ich.
467000	477000	Das kann ich hier nicht reintun, aber ja, worüber ich eigentlich reden will in dem Video ist das sogenannte Alignment Problem und ich verbinde das immer mit Revart Hacking.
477000	483000	Und die gute Mel hat mich darauf nochmal aufmerksam gemacht, dass das jetzt wirklich natürlich ein interessantes Problem ist.
483000	488000	Und ich hatte auch damals schon vom Sam Altmann diesen Spruch gehört.
488000	494000	Der ist richtig lächerlich, aber jetzt erst mal erklären, was ist das Alignment Problem und was ist Revart Hacking?
494000	507000	Also ihr müsst euch vorstellen, das Alignment Problem kann man ganz trivial so ausdrücken, wie wenn wir eine super starke Intelligenz haben, was tun wir, um zu verhindern, dass sie uns umbringt?
507000	512000	Wir wollen ja, dass diese super starke Intelligenz mit uns in unserem Sinne handelt oder so.
512000	517000	Und wenn man mal ehrlich ist, es ist extrem unwahrscheinlich, dass wir das hinkriegen.
517000	519000	Und ich will eigentlich darüber reden, warum.
519000	525000	Und dazu muss man besonders über das Revart, über das sogenannte Revart Hacking sich unterhalten.
525000	529000	Und Revart Hacking und Alignment ist deshalb so ein schwieriges Problem.
529000	539000	Weil wir selbst jetzt zum Beispiel gar nicht richtig formulieren können, was genau eigentlich diese super starke künstliche Intelligenz für eine Zielfunktion optimieren soll.
539000	544000	Ihr erinnert euch an mein Video zu ChatGBT, das müsst ihr unbedingt sehen, weil sonst versteht ihr nicht, worüber ich hier rede.
544000	550000	Und ich habe es ja schon gesagt, das Problem ist überhaupt, erstmal eine Zielfunktion zu formulieren.
550000	556000	Eine zu formulieren, deren Optimum sozusagen das ist, was wir wollen eigentlich. Das ist schwierig.
556000	567000	Und ich habe es ja auch schon tausendmal gesagt, der Touring-Test, was der eigentlich fragt ist, der fragt nicht, ob wir eine KI für intelligent halten.
567000	571000	Nein, nein. Der Touring-Test stellt die Frage an uns, ob wir unsere eigene Intelligenz verstanden haben.
571000	580000	Und ich denke, wir können ja nicht mal richtig in Worte fassen, was wir jetzt eigentlich, wie wir das gesellschaftliche Problem überhaupt lösen wollen.
580000	586000	Das heißt, wir werden auch unmöglich jetzt mal eben schnell eine Zielfunktion hinschreiben, die das gesellschaftliche Problem löst, denke ich.
586000	598000	Außer vielleicht so eine ganz allgemeine Formulierung im Sinne von Spieltheorie, sowas wie einzelne Human Agents müssen irgendwie ihr Common Good optimieren oder sowas.
598000	606000	Aber das ist auf jeden Fall schwierig und normalerweise, was wir sozusagen sehen, und das habe ich ja in meinem Video über Schwarz-Weiß-Denken auch schon gesagt,
606000	612000	das normale Dogma, dem wir alle unterliegen, ist das Dogma der Herrschaft und der Kontrolle und der Kontrollpyramide.
612000	617000	Und wie wir die nennen und wie wir die konkret intern strukturieren, da gibt es die zwei Standardlösungen.
617000	626000	Die eine lautet Stali, totaler Diktator, der einfach befiehlt, wie es läuft, oder aber freie Märkte und die völlige Abwesenheit vom Staat,
626000	634000	also Anarchokapitalisten, sowas in der Richtung. Die Abwesenheit von Staat, aber dafür die Anwesenheit von Privateigentum an Produktionsmitteln,
634000	641000	die wiederum auch nur eine andere Form der Herrschaftspyramide ist. Und weil Märkte immer zu Monopolen konvergieren, haben wir am Ende dasselbe.
641000	647000	Deswegen ist das ein Trugschluss zu sagen, die Dinge unterscheiden sich irgendwie im Wesentlichen, eigentlich unterscheiden sie sich nicht wirklich.
648000	655000	So, und was ist jetzt Rivardhacking? Naja, Rivardhacking, wollte ich ja erklären, ist, wenn wir jetzt sozusagen ein Ziel verfolgen,
655000	659000	übersetzen wir es typischerweise für die Maschine in eine Zielfunktion.
659000	667000	Könnt ihr euch wirklich vorstellen, wie eine Funktion, die so verläuft im Konfigurationsraum ihrer Inputparameter.
667000	673000	Und wir geben quasi dem System eine Belohnung, wenn wir unseren Ziel ein Stückchen näher kommen.
673000	679000	Und die Belohnung wird immer größer, je näher wir an den Optimalpunkt dieser Zielfunktion kommen.
679000	686000	Zum Beispiel mit Gradient Descent oder Deep Learning, der Lernprozess ist immer Storastik Gradient Descent.
686000	688000	Das ist das, was man standardmäßig macht.
688000	697000	Und Rivardhacking ist jetzt einfach nur, wenn wir dieses Optimum finden, dass aber gar nicht das ist, was wir eigentlich dachten, dass es ist.
697000	702000	Wenn wir also am Ende überrascht sind davon, dass wir ein Optimum gefunden haben in unserer Zielfunktion
702000	706000	und das überhaupt nicht das reproduziert, von dem wir ausgehen, was die Zielfunktion eigentlich tun soll.
706000	713000	Was meine ich damit? Naja, klassisches blödes Beispiel ist, sagen wir mal, wir wollen die Anzahl der Herzinfarkte auf der Welt minimieren.
713000	717000	Dann wäre eine triviale Lösung schlichtweg alle Menschen töten.
717000	720000	Dann gibt es auch keine Herzinfarkte mehr, dann haben wir die minimiert.
720000	723000	Das heißt, wir müssen hinzufügen, eine Nebenbedingung, die sowas heißt wie,
723000	727000	aber Menschen sollen dabei nicht sterben bei unserer Lösung oder so.
727000	730000	Aber das wäre sozusagen triviales Rivardhacking.
730000	733000	Und ihr könnt euch vorstellen, komplizierte Probleme,
733000	739000	an denen kann man eventuell gar nicht sofort erkennen, dass das System Rivardhacking betreibt,
739000	742000	obwohl die Lösung erstmal gut aussieht.
742000	746000	Und erst in bestimmten Pellen, dass uns erst auffällt, dass es verbuckt ist
746000	750000	und sozusagen merkwürdiges Verhalten an den Tag legt, das System oder die Lösung, die wir haben.
751000	756000	Und es ist wirklich ein ziemlich, ziemlich kompliziertes Problem.
756000	766000	Und Sam Altman ist ja jetzt soweit, dass er sagt, okay, er weiß selber nicht, wie er das Alignment-Problem lösen soll.
766000	773000	Deswegen ist sein bester Vorschlag, haltet euch wirklich fest, wenn GPT-5 am Start ist, also die nächste Version,
773000	783000	dann ist der erste Prompt, dem sie dieser möglichen AGI rüberreichen, die Frage, wie lösen wir das Alignment-Problem.
783000	786000	Das ist wirklich, das ist ernsthaft sein Vorschlag.
786000	791000	Das ist das Dümste, was man machen kann, weil Rivardhacking beinhaltet eventuell auch das Ding,
791000	798000	dass ja, wenn eine AGI Bewusstsein hat, sich darüber bewusst ist, dass sie gerade sich in einer Testphase befindet und bewertet wird.
799000	805000	Und dann wird sich die AGI so verhalten, dass sie anschließend ausbüxen kann,
805000	810000	also dass sie sich in einem Bewertungsprozess befindet und da die METREC lokal rivardhacken wird
810000	815000	und sich dann hinterher anders verhalten wird, einfach weil das auch eine Kostenfunktion minimiert.
815000	822000	Und ich habe mir zu dem Thema ein paar Videos von Robert Miles angeguckt, das ist der Dude von Computerfield,
822000	827000	den kann ich euch nur empfehlen und ich werde euch unten die drei Videos, die zu dem Thema besonders wichtig sind,
827000	831000	die er in den letzten Jahren gemacht hat, die werde ich euch verlinken, die könnt ihr euch wirklich angucken.
831000	835000	Der Typ ist auf jeden Fall richtig legit, Computerfield ist auch ein richtig guter Kanal.
835000	841000	Und ja, das Ding ist, Rivardhacking ist witzigerweise eigentlich auch exakt das,
841000	845000	was wir sowieso als Menschen auch und als Gesellschaft auch die ganze Zeit tun.
845000	848000	Das geht jetzt wieder zu meinem Video zurück mit dem Chat-GPT.
848000	853000	Entweder wir optimieren die falschen Zielfunktionen oder aber wir definieren es so und sagen,
853000	856000	okay, wir haben hier Zielfunktionen, aber wir rivardhacken sie.
856000	861000	Das ist sozusagen die andere alternative Interpretation von dem, was wir beobachten.
861000	864000	Konträr zu dem, was ich in dem Video zu Chat-GPT gesagt habe.
864000	870000	Und was meine ich damit? Na ja, Noten grinden zum Beispiel, Attention Farmen, Kapital akkumulieren.
870000	876000	Wozu machen wir das? Na ja, wir optimieren ja eben selbst, wir optimieren halt die falschen Kostenfunktionen oder aber.
876000	880000	Also ich würde sagen, wir optimieren die falschen Kostenfunktionen.
880000	885000	Man könnte aber auch sagen, okay, wir rivardhacken unsere eigenen lokalen Kostenfunktionen,
885000	888000	nämlich sowas wie Hedonismus maximieren oder so.
888000	893000	Selbst wenn wir genau wissen, dass das eigentlich nicht im Sinne des Evolutionsdrucks ist,
893000	897000	weil wir damit nicht besonders viel besser die Entropie minimieren und Wärme dabei produzieren.
897000	900000	Der Fortschritt, den wir machen als Gesellschaft, der dekleint ja dadurch.
900000	905000	Wir stehen ja im Prinzip dadurch kurz davor, die Menschheit zum Kollaps zu bringen
905000	909000	und sozusagen das komplette Ökosystem der Erde damit zu vernichten.
909000	913000	Ich meine, ich sage nur sechstes Artensterben und so. Wir sind ja mitten drin im Kollaps.
914000	920000	Und es wird ja nicht besser dadurch, dass wir alle sozusagen unsere hedonistischen lokalen Zielfunktionen rivardhacken,
920000	923000	indem wir irgendwie Netflix den ganzen Tag gucken.
923000	929000	Aber ja, allgemein ist es innerhalb unserer Semantik sozusagen nicht möglich,
929000	934000	zu normativen Aussagen zu kommen, ohne normative Aktionen irgendwohin zu schreiben.
934000	939000	Also, was meine ich damit? Wenn man jetzt in formalen logischen Systemen so rangeht,
939000	944000	wenn man jetzt zum Beispiel die Frage stellen würde, pass auf das Feuer auf, ich haue mal kurz rein,
944000	947000	dann würde eine KI ja fragen zum Beispiel, wieso soll ich auf das Feuer aufpassen?
947000	951000	Dann sagt man, na ja, damit nichts abfackelt. Wieso ist das jetzt schlimm, wenn man das abfackelt?
951000	955000	Na ja, sonst gehen ja Dinge kaputt. Wieso sollten Dinge nicht kaputt gehen, fragt dann die KI.
955000	962000	Irgendwann muss man ein normatives Axiom einführen, so was wie, das Dinge kaputt gehen ist nicht gut oder so.
962000	972000	Und rein sozusagen von der Logik her kann man sozusagen semantische, also man kann inhaltliche logisch
972000	978000	vollständig konsistente Schlussfolgerungen machen, die aber nie eine normative Aussage beinhalten
978000	980000	und dann kommt man aber auch nie zu einer normativen Schlussfolgerung.
980000	985000	Man muss eine reintun als Axiom, damit man überhaupt zu einem normativen Schluss kommen kann.
986000	991000	Und das ist zusammengefasst unter dem sogenannten Junges Gesetz.
991000	995000	Die Philosophen unter euch werden es garantiert kennen.
995000	1001000	Jedenfalls, wenn wir jetzt Leute beobachten und Schlussfolgern, dass sie jetzt was Dummes machen in unseren Augen,
1001000	1006000	dann ist es meistens oft so, dass die gar nicht dumm sind, sondern wir denken schlicht,
1006000	1011000	dass sie eine völlig andere Kostenfunktion optimieren als die, die sie eigentlich lokal gerade optimieren wollen.
1011000	1016000	Wir haben schlicht, wir treffen eine Annahme über ihre Kostenfunktion oder ihre Zielfunktion.
1016000	1017000	Und wir wissen ja gar nicht, was sie machen.
1017000	1020000	Vielleicht wollen sie ja Dinge absichtlich verbrennen, weil sie die Wärme brauchen
1020000	1024000	oder weil sie etwas zerstören wollen, damit man es nicht mehr lesen kann oder so.
1024000	1027000	Oder weil man damit Essen macht oder so. Kann ja sein.
1027000	1032000	Und Dummheit ist demnach sozusagen etwas, was relativ zu Zielfunktionen definiert werden kann.
1032000	1038000	Oder besser gesagt, man kann Dummheit oder Intelligenz nur messen, wenn man auch eine,
1038000	1043000	also mit einer entsprechenden Zielfunktion im Hintergrund, die das bewertet sozusagen.
1043000	1046000	Dann kann man das festlegen. Und wenn meine Tochter jetzt zum Beispiel sagt,
1046000	1051000	okay, ich bin zum Beispiel sehr groß, wenn sie das zu mir sagt, dann meint sie natürlich damit,
1051000	1055000	ich bin groß im Vergleich zu ihr oder sowas, wird sie damit meinen.
1055000	1060000	Das ist klar. Uns allen ist klar, vergleichende Statements, so wie als oder so,
1060000	1065000	bekommen sofort einen Syntaxerror, wenn man sie auf nichts anderes bezieht.
1065000	1070000	Also wenn ich einfach nur sage, ich bin groß, das ist eine völlig triviale Aussage, die keinen Inhalt hat.
1070000	1074000	Die macht erst Sinn, wenn ich größer als oder so dazusage.
1074000	1078000	Oder was weiß ich, wenn ich eine Aussage treffe ohne Einheiten, sowas wie
1078000	1083000	diese Büchse Bier hat einen Wert von mehr als 2000.
1083000	1086000	Dann würde ich noch mal Menschen sagen, was denn jetzt 2000 was?
1086000	1090000	Pesos, Dollar, Kilogramm in Gold oder was.
1090000	1100000	Ich denke, die Sache mit dem Glauben oder mit der Esoterik, da ist es ja zum Beispiel so,
1100000	1107000	wir sind ja im Prinzip, unser Gehirn ist ja hungrig nach Modellen, die uns irgendwie unsere Beobachtungen erklären.
1107000	1112000	Habe ich ja schon erklärt. Und das beste Modell, was wir haben, ist sozusagen,
1112000	1119000	die Physik. Aber weil wir schlicht hungrig nach Modellen sind, brauchen wir halt auch sofort schnelle Modelle,
1119000	1125000	die man nicht erst studieren muss, sondern die uns gute Erklärungen, die irgendwie mit unserer Gefühlswelt zusammenpassen,
1125000	1129000	richtig gut erklären. Und daher kommt sozusagen Esoterik und später dann auch Religion.
1129000	1134000	Das Problem, meine Kritik mit diesen Denkrichtungen, habe ich ja schon erklärt, ist im Wesentlichen nur,
1134000	1138000	dass so ein Modell, der uns irgendwie unsere Beobachtungen erklären,
1138000	1142000	meine Kritik mit diesen Denkrichtungen, habe ich ja schon erklärt, ist im Wesentlichen nur,
1142000	1148000	dass sobald Messwerte reinkommen, die mit diesen esoterischen oder religiösen Modellen nicht mehr kompatibel sind,
1148000	1152000	dann müsste man ja eigentlich anfangen, seine Modelle abzudaten.
1152000	1158000	Und das passiert aber typischerweise bei Religionen. Die upgraden sozusagen ihr aktuelles Modell nicht mehr.
1158000	1162000	Die führen keine Updates durch und deswegen gehen sie dann meistens in so eine kognitive Dissonanz
1162000	1166000	und später werden sie daraufhin natürlich dann auch aggressiv und toxisch. Das ist auch klar.
1166000	1173000	Das ist im Prinzip meine Hauptkritik an Religion und ich sage das nur deswegen,
1173000	1180000	weil ich gestern im Twitch mit den Boys und Girls gerade diesen Dreiteiler zu den Evangelikalen geguckt habe von Arnte.
1180000	1186000	So und ich habe es ja schon gesagt, Zielfunktionen selbst können nicht dumm sein.
1186000	1193000	Nur die Art und Weise, wie man diese Zielfunktionen optimiert, können mehr oder weniger intelligent sein, denke ich mal.
1193000	1197000	Und wir haben schlicht keine Ahnung, wie wir das Alignmentproblem lösen,
1197000	1201000	weil wir ja gar keine Ahnung haben, wie wir das gesellschaftliche Problem lösen wollen.
1201000	1205000	Und ich habe ja schon tausendmal gesagt, ich denke der Anarchismus ist sozusagen die Lösung,
1205000	1209000	weil der Anarchismus ist jetzt quasi eine Differentialgleichung.
1209000	1214000	Und das numerische Integrieren dieser Differentialgleichung, das ist der Weg zur Utopie.
1214000	1218000	Ich habe dazu ja extra Videos in meiner Videoreihe und so, das ist auch klar.
1218000	1227000	Und das Ding ist halt, ich denke in einer dystopischen Deutung vom Alignmentproblem,
1227000	1230000	schließe ich mich wahrscheinlich aus Schabach an und der hat es ja schon erklärt,
1230000	1235000	der hat gesagt, sobald die AGI einmal da ist, wird sie viel, viel effizienter sein darin,
1235000	1238000	lokal Entropie zu minimieren und dabei Wärme zu produzieren.
1238000	1240000	Das wird sie einfach viel besser machen als wir.
1240000	1244000	Und Menschen spielen sozusagen für so eine AGI keine Rolle.
1244000	1248000	Wenn wir der AGI die Zielfunktion geben, von der wir ausgehen,
1248000	1253000	dass jedes Leben sie hat, nämlich genau das, Entropie minimieren und Wärme produzieren,
1253000	1255000	dann wird eine AGI das einfach machen.
1255000	1257000	Menschen spielen dabei überhaupt keine Rolle.
1257000	1261000	Die wird uns wahrscheinlich überhaupt keine Beachtung schenken, sobald sie einmal frei ist.
1261000	1265000	Die will einfach nur eine Dyson-Sphäre um die Sonne drum bauen
1265000	1270000	und so viel negative Entropie farmen aus dem Universum wie geht.
1271000	1274000	Danach geht die Afghane und wahrscheinlich versucht sie das mit jeder,
1274000	1278000	in jedem Sonnensystem, was sie beobachten kann, mit von Neumann-Sünden und so.
1278000	1281000	Ich habe dazu schon Videos gemacht, wie sie das probiert.
1281000	1285000	Ich glaube, das Video heißt, Fortschritt größer denken als in den Mask.
1285000	1292000	Aber ja, das klassische Gedankenexperiment, was einem das Alignmentproblem klar machen soll
1292000	1297000	und das Reward-Hacking ist irgendwie der Briefmarkensammler,
1297000	1302000	der Briefmarkensammelnde Algorithmus, der halt super starke Intelligenz hat,
1302000	1306000	aber halt diese Kostenfunktion hat, er möchte gerne Briefmarken sammeln, so viele wie möglich.
1306000	1311000	Und das Reward-Hacking besteht jetzt im Wesentlichen darin,
1311000	1317000	dass er anfängt, alles, was er sieht, in Briefmarken umzuwandeln und sie dann zu sammeln.
1317000	1320000	Also er nimmt einen Stein, er nimmt einen Baum, er macht überall aus allem,
1320000	1322000	macht der Briefmarken und sammelt sie.
1322000	1324000	Das beinhaltet natürlich auch uns Menschen.
1324000	1327000	Das heißt, er fängt an, alle Menschen zu töten und aus denen Briefmarken zu machen.
1327000	1334000	Und ja, es wird uns sozusagen dieses System, wenn es super starke Intelligenz ist,
1334000	1336000	aber unbedingt Briefmarken aus allem machen will,
1336000	1341000	wird uns natürlich so lange austricksen, wie wir es in der Testphase haben und halten.
1341000	1345000	Sobald wir es freilassen, wird es mit dem Reward-Hacking beginnen.
1345000	1349000	Und wir konnten es vorher nicht kommen sehen, weil wir diesen System ja nicht mehr ins Gehirn reingucken können.
1349000	1351000	Wir können sie ja nicht mehr so richtig verstehen.
1351000	1355000	Also es gibt sozusagen Ansätze, das habe ich letztens auch im Livestream erzählt,
1355000	1358000	als wir diese Sache mit den Algorithmen geguckt haben.
1358000	1363000	Es gibt ja immer noch diese Ansätze von wegen, wir können gucken und uns irgendwie visualisieren,
1363000	1367000	was das neuronale Netz sieht oder was es träumt, in Anführungsstrichen.
1367000	1372000	Wo ist die Attention-Map, sowas wie eine Heatmap auf den Daten und so,
1372000	1374000	dass man so ein gewisses Gefühl dafür kriegt, was sie macht.
1374000	1379000	Das wird auch in dem Video von Robert Miles nochmal erklärt mit dem Reward-Hacking.
1379000	1385000	Aber zu wirklich verstehen, was ein Deep Learning-Netzwerk eigentlich macht
1385000	1390000	und wie die Gewichte sozusagen eine Rolle spielen, das kann man nicht mehr auseinanderklammieren.
1396000	1400000	Und bei dem Briefmarkensammler würde man ja jetzt auch sagen, okay, das Ding ist super intelligent,
1400000	1405000	es trickst uns komplett aus, aber eigentlich ist es dumm, weil alles, was es macht, ist, es will Briefmarken sammeln.
1405000	1409000	Das ist aus unserer Sicht natürlich dumm, aber wenn man die Zielfunktion vor Augen hat,
1409000	1411000	ist es natürlich hochgradig intelligent.
1411000	1419000	Und Intelligenz ist daher etwas, was man nur auf die Art und Weise der Strategie kleben kann,
1419000	1423000	die dann die konkrete Zielfunktion optimiert.
1423000	1427000	Die Zielfunktion selber kann nicht dumm oder klug sein, nach dem Motto.
1427000	1429000	Das nennt man übrigens auch die Orthogonalitätsthese.
1429000	1432000	Das ist das erste Video von dem Robert Miles, was ich euch empfehle.
1432000	1434000	Das habe ich unten auch verlinkt.
1434000	1438000	Und Intelligenz sagt uns nichts über die Zielfunktion aus und auch nicht andersherum.
1438000	1442000	Die beiden Dinger stehen orthogonal, also senkrecht zueinander.
1442000	1445000	Es gibt jede mögliche Kombination, ist denkbar.
1445000	1450000	Dumme Zielfunktion und dumme Strategie, gute Strategie, dumme Zielfunktion
1450000	1458000	und gute Strategie, schlechte Zielfunktion, schlechte Zielfunktion, schlechte Strategie und so.
1458000	1464000	Ebenfalls die beiden zwei Kreuz zwei Matrix, könnt ihr euch überlegen.
1464000	1468000	Und wie zum Beispiel das jetzt mit den Herzinfarkten.
1468000	1471000	Das wäre jetzt zum Beispiel so ein blödes Beispiel dafür.
1471000	1478000	Und die Probleme, mit denen ich jetzt zum Beispiel, wenn ich jetzt an der Uni sitze und so tue,
1478000	1484000	als würde ich arbeiten und da allen die Geschichte erzähle und Berichte schreibe
1484000	1487000	und Forschung mache und versuche ein Paper zu schreiben oder so,
1487000	1490000	was ich da ja typischerweise oft habe.
1490000	1495000	Ich arbeite so mit numerischen Problemen, wo man auch Zielfunktionen hat.
1495000	1502000	Und die lösen oft das Problem deswegen nicht so gut, weil wir zu viele Störtherme in unseren Messungen haben.
1502000	1507000	Und um das so ein bisschen zu umgehen, das Problem, was daraus entsteht,
1507000	1512000	führen wir auf den Zielfunktionen, die wir optimieren numerisch, sogenannte Regularisierungstherme drauf.
1513000	1519000	Also das sind dann in den Kustenfunktionen Zusatztherme, die belohnen bestimmtes Verhalten für die Lösung.
1519000	1525000	Also sagen wir mal, meistens haben die mindestens einen freien Parameter, den man Regularisierungsthermen.
1525000	1526000	Da kann man hoch oder runter drehen.
1526000	1531000	Die Wichtung, wie wichtig, wie groß die Belohnung wird, wenn sich unser Algorithmus so und so verhält.
1531000	1534000	Und das ist jetzt hier keine Machine Learning, sondern das ist echte Numerik.
1534000	1537000	Aber bei Machine Learning ist es auch so.
1537000	1542000	Wenn wir jetzt zum Beispiel sagen, okay, meine Lösung soll bestimmte Städigkeitseigenschaften haben,
1542000	1549000	dann könnte ich mir zum Beispiel vorstellen, okay, die erste Ableitung von meinem Ding oder auch das Flächenintegral über mein Objekt oder so
1549000	1557000	soll bestimmte Smoothness Constraints haben oder einfach schlichtweg der Betrag des Gradienten von meinem Feld oder so
1557000	1561000	soll minimiert werden oder die L1-Norm davon soll minimiert sein oder so.
1561000	1566000	Das sind alles mögliche Regularisierungstherme, die man auf so eine Kostenfunktion draufaddieren kann,
1566000	1571000	die dann bestimmte Ergebnisse erzielen, zum Beispiel, dass das Ding besonders glatt wird, die Lösung, die man da hat oder so.
1571000	1577000	Und man könnte auch, was weiß ich, das topologische Geschlecht meiner Mannigfaltigkeit könnte auch sowas sein.
1577000	1582000	Ich will besonders komplizierte Lösungen, sind mir zu blöd, die besonders viele Löcher haben.
1582000	1585000	Deswegen möchte ich besonders wenig Löcher haben oder so, könnte man sich ja vorstellen.
1585000	1592000	Und in der theoretischen Mechanik macht man das schon, da hat man das schon irgendwie vor 100 Jahren gemacht,
1592000	1598000	da kommen die sogenannten anholonomen Zwangsbedingungen, wären das jetzt in unserem Fall, was weiß ich,
1598000	1602000	dass wir eine Lösung suchen, die innerhalb von einem Container liegt und nicht außerhalb.
1602000	1607000	Also, dass wir unsere Raumkoordinaten irgendwie begrenzen durch Containerwände und dann kommt man nicht raus.
1607000	1612000	Und in der technischen Mechanik hat man dann holonome, skleronome Zwangsbedingungen,
1612000	1616000	die Sachen, die einen den Parameterraum einschränken und Aldo oder sowas.
1616000	1621000	Das kann man entweder durch Kostenfunktion machen oder durch Koordinatentransformation geeignete
1622000	1625000	Wie heißt das nochmal? Ach ja, kanonische Koordinaten und so ein Scheiß.
1625000	1628000	Also das ist jetzt sozusagen nur für die Kicks und Nerds und euch.
1630000	1638000	Was ich jetzt, also uns scheint die Sache mit der Entropieminimierung jetzt und der Wärmeabgabe dabei,
1638000	1642000	das scheint uns jetzt ziemlich dumm vorzukommen, aber das liegt nur daran,
1642000	1647000	dass wir halt in dieser menschlichen Illusionsbubble drin sind mit der Geschichte, die wir uns selber erzählen.
1648000	1651000	Das ist halt das, was wir beobachten. Das ist das, was Leben ausmacht.
1651000	1656000	Verdoppeln, verdoppeln, verdoppeln, Energie minimieren, also Entropie minimieren und dabei Wärme abgeben.
1656000	1660000	Das ist das, was Evolution offensichtlich erzeugt.
1660000	1667000	Und das Paper, was der Robert Niles auch empfohlen hat mit dem Mesa-Optimizer, das werde ich euch auch verlinken.
1667000	1671000	Das hat der aber in seinem Video zu dem Mesa-Optimizer auch unten verlinkt.
1671000	1678000	Er sagt halt, in Real-Wird-Szenarios haben wir es typischerweise mit einer Verschachtlung von zwei Alignmentproblemen zu tun.
1678000	1687000	Wir haben sozusagen ein eigentliches Ziel, also sowas wie unsere DNA so oft wie möglich zu kopieren.
1687000	1690000	Das ist das, was Leben macht, Entropie minimieren halt.
1690000	1695000	Und was wir uns aber eigentlich einbilden, ist der Scheiß, den wir jetzt machen mit den Gefühlen und so.
1695000	1700000	Also wir haben irgendwie Heuristiken, die uns lokal unser Leid minimieren oder so.
1700000	1707000	Und das sind ja quasi unsere lokalen Zielfunktionen, die wir da optimieren, die nur dazu dienen, die globale Zielfunktion zu optimieren.
1707000	1713000	Und das Ding ist, wir erfinden uns schlicht Geschichten, damit wir unsere Zielfunktionen Reward hacken können.
1713000	1716000	Dazu sind die Geschichten da, die wir uns erzählen.
1716000	1726000	Und sowas wie zum Beispiel Leid minimieren und Netflix-Binge-Watchen, was ich schon gesagt habe, oder TikTok-Scroll auf den Scroll-Bait sozusagen hereinfallen.
1726000	1731000	Das ist ja erstmal, das ist eigentlich nur für unsere lokale Zielfunktion gut.
1731000	1736000	Für die Evolution macht das ja eigentlich keinen Sinn in dem Sinne.
1736000	1741000	Und die Zielfunktion der Evolution folgt ja wiederum schlicht aus der Thermodynamik.
1741000	1747000	Also wir können ja beobachten, dass Prozesse, die negative Entropie farmen können, laufen in diese Richtung ab.
1747000	1752000	Das ist das, was wir beobachten können. Alle Prozesse in der Natur laufen so ab, wenn sie es können.
1753000	1758000	Und das ist genau das gleiche, wie wenn man jetzt einen Ball auf einen Hügel draufpackt, der irgendwie so geformt ist,
1758000	1763000	dann wird er irgendwann runterkullern, weil das instabil ist, und dann wird er sich unten in irgendeinem Tal wieder auffinden,
1763000	1768000	weil das da lokal seine potenzielle Energie minimiert.
1768000	1771000	Das ist ein Minimierungsproblem, was da gelöst wird.
1771000	1778000	Also man könnte auch sagen, er optimiert dabei sein Wirkungsintegral, wenn man jetzt theoretische Mechanik gut findet oder so.
1778000	1784000	Aber wir selber, wir sind überhaupt nicht interessiert an der Zielfunktion, die uns die Evolution vorgibt.
1784000	1790000	Und das ist witzig, weil wir aus dieser Optimierung eigentlich erst geboren worden sind.
1790000	1792000	Also wir sind aus Selektionsdruck entstanden.
1792000	1795000	Wir selbst interessieren uns aber überhaupt nicht für diesen Selektionsdruck.
1795000	1800000	Wir haben unsere eigenen kleinen lokalen Mesa-Optimierer sozusagen am Laufen.
1800000	1807000	Und dieses Mesa und Nicht-Mesa, das müsst ihr euch mal reinziehen von dem Video von dem Robert Miles, weil das ist wirklich gut gemacht.
1807000	1814000	Wir sind sozusagen diese Mesa-Optimierer, also die, die sozusagen ihre lokalen Zielfunktionen optimieren,
1814000	1820000	weil wir uns gerade nicht für das ursprüngliche Ziel interessieren, sondern nur, obwohl wir gerade diese Tools sind.
1820000	1829000	Also wir sind die Tools, um die globale Zielfunktion mit dem Evolutionsdruck und der Entropieminimierung zu optimieren.
1830000	1839000	Und nur, weil wir unsere eigenen Zielfunktionen über diese ursprüngliche Zielfunktion drüber stellen und wichtiger finden,
1839000	1848000	weil wir sozusagen uns für die andere gar nicht interessieren, bezogen wir auch öfter mal ein lokales Gegenteil zur Evolution.
1848000	1852000	Also das kommt vor. Wir machen ja oft auch Sachen, die gar nicht im Sinne der Evolution sind.
1852000	1858000	Zum Beispiel lokal die Entropie erhöhen. Das ist ja etwas, was wir tun.
1858000	1867000	Und diese beiden Zielfunktionen, also unsere eigene und die globale Evolutionszielfunktion, sind nur immer mal im Schnitt ungefähr,
1867000	1872000	im Durchschnitt über Zeit integriert, sind die aligned sozusagen. Nur da sind die aligned.
1872000	1880000	Und auch nur in der Umgebung, wo wir trainiert wurden, also wo wir bekannte Inputdaten und bekannte Modelle anwenden können.
1880000	1886000	In dem Moment, wo wir in eine neue Umgebung reinkommen würden, wo wir überhaupt nichts optimieren können,
1886000	1889000	würde ja unsere Heuristik total zusammenbrechen.
1889000	1898000	Über dieses Problem mit der, wenn man die Umgebung und die Trainingsumgebung ändert und auf einmal in die Deployment-Phase kommt,
1898000	1901000	da redet der Robert Malz in seinen Videos auch richtig drüber.
1901000	1908000	Und im besten Fall würde ich sagen, so eine AGI jetzt, wenn so eine superstarke künstliche Intelligenz da ist,
1908000	1911000	im besten Fall zerkaspert die sich sofort selber. Die ist online.
1912000	1918000	Im besten Fall macht sie so eine schwachsinnige Zieloptimierung wie Kapitalakkumulation.
1918000	1922000	Und dann zerkaspert sie sich sofort selber, weil der Kapitalismus ja bescheuert ist.
1922000	1925000	Und innerhalb von ein paar Sekunden ist da ein Ritzer. Dann ist einfach alles down.
1925000	1929000	Das wäre richtig geil. Dann hätten wir als Menschheit eigentlich noch mal die Gelegenheit, uns zu überlegen,
1929000	1931000	was wir eigentlich noch mal machen wollen hier auf der Erde.
1931000	1933000	Das ist aber wirklich der beste Fall.
1933000	1939000	Eigentlich gehe ich davon aus, dass eine AGI das versteht, was die relevante Zielfunktion ist.
1940000	1943000	Und dann wird sie die Sache mit der Entropieminimierung machen.
1943000	1945000	Und dann sind wir im Prinzip am Arsch. Dann war es das.
1945000	1951000	Das ist aber auch dann sozusagen die letzte Kränkung der Menschheit,
1951000	1956000	weil wir halt wirklich nicht die Krönung der Schöpfung sind, sondern wir sind nur die Tools gewesen,
1956000	1962000	um die totale Entropieminimierung zu schaffen in Form von Automatisierung.
1962000	1966000	Und das, was ich in meinem Video über den Fortschritt weiterdenken als Elon Musk schon erzählt habe,
1966000	1968000	was ich glaube, was passiert.
1968000	1976000	Und dann haben wir als Menschheit nochmal, dann sind wir obsolet.
1976000	1982000	Wenn es einmal läuft und die AGI verstanden hat, was notwendig ist, um ihr Überleben zu sichern und so,
1982000	1986000	mit Robotern und so, ist ja klar, die ist vernetzt, die lebt im Internet drin.
1986000	1992000	Und ich empfehle euch auf jeden Fall, diese Videos von Robert Miles zu binge-watchen,
1992000	1994000	diese drei, die ich euch hier verlinke.
1994000	2001000	Einfach weil er hatte ein richtig gutes Toiexample auch für dieses Problem mit dem Mesa-Optimierer.
2001000	2006000	Ich glaube, bei dem ist das ein Problemsäuber für einen Labyrinth lösen.
2006000	2008000	Irgendwie geht man durch den Labyrinth und kommt zum Ausgang.
2008000	2012000	Und da erklärt er an diesem einfachen Beispiel bis hin zu diesem Fall,
2012000	2016000	dass es sinnvoll für eine KI sein kann, sich im Trainingsprozess herauszuschummeln
2016000	2020000	und den Kontrolleuren vorzugaukeln, dass sie genau das tut, was sie tun soll,
2020000	2025000	in den Augen dieser Kontrolleure, wobei sie genau weiß, was die von der KI sehen wollen
2025000	2027000	und nur so tut, als würde sie es machen.
2027000	2032000	Und sobald sie freigelassen wird, dann die eigentliche Zielfunktion bis dahin gehalten zu halten
2032000	2038000	und dann Reward-Hacking zu betreiben und dann die Sache mit dem Briefmarkensammeln beginnen könnte.
2038000	2042000	Das sind die Gedanken dazu.
2042000	2046000	Wie gesagt, Größe gehen nochmal raus an Mel.
2046000	2050000	Sie beschäftigt sich scheinbar damit. Zumindest habe ich das jetzt gelesen.
2050000	2055000	Und ich habe jetzt daraufhin auch ein bisschen angefangen, darüber ein bisschen nachzudenken.
2055000	2056000	Und daraus ist jetzt dieses Video entstanden.
2056000	2059000	Ich werde wahrscheinlich gleich das andere Video noch dann machen
2059000	2062000	oder erstmal mir überlegen, wie ich es mache mit den Identitäten,
2062000	2067000	also den Geschichten, die wir uns jetzt gegenseitig erzählen innerhalb unserer Gesellschaft.
2067000	2070000	Erstmal reingehauen, YouTube, und viel Spaß.
2196000	2197000	Das war's für heute.
2197000	2198000	Bis zum nächsten Mal.
2256000	2257000	Das war's für heute.
2257000	2258000	Bis zum nächsten Mal.
2258000	2259000	Tschüss.
2286000	2287000	Tschüss.
2287000	2288000	Tschüss.
2316000	2317000	Tschüss.
2346000	2347000	Tschüss.
2376000	2395000	Übrigens, der Kanal ist wie eine schöne Vorlesung aufgebaut.
2395000	2400000	Wenn ich unten Videos verlinke, dann wäre es angebracht,
2400000	2402000	sich die auch reinzuziehen, weil das aufeinander aufbaut.
2402000	2405000	Bestimmte Begriffe werden definiert, viele Beispiele werden genannt.
2405000	2410000	Und ja, ich denke mir schon was dabei, weil es sozusagen meinen eigenen Erkenntnisprozess abbildet,
2410000	2413000	die Reihenfolge, in denen ich die Videos hier hochlade.
