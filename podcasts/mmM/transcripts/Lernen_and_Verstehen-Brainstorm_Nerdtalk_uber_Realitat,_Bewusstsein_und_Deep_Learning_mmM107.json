{"text": " Das Gehirn erz\u00e4hlt sich ja selber eine Geschichte dar\u00fcber, wie es w\u00e4re, wenn es eine Person g\u00e4be, von die so ist wie wir und die in einer Situation ist, die wir Gegenwart nennen. Und wir konstruieren, w\u00e4hrend wir den sensorischen Input \u00fcber die Gegenwart bekommen, diese Person und stellen uns vor, wie es w\u00e4re, so eine Person zu sein. Wir simulieren das. Das ist das, was das Gehirn macht. Deswegen sagt er, nur eine Simulation kann \u00fcberhaupt Bewusstsein haben. Riesen Sorry schonmal an alle, die sich dieses Video jetzt hier rein g\u00f6nnen. Ich glaube, es ist absolut unm\u00f6glich zu verstehen, was ich jetzt hier in diesem Video quatsche, wenn ihr mein Video zum Bewusstsein nicht zuerst gesehen habt. Das baut er halt darauf auf und ich benutze viele komische Fremdworte. Ich glaube, die ergeben alle \u00fcberhaupt keinen Sinn, wenn ihr das Video mit dem Bewusstsein nicht vorher geguckt habt. Das ist nur so schon mal als Warnung. Das ist ein absolutes Nerd-Video. Ich habe das gestern aufgenommen und da hatte ich auch schon einen im Tee. Und jetzt habe ich mir halt angeguckt und dachte mir so, ja, okay, schon klar. Aber das kann glaube ich keiner verstehen, wenn man nicht irgendwie das mindset daf\u00fcr hat. Und daf\u00fcr ist das andere Video auch gut geeignet. Also guckt euch vorher an, bevor ihr jetzt hier weiter guckt. So Leute, jetzt mal wieder eins von diesen Deep Philosophical U-Bow-Videos des Todes und der Vorl\u00e4ufer zu diesem Video, w\u00fcrde ich sagen, ist das Video zum Bewusstsein. Und ich m\u00f6chte hier erst mal mit euch \u00fcber Emergenz reden, weil das braucht man irgendwie st\u00e4ndig. Und dann so ein bisschen mehr \u00fcber, naja, was ist die Realit\u00e4t? Was ist das Bewusstsein? Und was soll das alles? Einfach mal ein bisschen euch mal ein bisschen mal was rein knallen an krasser Erkenntnis, die ich so in den letzten Jahren, ich w\u00fcrde sagen im letzten Jahr so ungef\u00e4hr hatte. Und ja, also was ist jetzt Emergenz? Ich habe schon ein paar Mal verwendet das Wort. Emergenz ist immer dann etwas, wenn ihr etwas beobachtet, was erstmal total kompliziert wirkt, wenn ihr da reingucken wollt. Von au\u00dfen sieht es irgendwie krass aus und ihr denkt, okay, das hat irgendwie einen Mechanismus, das ist heftig, aber wenn man reinguckt, steht man fest, es besteht aus ganz vielen simplen Teilen. Und irgendwie ist die Summe dieser Teile mehr als einfach nur, also die Summe der Teile ergibt mehr als einfach nur das, was man sich vorstellen w\u00fcrde, wenn man die Summe der Teile tats\u00e4chlich addieren w\u00fcrde. Was meine ich damit? Na ja, zum Beispiel ein Ameisenhaufen. Die kleinen Ameisen, die sind alle sehr einfach, wie die funktionieren. Trotzdem ist die Gesamtheit der Ameisen, das Konglomerat, so eine Art komplizierter Lebens, ein komplizierter Metabolismus, der sich verh\u00e4lt wie ein Lebewesen. Und wenn das Objekt, was ihr seht, nur aus Verbindungen besteht, die irgendwie auch rein virtuell dadurch sind, die sind ja nicht anfassbar. Man kann ja nicht sagen, ach, diese Wechselwirkung zwischen den Ameisen, die macht das erst so interessant. Aber genau darum geht es. Das ist die Emergenz. Das ist etwas, was man nicht so greifen kann, sondern es entsteht als ein Prozess h\u00f6herer Ordnung zwischen Wechselwirkungen von einfacheren Mechanismen, die man sehr wohl verstehen kann. Und darum ist Emergenz nicht greifbar. Und das ist immer das Problem. Das seht ihr ganz oft. Das ist immer, wenn es um gesellschaftliche Themen geht. Gesellschaft ist zum Beispiel eine emergente Struktur. Sie entsteht aus den ganz vielen Interaktionen zwischen Menschen. Wenn wir die alle addieren, kriegen wir die Gesellschaft raus. Und Evolution ist auch so was. Evolution entsteht ja nur dadurch, dass wir einen Selektionsdruck haben, der nur dadurch entsteht, dass es sozusagen eine \u00e4u\u00dfere Umgebung gibt, die sozusagen dazu f\u00fchrt, dass bestimmte Lebensformen, die besser, ich w\u00fcrde mal sagen, negative Entropie farmen k\u00f6nnen oder besser den Gradienten in die Entropie runtergehen k\u00f6nnen als andere und dadurch sozusagen einen Wettbewerbsvorteil gegeneinander haben und sich dann durchsetzen. Und Schwarmintelligenz ist auch so was wie Emergenz. Und der Markt, das ist das, was die FDP-Menschen nicht verstehen. Die raffen es ja nicht. Aber der Markt ist eine emergente Struktur. Und weil das so unbegreifbar ist, dachte damals Hayek auch, und das ist sozusagen so eine Art Gott f\u00fcr ihn und versteht mich da nicht falsch. Es ist nat\u00fcrlich eine emergente Struktur und es ist nat\u00fcrlich interessant, dar\u00fcber nachzudenken. Aber nur weil es kompliziert ist oder irgendwie geil aussieht oder man ein bisschen Mathematik damit machen kann, hei\u00dft es ja nicht, dass das sozusagen die L\u00f6sung aller Probleme ist. Aber genau so geht Hayek daran. Das habe ich auch schon mal in einem anderen Video erkl\u00e4rt, von wegen welches Problem l\u00f6sen die M\u00e4rkte eigentlich. So und was soll ich sagen, wenn es etwas in den letzten Jahren gab, was ich mir sehr gerne reingezogen habe, dann sind das Videos von Joscha Bach. Und ich kann euch wirklich nur empfehlen, schmei\u00dft alles in die Ecke. Jo Roggen Podcasts oder alle m\u00f6glichen Lex Friedman Interviews oder sonst welche Sachen, die ihr im englischen YouTube drin gesehen habt. Das n\u00e4chste, was ihr bei euch auf die Watchlist drauf tut, ist der erste Talk von Joscha Bach. Und der zweite, den k\u00f6nnt ihr gleich noch hinten dran h\u00e4ngen. Dann habt ihr sechs Stunden lang komprimierteste Todesweisheit des Todes. Der Typ ist der lebende Sheldon Cooper. Das ist der krasseste Wichser, den ich je irgendwo geh\u00f6rt habe. Ich glaube, das ist so ein verdammtes Genie. Und der lebt heutzutage und ich habe ihn schon auf Twitter vollgequatscht, er soll mal mit mir reden, aber nat\u00fcrlich, warum sollte der das tun? Der hat einfach eine volle Inbox, der hat keine Zeit mit einem schei\u00df Ossi zu reden. By the way, Joscha Bach ist selber ein Ossi. Aber ihr m\u00fcsst euch den Typen reinziehen. Der alles, was er jemals in seinem Leben gelesen hat, hat er hier oben drin und kann sofort abrufen. Der ist krass, der Typ. Und ich bin \u00fcbrigens nur auf den gesto\u00dfen, weil jemand mir den empfohlen hat unter einem englischen Matrix-Video. Hat mir jemand Joscha Bach empfohlen. Daf\u00fcr hat sich der englische Kanal schon gelohnt. Das ist super. Und jetzt wollte ich ein bisschen, jetzt haben wir das gekl\u00e4rt mit der Emergenz, ich habe jetzt dieses Wort etabliert. Jetzt will ich ein anderes wichtiges Wort f\u00fcr euch etablieren, weil das ist nat\u00fcrlich auch so ein Ding, wo die Leute immer rumschlavieren. Und irgendwie so so eine Kr\u00e4mer-Laber-Attit\u00fcde haben. Was ist jetzt eigentlich Intelligenz? Also guter Freund von mir hat damals zu mir gesagt, Intelligenz ist die F\u00e4higkeit, Analogien zu bilden. Und das ist eine ziemlich gute Definition, wenn man da mal dr\u00fcber nachdenkt, weil damit kann man eine ganze Menge erschlagen. Quasi ich habe hier, sagen wir mal, verstanden, wie das eine Ding funktioniert. Und jetzt habe ich hier ein anderes Ding, das hei\u00dft anders. Aber der Mechanismus, der hier abl\u00e4uft, ist genau derselbe Mechanismus wie in dem Ding, was ich schon verstanden habe. Wenn ich also die Analogie bilden kann und feststellen kann, dass abgesehen von den Bezeichnungen, dass da was da passiert, dasselbe oder das gleiche ist, dann habe ich eine Analogie gebildet und dann bin ich krass. Und der King der Analogien, meiner Meinung nach, ist Slavoj G. Jack. Der hat die witzigsten Analogien. So und Joscha Bach jetzt, der sagt, Intelligenz ist die F\u00e4higkeit, Modelle zu bauen. Also Modelle \u00fcber das, was man sieht. Und was meint man jetzt damit? Ein mathematisches Modell typischerweise hat was damit zu tun, dass man eine Komprimierung macht. Man kompressiert, man komprimiert irgendwie etwas, was man sieht, auf das Wesentliche herunter. Das ist das, was beim Komprimieren passiert. Und da muss man nat\u00fcrlich sich genau fragen, in welcher Sprache habe ich es auf das Wesentliche, auf das Minimum runtergekocht? Und das ist genau die Fragestellung, die man beim Archivieren von Dateien auch hat. Kann man sozusagen ein Muster finden, auf die man es runterbrechen kann? Kann ich das Muster sozusagen einmal abspeichern und dann nur noch sagen, jetzt kommt das Muster und dann spare ich mir 500 Bits oder so? So eine Idee ist das. Und darauf basieren dann sozusagen die ganzen modernen Komprimierungsverfahren auch. Im Wesentlichen, in der Mathematik w\u00fcrde man sagen, man findet eine Vektorbasis oder im Kompress-Censing w\u00fcrde man sagen, man findet eine Basis, die irgendwie im Sinne der L2-Norm oder beziehungsweise in der L1-Norm, eigentlich in der L0-Norm, ich glaube es ist tats\u00e4chlich die L0-Norm oder die L1-Norm, da wo das sozusagen ein Minimum annimmt. Das ist quasi Kompress-Censing, aber nur f\u00fcr die Geeks und Nerds, die sich damit auskennen. Ansonsten, ihr k\u00f6nnt euch einfach vorstellen, wann kann ich sozusagen etwas aufs Wesentliche so weit runterbrechen, bis es nicht mehr weiter runtergeht? Bis ich es sozusagen auf die Einsen und Nullen runtergebrochen habe. Und Sprache ist sozusagen in diesem Kontext sozusagen eine High-Level-Abstraktion, weil sie ist sehr schwammig und Mathematik ist quasi Low-Level. Man kann sozusagen das eine in das andere \u00fcberf\u00fchren, aber es ist sehr langwierig. Bestimmte Sachverhalte lassen sich mit Sprache in ein, zwei S\u00e4tzen erkl\u00e4ren und man m\u00fcsste einen riesen Apparat Mathematik drauf werfen, um dasselbe in Mathematik auszudr\u00fccken. Joscha Bach sagte, Mathematik ist die Dom\u00e4ne aller Sprachen und mittlerweile glaube ich, da hat er nat\u00fcrlich recht. Das ist ziemlich krass, denn es ist so, Sprache in diesem Sinne, das Kompress-Censing zum Beispiel, ist eine Projektion von Inhalt. Wir haben hier dieses Ding, wir haben diese echten Inhalte, die sind kompliziert und mit Sprache vereinfachen wir die. Wir machen schon so eine Art Komprimierung, wir komprimieren schon den Inhalt, sodass wir ihn r\u00fcberreichen k\u00f6nnen und nat\u00fcrlich ist dann sozusagen Informationsverlust. Und das ist bei jeder Projektion, jeder der lineare Algebra schon mal geh\u00f6rt hat, wei\u00df, Projektionen verlieren Informationen und das ist da auch der Fall. Und das ist ja klar, Sprache kann beim Empf\u00e4nger anders ankommen, als wir es eigentlich als der Empf\u00e4nger, als der Aussender der Sprache es eigentlich sagen wollte. Und ja, Wittgenstein zum Beispiel dachte, dass man alles mit Sprache aufbauen kann und das war sozusagen sein Fail. Er hat sozusagen nicht runterbrechen k\u00f6nnen auf Mathematik, sondern er dachte Sprache ist sozusagen der Shit. Und deswegen sage ich ja auch, Wittgenstein h\u00e4tte die Sache mit dem Word Embeddings in Deep Learning auch richtig gut gefallen. Ja, aber so Sachen wie Bilder oder Geometrie mit Sprache zu beschreiben, ist halt v\u00f6llig Fail. Da ist v\u00f6llig klar, dass die etabliert, also die bessere Variante irgendwie Mathematik sein muss. Und jetzt wollte ich mal erkl\u00e4ren, was nennen wir eigentlich Realit\u00e4t bzw. was nennen wir unser Universum? Na ja, wir bezeichnen mit Realit\u00e4t typischerweise das, was unser Modell als Ursache angibt f\u00fcr den sensorischen Input, den wir erfahren. Das einzige, was unser Gehirn bekommt, ist der sensorische Input \u00fcber unsere Augen, unsere Sinnesvornehmungen und so. Wir wissen nichts \u00fcber die Welt da drau\u00dfen, au\u00dfer wir sind nur beschr\u00e4nkt darauf auf den sensorischen Input, den wir haben und daraus schlussfolgern wir alles. Und das Gehirn, das habe ich ja in meinem Bewusstheitsvideo schon erkl\u00e4rt, versucht sozusagen das mathematische schlechtgestellte inverse Problem zu l\u00f6sen. Was hat die Inputdaten verursacht? Wir wissen ja nicht, was die verursacht haben. Unser Gehirn, quasi unser Verstand, unser was auch immer, unser Gehirn versucht zu beschreiben und zu erkl\u00e4ren, was diese Inputdaten verursacht hat. Und dabei konstruieren wir das, was wir Realit\u00e4t nennen. Und das beste, was Urscha Bachter immer sagt, ist, es gibt in dieser, also die beste Sprache, die wir im Moment daf\u00fcr haben, ist sozusagen die Physik, die physikalischen Gesetze. Und in dieser Sprache gibt es keine Ger\u00e4usche und es gibt auch keine Farben. Das sind einfach nur h\u00f6here Ordnung von Strukturen, denen wir diese Bedeutung zuweisen im Sinne von einer Komprimierung. Und jetzt ist es so... \u00dcbrigens, das ist der Grund, wenn man jetzt irgendwie, sagen wir mal, ich nehme mir jetzt das Lineal, tue das hier auf den Tisch und knall hiergegen, dann h\u00f6r ich so ein Brrrrrrr. Und diese h\u00f6heren Schwingungsmoden und so, diese ganzen Sachen mit den Schwingungen, Sounds sind auch fraktale. Da gibt es ein ziemlich geiles Video von Adam Neely, falls ihr den kennt. Das ist ein sehr witziges Video, das werde ich verlinken. Dass sozusagen sich auch da durch die h\u00f6heren Schwingungsmoden, Sounds wie, also Ger\u00e4usche und T\u00f6ne wie fraktale verhalten. Also man kann da immer weiter reinzoomen und die werden dann immer wieder selbst \u00e4hnlich. Und das ist super geil, denn das bedeutet im Prinzip, dass man alles auf eine Ringstruktur abbilden kann. Deswegen kann man Farben auch in einem Ring anordnen und bei T\u00f6nen kann man die sozusagen auf diese Oktaven abbilden. Und das hat damit zu tun, dass sie sozusagen fraktale darstellen. Und in echt existiert das nicht. Das ist was, was unser Gehirn konstruiert, genauso wie, was wei\u00df ich, wenn wir irgendwie rumgucken mit unseren Augen, dann wird der Hauptinput, der wird interpoliert und auch extrapoliert hier am Rand und so. Unser Gehirn kann sozusagen die Daten, die eigentlich aufgenommen werden m\u00fcssten, um das, was wir hier sehen, zu konstruieren, kann es gar nicht verarbeiten. Das braucht das Gehirn nicht. Unser Gehirn ist so krass, dass es \u00fcbelst Rechenleistung investiert, um das alles zu interpolieren. Und in dem Sinne ist es halt so, Unser Gehirn leistet diese Komprimierungsarbeit schon f\u00fcr uns und diese periodischen Eigenschaften von zum Beispiel Ger\u00e4uschen und Farben machen, dass man die auf einem Ring anordnen kann. Und wir existieren jetzt, also das, was wir als wir bezeichnen, wir existieren in dieser virtuellen Realit\u00e4t, die unser Gehirn konstruiert, um zu erkl\u00e4ren, was die Messdaten verursacht hat. Und das habe ich ja in dem Video \u00fcber das Bewusstsein schon erkl\u00e4rt. Also unser Gehirn konstruiert diese virtuelle Realit\u00e4t und auf diesem, weil das Ding ist, auf diesem Level von diesem \u00e4u\u00dferst merkw\u00fcrdigen Quantengraf, den wir jetzt, von dem wir annehmen, dass es, dass der existiert durch unsere mathematischen Modelle und unsere Physik und so, da existiert wirklich nur kalte Algebra. Von einem Zustand des Universums zum n\u00e4chsten transformieren die physikalischen Gesetze diesen Quantengraf und wir k\u00f6nnen nur einen ganz kleinen Auschnitt davon messen. Durch unseren sensorischen Input. Und das, was die Philosophen dann sozusagen als Qualia bezeichnen, das ist genauso eine Illusion wie der Freie Wille. Das ist einfach nur eine emergente Struktur, die auf unserem Gehirn entsteht, Qualia. Und Joscha Bach nimmt jetzt als Beispiel f\u00fcr Realit\u00e4t, ziemlich geiles Beispiel, wie ich finde, das Mandelbrot-Fraktal. Vielleicht binde ich das jetzt hier auch in das Video ein, da gibt es bestimmt ein cooles Gift oder so. Das ist so ein Fraktal, das entsteht im Wesentlichen, wenn man eine iterative Abbildung l\u00f6sen will. Ja genau, man hat eine quadratische iterative Abbildung in einer komplexen Ebene und guckt f\u00fcr welche Werte von einer konstanten komplexen Zahl die konvergiert oder nicht. Und wenn man das sozusagen macht f\u00fcr die komplexe Zahlenebene und alles was divergiert mit schwarz abbildet, dann kriegt man dieses Fraktal und so baut man das. Das ist relativ einfach das zu bauen tats\u00e4chlich, aber das Muster was rauskommt, das sieht halt super krass aus. Und wenn man sich jetzt vorstellt, wir leben auf so einer fraktalen Oberfl\u00e4che von dem Mandelbrot-Fraktal, dann w\u00fcrden wir sehen, okay, da gibt es so eine rechtsdrehende Spirale. Wenn man die jetzt reingeht, w\u00fcrde man irgendwann an einer Singularit\u00e4t ankommen. Und wenn man die sozusagen, wenn man dar\u00fcber hinweg guckt, gibt es quasi Periodenverdopplung oder ich wei\u00df gar nicht mehr wie dieser Effekt hei\u00dft, auf jeden Fall, dann beginnt diese Spirale von vorne. Dann beginnt sozusagen die Oberfl\u00e4che von vorne zu wachsen, dann kommt diese Selbst\u00e4hnlichkeit, die kickt rein bei dem Fraktal und dann sieht es wieder von vorne so aus. Und man m\u00fcsste wieder die Spirale weiter reingehen, bis man wieder zur Singularit\u00e4t kommt und dann wieder reingehen und so weiter. Und wenn man so mehrere Layers bauen w\u00fcrde von seiner eigenen Realit\u00e4t, weil wir leben ja auf diesem Fraktal, dann w\u00e4re das eine gute Approximation f\u00fcr die Realit\u00e4t. Was wir niemals sehen w\u00fcrden, ist sozusagen das komplette Picture, weil das Fraktal ist. Es ist ein unendlich gro\u00dfes und selbst\u00e4hnliches periodisches Muster. So und so ist es mit unserem Universum auch. Wir haben halt diese physikalischen Gesetze, sie sind nicht perfekt, sie sind aber eine gute Beschreibung f\u00fcr das, was wir haben, was wir sehen und so. Und je n\u00e4her wir uns ran tasten an die Realit\u00e4t, desto besser werden wir. Irgendwann w\u00fcrden wir, wenn wir auf diesem Fraktal leben, in der Mandelbrotmenge, irgendwann w\u00fcrden wir diese paar Zeilen Code, die das braucht, um das zu generieren, vielleicht bauen k\u00f6nnen. Und dann h\u00e4tten wir es gekickt, dann h\u00e4tten wir die Realit\u00e4t vollst\u00e4ndig beschrieben. Das hei\u00dft, was man machen muss, um die Realit\u00e4t zu beschreiben, ist man muss sich aus First Principles Schluss folgern. Und das geht nat\u00fcrlich nicht so als Schlussfolgerung, sondern das ist im Prinzip zwar ein Error. Also man muss Mathematik machen, betreiben und man muss es solange mit 3 in Error machen, bis man irgendwie auf das kommt, was sozusagen die minimale Information beinhaltet, die alles beschreibt, was wir haben. Das ist das Prinzip von Okams Racer auch. Also wir wollen so wenig wie m\u00f6glich voraussetzen oder so wenig wie m\u00f6glich annehmen. Und das soll maximal viel von dem, was wir beobachten, beschreiben. Und wenn wir etwas haben, was alles mit einem Formalismus beschreibt, dann sind wir fertig. Das ist die Idee. Das ist sozusagen das Ideal, an das ich sozusagen die Wissenschaft da ran tasten m\u00f6chte. So und wenn wir also, Joshua Bach gibt auch noch so einen anderen coolen Spruch, n\u00e4mlich er sagt, die physikalische Realit\u00e4t hat kein Bewusstsein und da gibt es auch kein Bewusstsein. Nur eine Simulation kann Bewusstsein haben. Das ist das, was er sagt und das ist ein ziemlich cooler Spruch, weil was er damit meint ist, wenn wir jetzt ein Buch lesen und da ist ein Protagonist drin und der f\u00fchlt irgendwas oder dem geht es irgendwie schei\u00dfe. Dann empfinden wir das, w\u00e4hrend wir das lesen. Das liegt daran, dass wir in unserem Kopf uns ein Bild davon, ein Abbild dieser Person in dem Buch, die da beschrieben wird durch Sprache \u00fcbrigens, bauen und dadurch wird es real. Und der Witz ist, zwischen dieser Person in dem Buch und dem, was wir uns selbst nennen, gibt es eigentlich keinen Unterschied. Denn das Gehirn erz\u00e4hlt sich ja selber eine Geschichte dar\u00fcber, wie es w\u00e4re, wenn es eine Person g\u00e4be, die so ist wie wir und die in einer Situation ist, die wir Gegenwart nennen. Und wir konstruieren, w\u00e4hrend wir den sensorischen Input \u00fcber die Gegenwart bekommen, diese Person und stellen uns vor, wie es w\u00e4re, so eine Person zu sein. Wir simulieren das. Das ist das, was das Gehirn macht. Deswegen sagt er, nur eine Simulation kann \u00fcberhaupt Bewusstsein haben. Nur eine Simulation ist f\u00e4hig, diese emergente Struktur zu haben, die dann wieder, also die emergente Struktur, die sozusagen das Bewusstsein als Nebenprodukt hat. Und das ist krass. Das ist ziemlich heftig. Also wenn ihr, wenn ihr das nicht kennt, diese Joscha Bach Talks, ihr m\u00fcsst euch die reinziehen. Ich verlinke die beiden von Lex Friedman. Es gibt aber noch viele mehr, die sozusagen ins Detail gehen, wenn Joscha Bach seine Meinung zu GPT-3 erz\u00e4hlt. Das ist super krass oder generell. Es gibt, glaube ich, auch zwei, drei deutsche Talks, von denen einer mindestens schon mal ziemlich gut ist f\u00fcr Einsteiger. Die anderen sind zu abgefahren, absolute Nord-Talks. Aber wie gesagt, der Typ ist ein Ostdeutscher, der kann also auch flie\u00dfend Deutsch, ist gar kein Problem. Man findet blo\u00df nicht so viel von dem auf Deutsch. Aber hammerm\u00e4\u00dfig. Also wirklich alles andere, alle anderen Talks, alle anderen Podcasts, die es auf dieser Welt gibt, sind, meiner Meinung nach, Zeitverschwendung, bevor man nicht vollst\u00e4ndig durchdrungen hat, was Joscha Bach einen in drei Stunden reinquetscht an Wissen. Das ist so viel. Ich habe den Talk jetzt bestimmt vier, f\u00fcnf Mal schon durchgeknallt und ich komme immer wieder auf neue Erkenntnisse, weil der Typ in einem Satz so viel Informationen und so viel Verst\u00e4ndnis reinsteckt, dass man das wirklich erst nach einem Jahr nochmal dann checkt oder so. So und in Deep Learning ist es jetzt nur so. Man muss dazu wissen, das habe ich auch noch nie so erw\u00e4hnt, aber das ist quasi bekannt. Das ist wahrscheinlich erste Vorlesung, Deep Learning im Informatikstudium. Der sogenannte universelle Funktions-Approximator, das ist das Relevante. Der sagt Folgendes aus, wenn ihr ein neuronales Netz macht mit N-Input, N-dimensionalen Input und ihr wollt nur ein Output haben, also sozusagen eine Funktion mit beliebigem Input und die soll was ausgeben. Und ihr habt ein Layer dazwischen, der hat beliebig viele, also auch beliebig viele Knoten. Dann k\u00f6nnt ihr jede mathematische Funktion damit approximieren. Das ist erstmal irgendwie klar, aber es wurde sozusagen bewiesen, dass das so ist. Und der Witz ist jetzt Folgendes. In der Informatik geht es jetzt darum, wir nehmen an, dass unsere Welt determiniert ist und deswegen kann man alles mit einer Funktion quasi hinschreiben. Die kann beliebig kompliziert sein, aber es muss durch eine Funktion approximierbar sein. Das bedeutet, dass alles mit einem Deep Learning Netz, also mit einem neuronalen Netz beschreibbar ist. Das ist die Voraussetzung. Und jetzt ist nur die Frage, wenn wir beliebig viele Knoten haben, dann ist das klar. Was man dann einfach macht, ist Overfitting. Also ihr k\u00f6nnt euch quasi vorstellen, ich habe eine Aufgabenstellung und ich verstehe die Aufgabenstellung nicht, sondern ich lerne sie einfach auswendig. Sagen wir mal, mein Konfigurationsraum hat 10 hoch 32 viele M\u00f6glichkeiten und 10 hoch 32 viele Outputs zu einem gegebenen Problem, eins aus diesen 10 hoch 32. Dann kann ich doch, wenn ich genug Knoten, also Speicherpunkte habe, kann ich die einfach auswendig lernen. Ich kann alle Antworten auf alle Fragen auswendig lernen. Also stellt euch einfach nur vor, ihr wollt eine Software schreiben, die Bilder erkennt und ihr gebt der Software vor. Man gibt ein Bild rein, das ist 256 mal 256 Pixel gro\u00df. Und jetzt macht ihr eine feine Absch\u00e4tzung und steckt sozusagen einfach alle m\u00f6glichen Bilder, die \u00fcberhaupt existieren k\u00f6nnen im Konfigurationsraum. Also alle Kombinationen von Pixelwerten, die 256 mal 256 Bilder haben k\u00f6nnen \u00fcberhaupt, die steckt ihr rein und lernt sie auswendig mit dem Satz darunter, was sieht man auf dem Bild. Das lernt ihr auswendig. Wenn ihr unendlich viel Speicherplatz habt und unendlich gro\u00df sozusagen das Lehr machen k\u00f6nnt, ist das nat\u00fcrlich kein Problem. Dabei habt ihr aber sozusagen, ihr habt das Problem nur gelernt, aber ihr habt es nicht verstanden. Und die Idee ist sozusagen, kann man das runter kochen auf irgendetwas, was weniger ist als Overfitting und sozusagen die Datenmenge besser abbildet, sozusagen eine sehr gro\u00dfe Datenmenge, zum Beispiel dieser riesige Konfigurationsraum von allen m\u00f6glichen Bildern, die 256 Quadratviele Pixel haben, irgendwie abzubilden auf irgendwas, wo wir trotzdem das Problem l\u00f6sen k\u00f6nnen, n\u00e4mlich eine Software, die uns am Ende sagt, was sieht man auf dem Bild oder so. Und das ist quasi das, was ich hier sozusagen jetzt mit Overfitting auch erkl\u00e4ren wollte, aber wie gesagt, das Video ist ein bisschen heftig, ich wei\u00df schon. Das nennt man Overfitting. Viel besser w\u00e4re es doch, wenn ich runter gehe mit meiner Komplexit\u00e4t in meinem Deep Learning Netzwerk, also in diesem Zwischenlayer, auf viel weniger Dimension und trotzdem noch eine sehr hohe Approximationsg\u00fcte kriege, also 99,999 Prozent oder so. Denn dann habe ich ja Folgendes geschafft. Ich habe mein Problem komprimiert, was konkret, ich habe aus dem Auswendiglernen Verstehen gemacht. Ich habe sozusagen, und das ist ja das, was grunds\u00e4tzlich irgendwie nie jemand mal rafft, was ist jetzt der Unterschied zwischen lehnen und lernen und verstehen? Verstehen bedeutet, mit dem minimalen Satz an Handlungsanweisungen das Problem vollst\u00e4ndig zu l\u00f6sen und aus verschiedensten Wissenssachen, aus Kombinationen von Wissen, neues Wissen zu erzeugen. Ja, eine Anwendung zu finden, die sozusagen nur darauf basiert, dass ich hier das eine schon mal verstanden habe und ich direkt auf das andere anwenden kann. So, und das ist auch da der Fall. Wenn man also den minimalen Satz quasi die beste Architektur von Layern erzeugen kann, um zum Beispiel Energie zu sparen, Speicher zu sparen, dann komprimiert das sozusagen. Ich mache eine Kompression dabei. Und da kommen dann sozusagen die mehreren Layer dann typischerweise im Deep Learning dazu. Es ist nicht einer, sondern es sind viele verschachtelte Layer, die runter und hoch gehen in ihrer Dimension. Da gibt es sozusagen Konvolutiones und was auch immer. Es gibt \u00fcbelst viele krasse Architekturen mittlerweile, die alle sehr tricky sind. Aber das ist sozusagen erstmal die Idee. Es gibt diesen universellen Funktions-Approximator und man kann mit einem Neuronalen jetzt jede beliebige Funktion, insbesondere alles, was wir in unserer Realit\u00e4t beobachten, wenn es dem geht. Das ist die Annahme. Und warum sollte man also nicht das Gehirn damit beschreiben k\u00f6nnen? Und warum sollte man nicht die gesamte Realit\u00e4t und das gesamte Universum damit beschreiben k\u00f6nnen? Ich habe jetzt schon erkl\u00e4rt in meinem anderen Video, was das Bewusstsein ist. Und Joshua Bach hat es ziemlich kompakt nochmal ausgedr\u00fcckt und hat gesagt, das Bewusstsein ist ein Modell f\u00fcr den Inhalt unseres Aufmerksamkeitsmodells. Und was meint ihr damit? Na ja, es ist so eine Art Meta-Aufmerksamkeit. Ihr m\u00fcsst euch vorstellen, es ist so eine Art Meta-Learning. Was meint jetzt eigentlich Aufmerksamkeit? Na ja, also erstmal gibt es ja mittlerweile schon Transformatoren. Also es gibt schon Attention-Networks im Deep Learning. Diese Sprache wurde schon etabliert und das macht auch ziemlich genau das, was man sich eigentlich unter dem Konzept von Aufmerksamkeit vorstellt. Also erstmal Aufmerksamkeit und Bewusstheit, das h\u00e4ngt schon miteinander zusammen, auch vom Wort, sozusagen von der w\u00f6rtlichen Bedeutung. Und im Moment ist es so, normale Deep Learning-Netzwerke machen Folgendes. Die haben einen Fehler-Signal, also sagen wir mal, wir haben eine Beobachtung und eine Prediktion von unserem Modell und das ist falsch. Und dieses Fehler-Signal, die Differenz davon, die versuchen wir zur\u00fcckzuf\u00fchren, in unser Modell wegzutrecken. Wo kommt sozusagen der Fehler her? Und dabei sozusagen bei vielen Daten entstehen nat\u00fcrlich eine ganze Menge in diesen statistischen Gewichten, wo irgendwas sich \u00e4ndert. Wenn man jetzt mit vielen Daten lernt, mitteln sich sozusagen die unwichtigen Gewichte raus und was \u00fcberbleibt ist das, wo der wirkliche Fehler herkommt, also das sind dann meistens nur noch weniger Gewichte. So, das ist aber ein Prozess, der super ineffizient ist und langsam, der ist super langsam, weil man immer wieder den kompletten Backpropagating-Step durchmachen muss. Viel geiler w\u00e4re, wenn man ein Netzwerk hat, was oben da bei diesem Lernprozess draufguckt und feststellt, wo sind die gr\u00f6\u00dften Gewichte im Gradienten, also quasi in dem Backpropagating und kann ich das beobachten, wenn ich das sozusagen \u00f6fter mache. Und dann kann man Statistiken betreiben und sozusagen das abk\u00fcrzen. Man kann diesen Lernprozess abk\u00fcrzen, weil man sozusagen den Shortcut geht. Das hat so ein bisschen was von, wenn ihr euch mit iterativen L\u00f6sungsverfahren von algebraischen Gleichungen auskennt, das hat ein bisschen was von Order-Zapsets, finde ich. Aber das ist wirklich nur meine Physiker-Intubation, da bin ich sozusagen mit gef\u00e4hrlichem Halbwissen hier unterwegs. Jedenfalls, das k\u00fcrzt den Lernprozess ab und das erkl\u00e4rt nat\u00fcrlich auch, warum sozusagen heutzutage so ein Alpha Star, also die KI, die StarCraft gewinnt, die hat ja so was wie umgerechnet 200 Jahre StarCraft gespielt, bevor sie auf einem Level ist, wie ein normaler Master-Spieler im High MMR Blizzard Elo. Und ein Mensch braucht nat\u00fcrlich viel weniger Daten. Warum? Na ja, weil unsere Hirnarchitektur krasser ist. Wir leben schon auf dem n\u00e4chsten Level von Deep Learning. Was sozusagen die State of the Art ist im Moment, das erste Level war quasi, wir bringen ein Programm bei Schach zu spielen und das kriegen wir einfach hin. Das n\u00e4chste Level war, wir schreiben eine Architektur, zum Beispiel bei Go, da geht das nicht mehr, das Spiel ist zu kompliziert, man kann nicht in einem Programm einfach schreiben, was das kann. Sondern wir schreiben ein Netzwerk, was selber lernt, sich Go beizubringen. Und Joshua Bach sagt jetzt, das ist aber immer noch nicht das, was wir k\u00f6nnen, wir Menschen sind noch ein Level krasser. Wir k\u00f6nnen Meta-Learning machen, wir k\u00f6nnen also ein Programm schreiben, was ein Programm findet, um ein Problem zu lernen. Das ist sozusagen das, von dem er sagt, was wir als n\u00e4chsten Schritt brauchen. Und ich nehme an, das passiert erst nach dem n\u00e4chsten KI-Winter. Also wenn sozusagen nur Slow reinkickt und wir noch mehr Rechnerkapazit\u00e4ten haben, um das zu machen. Weil ihr d\u00fcrft mich vergessen, wie unfassbar viele Neuronen wir in unserem Kopf drin haben. Also die Komplexit\u00e4t unseres Gehirns ist schon ziemlich krass. Und man kann sozusagen sagen, da ist eine ganze Menge krasser, universeller Funktions-Approximator mit drin. Und das Gehirn hat halt wirklich jetzt, das beobachtet halt, wo die meiste Performance herkommt beim Lernen. Und speichert quasi diese Aufmerksamkeit, das speichert es zusammen mit dem Kontext, in dem es gelernt hat, ab. Und das ist sozusagen der Inhalt der Aufmerksamkeit. Also das ist der Inhalt der Aufmerksamkeit, wo haben wir wann, was, wie am besten gelernt. Was sozusagen hat die meisten Performance Boost in unserer Performance gebracht, wenn wir an dem einen statistischen Gewicht oder an dem einen Neuronen wackeln. Und der Witz ist, das ist so eine Art Meta-Learning, weil es sozusagen eine Aufmerksamkeit \u00fcber die Aufmerksamkeit macht. Es speichert sozusagen den Kontext und wo die Aufmerksamkeit war. Und das muss es beobachten, das Gehirn. Und das tut es. Das ist sozusagen Meta-Learning, weil Aufmerksamkeit \u00fcber die Aufmerksamkeit quasi oder der Inhalt der Aufmerksamkeit zusammen mit dem Kontext. So ungef\u00e4hr muss man sich das vorstellen. Und ich finde, an diesem Punkt ist sozusagen diese maximale Selbstbez\u00fcglichkeit auch klar. Und das ist auch das, was die Leute so schizophren finden an der Frage, habe ich einen freien Willen oder nicht. Oder was ist das Bewusstsein? Das ist halt sozusagen, das ist das, was Douglas Hofstadter wahrscheinlich meinte, mit einem Estrange Loop. Und das Buch habe ich bis jetzt noch nicht gelesen, aber das steht auch auf meinem Pile of Shame. Und das H\u00f6ren muss sozusagen auch genau darauf achten, also es muss ich selbst beobachten, worauf ich jetzt genau Aufmerksamkeit lege. Also was gucke ich mir gerade an? Und der Punkt ist irgendwie diese maximale Selbstbez\u00fcglichkeit, die macht es halt. Und falls ihr das kennt, also bei mir war es so, wenn ich mal, ich hatte mal so einen Moment, ich war auf einer LAN-Party und war wirklich komplett im Tunnel. Also ich hatte das nicht oft in meinem Leben, aber manchmal war ich vollst\u00e4ndig im Tunnel, zum Beispiel beim Zocken oder auch mal beim Schlagzeugspielen. Und wenn man einmal so vollst\u00e4ndig in diesem Modus drin ist, dass man seine eigene Aufmerksamkeit overfittet, wenn man irgendwie so krass sich selber dabei beobachtet, wie heftig man unterwegs ist, dann ist man wie bei Matrix, wie bei Neo und sieht alles in Slow-Motion. Man ist auf einmal superschnell. Und ich hatte das, ich war normalerweise so ein Spieler, ich hatte so 250 APM vielleicht, wenn es hochkommt. Und ich hatte mal auf einer LAN-Party so 320 bis 350. Und ich kann mich auch im Nachhinein noch erinnern, das ging runter wie Butter. Meine Finger, das hat sich angef\u00fchlt wie das geschmeidigste, konstante Tippen der Welt. Ich konnte nie wieder diese Performance erlangen wie in diesem einen Game, was ich mal hatte in meinem Leben. Und ich glaube, das war so ein Zustand, der hat sich schon fast so ein bisschen transcendent angef\u00fchlt. Und ich glaube, das war, wo meine Aufmerksamkeit diesen Zustand erreicht hat, dass sie sich sozusagen selber vollst\u00e4ndig beobachtet und jederzeit eingreifen kann, v\u00f6llig problemlos. Ich glaube, so einen Zustand kann man auch durch Meditation erreichen, wenn man irgendwie, wenn man das lang genug macht. Jedenfalls, das ist irgendwie krass und das merkt ihr, wenn ihr irgendwann mal, wenn ihr irgendwas habt, wo ihr euch krass konzentrieren m\u00fcsst. Und das genaue Gegenteil ist sowas wie mit dem Fahrrad irgendwo langfahren, zur Arbeit oder so. Da denkt man gar nicht mehr dr\u00fcber nach, muss ich jetzt hier abbiegen, muss ich hier meinen rechten Fu\u00df nach unten dr\u00fccken, muss ich jetzt hier die Bremse dr\u00fccken, muss ich hier irgendwie lenken. Das ist alles Autopilot, ne? Da ist genau das Gegenteil, keine Aufmerksamkeit mehr quasi. Und \u00fcbrigens das wichtige, meilensteinartige Paper dazu zu den Transformers hei\u00dft Attention is all you need. Das ist schon so ein geiler Titel. Ich glaube, das war auch von so Boys, die bei Google in der AI-Abteilung sitzen. Das ist auch cool geschrieben, das kann man sich wirklich mal reinziehen. Also wenn ihr euch damit auskennt, wenn ihr euch damit auskennt, kennt ihr das Paper eh, was erz\u00e4hle ich hier. Ich bin ja sozusagen der Laie, weil ich hab mir nur mal so einen w\u00f6chentlichen Crashkurs bei uns in der Informatikfakult\u00e4t mal reingeg\u00f6nnt dazu. Ja und da wird dann sozusagen auch klar, dass diese Sache mit dem Attention so geil ist, weil wenn man jetzt zum Beispiel Text\u00fcbersetzung macht und man geht Brutforce von damals vor und \u00fcbersetzt Wort f\u00fcr Wort oder so, dann geht das immer krachen, was man braucht, das Kontext. Und Attention ist richtig gut daf\u00fcr, weil Attention kann sozusagen den Finger auf ein Wort legen und sagen, also dieses Wort jetzt hier, zum Beispiel der Arzt oder der K\u00fcnstler, zum Beispiel Cain West. Sp\u00e4ter in einem anderen Satz steht dann nur noch R und dieses R muss bez\u00fcglich auf Cain West sein. Dieser Kontext, das kann Attention richtig gut lernen. Und das konnten sozusagen die Learning-Strukturen vor den Transformern nicht so gut. Und das ist sozusagen der erste Schritt in Sachen jetzt mal Attention machen und das ist halt krass. Der n\u00e4chste Schritt, also das ist jetzt sozusagen State of the Art, das ist das, was alle mittlerweile k\u00f6nnen und das ist auch das, was GPT-3 maximal krass macht. Und man, also Ben G\u00fcrtel hat den Tag auch so was gesagt wie, naja, GPT-3 hat halt auch so unendlich viele Knoten, dass wahrscheinlich auch Overfitting schon dabei ist. Das ist einfach nur ein v\u00f6llig \u00fcbertrieben gro\u00dfes neuronales Netzwerk, was das komplette Internet gelernt hat einmal und deswegen mit dir reden kann wie ein Mensch, kurz mal. Das habe ich in meinem Live-Talk dann \u00fcber K\u00fcnstliche Intelligenz auch schon mal gezeigt, so ein Beispiel. Und jetzt ist es halt so, die normalen Transformer-Networks, die haben halt einen begrenzten Arbeitsspeicher, das hei\u00dft, sie kriegen auch nur einen begrenzten Text in sich rein. Was wir aber machen ist, wir haben sozusagen eine besseren Komprimierungsalgorithmus und k\u00f6nnen deswegen viel mehr Daten in Attention, also in Kontext zueinandersetzen. Und das, was jetzt sozusagen Cain West und er und sozusagen der Kontext f\u00fcr so ein Deep Learning Netzwerk ist, ist f\u00fcr uns das gesamte Universum. Das hei\u00dft erst, wenn wir auf diesem Level sind, quasi ein Level h\u00f6her in der Attention, dann k\u00f6nnen wir auch so was wie k\u00fcnstliches Bewusstsein konstruieren. Also das ist zumindest das, was Joshua Bach sagt und was soll ich sagen, alles, was Joshua Bach sagt, klingt extrem plausibel und hier muss ich auf jeden Fall eingestehen, dass ich nicht so krass bin wie Stanislav Lemb und ich finde im Prinzip fast alles, was Joshua Bach sagt, ist des Todes legit des Todes. Der Typ ist einfach nur zu geil. Und auf der Ebene der Neuronen \u00fcbrigens kann man das Bewusstsein auch noch mal genauso erkl\u00e4ren, n\u00e4mlich mit Emergenz, da kann man einfach sagen, naja, so ein Neuron muss jetzt sozusagen eine Funktion lernen, wann es feuert. Ein Neuron hat sozusagen, kann eine nicht-linearer Funktion approximieren. Es hat ja ein multidimensionales Input, die ganzen Daten, die reinkommen. Wann feuert es? Das ist der Output. Und das Neuron muss jetzt sozusagen das Output an das Universum liefern. Also es hat ja nur diesen Ausgang, es wei\u00df ja nichts von der \u00e4u\u00dferen Welt. Irgendwann kommt wieder Input rein und dieser Input f\u00fcttert das Neuron ja. Wenn kein Input mehr kommt, dann stirbt das Neuron ja ab. Das hei\u00dft, Neuronen haben ein Interesse daran, diese Funktion sehr gut zu approximieren, um lange zu leben und fetter zu werden. Und das ist genau das, was wir haben. Und in unserem Hirn gibt es sozusagen diese goldene Regel, Wires Together, Fires Together oder so. Das ist so eine Grundregel, die wird um Deep Learning sozusagen, diese Intuition wird da auch verwendet, weil sozusagen die ganze Struktur der neuronalen Netze auf dieser Analogie funktionieren, von den Axionen und den Neuronen und so. Dem Schei\u00df, den wir sozusagen auf der biologischen Ebene schon sehen k\u00f6nnen, wie unser Gehirn im Prinzip funktionieren muss. Wir sehen sozusagen nur diese unendlich komplizierten Netzwerke und checken nicht, wie die gehen, so ungef\u00e4hr. So und nur die Neuronen, die das richtig machen, die \u00fcberleben und die anderen sterben halt aus, die das nicht machen, also da ist sozusagen wieder evolution\u00e4rer Selektionsdruck. Und Joshua Bach sagt halt jetzt, und das finde ich auch schon wieder so geil, ein Neuron ist halt einfach nur ein normaler kleiner Reinforcement Learning Agent. Das ist ein kleines Reinforcement Agent Konzept, der sozusagen sich selbst immer verbessert. Und ja, es sendet halt einfach diese Signale in das Universum, also nach drau\u00dfen. Das ist zum Beispiel das, was wir hier machen. Das beeinflusst unsere Handlung und den Willen und den freien Willen und diesen ganzen Schei\u00df, nur um eine positive Antwort zu bekommen. Und das Konglomerat von den ganzen Neuronen, die wir halt im Kopf haben, ist halt so angelegt, von der Architektur her auch, aber auch von r\u00e4umlich getrennten Kostenfunktionen, also bestimmte Hirnareale machen bestimmte Sachen, dass es robust w\u00e4chst in der meisten, bei fast allen Menschen, die geboren werden. Und die Gesamtheit davon nennen wir halt Gehirn. Das ist das. Und das ist krass. Das ist wirklich erstaunlich, dass die Evolution das herausgebracht hat. Aber der Gehirn, habe ich ja auch glaube ich schon gesagt in dem anderen Video, was f\u00fcr krasse Spr\u00fcnge da dazu geh\u00f6ren. Weil es ein super schlechter Trade-off war, f\u00fcr so eine Spezies wie den Menschenaffen mehr in seinen Gehirn zu investieren und daf\u00fcr mehr Ressourcen irgendwo anders einzub\u00fc\u00dfen. Zum Beispiel war er nicht mehr so stark. Das musste sozusagen in einer Zeit der Erbgeschichte auch entstehen, wo das gerade so ging, damit wir \u00fcber diesen Potenzialwald r\u00fcberkommen und dann uns als Menschen \u00fcberhaupt als Homo sapiens etablieren konnten. Das war ein super Heft, da sind mehrere Sachen zusammengefallen, dass das durch Selektionsdruck dann passieren konnte und der Mensch daraus entstehen konnte. Weil sowas wie Augen zum Beispiel, das wurde glaube ich drei, viermal separat voneinander in der Evolution festgestellt, dass sich Augen gebildet haben. Zuf\u00e4llig. Durch Evolutionsdruck. Das ist zum Beispiel was, was man \u00f6fter beobachtet, aber so ein Shit wie krasses Gehirn und daf\u00fcr nat\u00fcrlich weniger Robustheit, k\u00f6rperliche, physische Robustheit, das ist selten. Und ja, was soll ich dazu sagen? Die emergente Struktur von dem, was ich sozusagen gerade als Gehirn beschrieben habe, das ist das Bewusstsein. So kann man es auch artikulieren. Und ich wollte jetzt wirklich nur mal dieses Video machen, weil ich finde das so krass. Immer wenn ich mir wieder was von Erscha Bach einziehe, denke ich mir so, ja, Dieser Typ, den jeder muss wissen, wer das ist. Ich finde, das ist der krasseste Dude, den es gibt. Der basht halt auch die anderen Leute, also er basht die nat\u00fcrlich nicht weg, macht kein Reaction-Video oder ratet irgendwie auf irgendwen. Aber er macht sie alle platt, er macht, also meiner Meinung nach ist er v\u00f6llig \u00fcberlegen gegen\u00fcber zum Beispiel Roger Penrose oder so anderen Leuten, die sich nat\u00fcrlich auch den ganzen Tag, auch Douglas Hofstadter, die sich den ganzen Tag mit Bewusstsein und diesen Fragen besch\u00e4ftigen. Ich finde, Erscha Bach hat die perfekte, sozusagen die bestm\u00f6gliche verf\u00fcgbare Komprimierung schon da in seiner Sprache. Seine Sprache ist so kompakt, da steckt so viel drin, weil er sozusagen diese ganze Terminologie von den Deep Learning Netzwerken so krass gefressen hat und diese ganzen Konzepte auch problemlos auf alles M\u00f6gliche anwenden kann, weil er die Intelligenz da hat, um sozusagen die Analogien zu bilden, um das zu tun. Es ist einfach nur ein Genuss, dem Typen zuzuh\u00f6ren, auch wenn er extrem schnell redet, so wie Sheldon Cooper \u00fcbrigens auch. Aber ist kein Problem, man kann es einfach mehrmals h\u00f6ren und super. Und Lex Friedman macht auch immer einen guten Job, wenn er nochmal nachfragt und so. Also, ich wollte einfach ein Video dar\u00fcber machen. Reingehauen, YouTube. Das war's f\u00fcr heute. Vielen Dank f\u00fcr's Zuschauen. Bis zum n\u00e4chsten Mal. Der Kanal ist wie eine sch\u00f6ne Vorlesung aufgebaut. Wenn ich unten Videos verlinke, dann w\u00e4re es angebracht, sich die auch reinzuziehen, weil das aufeinander aufbaut. Bestimmte Begriffe werden definiert, viele Beispiele werden genannt. Und ja, ich denke mir schon was dabei, weil es sozusagen meinen eigenen Erkenntnisprozess abbildet, die Reihenfolge, in denen ich die Videos hier hochlade.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.68, "text": " Das Gehirn erz\u00e4hlt sich ja selber eine Geschichte dar\u00fcber, wie es w\u00e4re, wenn es eine Person g\u00e4be,", "tokens": [50364, 2846, 2876, 24118, 77, 47110, 3041, 2784, 23888, 3018, 28896, 21737, 11, 3355, 785, 14558, 11, 4797, 785, 3018, 8443, 37612, 650, 11, 50698], "temperature": 0.0, "avg_logprob": -0.15000402277166194, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.04531658813357353}, {"id": 1, "seek": 0, "start": 6.68, "end": 12.88, "text": " von die so ist wie wir und die in einer Situation ist, die wir Gegenwart nennen.", "tokens": [50698, 2957, 978, 370, 1418, 3355, 1987, 674, 978, 294, 6850, 22247, 1418, 11, 978, 1987, 38631, 29587, 297, 16043, 13, 51008], "temperature": 0.0, "avg_logprob": -0.15000402277166194, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.04531658813357353}, {"id": 2, "seek": 0, "start": 12.88, "end": 18.12, "text": " Und wir konstruieren, w\u00e4hrend wir den sensorischen Input \u00fcber die Gegenwart bekommen,", "tokens": [51008, 2719, 1987, 34208, 894, 5695, 11, 33624, 1987, 1441, 10200, 6282, 682, 2582, 4502, 978, 38631, 29587, 19256, 11, 51270], "temperature": 0.0, "avg_logprob": -0.15000402277166194, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.04531658813357353}, {"id": 3, "seek": 0, "start": 18.12, "end": 22.72, "text": " diese Person und stellen uns vor, wie es w\u00e4re, so eine Person zu sein.", "tokens": [51270, 6705, 8443, 674, 24407, 2693, 4245, 11, 3355, 785, 14558, 11, 370, 3018, 8443, 2164, 6195, 13, 51500], "temperature": 0.0, "avg_logprob": -0.15000402277166194, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.04531658813357353}, {"id": 4, "seek": 0, "start": 22.72, "end": 25.12, "text": " Wir simulieren das.", "tokens": [51500, 4347, 1034, 425, 5695, 1482, 13, 51620], "temperature": 0.0, "avg_logprob": -0.15000402277166194, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.04531658813357353}, {"id": 5, "seek": 0, "start": 25.12, "end": 26.76, "text": " Das ist das, was das Gehirn macht.", "tokens": [51620, 2846, 1418, 1482, 11, 390, 1482, 2876, 24118, 77, 10857, 13, 51702], "temperature": 0.0, "avg_logprob": -0.15000402277166194, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.04531658813357353}, {"id": 6, "seek": 2676, "start": 26.76, "end": 30.92, "text": " Deswegen sagt er, nur eine Simulation kann \u00fcberhaupt Bewusstsein haben.", "tokens": [50364, 24864, 15764, 1189, 11, 4343, 3018, 3998, 2776, 4028, 20023, 40512, 26340, 33042, 3084, 13, 50572], "temperature": 0.0, "avg_logprob": -0.18421771709735577, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.3838646113872528}, {"id": 7, "seek": 2676, "start": 30.92, "end": 35.6, "text": " Riesen Sorry schonmal an alle, die sich dieses Video jetzt hier rein g\u00f6nnen.", "tokens": [50572, 497, 30383, 4919, 4981, 5579, 364, 5430, 11, 978, 3041, 12113, 9777, 4354, 3296, 6561, 290, 3239, 2866, 13, 50806], "temperature": 0.0, "avg_logprob": -0.18421771709735577, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.3838646113872528}, {"id": 8, "seek": 2676, "start": 35.6, "end": 40.64, "text": " Ich glaube, es ist absolut unm\u00f6glich zu verstehen, was ich jetzt hier in diesem Video quatsche,", "tokens": [50806, 3141, 13756, 11, 785, 1418, 18757, 19334, 16277, 2164, 37352, 11, 390, 1893, 4354, 3296, 294, 10975, 9777, 421, 1720, 1876, 11, 51058], "temperature": 0.0, "avg_logprob": -0.18421771709735577, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.3838646113872528}, {"id": 9, "seek": 2676, "start": 40.64, "end": 44.8, "text": " wenn ihr mein Video zum Bewusstsein nicht zuerst gesehen habt.", "tokens": [51058, 4797, 5553, 10777, 9777, 5919, 40512, 26340, 33042, 1979, 2164, 16398, 21535, 23660, 13, 51266], "temperature": 0.0, "avg_logprob": -0.18421771709735577, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.3838646113872528}, {"id": 10, "seek": 2676, "start": 44.8, "end": 50.64, "text": " Das baut er halt darauf auf und ich benutze viele komische Fremdworte.", "tokens": [51266, 2846, 272, 1375, 1189, 12479, 18654, 2501, 674, 1893, 38424, 1381, 9693, 5207, 7864, 479, 2579, 67, 86, 12752, 13, 51558], "temperature": 0.0, "avg_logprob": -0.18421771709735577, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.3838646113872528}, {"id": 11, "seek": 2676, "start": 50.64, "end": 56.040000000000006, "text": " Ich glaube, die ergeben alle \u00fcberhaupt keinen Sinn, wenn ihr das Video mit dem Bewusstsein nicht vorher geguckt habt.", "tokens": [51558, 3141, 13756, 11, 978, 1189, 16702, 5430, 20023, 20624, 37962, 11, 4797, 5553, 1482, 9777, 2194, 1371, 40512, 26340, 33042, 1979, 29195, 23982, 47800, 23660, 13, 51828], "temperature": 0.0, "avg_logprob": -0.18421771709735577, "compression_ratio": 1.7064846416382253, "no_speech_prob": 0.3838646113872528}, {"id": 12, "seek": 5604, "start": 56.04, "end": 58.56, "text": " Das ist nur so schon mal als Warnung.", "tokens": [50364, 2846, 1418, 4343, 370, 4981, 2806, 3907, 343, 1083, 1063, 13, 50490], "temperature": 0.0, "avg_logprob": -0.20832640437756555, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.06648602336645126}, {"id": 13, "seek": 5604, "start": 58.56, "end": 60.68, "text": " Das ist ein absolutes Nerd-Video.", "tokens": [50490, 2846, 1418, 1343, 7305, 1819, 38367, 12, 46287, 13, 50596], "temperature": 0.0, "avg_logprob": -0.20832640437756555, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.06648602336645126}, {"id": 14, "seek": 5604, "start": 60.68, "end": 64.44, "text": " Ich habe das gestern aufgenommen und da hatte ich auch schon einen im Tee.", "tokens": [50596, 3141, 6015, 1482, 7219, 1248, 2501, 29270, 674, 1120, 13299, 1893, 2168, 4981, 4891, 566, 314, 1653, 13, 50784], "temperature": 0.0, "avg_logprob": -0.20832640437756555, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.06648602336645126}, {"id": 15, "seek": 5604, "start": 64.44, "end": 67.72, "text": " Und jetzt habe ich mir halt angeguckt und dachte mir so, ja, okay, schon klar.", "tokens": [50784, 2719, 4354, 6015, 1893, 3149, 12479, 15495, 70, 47800, 674, 39775, 3149, 370, 11, 2784, 11, 1392, 11, 4981, 14743, 13, 50948], "temperature": 0.0, "avg_logprob": -0.20832640437756555, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.06648602336645126}, {"id": 16, "seek": 5604, "start": 67.72, "end": 71.96, "text": " Aber das kann glaube ich keiner verstehen, wenn man nicht irgendwie das mindset daf\u00fcr hat.", "tokens": [50948, 5992, 1482, 4028, 13756, 1893, 37767, 37352, 11, 4797, 587, 1979, 20759, 1482, 12543, 13747, 2385, 13, 51160], "temperature": 0.0, "avg_logprob": -0.20832640437756555, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.06648602336645126}, {"id": 17, "seek": 5604, "start": 71.96, "end": 75.12, "text": " Und daf\u00fcr ist das andere Video auch gut geeignet.", "tokens": [51160, 2719, 13747, 1418, 1482, 10490, 9777, 2168, 5228, 24105, 788, 302, 13, 51318], "temperature": 0.0, "avg_logprob": -0.20832640437756555, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.06648602336645126}, {"id": 18, "seek": 5604, "start": 75.12, "end": 77.32, "text": " Also guckt euch vorher an, bevor ihr jetzt hier weiter guckt.", "tokens": [51318, 2743, 695, 19951, 10403, 29195, 364, 11, 37591, 5553, 4354, 3296, 8988, 695, 19951, 13, 51428], "temperature": 0.0, "avg_logprob": -0.20832640437756555, "compression_ratio": 1.628787878787879, "no_speech_prob": 0.06648602336645126}, {"id": 19, "seek": 8604, "start": 86.04, "end": 107.2, "text": " So Leute, jetzt mal wieder eins von diesen Deep Philosophical U-Bow-Videos des Todes und der Vorl\u00e4ufer zu diesem Video, w\u00fcrde ich sagen, ist das Video zum Bewusstsein.", "tokens": [50364, 407, 13495, 11, 4354, 2806, 6216, 21889, 2957, 12862, 14895, 31182, 5317, 804, 624, 12, 33, 305, 12, 53, 482, 329, 730, 2465, 279, 674, 1163, 12231, 22882, 84, 612, 2164, 10975, 9777, 11, 11942, 1893, 8360, 11, 1418, 1482, 9777, 5919, 40512, 26340, 33042, 13, 51422], "temperature": 0.0, "avg_logprob": -0.25767145930109797, "compression_ratio": 1.3618090452261307, "no_speech_prob": 0.011504906229674816}, {"id": 20, "seek": 8604, "start": 107.2, "end": 113.24000000000001, "text": " Und ich m\u00f6chte hier erst mal mit euch \u00fcber Emergenz reden, weil das braucht man irgendwie st\u00e4ndig.", "tokens": [51422, 2719, 1893, 14570, 3296, 11301, 2806, 2194, 10403, 4502, 18477, 1766, 89, 26447, 11, 7689, 1482, 22623, 587, 20759, 342, 38861, 13, 51724], "temperature": 0.0, "avg_logprob": -0.25767145930109797, "compression_ratio": 1.3618090452261307, "no_speech_prob": 0.011504906229674816}, {"id": 21, "seek": 11324, "start": 114.19999999999999, "end": 118.75999999999999, "text": " Und dann so ein bisschen mehr \u00fcber, naja, was ist die Realit\u00e4t?", "tokens": [50412, 2719, 3594, 370, 1343, 10763, 5417, 4502, 11, 1667, 2938, 11, 390, 1418, 978, 8467, 14053, 30, 50640], "temperature": 0.0, "avg_logprob": -0.2192454607981556, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.36954593658447266}, {"id": 22, "seek": 11324, "start": 118.75999999999999, "end": 120.83999999999999, "text": " Was ist das Bewusstsein?", "tokens": [50640, 3027, 1418, 1482, 40512, 26340, 33042, 30, 50744], "temperature": 0.0, "avg_logprob": -0.2192454607981556, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.36954593658447266}, {"id": 23, "seek": 11324, "start": 120.83999999999999, "end": 123.24, "text": " Und was soll das alles?", "tokens": [50744, 2719, 390, 7114, 1482, 7874, 30, 50864], "temperature": 0.0, "avg_logprob": -0.2192454607981556, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.36954593658447266}, {"id": 24, "seek": 11324, "start": 123.24, "end": 137.0, "text": " Einfach mal ein bisschen euch mal ein bisschen mal was rein knallen an krasser Erkenntnis, die ich so in den letzten Jahren, ich w\u00fcrde sagen im letzten Jahr so ungef\u00e4hr hatte.", "tokens": [50864, 6391, 6749, 2806, 1343, 10763, 10403, 2806, 1343, 10763, 2806, 390, 6561, 444, 31181, 364, 15913, 12129, 3300, 41838, 10661, 11, 978, 1893, 370, 294, 1441, 18226, 13080, 11, 1893, 11942, 8360, 566, 18226, 11674, 370, 41285, 13299, 13, 51552], "temperature": 0.0, "avg_logprob": -0.2192454607981556, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.36954593658447266}, {"id": 25, "seek": 11324, "start": 137.0, "end": 139.16, "text": " Und ja, also was ist jetzt Emergenz?", "tokens": [51552, 2719, 2784, 11, 611, 390, 1418, 4354, 18477, 1766, 89, 30, 51660], "temperature": 0.0, "avg_logprob": -0.2192454607981556, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.36954593658447266}, {"id": 26, "seek": 11324, "start": 139.16, "end": 142.04, "text": " Ich habe schon ein paar Mal verwendet das Wort.", "tokens": [51660, 3141, 6015, 4981, 1343, 16509, 5746, 1306, 20128, 302, 1482, 22748, 13, 51804], "temperature": 0.0, "avg_logprob": -0.2192454607981556, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.36954593658447266}, {"id": 27, "seek": 14204, "start": 142.04, "end": 152.6, "text": " Emergenz ist immer dann etwas, wenn ihr etwas beobachtet, was erstmal total kompliziert wirkt, wenn ihr da reingucken wollt.", "tokens": [50364, 18477, 1766, 89, 1418, 5578, 3594, 9569, 11, 4797, 5553, 9569, 312, 996, 48833, 11, 390, 38607, 3217, 24526, 43590, 1987, 2320, 11, 4797, 5553, 1120, 319, 278, 49720, 45826, 13, 50892], "temperature": 0.0, "avg_logprob": -0.1645097620346967, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.4176711142063141}, {"id": 28, "seek": 14204, "start": 152.6, "end": 161.88, "text": " Von au\u00dfen sieht es irgendwie krass aus und ihr denkt, okay, das hat irgendwie einen Mechanismus, das ist heftig, aber wenn man reinguckt, steht man fest, es besteht aus ganz vielen simplen Teilen.", "tokens": [50892, 20700, 1609, 8989, 14289, 785, 20759, 15913, 640, 3437, 674, 5553, 38658, 11, 1392, 11, 1482, 2385, 20759, 4891, 30175, 25327, 11, 1482, 1418, 415, 34765, 11, 4340, 4797, 587, 319, 278, 47800, 11, 16361, 587, 6633, 11, 785, 43680, 3437, 6312, 19885, 6883, 268, 16357, 268, 13, 51356], "temperature": 0.0, "avg_logprob": -0.1645097620346967, "compression_ratio": 1.5707317073170732, "no_speech_prob": 0.4176711142063141}, {"id": 29, "seek": 16188, "start": 161.88, "end": 174.51999999999998, "text": " Und irgendwie ist die Summe dieser Teile mehr als einfach nur, also die Summe der Teile ergibt mehr als einfach nur das, was man sich vorstellen w\u00fcrde, wenn man die Summe der Teile tats\u00e4chlich addieren w\u00fcrde.", "tokens": [50364, 2719, 20759, 1418, 978, 8626, 1398, 9053, 1989, 794, 5417, 3907, 7281, 4343, 11, 611, 978, 8626, 1398, 1163, 1989, 794, 26585, 13651, 5417, 3907, 7281, 4343, 1482, 11, 390, 587, 3041, 34346, 11942, 11, 4797, 587, 978, 8626, 1398, 1163, 1989, 794, 20796, 909, 5695, 11942, 13, 50996], "temperature": 0.0, "avg_logprob": -0.12435719645615165, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.38405081629753113}, {"id": 30, "seek": 16188, "start": 174.51999999999998, "end": 175.32, "text": " Was meine ich damit?", "tokens": [50996, 3027, 10946, 1893, 9479, 30, 51036], "temperature": 0.0, "avg_logprob": -0.12435719645615165, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.38405081629753113}, {"id": 31, "seek": 16188, "start": 175.32, "end": 176.6, "text": " Na ja, zum Beispiel ein Ameisenhaufen.", "tokens": [51036, 6056, 2784, 11, 5919, 13772, 1343, 316, 1398, 11106, 71, 20748, 13, 51100], "temperature": 0.0, "avg_logprob": -0.12435719645615165, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.38405081629753113}, {"id": 32, "seek": 16188, "start": 176.6, "end": 179.8, "text": " Die kleinen Ameisen, die sind alle sehr einfach, wie die funktionieren.", "tokens": [51100, 3229, 26512, 316, 1398, 11106, 11, 978, 3290, 5430, 5499, 7281, 11, 3355, 978, 20454, 5695, 13, 51260], "temperature": 0.0, "avg_logprob": -0.12435719645615165, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.38405081629753113}, {"id": 33, "seek": 16188, "start": 179.8, "end": 190.04, "text": " Trotzdem ist die Gesamtheit der Ameisen, das Konglomerat, so eine Art komplizierter Lebens, ein komplizierter Metabolismus, der sich verh\u00e4lt wie ein Lebewesen.", "tokens": [51260, 1765, 23934, 1418, 978, 6761, 335, 3322, 270, 1163, 316, 1398, 11106, 11, 1482, 9832, 75, 14301, 267, 11, 370, 3018, 5735, 24526, 590, 811, 391, 21530, 11, 1343, 24526, 590, 811, 391, 6377, 14923, 25327, 11, 1163, 3041, 1306, 28068, 3355, 1343, 1456, 47465, 17403, 13, 51772], "temperature": 0.0, "avg_logprob": -0.12435719645615165, "compression_ratio": 1.7439446366782008, "no_speech_prob": 0.38405081629753113}, {"id": 34, "seek": 19004, "start": 190.12, "end": 201.07999999999998, "text": " Und wenn das Objekt, was ihr seht, nur aus Verbindungen besteht, die irgendwie auch rein virtuell dadurch sind, die sind ja nicht anfassbar.", "tokens": [50368, 2719, 4797, 1482, 4075, 14930, 11, 390, 5553, 369, 357, 11, 4343, 3437, 27034, 471, 5084, 43680, 11, 978, 20759, 2168, 6561, 4480, 13789, 35472, 3290, 11, 978, 3290, 2784, 1979, 33709, 640, 5356, 13, 50916], "temperature": 0.0, "avg_logprob": -0.12550367107828156, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.052592579275369644}, {"id": 35, "seek": 19004, "start": 201.07999999999998, "end": 206.28, "text": " Man kann ja nicht sagen, ach, diese Wechselwirkung zwischen den Ameisen, die macht das erst so interessant.", "tokens": [50916, 2458, 4028, 2784, 1979, 8360, 11, 2800, 11, 6705, 492, 21266, 86, 18610, 1063, 19875, 1441, 316, 1398, 11106, 11, 978, 10857, 1482, 11301, 370, 37748, 13, 51176], "temperature": 0.0, "avg_logprob": -0.12550367107828156, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.052592579275369644}, {"id": 36, "seek": 19004, "start": 206.28, "end": 207.16, "text": " Aber genau darum geht es.", "tokens": [51176, 5992, 12535, 27313, 7095, 785, 13, 51220], "temperature": 0.0, "avg_logprob": -0.12550367107828156, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.052592579275369644}, {"id": 37, "seek": 19004, "start": 207.16, "end": 208.04, "text": " Das ist die Emergenz.", "tokens": [51220, 2846, 1418, 978, 18477, 1766, 89, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12550367107828156, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.052592579275369644}, {"id": 38, "seek": 19004, "start": 208.04, "end": 218.44, "text": " Das ist etwas, was man nicht so greifen kann, sondern es entsteht als ein Prozess h\u00f6herer Ordnung zwischen Wechselwirkungen von einfacheren Mechanismen, die man sehr wohl verstehen kann.", "tokens": [51264, 2846, 1418, 9569, 11, 390, 587, 1979, 370, 6066, 25076, 4028, 11, 11465, 785, 35955, 357, 3907, 1343, 1705, 37575, 48045, 260, 29388, 15539, 19875, 492, 21266, 86, 18610, 5084, 2957, 7281, 5170, 30175, 1434, 268, 11, 978, 587, 5499, 24531, 37352, 4028, 13, 51784], "temperature": 0.0, "avg_logprob": -0.12550367107828156, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.052592579275369644}, {"id": 39, "seek": 21844, "start": 218.44, "end": 221.0, "text": " Und darum ist Emergenz nicht greifbar.", "tokens": [50364, 2719, 27313, 1418, 18477, 1766, 89, 1979, 6066, 351, 5356, 13, 50492], "temperature": 0.0, "avg_logprob": -0.0992902609018179, "compression_ratio": 1.7156549520766773, "no_speech_prob": 0.0021487772464752197}, {"id": 40, "seek": 21844, "start": 221.0, "end": 222.35999999999999, "text": " Und das ist immer das Problem.", "tokens": [50492, 2719, 1482, 1418, 5578, 1482, 11676, 13, 50560], "temperature": 0.0, "avg_logprob": -0.0992902609018179, "compression_ratio": 1.7156549520766773, "no_speech_prob": 0.0021487772464752197}, {"id": 41, "seek": 21844, "start": 222.35999999999999, "end": 223.24, "text": " Das seht ihr ganz oft.", "tokens": [50560, 2846, 369, 357, 5553, 6312, 11649, 13, 50604], "temperature": 0.0, "avg_logprob": -0.0992902609018179, "compression_ratio": 1.7156549520766773, "no_speech_prob": 0.0021487772464752197}, {"id": 42, "seek": 21844, "start": 223.24, "end": 225.88, "text": " Das ist immer, wenn es um gesellschaftliche Themen geht.", "tokens": [50604, 2846, 1418, 5578, 11, 4797, 785, 1105, 5019, 22055, 10185, 39229, 7095, 13, 50736], "temperature": 0.0, "avg_logprob": -0.0992902609018179, "compression_ratio": 1.7156549520766773, "no_speech_prob": 0.0021487772464752197}, {"id": 43, "seek": 21844, "start": 225.88, "end": 228.2, "text": " Gesellschaft ist zum Beispiel eine emergente Struktur.", "tokens": [50736, 30006, 1418, 5919, 13772, 3018, 4345, 70, 1576, 745, 31543, 13, 50852], "temperature": 0.0, "avg_logprob": -0.0992902609018179, "compression_ratio": 1.7156549520766773, "no_speech_prob": 0.0021487772464752197}, {"id": 44, "seek": 21844, "start": 228.2, "end": 233.0, "text": " Sie entsteht aus den ganz vielen Interaktionen zwischen Menschen.", "tokens": [50852, 3559, 35955, 357, 3437, 1441, 6312, 19885, 5751, 5886, 17068, 19875, 8397, 13, 51092], "temperature": 0.0, "avg_logprob": -0.0992902609018179, "compression_ratio": 1.7156549520766773, "no_speech_prob": 0.0021487772464752197}, {"id": 45, "seek": 21844, "start": 233.0, "end": 235.48, "text": " Wenn wir die alle addieren, kriegen wir die Gesellschaft raus.", "tokens": [51092, 7899, 1987, 978, 5430, 909, 5695, 11, 46882, 1987, 978, 30006, 17202, 13, 51216], "temperature": 0.0, "avg_logprob": -0.0992902609018179, "compression_ratio": 1.7156549520766773, "no_speech_prob": 0.0021487772464752197}, {"id": 46, "seek": 21844, "start": 235.48, "end": 237.96, "text": " Und Evolution ist auch so was.", "tokens": [51216, 2719, 40800, 1418, 2168, 370, 390, 13, 51340], "temperature": 0.0, "avg_logprob": -0.0992902609018179, "compression_ratio": 1.7156549520766773, "no_speech_prob": 0.0021487772464752197}, {"id": 47, "seek": 21844, "start": 237.96, "end": 247.64, "text": " Evolution entsteht ja nur dadurch, dass wir einen Selektionsdruck haben, der nur dadurch entsteht, dass es sozusagen eine \u00e4u\u00dfere Umgebung gibt, die sozusagen dazu f\u00fchrt,", "tokens": [51340, 40800, 35955, 357, 2784, 4343, 35472, 11, 2658, 1987, 4891, 1100, 306, 2320, 626, 67, 8161, 3084, 11, 1163, 4343, 35472, 35955, 357, 11, 2658, 785, 33762, 3018, 3078, 43796, 323, 3301, 10848, 1063, 6089, 11, 978, 33762, 13034, 39671, 11, 51824], "temperature": 0.0, "avg_logprob": -0.0992902609018179, "compression_ratio": 1.7156549520766773, "no_speech_prob": 0.0021487772464752197}, {"id": 48, "seek": 24764, "start": 247.72, "end": 261.88, "text": " dass bestimmte Lebensformen, die besser, ich w\u00fcrde mal sagen, negative Entropie farmen k\u00f6nnen oder besser den Gradienten in die Entropie runtergehen k\u00f6nnen als andere und dadurch sozusagen einen Wettbewerbsvorteil gegeneinander haben und sich dann durchsetzen.", "tokens": [50368, 2658, 35180, 975, 21530, 837, 268, 11, 978, 18021, 11, 1893, 11942, 2806, 8360, 11, 3671, 3951, 1513, 414, 1400, 2558, 6310, 4513, 18021, 1441, 16710, 1196, 268, 294, 978, 3951, 1513, 414, 33295, 24985, 6310, 3907, 10490, 674, 35472, 33762, 4891, 343, 3093, 650, 1554, 929, 85, 12752, 388, 23982, 1450, 20553, 3084, 674, 3041, 3594, 7131, 3854, 2904, 13, 51076], "temperature": 0.0, "avg_logprob": -0.183303463843561, "compression_ratio": 1.618705035971223, "no_speech_prob": 0.011156121268868446}, {"id": 49, "seek": 24764, "start": 261.88, "end": 266.59999999999997, "text": " Und Schwarmintelligenz ist auch so was wie Emergenz.", "tokens": [51076, 2719, 17576, 4452, 20761, 3213, 89, 1418, 2168, 370, 390, 3355, 18477, 1766, 89, 13, 51312], "temperature": 0.0, "avg_logprob": -0.183303463843561, "compression_ratio": 1.618705035971223, "no_speech_prob": 0.011156121268868446}, {"id": 50, "seek": 24764, "start": 266.59999999999997, "end": 271.47999999999996, "text": " Und der Markt, das ist das, was die FDP-Menschen nicht verstehen.", "tokens": [51312, 2719, 1163, 39774, 11, 1482, 1418, 1482, 11, 390, 978, 31763, 12, 44, 694, 2470, 1979, 37352, 13, 51556], "temperature": 0.0, "avg_logprob": -0.183303463843561, "compression_ratio": 1.618705035971223, "no_speech_prob": 0.011156121268868446}, {"id": 51, "seek": 24764, "start": 271.47999999999996, "end": 272.76, "text": " Die raffen es ja nicht.", "tokens": [51556, 3229, 367, 19182, 785, 2784, 1979, 13, 51620], "temperature": 0.0, "avg_logprob": -0.183303463843561, "compression_ratio": 1.618705035971223, "no_speech_prob": 0.011156121268868446}, {"id": 52, "seek": 24764, "start": 272.76, "end": 274.44, "text": " Aber der Markt ist eine emergente Struktur.", "tokens": [51620, 5992, 1163, 39774, 1418, 3018, 4345, 70, 1576, 745, 31543, 13, 51704], "temperature": 0.0, "avg_logprob": -0.183303463843561, "compression_ratio": 1.618705035971223, "no_speech_prob": 0.011156121268868446}, {"id": 53, "seek": 27444, "start": 274.44, "end": 283.16, "text": " Und weil das so unbegreifbar ist, dachte damals Hayek auch, und das ist sozusagen so eine Art Gott f\u00fcr ihn und versteht mich da nicht falsch.", "tokens": [50364, 2719, 7689, 1482, 370, 517, 650, 33248, 351, 5356, 1418, 11, 39775, 26067, 8721, 916, 2168, 11, 674, 1482, 1418, 33762, 370, 3018, 5735, 19133, 2959, 14534, 674, 22442, 357, 6031, 1120, 1979, 43340, 13, 50800], "temperature": 0.0, "avg_logprob": -0.14677452805018662, "compression_ratio": 1.587360594795539, "no_speech_prob": 0.674347460269928}, {"id": 54, "seek": 27444, "start": 283.16, "end": 288.36, "text": " Es ist nat\u00fcrlich eine emergente Struktur und es ist nat\u00fcrlich interessant, dar\u00fcber nachzudenken.", "tokens": [50800, 2313, 1418, 8762, 3018, 4345, 70, 1576, 745, 31543, 674, 785, 1418, 8762, 37748, 11, 21737, 5168, 89, 32940, 2653, 13, 51060], "temperature": 0.0, "avg_logprob": -0.14677452805018662, "compression_ratio": 1.587360594795539, "no_speech_prob": 0.674347460269928}, {"id": 55, "seek": 27444, "start": 288.36, "end": 296.92, "text": " Aber nur weil es kompliziert ist oder irgendwie geil aussieht oder man ein bisschen Mathematik damit machen kann, hei\u00dft es ja nicht, dass das sozusagen die L\u00f6sung aller Probleme ist.", "tokens": [51060, 5992, 4343, 7689, 785, 24526, 43590, 1418, 4513, 20759, 47165, 5730, 39850, 4513, 587, 1343, 10763, 15776, 8615, 1035, 9479, 7069, 4028, 11, 13139, 785, 2784, 1979, 11, 2658, 1482, 33762, 978, 46934, 8722, 32891, 1418, 13, 51488], "temperature": 0.0, "avg_logprob": -0.14677452805018662, "compression_ratio": 1.587360594795539, "no_speech_prob": 0.674347460269928}, {"id": 56, "seek": 29692, "start": 297.0, "end": 298.52000000000004, "text": " Aber genau so geht Hayek daran.", "tokens": [50368, 5992, 12535, 370, 7095, 8721, 916, 24520, 13, 50444], "temperature": 0.0, "avg_logprob": -0.18713317949747302, "compression_ratio": 1.4798387096774193, "no_speech_prob": 0.38438764214515686}, {"id": 57, "seek": 29692, "start": 298.52000000000004, "end": 304.12, "text": " Das habe ich auch schon mal in einem anderen Video erkl\u00e4rt, von wegen welches Problem l\u00f6sen die M\u00e4rkte eigentlich.", "tokens": [50444, 2846, 6015, 1893, 2168, 4981, 2806, 294, 6827, 11122, 9777, 27570, 24802, 11, 2957, 32855, 2214, 3781, 11676, 25209, 6748, 978, 46084, 18844, 10926, 13, 50724], "temperature": 0.0, "avg_logprob": -0.18713317949747302, "compression_ratio": 1.4798387096774193, "no_speech_prob": 0.38438764214515686}, {"id": 58, "seek": 29692, "start": 307.0, "end": 317.24, "text": " So und was soll ich sagen, wenn es etwas in den letzten Jahren gab, was ich mir sehr gerne reingezogen habe, dann sind das Videos von Joscha Bach.", "tokens": [50868, 407, 674, 390, 7114, 1893, 8360, 11, 4797, 785, 9569, 294, 1441, 18226, 13080, 17964, 11, 390, 1893, 3149, 5499, 15689, 319, 278, 4371, 8799, 6015, 11, 3594, 3290, 1482, 25903, 2957, 18541, 4413, 30920, 13, 51380], "temperature": 0.0, "avg_logprob": -0.18713317949747302, "compression_ratio": 1.4798387096774193, "no_speech_prob": 0.38438764214515686}, {"id": 59, "seek": 29692, "start": 317.24, "end": 321.8, "text": " Und ich kann euch wirklich nur empfehlen, schmei\u00dft alles in die Ecke.", "tokens": [51380, 2719, 1893, 4028, 10403, 9696, 4343, 4012, 33865, 6698, 11, 46459, 11539, 7874, 294, 978, 462, 18627, 13, 51608], "temperature": 0.0, "avg_logprob": -0.18713317949747302, "compression_ratio": 1.4798387096774193, "no_speech_prob": 0.38438764214515686}, {"id": 60, "seek": 32180, "start": 322.12, "end": 328.92, "text": " Jo Roggen Podcasts oder alle m\u00f6glichen Lex Friedman Interviews oder sonst welche Sachen, die ihr im englischen YouTube drin gesehen habt.", "tokens": [50380, 3139, 11860, 1766, 29972, 82, 4513, 5430, 16294, 268, 24086, 17605, 1601, 35599, 82, 4513, 26309, 24311, 26074, 11, 978, 5553, 566, 1741, 75, 6282, 3088, 24534, 21535, 23660, 13, 50720], "temperature": 0.0, "avg_logprob": -0.17205187678337097, "compression_ratio": 1.5537459283387622, "no_speech_prob": 0.5648336410522461}, {"id": 61, "seek": 32180, "start": 328.92, "end": 334.68, "text": " Das n\u00e4chste, was ihr bei euch auf die Watchlist drauf tut, ist der erste Talk von Joscha Bach.", "tokens": [50720, 2846, 30661, 11, 390, 5553, 4643, 10403, 2501, 978, 7277, 8264, 22763, 3672, 11, 1418, 1163, 20951, 8780, 2957, 18541, 4413, 30920, 13, 51008], "temperature": 0.0, "avg_logprob": -0.17205187678337097, "compression_ratio": 1.5537459283387622, "no_speech_prob": 0.5648336410522461}, {"id": 62, "seek": 32180, "start": 334.68, "end": 336.84000000000003, "text": " Und der zweite, den k\u00f6nnt ihr gleich noch hinten dran h\u00e4ngen.", "tokens": [51008, 2719, 1163, 37456, 11, 1441, 22541, 5553, 11699, 3514, 36417, 32801, 276, 43921, 13, 51116], "temperature": 0.0, "avg_logprob": -0.17205187678337097, "compression_ratio": 1.5537459283387622, "no_speech_prob": 0.5648336410522461}, {"id": 63, "seek": 32180, "start": 336.84000000000003, "end": 341.88, "text": " Dann habt ihr sechs Stunden lang komprimierteste Todesweisheit des Todes.", "tokens": [51116, 7455, 23660, 5553, 41945, 30496, 2265, 5207, 1424, 332, 4859, 8887, 2465, 279, 35033, 8480, 730, 2465, 279, 13, 51368], "temperature": 0.0, "avg_logprob": -0.17205187678337097, "compression_ratio": 1.5537459283387622, "no_speech_prob": 0.5648336410522461}, {"id": 64, "seek": 32180, "start": 341.88, "end": 344.36, "text": " Der Typ ist der lebende Sheldon Cooper.", "tokens": [51368, 5618, 17722, 1418, 1163, 17111, 5445, 1160, 5957, 266, 20355, 13, 51492], "temperature": 0.0, "avg_logprob": -0.17205187678337097, "compression_ratio": 1.5537459283387622, "no_speech_prob": 0.5648336410522461}, {"id": 65, "seek": 32180, "start": 344.36, "end": 348.6, "text": " Das ist der krasseste Wichser, den ich je irgendwo geh\u00f6rt habe.", "tokens": [51492, 2846, 1418, 1163, 15913, 640, 8887, 343, 480, 12484, 11, 1441, 1893, 1506, 40865, 21544, 6015, 13, 51704], "temperature": 0.0, "avg_logprob": -0.17205187678337097, "compression_ratio": 1.5537459283387622, "no_speech_prob": 0.5648336410522461}, {"id": 66, "seek": 34860, "start": 348.6, "end": 351.96000000000004, "text": " Ich glaube, das ist so ein verdammtes Genie.", "tokens": [50364, 3141, 13756, 11, 1482, 1418, 370, 1343, 6387, 5136, 7269, 3632, 414, 13, 50532], "temperature": 0.0, "avg_logprob": -0.1765558655197556, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.1622174233198166}, {"id": 67, "seek": 34860, "start": 351.96000000000004, "end": 358.76000000000005, "text": " Und der lebt heutzutage und ich habe ihn schon auf Twitter vollgequatscht, er soll mal mit mir reden, aber nat\u00fcrlich, warum sollte der das tun?", "tokens": [50532, 2719, 1163, 476, 4517, 415, 12950, 325, 609, 674, 1893, 6015, 14534, 4981, 2501, 5794, 15593, 432, 358, 1720, 4701, 11, 1189, 7114, 2806, 2194, 3149, 26447, 11, 4340, 8762, 11, 24331, 18042, 1163, 1482, 4267, 30, 50872], "temperature": 0.0, "avg_logprob": -0.1765558655197556, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.1622174233198166}, {"id": 68, "seek": 34860, "start": 358.76000000000005, "end": 362.20000000000005, "text": " Der hat einfach eine volle Inbox, der hat keine Zeit mit einem schei\u00df Ossi zu reden.", "tokens": [50872, 5618, 2385, 7281, 3018, 1650, 2447, 682, 4995, 11, 1163, 2385, 9252, 9394, 2194, 6827, 25690, 6230, 422, 3810, 72, 2164, 26447, 13, 51044], "temperature": 0.0, "avg_logprob": -0.1765558655197556, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.1622174233198166}, {"id": 69, "seek": 34860, "start": 362.20000000000005, "end": 364.44, "text": " By the way, Joscha Bach ist selber ein Ossi.", "tokens": [51044, 3146, 264, 636, 11, 18541, 4413, 30920, 1418, 23888, 1343, 422, 3810, 72, 13, 51156], "temperature": 0.0, "avg_logprob": -0.1765558655197556, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.1622174233198166}, {"id": 70, "seek": 34860, "start": 364.44, "end": 366.68, "text": " Aber ihr m\u00fcsst euch den Typen reinziehen.", "tokens": [51156, 5992, 5553, 49481, 10403, 1441, 5569, 5200, 6561, 28768, 13, 51268], "temperature": 0.0, "avg_logprob": -0.1765558655197556, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.1622174233198166}, {"id": 71, "seek": 34860, "start": 366.68, "end": 371.40000000000003, "text": " Der alles, was er jemals in seinem Leben gelesen hat, hat er hier oben drin und kann sofort abrufen.", "tokens": [51268, 5618, 7874, 11, 390, 1189, 361, 443, 1124, 294, 29187, 15399, 4087, 17403, 2385, 11, 2385, 1189, 3296, 21279, 24534, 674, 4028, 33168, 410, 894, 6570, 13, 51504], "temperature": 0.0, "avg_logprob": -0.1765558655197556, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.1622174233198166}, {"id": 72, "seek": 34860, "start": 371.40000000000003, "end": 372.92, "text": " Der ist krass, der Typ.", "tokens": [51504, 5618, 1418, 15913, 640, 11, 1163, 17722, 13, 51580], "temperature": 0.0, "avg_logprob": -0.1765558655197556, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.1622174233198166}, {"id": 73, "seek": 37292, "start": 373.88, "end": 381.88, "text": " Und ich bin \u00fcbrigens nur auf den gesto\u00dfen, weil jemand mir den empfohlen hat unter einem englischen Matrix-Video.", "tokens": [50412, 2719, 1893, 5171, 38215, 4343, 2501, 1441, 7219, 78, 8989, 11, 7689, 21717, 3149, 1441, 4012, 69, 1445, 6698, 2385, 8662, 6827, 1741, 75, 6282, 36274, 12, 46287, 13, 50812], "temperature": 0.0, "avg_logprob": -0.19709853271939862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.6570453643798828}, {"id": 74, "seek": 37292, "start": 381.88, "end": 383.40000000000003, "text": " Hat mir jemand Joscha Bach empfohlen.", "tokens": [50812, 15867, 3149, 21717, 18541, 4413, 30920, 4012, 69, 1445, 6698, 13, 50888], "temperature": 0.0, "avg_logprob": -0.19709853271939862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.6570453643798828}, {"id": 75, "seek": 37292, "start": 383.40000000000003, "end": 385.56, "text": " Daf\u00fcr hat sich der englische Kanal schon gelohnt.", "tokens": [50888, 35865, 2385, 3041, 1163, 1741, 75, 7864, 38643, 4981, 4087, 1445, 580, 13, 50996], "temperature": 0.0, "avg_logprob": -0.19709853271939862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.6570453643798828}, {"id": 76, "seek": 37292, "start": 385.56, "end": 387.24, "text": " Das ist super.", "tokens": [50996, 2846, 1418, 1687, 13, 51080], "temperature": 0.0, "avg_logprob": -0.19709853271939862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.6570453643798828}, {"id": 77, "seek": 37292, "start": 387.24, "end": 392.6, "text": " Und jetzt wollte ich ein bisschen, jetzt haben wir das gekl\u00e4rt mit der Emergenz, ich habe jetzt dieses Wort etabliert.", "tokens": [51080, 2719, 4354, 24509, 1893, 1343, 10763, 11, 4354, 3084, 1987, 1482, 14037, 75, 24802, 2194, 1163, 18477, 1766, 89, 11, 1893, 6015, 4354, 12113, 22748, 1030, 455, 2753, 83, 13, 51348], "temperature": 0.0, "avg_logprob": -0.19709853271939862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.6570453643798828}, {"id": 78, "seek": 37292, "start": 392.6, "end": 401.08000000000004, "text": " Jetzt will ich ein anderes wichtiges Wort f\u00fcr euch etablieren, weil das ist nat\u00fcrlich auch so ein Ding, wo die Leute immer rumschlavieren.", "tokens": [51348, 12592, 486, 1893, 1343, 31426, 13621, 279, 22748, 2959, 10403, 1030, 455, 2753, 268, 11, 7689, 1482, 1418, 8762, 2168, 370, 1343, 20558, 11, 6020, 978, 13495, 5578, 8347, 6145, 36243, 5695, 13, 51772], "temperature": 0.0, "avg_logprob": -0.19709853271939862, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.6570453643798828}, {"id": 79, "seek": 40108, "start": 401.08, "end": 404.28, "text": " Und irgendwie so so eine Kr\u00e4mer-Laber-Attit\u00fcde haben.", "tokens": [50364, 2719, 20759, 370, 370, 3018, 6332, 737, 936, 12, 43, 23298, 12, 38151, 270, 774, 1479, 3084, 13, 50524], "temperature": 0.0, "avg_logprob": -0.13354679368297506, "compression_ratio": 1.6839080459770115, "no_speech_prob": 0.13829387724399567}, {"id": 80, "seek": 40108, "start": 404.28, "end": 406.52, "text": " Was ist jetzt eigentlich Intelligenz?", "tokens": [50524, 3027, 1418, 4354, 10926, 18762, 3213, 89, 30, 50636], "temperature": 0.0, "avg_logprob": -0.13354679368297506, "compression_ratio": 1.6839080459770115, "no_speech_prob": 0.13829387724399567}, {"id": 81, "seek": 40108, "start": 406.52, "end": 412.91999999999996, "text": " Also guter Freund von mir hat damals zu mir gesagt, Intelligenz ist die F\u00e4higkeit, Analogien zu bilden.", "tokens": [50636, 2743, 5228, 260, 29685, 2957, 3149, 2385, 26067, 2164, 3149, 12260, 11, 18762, 3213, 89, 1418, 978, 479, 6860, 16626, 11, 16128, 664, 1053, 2164, 22105, 268, 13, 50956], "temperature": 0.0, "avg_logprob": -0.13354679368297506, "compression_ratio": 1.6839080459770115, "no_speech_prob": 0.13829387724399567}, {"id": 82, "seek": 40108, "start": 412.91999999999996, "end": 419.0, "text": " Und das ist eine ziemlich gute Definition, wenn man da mal dr\u00fcber nachdenkt, weil damit kann man eine ganze Menge erschlagen.", "tokens": [50956, 2719, 1482, 1418, 3018, 28901, 21476, 46245, 849, 11, 4797, 587, 1120, 2806, 1224, 12670, 5168, 1556, 2320, 11, 7689, 9479, 4028, 587, 3018, 18898, 40723, 33743, 44496, 13, 51260], "temperature": 0.0, "avg_logprob": -0.13354679368297506, "compression_ratio": 1.6839080459770115, "no_speech_prob": 0.13829387724399567}, {"id": 83, "seek": 40108, "start": 419.0, "end": 422.36, "text": " Quasi ich habe hier, sagen wir mal, verstanden, wie das eine Ding funktioniert.", "tokens": [51260, 2326, 8483, 1893, 6015, 3296, 11, 8360, 1987, 2806, 11, 1306, 33946, 11, 3355, 1482, 3018, 20558, 26160, 13, 51428], "temperature": 0.0, "avg_logprob": -0.13354679368297506, "compression_ratio": 1.6839080459770115, "no_speech_prob": 0.13829387724399567}, {"id": 84, "seek": 40108, "start": 422.36, "end": 425.15999999999997, "text": " Und jetzt habe ich hier ein anderes Ding, das hei\u00dft anders.", "tokens": [51428, 2719, 4354, 6015, 1893, 3296, 1343, 31426, 20558, 11, 1482, 13139, 17999, 13, 51568], "temperature": 0.0, "avg_logprob": -0.13354679368297506, "compression_ratio": 1.6839080459770115, "no_speech_prob": 0.13829387724399567}, {"id": 85, "seek": 40108, "start": 425.15999999999997, "end": 430.68, "text": " Aber der Mechanismus, der hier abl\u00e4uft, ist genau derselbe Mechanismus wie in dem Ding, was ich schon verstanden habe.", "tokens": [51568, 5992, 1163, 30175, 25327, 11, 1163, 3296, 410, 22882, 25005, 11, 1418, 12535, 39636, 338, 650, 30175, 25327, 3355, 294, 1371, 20558, 11, 390, 1893, 4981, 1306, 33946, 6015, 13, 51844], "temperature": 0.0, "avg_logprob": -0.13354679368297506, "compression_ratio": 1.6839080459770115, "no_speech_prob": 0.13829387724399567}, {"id": 86, "seek": 43068, "start": 430.68, "end": 437.48, "text": " Wenn ich also die Analogie bilden kann und feststellen kann, dass abgesehen von den Bezeichnungen, dass da was da passiert, dasselbe oder das gleiche ist,", "tokens": [50364, 7899, 1893, 611, 978, 16128, 39031, 22105, 268, 4028, 674, 6633, 17538, 4028, 11, 2658, 410, 70, 18380, 2957, 1441, 879, 32338, 77, 5084, 11, 2658, 1120, 390, 1120, 21671, 11, 2658, 338, 650, 4513, 1482, 11699, 68, 1418, 11, 50704], "temperature": 0.0, "avg_logprob": -0.18820775788405847, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0022168040741235018}, {"id": 87, "seek": 43068, "start": 437.48, "end": 440.28000000000003, "text": " dann habe ich eine Analogie gebildet und dann bin ich krass.", "tokens": [50704, 3594, 6015, 1893, 3018, 16128, 39031, 1519, 16248, 302, 674, 3594, 5171, 1893, 15913, 640, 13, 50844], "temperature": 0.0, "avg_logprob": -0.18820775788405847, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0022168040741235018}, {"id": 88, "seek": 43068, "start": 440.28000000000003, "end": 443.08, "text": " Und der King der Analogien, meiner Meinung nach, ist Slavoj G. Jack.", "tokens": [50844, 2719, 1163, 3819, 1163, 16128, 664, 1053, 11, 20529, 36519, 5168, 11, 1418, 6187, 25713, 73, 460, 13, 4718, 13, 50984], "temperature": 0.0, "avg_logprob": -0.18820775788405847, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0022168040741235018}, {"id": 89, "seek": 43068, "start": 443.08, "end": 445.24, "text": " Der hat die witzigsten Analogien.", "tokens": [50984, 5618, 2385, 978, 261, 6862, 328, 6266, 16128, 664, 1053, 13, 51092], "temperature": 0.0, "avg_logprob": -0.18820775788405847, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0022168040741235018}, {"id": 90, "seek": 43068, "start": 445.24, "end": 450.2, "text": " So und Joscha Bach jetzt, der sagt, Intelligenz ist die F\u00e4higkeit, Modelle zu bauen.", "tokens": [51092, 407, 674, 18541, 4413, 30920, 4354, 11, 1163, 15764, 11, 18762, 3213, 89, 1418, 978, 479, 6860, 16626, 11, 6583, 4434, 2164, 43787, 13, 51340], "temperature": 0.0, "avg_logprob": -0.18820775788405847, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0022168040741235018}, {"id": 91, "seek": 43068, "start": 450.2, "end": 452.92, "text": " Also Modelle \u00fcber das, was man sieht.", "tokens": [51340, 2743, 6583, 4434, 4502, 1482, 11, 390, 587, 14289, 13, 51476], "temperature": 0.0, "avg_logprob": -0.18820775788405847, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0022168040741235018}, {"id": 92, "seek": 43068, "start": 452.92, "end": 454.6, "text": " Und was meint man jetzt damit?", "tokens": [51476, 2719, 390, 385, 686, 587, 4354, 9479, 30, 51560], "temperature": 0.0, "avg_logprob": -0.18820775788405847, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.0022168040741235018}, {"id": 93, "seek": 45460, "start": 454.68, "end": 461.64000000000004, "text": " Ein mathematisches Modell typischerweise hat was damit zu tun, dass man eine Komprimierung macht.", "tokens": [50368, 6391, 11619, 35889, 6583, 898, 2125, 19674, 13109, 2385, 390, 9479, 2164, 4267, 11, 2658, 587, 3018, 14286, 1424, 332, 11651, 10857, 13, 50716], "temperature": 0.0, "avg_logprob": -0.12302442235270823, "compression_ratio": 1.6891385767790261, "no_speech_prob": 0.3588358461856842}, {"id": 94, "seek": 45460, "start": 461.64000000000004, "end": 469.88, "text": " Man kompressiert, man komprimiert irgendwie etwas, was man sieht, auf das Wesentliche herunter.", "tokens": [50716, 2458, 5207, 11637, 4859, 11, 587, 5207, 1424, 332, 4859, 20759, 9569, 11, 390, 587, 14289, 11, 2501, 1482, 23843, 7698, 68, 720, 21777, 13, 51128], "temperature": 0.0, "avg_logprob": -0.12302442235270823, "compression_ratio": 1.6891385767790261, "no_speech_prob": 0.3588358461856842}, {"id": 95, "seek": 45460, "start": 469.88, "end": 472.04, "text": " Das ist das, was beim Komprimieren passiert.", "tokens": [51128, 2846, 1418, 1482, 11, 390, 13922, 14286, 1424, 332, 5695, 21671, 13, 51236], "temperature": 0.0, "avg_logprob": -0.12302442235270823, "compression_ratio": 1.6891385767790261, "no_speech_prob": 0.3588358461856842}, {"id": 96, "seek": 45460, "start": 472.04, "end": 479.8, "text": " Und da muss man nat\u00fcrlich sich genau fragen, in welcher Sprache habe ich es auf das Wesentliche, auf das Minimum runtergekocht?", "tokens": [51236, 2719, 1120, 6425, 587, 8762, 3041, 12535, 39129, 11, 294, 2214, 6759, 7702, 6000, 6015, 1893, 785, 2501, 1482, 23843, 7698, 68, 11, 2501, 1482, 2829, 332, 449, 33295, 432, 4093, 4701, 30, 51624], "temperature": 0.0, "avg_logprob": -0.12302442235270823, "compression_ratio": 1.6891385767790261, "no_speech_prob": 0.3588358461856842}, {"id": 97, "seek": 45460, "start": 479.8, "end": 483.40000000000003, "text": " Und das ist genau die Fragestellung, die man beim Archivieren von Dateien auch hat.", "tokens": [51624, 2719, 1482, 1418, 12535, 978, 5849, 2629, 898, 1063, 11, 978, 587, 13922, 10984, 592, 5695, 2957, 31805, 1053, 2168, 2385, 13, 51804], "temperature": 0.0, "avg_logprob": -0.12302442235270823, "compression_ratio": 1.6891385767790261, "no_speech_prob": 0.3588358461856842}, {"id": 98, "seek": 48340, "start": 483.4, "end": 488.67999999999995, "text": " Kann man sozusagen ein Muster finden, auf die man es runterbrechen kann?", "tokens": [50364, 29074, 587, 33762, 1343, 376, 8393, 20734, 11, 2501, 978, 587, 785, 33295, 2672, 2470, 4028, 30, 50628], "temperature": 0.0, "avg_logprob": -0.1888586680094401, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.05661555007100105}, {"id": 99, "seek": 48340, "start": 488.67999999999995, "end": 494.67999999999995, "text": " Kann ich das Muster sozusagen einmal abspeichern und dann nur noch sagen, jetzt kommt das Muster und dann spare ich mir 500 Bits oder so?", "tokens": [50628, 29074, 1893, 1482, 376, 8393, 33762, 11078, 1950, 494, 480, 1248, 674, 3594, 4343, 3514, 8360, 11, 4354, 10047, 1482, 376, 8393, 674, 3594, 13798, 1893, 3149, 5923, 363, 1208, 4513, 370, 30, 50928], "temperature": 0.0, "avg_logprob": -0.1888586680094401, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.05661555007100105}, {"id": 100, "seek": 48340, "start": 494.67999999999995, "end": 496.67999999999995, "text": " So eine Idee ist das.", "tokens": [50928, 407, 3018, 32651, 1418, 1482, 13, 51028], "temperature": 0.0, "avg_logprob": -0.1888586680094401, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.05661555007100105}, {"id": 101, "seek": 48340, "start": 496.67999999999995, "end": 500.35999999999996, "text": " Und darauf basieren dann sozusagen die ganzen modernen Komprimierungsverfahren auch.", "tokens": [51028, 2719, 18654, 987, 5695, 3594, 33762, 978, 23966, 4363, 268, 14286, 1424, 332, 40908, 331, 34394, 2168, 13, 51212], "temperature": 0.0, "avg_logprob": -0.1888586680094401, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.05661555007100105}, {"id": 102, "seek": 48340, "start": 500.35999999999996, "end": 507.64, "text": " Im Wesentlichen, in der Mathematik w\u00fcrde man sagen, man findet eine Vektorbasis oder im Kompress-Censing w\u00fcrde man sagen, man findet eine Basis,", "tokens": [51212, 4331, 23843, 7698, 268, 11, 294, 1163, 15776, 8615, 1035, 11942, 587, 8360, 11, 587, 27752, 3018, 691, 8192, 284, 16342, 271, 4513, 566, 591, 8586, 735, 12, 34, 22481, 11942, 587, 8360, 11, 587, 27752, 3018, 5859, 271, 11, 51576], "temperature": 0.0, "avg_logprob": -0.1888586680094401, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.05661555007100105}, {"id": 103, "seek": 50764, "start": 507.71999999999997, "end": 519.96, "text": " die irgendwie im Sinne der L2-Norm oder beziehungsweise in der L1-Norm, eigentlich in der L0-Norm, ich glaube es ist tats\u00e4chlich die L0-Norm oder die L1-Norm, da wo das sozusagen ein Minimum annimmt.", "tokens": [50368, 978, 20759, 566, 47041, 1163, 441, 17, 12, 45, 687, 4513, 312, 28213, 5846, 13109, 294, 1163, 441, 16, 12, 45, 687, 11, 10926, 294, 1163, 441, 15, 12, 45, 687, 11, 1893, 13756, 785, 1418, 20796, 978, 441, 15, 12, 45, 687, 4513, 978, 441, 16, 12, 45, 687, 11, 1120, 6020, 1482, 33762, 1343, 2829, 332, 449, 2324, 15314, 13, 50980], "temperature": 0.0, "avg_logprob": -0.12775773366292317, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.4179893434047699}, {"id": 104, "seek": 50764, "start": 519.96, "end": 524.04, "text": " Das ist quasi Kompress-Censing, aber nur f\u00fcr die Geeks und Nerds, die sich damit auskennen.", "tokens": [50980, 2846, 1418, 20954, 591, 8586, 735, 12, 34, 22481, 11, 4340, 4343, 2959, 978, 2876, 24785, 674, 38367, 82, 11, 978, 3041, 9479, 3437, 74, 16043, 13, 51184], "temperature": 0.0, "avg_logprob": -0.12775773366292317, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.4179893434047699}, {"id": 105, "seek": 50764, "start": 524.04, "end": 530.6, "text": " Ansonsten, ihr k\u00f6nnt euch einfach vorstellen, wann kann ich sozusagen etwas aufs Wesentliche so weit runterbrechen, bis es nicht mehr weiter runtergeht?", "tokens": [51184, 1107, 3015, 6266, 11, 5553, 22541, 10403, 7281, 34346, 11, 38064, 4028, 1893, 33762, 9569, 2501, 82, 23843, 7698, 68, 370, 15306, 33295, 2672, 2470, 11, 7393, 785, 1979, 5417, 8988, 33295, 46227, 30, 51512], "temperature": 0.0, "avg_logprob": -0.12775773366292317, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.4179893434047699}, {"id": 106, "seek": 50764, "start": 530.6, "end": 533.88, "text": " Bis ich es sozusagen auf die Einsen und Nullen runtergebrochen habe.", "tokens": [51512, 25271, 1893, 785, 33762, 2501, 978, 22790, 268, 674, 426, 32516, 33295, 432, 9120, 2470, 6015, 13, 51676], "temperature": 0.0, "avg_logprob": -0.12775773366292317, "compression_ratio": 1.664516129032258, "no_speech_prob": 0.4179893434047699}, {"id": 107, "seek": 53388, "start": 533.96, "end": 544.76, "text": " Und Sprache ist sozusagen in diesem Kontext sozusagen eine High-Level-Abstraktion, weil sie ist sehr schwammig und Mathematik ist quasi Low-Level.", "tokens": [50368, 2719, 7702, 6000, 1418, 33762, 294, 10975, 20629, 3828, 33762, 3018, 5229, 12, 11020, 779, 12, 32, 9690, 39694, 11, 7689, 2804, 1418, 5499, 17932, 5136, 328, 674, 15776, 8615, 1035, 1418, 20954, 17078, 12, 11020, 779, 13, 50908], "temperature": 0.0, "avg_logprob": -0.13518412907918295, "compression_ratio": 1.609375, "no_speech_prob": 0.15594491362571716}, {"id": 108, "seek": 53388, "start": 544.76, "end": 550.04, "text": " Man kann sozusagen das eine in das andere \u00fcberf\u00fchren, aber es ist sehr langwierig.", "tokens": [50908, 2458, 4028, 33762, 1482, 3018, 294, 1482, 10490, 4502, 69, 29540, 11, 4340, 785, 1418, 5499, 2265, 40717, 328, 13, 51172], "temperature": 0.0, "avg_logprob": -0.13518412907918295, "compression_ratio": 1.609375, "no_speech_prob": 0.15594491362571716}, {"id": 109, "seek": 53388, "start": 550.04, "end": 558.36, "text": " Bestimmte Sachverhalte lassen sich mit Sprache in ein, zwei S\u00e4tzen erkl\u00e4ren und man m\u00fcsste einen riesen Apparat Mathematik drauf werfen, um dasselbe in Mathematik auszudr\u00fccken.", "tokens": [51172, 9752, 6753, 975, 25626, 331, 4947, 975, 16168, 3041, 2194, 7702, 6000, 294, 1343, 11, 12002, 318, 45721, 46528, 674, 587, 42962, 4891, 23932, 268, 3132, 18452, 15776, 8615, 1035, 22763, 2612, 6570, 11, 1105, 2658, 338, 650, 294, 15776, 8615, 1035, 3437, 89, 532, 81, 26037, 13, 51588], "temperature": 0.0, "avg_logprob": -0.13518412907918295, "compression_ratio": 1.609375, "no_speech_prob": 0.15594491362571716}, {"id": 110, "seek": 55836, "start": 558.6800000000001, "end": 564.44, "text": " Joscha Bach sagte, Mathematik ist die Dom\u00e4ne aller Sprachen und mittlerweile glaube ich, da hat er nat\u00fcrlich recht.", "tokens": [50380, 18541, 4413, 30920, 36771, 11, 15776, 8615, 1035, 1418, 978, 16674, 737, 716, 8722, 7702, 11646, 674, 41999, 13756, 1893, 11, 1120, 2385, 1189, 8762, 24261, 13, 50668], "temperature": 0.0, "avg_logprob": -0.22210483358363914, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.14209608733654022}, {"id": 111, "seek": 55836, "start": 564.44, "end": 574.84, "text": " Das ist ziemlich krass, denn es ist so, Sprache in diesem Sinne, das Kompress-Censing zum Beispiel, ist eine Projektion von Inhalt.", "tokens": [50668, 2846, 1418, 28901, 15913, 640, 11, 10471, 785, 1418, 370, 11, 7702, 6000, 294, 10975, 47041, 11, 1482, 591, 8586, 735, 12, 34, 22481, 5919, 13772, 11, 1418, 3018, 34804, 313, 2957, 682, 20731, 13, 51188], "temperature": 0.0, "avg_logprob": -0.22210483358363914, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.14209608733654022}, {"id": 112, "seek": 55836, "start": 576.6800000000001, "end": 583.72, "text": " Wir haben hier dieses Ding, wir haben diese echten Inhalte, die sind kompliziert und mit Sprache vereinfachen wir die.", "tokens": [51280, 4347, 3084, 3296, 12113, 20558, 11, 1987, 3084, 6705, 308, 21043, 682, 4947, 975, 11, 978, 3290, 24526, 43590, 674, 2194, 7702, 6000, 49162, 6749, 268, 1987, 978, 13, 51632], "temperature": 0.0, "avg_logprob": -0.22210483358363914, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.14209608733654022}, {"id": 113, "seek": 58372, "start": 583.72, "end": 591.64, "text": " Wir machen schon so eine Art Komprimierung, wir komprimieren schon den Inhalt, sodass wir ihn r\u00fcberreichen k\u00f6nnen und nat\u00fcrlich ist dann sozusagen Informationsverlust.", "tokens": [50364, 4347, 7069, 4981, 370, 3018, 5735, 14286, 1424, 332, 11651, 11, 1987, 5207, 1424, 332, 5695, 4981, 1441, 682, 20731, 11, 15047, 640, 1987, 14534, 367, 12670, 29119, 6310, 674, 8762, 1418, 3594, 33762, 34301, 763, 331, 75, 381, 13, 50760], "temperature": 0.0, "avg_logprob": -0.20048333790676653, "compression_ratio": 1.745583038869258, "no_speech_prob": 0.04532858356833458}, {"id": 114, "seek": 58372, "start": 591.64, "end": 597.96, "text": " Und das ist bei jeder Projektion, jeder der lineare Algebra schon mal geh\u00f6rt hat, wei\u00df, Projektionen verlieren Informationen und das ist da auch der Fall.", "tokens": [50760, 2719, 1482, 1418, 4643, 19610, 34804, 313, 11, 19610, 1163, 1622, 543, 967, 19983, 4981, 2806, 21544, 2385, 11, 13385, 11, 34804, 17068, 49331, 268, 46753, 674, 1482, 1418, 1120, 2168, 1163, 7465, 13, 51076], "temperature": 0.0, "avg_logprob": -0.20048333790676653, "compression_ratio": 1.745583038869258, "no_speech_prob": 0.04532858356833458}, {"id": 115, "seek": 58372, "start": 597.96, "end": 606.84, "text": " Und das ist ja klar, Sprache kann beim Empf\u00e4nger anders ankommen, als wir es eigentlich als der Empf\u00e4nger, als der Aussender der Sprache es eigentlich sagen wollte.", "tokens": [51076, 2719, 1482, 1418, 2784, 14743, 11, 7702, 6000, 4028, 13922, 8599, 69, 35174, 17999, 364, 13675, 11, 3907, 1987, 785, 10926, 3907, 1163, 8599, 69, 35174, 11, 3907, 1163, 21286, 3216, 1163, 7702, 6000, 785, 10926, 8360, 24509, 13, 51520], "temperature": 0.0, "avg_logprob": -0.20048333790676653, "compression_ratio": 1.745583038869258, "no_speech_prob": 0.04532858356833458}, {"id": 116, "seek": 60684, "start": 606.84, "end": 613.8000000000001, "text": " Und ja, Wittgenstein zum Beispiel dachte, dass man alles mit Sprache aufbauen kann und das war sozusagen sein Fail.", "tokens": [50364, 2719, 2784, 11, 343, 593, 1766, 9089, 5919, 13772, 39775, 11, 2658, 587, 7874, 2194, 7702, 6000, 2501, 65, 11715, 4028, 674, 1482, 1516, 33762, 6195, 39094, 13, 50712], "temperature": 0.0, "avg_logprob": -0.1750730105808803, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.061833981424570084}, {"id": 117, "seek": 60684, "start": 613.8000000000001, "end": 620.2800000000001, "text": " Er hat sozusagen nicht runterbrechen k\u00f6nnen auf Mathematik, sondern er dachte Sprache ist sozusagen der Shit.", "tokens": [50712, 3300, 2385, 33762, 1979, 33295, 2672, 2470, 6310, 2501, 15776, 8615, 1035, 11, 11465, 1189, 39775, 7702, 6000, 1418, 33762, 1163, 19593, 13, 51036], "temperature": 0.0, "avg_logprob": -0.1750730105808803, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.061833981424570084}, {"id": 118, "seek": 60684, "start": 620.2800000000001, "end": 627.1600000000001, "text": " Und deswegen sage ich ja auch, Wittgenstein h\u00e4tte die Sache mit dem Word Embeddings in Deep Learning auch richtig gut gefallen.", "tokens": [51036, 2719, 26482, 19721, 1893, 2784, 2168, 11, 343, 593, 1766, 9089, 20041, 978, 31452, 2194, 1371, 8725, 24234, 292, 29432, 294, 14895, 15205, 2168, 13129, 5228, 39935, 13, 51380], "temperature": 0.0, "avg_logprob": -0.1750730105808803, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.061833981424570084}, {"id": 119, "seek": 60684, "start": 627.1600000000001, "end": 632.2800000000001, "text": " Ja, aber so Sachen wie Bilder oder Geometrie mit Sprache zu beschreiben, ist halt v\u00f6llig Fail.", "tokens": [51380, 3530, 11, 4340, 370, 26074, 3355, 44719, 4513, 2876, 649, 5469, 2194, 7702, 6000, 2164, 17498, 25946, 11, 1418, 12479, 35670, 39094, 13, 51636], "temperature": 0.0, "avg_logprob": -0.1750730105808803, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.061833981424570084}, {"id": 120, "seek": 63228, "start": 632.52, "end": 637.64, "text": " Da ist v\u00f6llig klar, dass die etabliert, also die bessere Variante irgendwie Mathematik sein muss.", "tokens": [50376, 3933, 1418, 35670, 14743, 11, 2658, 978, 1030, 455, 2753, 83, 11, 611, 978, 42410, 323, 32511, 2879, 20759, 15776, 8615, 1035, 6195, 6425, 13, 50632], "temperature": 0.0, "avg_logprob": -0.14970910030862558, "compression_ratio": 1.4786324786324787, "no_speech_prob": 0.3203003406524658}, {"id": 121, "seek": 63228, "start": 637.64, "end": 645.56, "text": " Und jetzt wollte ich mal erkl\u00e4ren, was nennen wir eigentlich Realit\u00e4t bzw. was nennen wir unser Universum?", "tokens": [50632, 2719, 4354, 24509, 1893, 2806, 46528, 11, 390, 297, 16043, 1987, 10926, 8467, 14053, 39998, 13, 390, 297, 16043, 1987, 12977, 14052, 449, 30, 51028], "temperature": 0.0, "avg_logprob": -0.14970910030862558, "compression_ratio": 1.4786324786324787, "no_speech_prob": 0.3203003406524658}, {"id": 122, "seek": 63228, "start": 645.56, "end": 657.0799999999999, "text": " Na ja, wir bezeichnen mit Realit\u00e4t typischerweise das, was unser Modell als Ursache angibt f\u00fcr den sensorischen Input, den wir erfahren.", "tokens": [51028, 6056, 2784, 11, 1987, 312, 32338, 2866, 2194, 8467, 14053, 2125, 19674, 13109, 1482, 11, 390, 12977, 6583, 898, 3907, 41303, 6000, 2562, 13651, 2959, 1441, 10200, 6282, 682, 2582, 11, 1441, 1987, 49472, 13, 51604], "temperature": 0.0, "avg_logprob": -0.14970910030862558, "compression_ratio": 1.4786324786324787, "no_speech_prob": 0.3203003406524658}, {"id": 123, "seek": 65708, "start": 657.08, "end": 663.24, "text": " Das einzige, was unser Gehirn bekommt, ist der sensorische Input \u00fcber unsere Augen, unsere Sinnesvornehmungen und so.", "tokens": [50364, 2846, 47743, 11, 390, 12977, 2876, 24118, 77, 33429, 11, 1418, 1163, 10200, 7864, 682, 2582, 4502, 14339, 29692, 11, 14339, 11187, 4081, 8453, 716, 8587, 5084, 674, 370, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1444736088023466, "compression_ratio": 1.6784565916398715, "no_speech_prob": 0.05106191709637642}, {"id": 124, "seek": 65708, "start": 663.24, "end": 670.84, "text": " Wir wissen nichts \u00fcber die Welt da drau\u00dfen, au\u00dfer wir sind nur beschr\u00e4nkt darauf auf den sensorischen Input, den wir haben und daraus schlussfolgern wir alles.", "tokens": [50672, 4347, 16331, 13004, 4502, 978, 14761, 1120, 44602, 11, 39428, 1987, 3290, 4343, 17498, 33766, 2320, 18654, 2501, 1441, 10200, 6282, 682, 2582, 11, 1441, 1987, 3084, 674, 274, 46483, 956, 75, 2023, 23910, 1248, 1987, 7874, 13, 51052], "temperature": 0.0, "avg_logprob": -0.1444736088023466, "compression_ratio": 1.6784565916398715, "no_speech_prob": 0.05106191709637642}, {"id": 125, "seek": 65708, "start": 670.84, "end": 679.32, "text": " Und das Gehirn, das habe ich ja in meinem Bewusstheitsvideo schon erkl\u00e4rt, versucht sozusagen das mathematische schlechtgestellte inverse Problem zu l\u00f6sen.", "tokens": [51052, 2719, 1482, 2876, 24118, 77, 11, 1482, 6015, 1893, 2784, 294, 24171, 40512, 26340, 24260, 40876, 4981, 27570, 24802, 11, 36064, 33762, 1482, 11619, 7864, 32427, 2629, 898, 975, 17340, 11676, 2164, 25209, 6748, 13, 51476], "temperature": 0.0, "avg_logprob": -0.1444736088023466, "compression_ratio": 1.6784565916398715, "no_speech_prob": 0.05106191709637642}, {"id": 126, "seek": 65708, "start": 679.32, "end": 684.5200000000001, "text": " Was hat die Inputdaten verursacht? Wir wissen ja nicht, was die verursacht haben.", "tokens": [51476, 3027, 2385, 978, 682, 2582, 67, 7186, 1306, 2156, 3589, 30, 4347, 16331, 2784, 1979, 11, 390, 978, 1306, 2156, 3589, 3084, 13, 51736], "temperature": 0.0, "avg_logprob": -0.1444736088023466, "compression_ratio": 1.6784565916398715, "no_speech_prob": 0.05106191709637642}, {"id": 127, "seek": 68452, "start": 684.6, "end": 691.72, "text": " Unser Gehirn, quasi unser Verstand, unser was auch immer, unser Gehirn versucht zu beschreiben und zu erkl\u00e4ren, was diese Inputdaten verursacht hat.", "tokens": [50368, 1156, 12484, 2876, 24118, 77, 11, 20954, 12977, 4281, 1115, 11, 12977, 390, 2168, 5578, 11, 12977, 2876, 24118, 77, 36064, 2164, 17498, 25946, 674, 2164, 46528, 11, 390, 6705, 682, 2582, 67, 7186, 1306, 2156, 3589, 2385, 13, 50724], "temperature": 0.0, "avg_logprob": -0.181713133591872, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.37345752120018005}, {"id": 128, "seek": 68452, "start": 691.72, "end": 695.4, "text": " Und dabei konstruieren wir das, was wir Realit\u00e4t nennen.", "tokens": [50724, 2719, 14967, 34208, 894, 5695, 1987, 1482, 11, 390, 1987, 8467, 14053, 297, 16043, 13, 50908], "temperature": 0.0, "avg_logprob": -0.181713133591872, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.37345752120018005}, {"id": 129, "seek": 68452, "start": 695.4, "end": 707.96, "text": " Und das beste, was Urscha Bachter immer sagt, ist, es gibt in dieser, also die beste Sprache, die wir im Moment daf\u00fcr haben, ist sozusagen die Physik, die physikalischen Gesetze.", "tokens": [50908, 2719, 1482, 22245, 11, 390, 41303, 4413, 363, 3589, 260, 5578, 15764, 11, 1418, 11, 785, 6089, 294, 9053, 11, 611, 978, 22245, 7702, 6000, 11, 978, 1987, 566, 19093, 13747, 3084, 11, 1418, 33762, 978, 15542, 1035, 11, 978, 2529, 41216, 6282, 6761, 302, 1381, 13, 51536], "temperature": 0.0, "avg_logprob": -0.181713133591872, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.37345752120018005}, {"id": 130, "seek": 68452, "start": 707.96, "end": 711.96, "text": " Und in dieser Sprache gibt es keine Ger\u00e4usche und es gibt auch keine Farben.", "tokens": [51536, 2719, 294, 9053, 7702, 6000, 6089, 785, 9252, 9409, 31611, 1876, 674, 785, 6089, 2168, 9252, 9067, 1799, 13, 51736], "temperature": 0.0, "avg_logprob": -0.181713133591872, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.37345752120018005}, {"id": 131, "seek": 71196, "start": 712.6, "end": 721.32, "text": " Das sind einfach nur h\u00f6here Ordnung von Strukturen, denen wir diese Bedeutung zuweisen im Sinne von einer Komprimierung.", "tokens": [50396, 2846, 3290, 7281, 4343, 13531, 6703, 29388, 15539, 2957, 745, 19977, 9873, 11, 19998, 1987, 6705, 363, 4858, 325, 1063, 2164, 40196, 566, 47041, 2957, 6850, 14286, 1424, 332, 11651, 13, 50832], "temperature": 0.4, "avg_logprob": -0.344009760806435, "compression_ratio": 1.4954128440366972, "no_speech_prob": 0.026346780359745026}, {"id": 132, "seek": 71196, "start": 726.6800000000001, "end": 729.0, "text": " Und jetzt ist es so...", "tokens": [51100, 2719, 4354, 1418, 785, 370, 485, 51216], "temperature": 0.4, "avg_logprob": -0.344009760806435, "compression_ratio": 1.4954128440366972, "no_speech_prob": 0.026346780359745026}, {"id": 133, "seek": 71196, "start": 731.0, "end": 737.72, "text": " \u00dcbrigens, das ist der Grund, wenn man jetzt irgendwie, sagen wir mal, ich nehme mir jetzt das Lineal, tue das hier auf den Tisch und knall hiergegen, dann h\u00f6r ich so ein Brrrrrrr.", "tokens": [51316, 10713, 21674, 694, 11, 1482, 1418, 1163, 13941, 11, 4797, 587, 4354, 20759, 11, 8360, 1987, 2806, 11, 1893, 48276, 3149, 4354, 1482, 14670, 304, 11, 256, 622, 1482, 3296, 2501, 1441, 48192, 674, 444, 336, 3296, 432, 1766, 11, 3594, 42651, 1893, 370, 1343, 1603, 16115, 16115, 16115, 13, 51652], "temperature": 0.4, "avg_logprob": -0.344009760806435, "compression_ratio": 1.4954128440366972, "no_speech_prob": 0.026346780359745026}, {"id": 134, "seek": 73772, "start": 738.28, "end": 745.24, "text": " Und diese h\u00f6heren Schwingungsmoden und so, diese ganzen Sachen mit den Schwingungen, Sounds sind auch fraktale.", "tokens": [50392, 2719, 6705, 48045, 268, 2065, 7904, 5846, 8014, 268, 674, 370, 11, 6705, 23966, 26074, 2194, 1441, 2065, 7904, 5084, 11, 14576, 3290, 2168, 6600, 2320, 1220, 13, 50740], "temperature": 0.0, "avg_logprob": -0.16617913399973222, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.32031434774398804}, {"id": 135, "seek": 73772, "start": 745.24, "end": 748.76, "text": " Da gibt es ein ziemlich geiles Video von Adam Neely, falls ihr den kennt.", "tokens": [50740, 3933, 6089, 785, 1343, 28901, 1519, 4680, 9777, 2957, 7938, 1734, 736, 11, 8804, 5553, 1441, 37682, 13, 50916], "temperature": 0.0, "avg_logprob": -0.16617913399973222, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.32031434774398804}, {"id": 136, "seek": 73772, "start": 748.76, "end": 750.76, "text": " Das ist ein sehr witziges Video, das werde ich verlinken.", "tokens": [50916, 2846, 1418, 1343, 5499, 261, 6862, 20609, 9777, 11, 1482, 24866, 1893, 19441, 35061, 13, 51016], "temperature": 0.0, "avg_logprob": -0.16617913399973222, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.32031434774398804}, {"id": 137, "seek": 73772, "start": 750.76, "end": 758.84, "text": " Dass sozusagen sich auch da durch die h\u00f6heren Schwingungsmoden, Sounds wie, also Ger\u00e4usche und T\u00f6ne wie fraktale verhalten.", "tokens": [51016, 22306, 33762, 3041, 2168, 1120, 7131, 978, 48045, 268, 2065, 7904, 5846, 8014, 268, 11, 14576, 3355, 11, 611, 9409, 31611, 1876, 674, 314, 973, 716, 3355, 6600, 2320, 1220, 1306, 15022, 13, 51420], "temperature": 0.0, "avg_logprob": -0.16617913399973222, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.32031434774398804}, {"id": 138, "seek": 73772, "start": 758.84, "end": 763.4, "text": " Also man kann da immer weiter reinzoomen und die werden dann immer wieder selbst \u00e4hnlich.", "tokens": [51420, 2743, 587, 4028, 1120, 5578, 8988, 6561, 4765, 4726, 674, 978, 4604, 3594, 5578, 6216, 13053, 49696, 13, 51648], "temperature": 0.0, "avg_logprob": -0.16617913399973222, "compression_ratio": 1.6985294117647058, "no_speech_prob": 0.32031434774398804}, {"id": 139, "seek": 76340, "start": 763.4, "end": 770.04, "text": " Und das ist super geil, denn das bedeutet im Prinzip, dass man alles auf eine Ringstruktur abbilden kann.", "tokens": [50364, 2719, 1482, 1418, 1687, 47165, 11, 10471, 1482, 27018, 566, 47572, 11, 2658, 587, 7874, 2501, 3018, 19844, 372, 31543, 410, 16248, 268, 4028, 13, 50696], "temperature": 0.0, "avg_logprob": -0.15453655463604887, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.2874550521373749}, {"id": 140, "seek": 76340, "start": 770.04, "end": 775.8, "text": " Deswegen kann man Farben auch in einem Ring anordnen und bei T\u00f6nen kann man die sozusagen auf diese Oktaven abbilden.", "tokens": [50696, 24864, 4028, 587, 9067, 1799, 2168, 294, 6827, 19844, 364, 765, 2866, 674, 4643, 314, 973, 2866, 4028, 587, 978, 33762, 2501, 6705, 422, 2320, 4940, 410, 16248, 268, 13, 50984], "temperature": 0.0, "avg_logprob": -0.15453655463604887, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.2874550521373749}, {"id": 141, "seek": 76340, "start": 775.8, "end": 779.16, "text": " Und das hat damit zu tun, dass sie sozusagen fraktale darstellen.", "tokens": [50984, 2719, 1482, 2385, 9479, 2164, 4267, 11, 2658, 2804, 33762, 6600, 2320, 1220, 4072, 17538, 13, 51152], "temperature": 0.0, "avg_logprob": -0.15453655463604887, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.2874550521373749}, {"id": 142, "seek": 76340, "start": 779.16, "end": 781.0, "text": " Und in echt existiert das nicht.", "tokens": [51152, 2719, 294, 13972, 2514, 4859, 1482, 1979, 13, 51244], "temperature": 0.0, "avg_logprob": -0.15453655463604887, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.2874550521373749}, {"id": 143, "seek": 76340, "start": 781.0, "end": 788.36, "text": " Das ist was, was unser Gehirn konstruiert, genauso wie, was wei\u00df ich, wenn wir irgendwie rumgucken mit unseren Augen,", "tokens": [51244, 2846, 1418, 390, 11, 390, 12977, 2876, 24118, 77, 34208, 894, 4859, 11, 37694, 3355, 11, 390, 13385, 1893, 11, 4797, 1987, 20759, 8347, 70, 49720, 2194, 25305, 29692, 11, 51612], "temperature": 0.0, "avg_logprob": -0.15453655463604887, "compression_ratio": 1.6431226765799256, "no_speech_prob": 0.2874550521373749}, {"id": 144, "seek": 78836, "start": 788.36, "end": 794.12, "text": " dann wird der Hauptinput, der wird interpoliert und auch extrapoliert hier am Rand und so.", "tokens": [50364, 3594, 4578, 1163, 30573, 259, 2582, 11, 1163, 4578, 44902, 4859, 674, 2168, 48224, 4859, 3296, 669, 23614, 674, 370, 13, 50652], "temperature": 0.0, "avg_logprob": -0.17689975510295639, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.1665412336587906}, {"id": 145, "seek": 78836, "start": 794.12, "end": 802.12, "text": " Unser Gehirn kann sozusagen die Daten, die eigentlich aufgenommen werden m\u00fcssten, um das, was wir hier sehen, zu konstruieren, kann es gar nicht verarbeiten.", "tokens": [50652, 1156, 12484, 2876, 24118, 77, 4028, 33762, 978, 31126, 11, 978, 10926, 2501, 29270, 4604, 28802, 6266, 11, 1105, 1482, 11, 390, 1987, 3296, 11333, 11, 2164, 34208, 894, 5695, 11, 4028, 785, 3691, 1979, 1306, 43918, 13, 51052], "temperature": 0.0, "avg_logprob": -0.17689975510295639, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.1665412336587906}, {"id": 146, "seek": 78836, "start": 802.12, "end": 803.64, "text": " Das braucht das Gehirn nicht.", "tokens": [51052, 2846, 22623, 1482, 2876, 24118, 77, 1979, 13, 51128], "temperature": 0.0, "avg_logprob": -0.17689975510295639, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.1665412336587906}, {"id": 147, "seek": 78836, "start": 803.64, "end": 807.88, "text": " Unser Gehirn ist so krass, dass es \u00fcbelst Rechenleistung investiert, um das alles zu interpolieren.", "tokens": [51128, 1156, 12484, 2876, 24118, 77, 1418, 370, 15913, 640, 11, 2658, 785, 3304, 5390, 372, 1300, 2470, 46820, 1063, 1963, 4859, 11, 1105, 1482, 7874, 2164, 44902, 5695, 13, 51340], "temperature": 0.0, "avg_logprob": -0.17689975510295639, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.1665412336587906}, {"id": 148, "seek": 78836, "start": 807.88, "end": 810.6, "text": " Und in dem Sinne ist es halt so,", "tokens": [51340, 2719, 294, 1371, 47041, 1418, 785, 12479, 370, 11, 51476], "temperature": 0.0, "avg_logprob": -0.17689975510295639, "compression_ratio": 1.6653225806451613, "no_speech_prob": 0.1665412336587906}, {"id": 149, "seek": 81060, "start": 810.6, "end": 823.5600000000001, "text": " Unser Gehirn leistet diese Komprimierungsarbeit schon f\u00fcr uns und diese periodischen Eigenschaften von zum Beispiel Ger\u00e4uschen und Farben machen, dass man die auf einem Ring anordnen kann.", "tokens": [50364, 1156, 12484, 2876, 24118, 77, 476, 468, 302, 6705, 14286, 1424, 332, 40908, 24024, 4981, 2959, 2693, 674, 6705, 2896, 6282, 40561, 20996, 268, 2957, 5919, 13772, 9409, 31611, 2470, 674, 9067, 1799, 7069, 11, 2658, 587, 978, 2501, 6827, 19844, 364, 765, 2866, 4028, 13, 51012], "temperature": 0.0, "avg_logprob": -0.16748215722256019, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.029302937909960747}, {"id": 150, "seek": 81060, "start": 823.5600000000001, "end": 836.9200000000001, "text": " Und wir existieren jetzt, also das, was wir als wir bezeichnen, wir existieren in dieser virtuellen Realit\u00e4t, die unser Gehirn konstruiert, um zu erkl\u00e4ren, was die Messdaten verursacht hat.", "tokens": [51012, 2719, 1987, 2514, 5695, 4354, 11, 611, 1482, 11, 390, 1987, 3907, 1987, 312, 32338, 2866, 11, 1987, 2514, 5695, 294, 9053, 4480, 13789, 268, 8467, 14053, 11, 978, 12977, 2876, 24118, 77, 34208, 894, 4859, 11, 1105, 2164, 46528, 11, 390, 978, 9847, 67, 7186, 1306, 2156, 3589, 2385, 13, 51680], "temperature": 0.0, "avg_logprob": -0.16748215722256019, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.029302937909960747}, {"id": 151, "seek": 81060, "start": 836.9200000000001, "end": 840.0400000000001, "text": " Und das habe ich ja in dem Video \u00fcber das Bewusstsein schon erkl\u00e4rt.", "tokens": [51680, 2719, 1482, 6015, 1893, 2784, 294, 1371, 9777, 4502, 1482, 40512, 26340, 33042, 4981, 27570, 24802, 13, 51836], "temperature": 0.0, "avg_logprob": -0.16748215722256019, "compression_ratio": 1.6063829787234043, "no_speech_prob": 0.029302937909960747}, {"id": 152, "seek": 84004, "start": 840.12, "end": 849.64, "text": " Also unser Gehirn konstruiert diese virtuelle Realit\u00e4t und auf diesem, weil das Ding ist, auf diesem Level von diesem \u00e4u\u00dferst merkw\u00fcrdigen Quantengraf,", "tokens": [50368, 2743, 12977, 2876, 24118, 77, 34208, 894, 4859, 6705, 20816, 2447, 8467, 14053, 674, 2501, 10975, 11, 7689, 1482, 20558, 1418, 11, 2501, 10975, 16872, 2957, 10975, 3078, 43796, 16398, 43541, 86, 39717, 3213, 26968, 1501, 10437, 11, 50844], "temperature": 0.0, "avg_logprob": -0.19550106389735772, "compression_ratio": 1.603896103896104, "no_speech_prob": 0.19172360002994537}, {"id": 153, "seek": 84004, "start": 849.64, "end": 856.12, "text": " den wir jetzt, von dem wir annehmen, dass es, dass der existiert durch unsere mathematischen Modelle und unsere Physik und so,", "tokens": [50844, 1441, 1987, 4354, 11, 2957, 1371, 1987, 364, 14669, 11, 2658, 785, 11, 2658, 1163, 2514, 4859, 7131, 14339, 11619, 6282, 6583, 4434, 674, 14339, 15542, 1035, 674, 370, 11, 51168], "temperature": 0.0, "avg_logprob": -0.19550106389735772, "compression_ratio": 1.603896103896104, "no_speech_prob": 0.19172360002994537}, {"id": 154, "seek": 84004, "start": 856.12, "end": 859.0, "text": " da existiert wirklich nur kalte Algebra.", "tokens": [51168, 1120, 2514, 4859, 9696, 4343, 7788, 975, 967, 19983, 13, 51312], "temperature": 0.0, "avg_logprob": -0.19550106389735772, "compression_ratio": 1.603896103896104, "no_speech_prob": 0.19172360002994537}, {"id": 155, "seek": 84004, "start": 859.0, "end": 869.9599999999999, "text": " Von einem Zustand des Universums zum n\u00e4chsten transformieren die physikalischen Gesetze diesen Quantengraf und wir k\u00f6nnen nur einen ganz kleinen Auschnitt davon messen.", "tokens": [51312, 20700, 6827, 46322, 474, 730, 14052, 8099, 5919, 19101, 4088, 5695, 978, 2529, 41216, 6282, 6761, 302, 1381, 12862, 26968, 1501, 10437, 674, 1987, 6310, 4343, 4891, 6312, 26512, 9039, 32064, 18574, 2082, 268, 13, 51860], "temperature": 0.0, "avg_logprob": -0.19550106389735772, "compression_ratio": 1.603896103896104, "no_speech_prob": 0.19172360002994537}, {"id": 156, "seek": 86996, "start": 870.9200000000001, "end": 872.9200000000001, "text": " Durch unseren sensorischen Input.", "tokens": [50412, 28557, 25305, 10200, 6282, 682, 2582, 13, 50512], "temperature": 0.0, "avg_logprob": -0.1693406142587737, "compression_ratio": 1.5748299319727892, "no_speech_prob": 0.0039442842826247215}, {"id": 157, "seek": 86996, "start": 872.9200000000001, "end": 880.0400000000001, "text": " Und das, was die Philosophen dann sozusagen als Qualia bezeichnen, das ist genauso eine Illusion wie der Freie Wille.", "tokens": [50512, 2719, 1482, 11, 390, 978, 31182, 404, 2932, 3594, 33762, 3907, 13616, 654, 312, 32338, 2866, 11, 1482, 1418, 37694, 3018, 10597, 5704, 3355, 1163, 6142, 414, 3099, 68, 13, 50868], "temperature": 0.0, "avg_logprob": -0.1693406142587737, "compression_ratio": 1.5748299319727892, "no_speech_prob": 0.0039442842826247215}, {"id": 158, "seek": 86996, "start": 880.0400000000001, "end": 885.64, "text": " Das ist einfach nur eine emergente Struktur, die auf unserem Gehirn entsteht, Qualia.", "tokens": [50868, 2846, 1418, 7281, 4343, 3018, 4345, 70, 1576, 745, 31543, 11, 978, 2501, 26792, 2876, 24118, 77, 35955, 357, 11, 13616, 654, 13, 51148], "temperature": 0.0, "avg_logprob": -0.1693406142587737, "compression_ratio": 1.5748299319727892, "no_speech_prob": 0.0039442842826247215}, {"id": 159, "seek": 86996, "start": 885.64, "end": 893.24, "text": " Und Joscha Bach nimmt jetzt als Beispiel f\u00fcr Realit\u00e4t, ziemlich geiles Beispiel, wie ich finde, das Mandelbrot-Fraktal.", "tokens": [51148, 2719, 18541, 4413, 30920, 38891, 4354, 3907, 13772, 2959, 8467, 14053, 11, 28901, 1519, 4680, 13772, 11, 3355, 1893, 17841, 11, 1482, 15458, 338, 1443, 310, 12, 37, 32249, 304, 13, 51528], "temperature": 0.0, "avg_logprob": -0.1693406142587737, "compression_ratio": 1.5748299319727892, "no_speech_prob": 0.0039442842826247215}, {"id": 160, "seek": 86996, "start": 893.24, "end": 898.52, "text": " Vielleicht binde ich das jetzt hier auch in das Video ein, da gibt es bestimmt ein cooles Gift oder so.", "tokens": [51528, 29838, 272, 8274, 1893, 1482, 4354, 3296, 2168, 294, 1482, 9777, 1343, 11, 1120, 6089, 785, 46871, 1343, 1627, 279, 44890, 4513, 370, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1693406142587737, "compression_ratio": 1.5748299319727892, "no_speech_prob": 0.0039442842826247215}, {"id": 161, "seek": 89852, "start": 898.52, "end": 905.56, "text": " Das ist so ein Fraktal, das entsteht im Wesentlichen, wenn man eine iterative Abbildung l\u00f6sen will.", "tokens": [50364, 2846, 1418, 370, 1343, 479, 32249, 304, 11, 1482, 35955, 357, 566, 23843, 7698, 268, 11, 4797, 587, 3018, 17138, 1166, 2847, 35588, 25209, 6748, 486, 13, 50716], "temperature": 0.0, "avg_logprob": -0.14308875401814777, "compression_ratio": 1.7125984251968505, "no_speech_prob": 0.02367885410785675}, {"id": 162, "seek": 89852, "start": 905.56, "end": 915.0, "text": " Ja genau, man hat eine quadratische iterative Abbildung in einer komplexen Ebene und guckt f\u00fcr welche Werte von einer konstanten komplexen Zahl die konvergiert oder nicht.", "tokens": [50716, 3530, 12535, 11, 587, 2385, 3018, 10787, 4481, 7864, 17138, 1166, 2847, 35588, 294, 6850, 5207, 18945, 268, 20418, 1450, 674, 695, 19951, 2959, 24311, 343, 10634, 2957, 6850, 34208, 29646, 5207, 18945, 268, 42592, 978, 5897, 331, 70, 4859, 4513, 1979, 13, 51188], "temperature": 0.0, "avg_logprob": -0.14308875401814777, "compression_ratio": 1.7125984251968505, "no_speech_prob": 0.02367885410785675}, {"id": 163, "seek": 89852, "start": 915.0, "end": 924.28, "text": " Und wenn man das sozusagen macht f\u00fcr die komplexe Zahlenebene und alles was divergiert mit schwarz abbildet, dann kriegt man dieses Fraktal und so baut man das.", "tokens": [51188, 2719, 4797, 587, 1482, 33762, 10857, 2959, 978, 5207, 18945, 68, 42592, 1450, 41605, 674, 7874, 390, 18558, 70, 4859, 2194, 956, 31991, 410, 16248, 302, 11, 3594, 25766, 10463, 587, 12113, 479, 32249, 304, 674, 370, 272, 1375, 587, 1482, 13, 51652], "temperature": 0.0, "avg_logprob": -0.14308875401814777, "compression_ratio": 1.7125984251968505, "no_speech_prob": 0.02367885410785675}, {"id": 164, "seek": 92428, "start": 924.36, "end": 928.8399999999999, "text": " Das ist relativ einfach das zu bauen tats\u00e4chlich, aber das Muster was rauskommt, das sieht halt super krass aus.", "tokens": [50368, 2846, 1418, 21960, 7281, 1482, 2164, 43787, 20796, 11, 4340, 1482, 376, 8393, 390, 17202, 74, 22230, 11, 1482, 14289, 12479, 1687, 15913, 640, 3437, 13, 50592], "temperature": 0.0, "avg_logprob": -0.18054820941044733, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.17989996075630188}, {"id": 165, "seek": 92428, "start": 928.8399999999999, "end": 943.48, "text": " Und wenn man sich jetzt vorstellt, wir leben auf so einer fraktalen Oberfl\u00e4che von dem Mandelbrot-Fraktal, dann w\u00fcrden wir sehen, okay, da gibt es so eine rechtsdrehende Spirale.", "tokens": [50592, 2719, 4797, 587, 3041, 4354, 4245, 372, 12783, 11, 1987, 26392, 2501, 370, 6850, 6600, 2320, 21745, 27664, 3423, 32664, 2957, 1371, 15458, 338, 1443, 310, 12, 37, 32249, 304, 11, 3594, 27621, 1987, 11333, 11, 1392, 11, 1120, 6089, 785, 370, 3018, 34305, 67, 9017, 5445, 1738, 347, 1220, 13, 51324], "temperature": 0.0, "avg_logprob": -0.18054820941044733, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.17989996075630188}, {"id": 166, "seek": 92428, "start": 943.48, "end": 948.12, "text": " Wenn man die jetzt reingeht, w\u00fcrde man irgendwann an einer Singularit\u00e4t ankommen.", "tokens": [51324, 7899, 587, 978, 4354, 319, 8735, 357, 11, 11942, 587, 34313, 364, 6850, 7474, 1040, 14053, 364, 13675, 13, 51556], "temperature": 0.0, "avg_logprob": -0.18054820941044733, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.17989996075630188}, {"id": 167, "seek": 94812, "start": 948.2, "end": 958.12, "text": " Und wenn man die sozusagen, wenn man dar\u00fcber hinweg guckt, gibt es quasi Periodenverdopplung oder ich wei\u00df gar nicht mehr wie dieser Effekt hei\u00dft, auf jeden Fall, dann beginnt diese Spirale von vorne.", "tokens": [50368, 2719, 4797, 587, 978, 33762, 11, 4797, 587, 21737, 14102, 12517, 695, 19951, 11, 6089, 785, 20954, 34976, 268, 331, 67, 404, 564, 1063, 4513, 1893, 13385, 3691, 1979, 5417, 3355, 9053, 34192, 8192, 13139, 11, 2501, 12906, 7465, 11, 3594, 1841, 580, 6705, 1738, 347, 1220, 2957, 32025, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1700626960167518, "compression_ratio": 1.7220338983050847, "no_speech_prob": 0.15795834362506866}, {"id": 168, "seek": 94812, "start": 958.12, "end": 964.76, "text": " Dann beginnt sozusagen die Oberfl\u00e4che von vorne zu wachsen, dann kommt diese Selbst\u00e4hnlichkeit, die kickt rein bei dem Fraktal und dann sieht es wieder von vorne so aus.", "tokens": [50864, 7455, 1841, 580, 33762, 978, 27664, 3423, 32664, 2957, 32025, 2164, 261, 31625, 11, 3594, 10047, 6705, 29712, 6860, 77, 41096, 11, 978, 4437, 83, 6561, 4643, 1371, 479, 32249, 304, 674, 3594, 14289, 785, 6216, 2957, 32025, 370, 3437, 13, 51196], "temperature": 0.0, "avg_logprob": -0.1700626960167518, "compression_ratio": 1.7220338983050847, "no_speech_prob": 0.15795834362506866}, {"id": 169, "seek": 94812, "start": 964.76, "end": 970.76, "text": " Und man m\u00fcsste wieder die Spirale weiter reingehen, bis man wieder zur Singularit\u00e4t kommt und dann wieder reingehen und so weiter.", "tokens": [51196, 2719, 587, 42962, 6216, 978, 1738, 347, 1220, 8988, 319, 8735, 2932, 11, 7393, 587, 6216, 7147, 7474, 1040, 14053, 10047, 674, 3594, 6216, 319, 8735, 2932, 674, 370, 8988, 13, 51496], "temperature": 0.0, "avg_logprob": -0.1700626960167518, "compression_ratio": 1.7220338983050847, "no_speech_prob": 0.15795834362506866}, {"id": 170, "seek": 97076, "start": 970.76, "end": 979.08, "text": " Und wenn man so mehrere Layers bauen w\u00fcrde von seiner eigenen Realit\u00e4t, weil wir leben ja auf diesem Fraktal, dann w\u00e4re das eine gute Approximation f\u00fcr die Realit\u00e4t.", "tokens": [50364, 2719, 4797, 587, 370, 44677, 20084, 433, 43787, 11942, 2957, 23114, 28702, 8467, 14053, 11, 7689, 1987, 26392, 2784, 2501, 10975, 479, 32249, 304, 11, 3594, 14558, 1482, 3018, 21476, 29551, 3081, 399, 2959, 978, 8467, 14053, 13, 50780], "temperature": 0.0, "avg_logprob": -0.14358269467073329, "compression_ratio": 1.6635220125786163, "no_speech_prob": 0.4679262042045593}, {"id": 171, "seek": 97076, "start": 979.08, "end": 984.36, "text": " Was wir niemals sehen w\u00fcrden, ist sozusagen das komplette Picture, weil das Fraktal ist.", "tokens": [50780, 3027, 1987, 2838, 34978, 11333, 27621, 11, 1418, 33762, 1482, 24526, 3007, 35730, 11, 7689, 1482, 479, 32249, 304, 1418, 13, 51044], "temperature": 0.0, "avg_logprob": -0.14358269467073329, "compression_ratio": 1.6635220125786163, "no_speech_prob": 0.4679262042045593}, {"id": 172, "seek": 97076, "start": 984.36, "end": 988.76, "text": " Es ist ein unendlich gro\u00dfes und selbst\u00e4hnliches periodisches Muster.", "tokens": [51044, 2313, 1418, 1343, 517, 521, 1739, 48875, 674, 13053, 6860, 77, 45502, 2896, 35889, 376, 8393, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14358269467073329, "compression_ratio": 1.6635220125786163, "no_speech_prob": 0.4679262042045593}, {"id": 173, "seek": 97076, "start": 988.76, "end": 991.08, "text": " So und so ist es mit unserem Universum auch.", "tokens": [51264, 407, 674, 370, 1418, 785, 2194, 26792, 14052, 449, 2168, 13, 51380], "temperature": 0.0, "avg_logprob": -0.14358269467073329, "compression_ratio": 1.6635220125786163, "no_speech_prob": 0.4679262042045593}, {"id": 174, "seek": 97076, "start": 991.08, "end": 996.92, "text": " Wir haben halt diese physikalischen Gesetze, sie sind nicht perfekt, sie sind aber eine gute Beschreibung f\u00fcr das, was wir haben, was wir sehen und so.", "tokens": [51380, 4347, 3084, 12479, 6705, 2529, 41216, 6282, 6761, 302, 1381, 11, 2804, 3290, 1979, 49134, 11, 2804, 3290, 4340, 3018, 21476, 30860, 38606, 1063, 2959, 1482, 11, 390, 1987, 3084, 11, 390, 1987, 11333, 674, 370, 13, 51672], "temperature": 0.0, "avg_logprob": -0.14358269467073329, "compression_ratio": 1.6635220125786163, "no_speech_prob": 0.4679262042045593}, {"id": 175, "seek": 99692, "start": 996.92, "end": 1001.0799999999999, "text": " Und je n\u00e4her wir uns ran tasten an die Realit\u00e4t, desto besser werden wir.", "tokens": [50364, 2719, 1506, 6433, 511, 1987, 2693, 5872, 2700, 268, 364, 978, 8467, 14053, 11, 2677, 78, 18021, 4604, 1987, 13, 50572], "temperature": 0.0, "avg_logprob": -0.1421030688759507, "compression_ratio": 1.7365269461077844, "no_speech_prob": 0.2841850519180298}, {"id": 176, "seek": 99692, "start": 1001.0799999999999, "end": 1011.0, "text": " Irgendwann w\u00fcrden wir, wenn wir auf diesem Fraktal leben, in der Mandelbrotmenge, irgendwann w\u00fcrden wir diese paar Zeilen Code, die das braucht, um das zu generieren, vielleicht bauen k\u00f6nnen.", "tokens": [50572, 9151, 9395, 86, 969, 27621, 1987, 11, 4797, 1987, 2501, 10975, 479, 32249, 304, 26392, 11, 294, 1163, 15458, 338, 1443, 310, 2558, 432, 11, 34313, 27621, 1987, 6705, 16509, 4853, 17471, 15549, 11, 978, 1482, 22623, 11, 1105, 1482, 2164, 1337, 5695, 11, 12547, 43787, 6310, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1421030688759507, "compression_ratio": 1.7365269461077844, "no_speech_prob": 0.2841850519180298}, {"id": 177, "seek": 99692, "start": 1011.0, "end": 1014.28, "text": " Und dann h\u00e4tten wir es gekickt, dann h\u00e4tten wir die Realit\u00e4t vollst\u00e4ndig beschrieben.", "tokens": [51068, 2719, 3594, 33278, 1987, 785, 14037, 40522, 11, 3594, 33278, 1987, 978, 8467, 14053, 15593, 16913, 328, 17498, 24027, 13, 51232], "temperature": 0.0, "avg_logprob": -0.1421030688759507, "compression_ratio": 1.7365269461077844, "no_speech_prob": 0.2841850519180298}, {"id": 178, "seek": 99692, "start": 1014.28, "end": 1020.04, "text": " Das hei\u00dft, was man machen muss, um die Realit\u00e4t zu beschreiben, ist man muss sich aus First Principles Schluss folgern.", "tokens": [51232, 2846, 13139, 11, 390, 587, 7069, 6425, 11, 1105, 978, 8467, 14053, 2164, 17498, 25946, 11, 1418, 587, 6425, 3041, 3437, 2386, 38372, 2622, 36573, 3339, 70, 1248, 13, 51520], "temperature": 0.0, "avg_logprob": -0.1421030688759507, "compression_ratio": 1.7365269461077844, "no_speech_prob": 0.2841850519180298}, {"id": 179, "seek": 99692, "start": 1020.04, "end": 1024.28, "text": " Und das geht nat\u00fcrlich nicht so als Schlussfolgerung, sondern das ist im Prinzip zwar ein Error.", "tokens": [51520, 2719, 1482, 7095, 8762, 1979, 370, 3907, 36573, 7082, 1321, 1063, 11, 11465, 1482, 1418, 566, 47572, 19054, 1343, 3300, 2874, 13, 51732], "temperature": 0.0, "avg_logprob": -0.1421030688759507, "compression_ratio": 1.7365269461077844, "no_speech_prob": 0.2841850519180298}, {"id": 180, "seek": 102428, "start": 1024.28, "end": 1038.52, "text": " Also man muss Mathematik machen, betreiben und man muss es solange mit 3 in Error machen, bis man irgendwie auf das kommt, was sozusagen die minimale Information beinhaltet, die alles beschreibt, was wir haben.", "tokens": [50364, 2743, 587, 6425, 15776, 8615, 1035, 7069, 11, 778, 25946, 674, 587, 6425, 785, 1404, 933, 2194, 805, 294, 3300, 2874, 7069, 11, 7393, 587, 20759, 2501, 1482, 10047, 11, 390, 33762, 978, 4464, 1220, 15357, 312, 10085, 39931, 11, 978, 7874, 17498, 31174, 11, 390, 1987, 3084, 13, 51076], "temperature": 0.0, "avg_logprob": -0.15205270929854045, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.07801661640405655}, {"id": 181, "seek": 102428, "start": 1038.52, "end": 1041.24, "text": " Das ist das Prinzip von Okams Racer auch.", "tokens": [51076, 2846, 1418, 1482, 47572, 2957, 3477, 4070, 497, 12858, 2168, 13, 51212], "temperature": 0.0, "avg_logprob": -0.15205270929854045, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.07801661640405655}, {"id": 182, "seek": 102428, "start": 1041.24, "end": 1045.72, "text": " Also wir wollen so wenig wie m\u00f6glich voraussetzen oder so wenig wie m\u00f6glich annehmen.", "tokens": [51212, 2743, 1987, 11253, 370, 20911, 3355, 16294, 371, 3252, 2023, 24797, 4513, 370, 20911, 3355, 16294, 364, 14669, 13, 51436], "temperature": 0.0, "avg_logprob": -0.15205270929854045, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.07801661640405655}, {"id": 183, "seek": 102428, "start": 1045.72, "end": 1048.92, "text": " Und das soll maximal viel von dem, was wir beobachten, beschreiben.", "tokens": [51436, 2719, 1482, 7114, 49336, 5891, 2957, 1371, 11, 390, 1987, 312, 996, 20806, 11, 17498, 25946, 13, 51596], "temperature": 0.0, "avg_logprob": -0.15205270929854045, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.07801661640405655}, {"id": 184, "seek": 102428, "start": 1048.92, "end": 1053.12, "text": " Und wenn wir etwas haben, was alles mit einem Formalismus beschreibt, dann sind wir fertig.", "tokens": [51596, 2719, 4797, 1987, 9569, 3084, 11, 390, 7874, 2194, 6827, 10126, 304, 25327, 17498, 31174, 11, 3594, 3290, 1987, 31362, 13, 51806], "temperature": 0.0, "avg_logprob": -0.15205270929854045, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.07801661640405655}, {"id": 185, "seek": 105312, "start": 1053.12, "end": 1058.0, "text": " Das ist die Idee. Das ist sozusagen das Ideal, an das ich sozusagen die Wissenschaft da ran tasten m\u00f6chte.", "tokens": [50364, 2846, 1418, 978, 32651, 13, 2846, 1418, 33762, 1482, 13090, 304, 11, 364, 1482, 1893, 33762, 978, 38774, 1120, 5872, 2700, 268, 14570, 13, 50608], "temperature": 0.0, "avg_logprob": -0.31066226959228516, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.1917661726474762}, {"id": 186, "seek": 105312, "start": 1063.84, "end": 1081.4399999999998, "text": " So und wenn wir also, Joshua Bach gibt auch noch so einen anderen coolen Spruch, n\u00e4mlich er sagt, die physikalische Realit\u00e4t hat kein Bewusstsein und da gibt es auch kein Bewusstsein.", "tokens": [50900, 407, 674, 4797, 1987, 611, 11, 24005, 30920, 6089, 2168, 3514, 370, 4891, 11122, 1627, 268, 7702, 625, 11, 21219, 1189, 15764, 11, 978, 2529, 41216, 7864, 8467, 14053, 2385, 13424, 40512, 26340, 33042, 674, 1120, 6089, 785, 2168, 13424, 40512, 26340, 33042, 13, 51780], "temperature": 0.0, "avg_logprob": -0.31066226959228516, "compression_ratio": 1.4948979591836735, "no_speech_prob": 0.1917661726474762}, {"id": 187, "seek": 108144, "start": 1081.44, "end": 1085.2, "text": " Nur eine Simulation kann Bewusstsein haben.", "tokens": [50364, 17612, 3018, 3998, 2776, 4028, 40512, 26340, 33042, 3084, 13, 50552], "temperature": 0.0, "avg_logprob": -0.11367820924328219, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.10223840177059174}, {"id": 188, "seek": 108144, "start": 1085.2, "end": 1097.16, "text": " Das ist das, was er sagt und das ist ein ziemlich cooler Spruch, weil was er damit meint ist, wenn wir jetzt ein Buch lesen und da ist ein Protagonist drin und der f\u00fchlt irgendwas oder dem geht es irgendwie schei\u00dfe.", "tokens": [50552, 2846, 1418, 1482, 11, 390, 1189, 15764, 674, 1482, 1418, 1343, 28901, 15566, 7702, 625, 11, 7689, 390, 1189, 9479, 385, 686, 1418, 11, 4797, 1987, 4354, 1343, 25818, 1512, 268, 674, 1120, 1418, 1343, 10019, 6709, 468, 24534, 674, 1163, 18813, 2282, 47090, 4513, 1371, 7095, 785, 20759, 25690, 47828, 13, 51150], "temperature": 0.0, "avg_logprob": -0.11367820924328219, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.10223840177059174}, {"id": 189, "seek": 108144, "start": 1097.16, "end": 1099.0800000000002, "text": " Dann empfinden wir das, w\u00e4hrend wir das lesen.", "tokens": [51150, 7455, 4012, 43270, 1987, 1482, 11, 33624, 1987, 1482, 1512, 268, 13, 51246], "temperature": 0.0, "avg_logprob": -0.11367820924328219, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.10223840177059174}, {"id": 190, "seek": 108144, "start": 1099.0800000000002, "end": 1110.04, "text": " Das liegt daran, dass wir in unserem Kopf uns ein Bild davon, ein Abbild dieser Person in dem Buch, die da beschrieben wird durch Sprache \u00fcbrigens, bauen und dadurch wird es real.", "tokens": [51246, 2846, 22421, 24520, 11, 2658, 1987, 294, 26792, 28231, 2693, 1343, 15746, 18574, 11, 1343, 2847, 16248, 9053, 8443, 294, 1371, 25818, 11, 978, 1120, 17498, 24027, 4578, 7131, 7702, 6000, 38215, 11, 43787, 674, 35472, 4578, 785, 957, 13, 51794], "temperature": 0.0, "avg_logprob": -0.11367820924328219, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.10223840177059174}, {"id": 191, "seek": 111004, "start": 1110.04, "end": 1117.32, "text": " Und der Witz ist, zwischen dieser Person in dem Buch und dem, was wir uns selbst nennen, gibt es eigentlich keinen Unterschied.", "tokens": [50364, 2719, 1163, 343, 6862, 1418, 11, 19875, 9053, 8443, 294, 1371, 25818, 674, 1371, 11, 390, 1987, 2693, 13053, 297, 16043, 11, 6089, 785, 10926, 20624, 41414, 13, 50728], "temperature": 0.0, "avg_logprob": -0.11805105209350586, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.0862547755241394}, {"id": 192, "seek": 111004, "start": 1117.32, "end": 1130.48, "text": " Denn das Gehirn erz\u00e4hlt sich ja selber eine Geschichte dar\u00fcber, wie es w\u00e4re, wenn es eine Person g\u00e4be, die so ist wie wir und die in einer Situation ist, die wir Gegenwart nennen.", "tokens": [50728, 19027, 1482, 2876, 24118, 77, 47110, 3041, 2784, 23888, 3018, 28896, 21737, 11, 3355, 785, 14558, 11, 4797, 785, 3018, 8443, 37612, 650, 11, 978, 370, 1418, 3355, 1987, 674, 978, 294, 6850, 22247, 1418, 11, 978, 1987, 38631, 29587, 297, 16043, 13, 51386], "temperature": 0.0, "avg_logprob": -0.11805105209350586, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.0862547755241394}, {"id": 193, "seek": 111004, "start": 1130.48, "end": 1139.0, "text": " Und wir konstruieren, w\u00e4hrend wir den sensorischen Input \u00fcber die Gegenwart bekommen, diese Person und stellen uns vor, wie es w\u00e4re, so eine Person zu sein.", "tokens": [51386, 2719, 1987, 34208, 894, 5695, 11, 33624, 1987, 1441, 10200, 6282, 682, 2582, 4502, 978, 38631, 29587, 19256, 11, 6705, 8443, 674, 24407, 2693, 4245, 11, 3355, 785, 14558, 11, 370, 3018, 8443, 2164, 6195, 13, 51812], "temperature": 0.0, "avg_logprob": -0.11805105209350586, "compression_ratio": 1.7127272727272727, "no_speech_prob": 0.0862547755241394}, {"id": 194, "seek": 114004, "start": 1140.28, "end": 1141.48, "text": " Wir simulieren das.", "tokens": [50376, 4347, 1034, 425, 5695, 1482, 13, 50436], "temperature": 0.0, "avg_logprob": -0.17678136825561525, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.2937192916870117}, {"id": 195, "seek": 114004, "start": 1142.68, "end": 1143.96, "text": " Das ist das, was das Gehirn macht.", "tokens": [50496, 2846, 1418, 1482, 11, 390, 1482, 2876, 24118, 77, 10857, 13, 50560], "temperature": 0.0, "avg_logprob": -0.17678136825561525, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.2937192916870117}, {"id": 196, "seek": 114004, "start": 1144.32, "end": 1147.52, "text": " Deswegen sagt er, nur eine Simulation kann \u00fcberhaupt Bewusstsein haben.", "tokens": [50578, 24864, 15764, 1189, 11, 4343, 3018, 3998, 2776, 4028, 20023, 40512, 26340, 33042, 3084, 13, 50738], "temperature": 0.0, "avg_logprob": -0.17678136825561525, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.2937192916870117}, {"id": 197, "seek": 114004, "start": 1147.92, "end": 1157.6399999999999, "text": " Nur eine Simulation ist f\u00e4hig, diese emergente Struktur zu haben, die dann wieder, also die emergente Struktur, die sozusagen das Bewusstsein als Nebenprodukt hat.", "tokens": [50758, 17612, 3018, 3998, 2776, 1418, 283, 6860, 328, 11, 6705, 4345, 70, 1576, 745, 31543, 2164, 3084, 11, 978, 3594, 6216, 11, 611, 978, 4345, 70, 1576, 745, 31543, 11, 978, 33762, 1482, 40512, 26340, 33042, 3907, 48193, 14314, 2320, 2385, 13, 51244], "temperature": 0.0, "avg_logprob": -0.17678136825561525, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.2937192916870117}, {"id": 198, "seek": 114004, "start": 1158.3999999999999, "end": 1159.1599999999999, "text": " Und das ist krass.", "tokens": [51282, 2719, 1482, 1418, 15913, 640, 13, 51320], "temperature": 0.0, "avg_logprob": -0.17678136825561525, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.2937192916870117}, {"id": 199, "seek": 114004, "start": 1159.36, "end": 1160.36, "text": " Das ist ziemlich heftig.", "tokens": [51330, 2846, 1418, 28901, 415, 34765, 13, 51380], "temperature": 0.0, "avg_logprob": -0.17678136825561525, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.2937192916870117}, {"id": 200, "seek": 116036, "start": 1160.36, "end": 1165.6, "text": " Also wenn ihr, wenn ihr das nicht kennt, diese Joscha Bach Talks, ihr m\u00fcsst euch die reinziehen.", "tokens": [50364, 2743, 4797, 5553, 11, 4797, 5553, 1482, 1979, 37682, 11, 6705, 18541, 4413, 30920, 8780, 82, 11, 5553, 49481, 10403, 978, 6561, 28768, 13, 50626], "temperature": 0.0, "avg_logprob": -0.1711293052224552, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.9433563947677612}, {"id": 201, "seek": 116036, "start": 1165.6, "end": 1167.1999999999998, "text": " Ich verlinke die beiden von Lex Friedman.", "tokens": [50626, 3141, 1306, 5045, 330, 978, 23446, 2957, 24086, 17605, 1601, 13, 50706], "temperature": 0.0, "avg_logprob": -0.1711293052224552, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.9433563947677612}, {"id": 202, "seek": 116036, "start": 1167.32, "end": 1172.24, "text": " Es gibt aber noch viele mehr, die sozusagen ins Detail gehen, wenn Joscha Bach seine Meinung zu GPT-3 erz\u00e4hlt.", "tokens": [50712, 2313, 6089, 4340, 3514, 9693, 5417, 11, 978, 33762, 1028, 4237, 864, 13230, 11, 4797, 18541, 4413, 30920, 15925, 36519, 2164, 26039, 51, 12, 18, 47110, 13, 50958], "temperature": 0.0, "avg_logprob": -0.1711293052224552, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.9433563947677612}, {"id": 203, "seek": 116036, "start": 1172.24, "end": 1174.28, "text": " Das ist super krass oder generell.", "tokens": [50958, 2846, 1418, 1687, 15913, 640, 4513, 41553, 285, 13, 51060], "temperature": 0.0, "avg_logprob": -0.1711293052224552, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.9433563947677612}, {"id": 204, "seek": 116036, "start": 1174.28, "end": 1179.9599999999998, "text": " Es gibt, glaube ich, auch zwei, drei deutsche Talks, von denen einer mindestens schon mal ziemlich gut ist f\u00fcr Einsteiger.", "tokens": [51060, 2313, 6089, 11, 13756, 1893, 11, 2168, 12002, 11, 16809, 47502, 8780, 82, 11, 2957, 19998, 6850, 1575, 42624, 4981, 2806, 28901, 5228, 1418, 2959, 6391, 2941, 4810, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1711293052224552, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.9433563947677612}, {"id": 205, "seek": 116036, "start": 1179.9599999999998, "end": 1182.36, "text": " Die anderen sind zu abgefahren, absolute Nord-Talks.", "tokens": [51344, 3229, 11122, 3290, 2164, 410, 13529, 7079, 11, 8236, 16229, 12, 33210, 82, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1711293052224552, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.9433563947677612}, {"id": 206, "seek": 116036, "start": 1182.8799999999999, "end": 1187.36, "text": " Aber wie gesagt, der Typ ist ein Ostdeutscher, der kann also auch flie\u00dfend Deutsch, ist gar kein Problem.", "tokens": [51490, 5992, 3355, 12260, 11, 1163, 17722, 1418, 1343, 34140, 1479, 3648, 6759, 11, 1163, 4028, 611, 2168, 932, 39245, 521, 12699, 11, 1418, 3691, 13424, 11676, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1711293052224552, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.9433563947677612}, {"id": 207, "seek": 116036, "start": 1187.36, "end": 1189.04, "text": " Man findet blo\u00df nicht so viel von dem auf Deutsch.", "tokens": [51714, 2458, 27752, 1749, 2536, 1979, 370, 5891, 2957, 1371, 2501, 12699, 13, 51798], "temperature": 0.0, "avg_logprob": -0.1711293052224552, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.9433563947677612}, {"id": 208, "seek": 119036, "start": 1190.4399999999998, "end": 1191.24, "text": " Aber hammerm\u00e4\u00dfig.", "tokens": [50368, 5992, 13017, 43132, 13, 50408], "temperature": 0.0, "avg_logprob": -0.16429426574707032, "compression_ratio": 1.5414012738853504, "no_speech_prob": 0.015421171672642231}, {"id": 209, "seek": 119036, "start": 1192.32, "end": 1198.9599999999998, "text": " Also wirklich alles andere, alle anderen Talks, alle anderen Podcasts, die es auf dieser Welt gibt, sind, meiner Meinung nach, Zeitverschwendung,", "tokens": [50462, 2743, 9696, 7874, 10490, 11, 5430, 11122, 8780, 82, 11, 5430, 11122, 29972, 82, 11, 978, 785, 2501, 9053, 14761, 6089, 11, 3290, 11, 20529, 36519, 5168, 11, 9394, 840, 339, 20128, 1063, 11, 50794], "temperature": 0.0, "avg_logprob": -0.16429426574707032, "compression_ratio": 1.5414012738853504, "no_speech_prob": 0.015421171672642231}, {"id": 210, "seek": 119036, "start": 1198.9599999999998, "end": 1204.52, "text": " bevor man nicht vollst\u00e4ndig durchdrungen hat, was Joscha Bach einen in drei Stunden reinquetscht an Wissen.", "tokens": [50794, 37591, 587, 1979, 15593, 16913, 328, 7131, 16753, 5084, 2385, 11, 390, 18541, 4413, 30920, 4891, 294, 16809, 30496, 6561, 358, 1385, 4701, 364, 343, 10987, 13, 51072], "temperature": 0.0, "avg_logprob": -0.16429426574707032, "compression_ratio": 1.5414012738853504, "no_speech_prob": 0.015421171672642231}, {"id": 211, "seek": 119036, "start": 1204.6, "end": 1205.4399999999998, "text": " Das ist so viel.", "tokens": [51076, 2846, 1418, 370, 5891, 13, 51118], "temperature": 0.0, "avg_logprob": -0.16429426574707032, "compression_ratio": 1.5414012738853504, "no_speech_prob": 0.015421171672642231}, {"id": 212, "seek": 119036, "start": 1205.4799999999998, "end": 1216.04, "text": " Ich habe den Talk jetzt bestimmt vier, f\u00fcnf Mal schon durchgeknallt und ich komme immer wieder auf neue Erkenntnisse, weil der Typ in einem Satz so viel Informationen und so viel Verst\u00e4ndnis", "tokens": [51120, 3141, 6015, 1441, 8780, 4354, 46871, 17634, 11, 28723, 5746, 4981, 7131, 432, 5457, 336, 83, 674, 1893, 31194, 5578, 6216, 2501, 16842, 3300, 41838, 31481, 11, 7689, 1163, 17722, 294, 6827, 5344, 89, 370, 5891, 46753, 674, 370, 5891, 4281, 16913, 10661, 51648], "temperature": 0.0, "avg_logprob": -0.16429426574707032, "compression_ratio": 1.5414012738853504, "no_speech_prob": 0.015421171672642231}, {"id": 213, "seek": 121604, "start": 1216.04, "end": 1220.8, "text": " reinsteckt, dass man das wirklich erst nach einem Jahr nochmal dann checkt oder so.", "tokens": [50364, 6561, 2941, 19951, 11, 2658, 587, 1482, 9696, 11301, 5168, 6827, 11674, 26509, 3594, 1520, 83, 4513, 370, 13, 50602], "temperature": 0.0, "avg_logprob": -0.20416213016884, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.729407548904419}, {"id": 214, "seek": 121604, "start": 1221.96, "end": 1225.6399999999999, "text": " So und in Deep Learning ist es jetzt nur so.", "tokens": [50660, 407, 674, 294, 14895, 15205, 1418, 785, 4354, 4343, 370, 13, 50844], "temperature": 0.0, "avg_logprob": -0.20416213016884, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.729407548904419}, {"id": 215, "seek": 121604, "start": 1226.2, "end": 1230.24, "text": " Man muss dazu wissen, das habe ich auch noch nie so erw\u00e4hnt, aber das ist quasi bekannt.", "tokens": [50872, 2458, 6425, 13034, 16331, 11, 1482, 6015, 1893, 2168, 3514, 2838, 370, 21715, 6860, 580, 11, 4340, 1482, 1418, 20954, 39167, 13, 51074], "temperature": 0.0, "avg_logprob": -0.20416213016884, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.729407548904419}, {"id": 216, "seek": 121604, "start": 1230.24, "end": 1233.56, "text": " Das ist wahrscheinlich erste Vorlesung, Deep Learning im Informatikstudium.", "tokens": [51074, 2846, 1418, 30957, 20951, 12231, 904, 1063, 11, 14895, 15205, 566, 34301, 267, 1035, 28349, 2197, 13, 51240], "temperature": 0.0, "avg_logprob": -0.20416213016884, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.729407548904419}, {"id": 217, "seek": 121604, "start": 1234.24, "end": 1237.8799999999999, "text": " Der sogenannte universelle Funktions-Approximator, das ist das Relevante.", "tokens": [51274, 5618, 37467, 9358, 6445, 2447, 11166, 2320, 626, 12, 9132, 340, 3081, 1639, 11, 1482, 1418, 1482, 1300, 28316, 2879, 13, 51456], "temperature": 0.0, "avg_logprob": -0.20416213016884, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.729407548904419}, {"id": 218, "seek": 123788, "start": 1238.3600000000001, "end": 1252.72, "text": " Der sagt Folgendes aus, wenn ihr ein neuronales Netz macht mit N-Input, N-dimensionalen Input und ihr wollt nur ein Output haben, also sozusagen eine Funktion mit beliebigem Input und die soll was ausgeben.", "tokens": [50388, 5618, 15764, 15255, 9395, 279, 3437, 11, 4797, 5553, 1343, 34090, 4229, 38889, 10857, 2194, 426, 12, 4575, 2582, 11, 426, 12, 18759, 268, 682, 2582, 674, 5553, 45826, 4343, 1343, 5925, 2582, 3084, 11, 611, 33762, 3018, 11166, 9780, 2194, 1351, 65, 328, 443, 682, 2582, 674, 978, 7114, 390, 3437, 16702, 13, 51106], "temperature": 0.0, "avg_logprob": -0.20947723065392446, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.5379957556724548}, {"id": 219, "seek": 123788, "start": 1253.16, "end": 1258.0, "text": " Und ihr habt ein Layer dazwischen, der hat beliebig viele, also auch beliebig viele Knoten.", "tokens": [51128, 2719, 5553, 23660, 1343, 35166, 274, 921, 86, 6282, 11, 1163, 2385, 1351, 37660, 9693, 11, 611, 2168, 1351, 37660, 9693, 591, 2247, 268, 13, 51370], "temperature": 0.0, "avg_logprob": -0.20947723065392446, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.5379957556724548}, {"id": 220, "seek": 123788, "start": 1259.2, "end": 1262.5600000000002, "text": " Dann k\u00f6nnt ihr jede mathematische Funktion damit approximieren.", "tokens": [51430, 7455, 22541, 5553, 34039, 11619, 7864, 11166, 9780, 9479, 8542, 5695, 13, 51598], "temperature": 0.0, "avg_logprob": -0.20947723065392446, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.5379957556724548}, {"id": 221, "seek": 123788, "start": 1263.1200000000001, "end": 1267.68, "text": " Das ist erstmal irgendwie klar, aber es wurde sozusagen bewiesen, dass das so ist.", "tokens": [51626, 2846, 1418, 38607, 20759, 14743, 11, 4340, 785, 11191, 33762, 17897, 30383, 11, 2658, 1482, 370, 1418, 13, 51854], "temperature": 0.0, "avg_logprob": -0.20947723065392446, "compression_ratio": 1.6766917293233083, "no_speech_prob": 0.5379957556724548}, {"id": 222, "seek": 126788, "start": 1268.5600000000002, "end": 1269.92, "text": " Und der Witz ist jetzt Folgendes.", "tokens": [50398, 2719, 1163, 343, 6862, 1418, 4354, 15255, 9395, 279, 13, 50466], "temperature": 0.0, "avg_logprob": -0.1763423283894857, "compression_ratio": 1.642570281124498, "no_speech_prob": 0.0031230461318045855}, {"id": 223, "seek": 126788, "start": 1270.3600000000001, "end": 1280.24, "text": " In der Informatik geht es jetzt darum, wir nehmen an, dass unsere Welt determiniert ist und deswegen kann man alles mit einer Funktion quasi hinschreiben.", "tokens": [50488, 682, 1163, 34301, 267, 1035, 7095, 785, 4354, 27313, 11, 1987, 19905, 364, 11, 2658, 14339, 14761, 15957, 4859, 1418, 674, 26482, 4028, 587, 7874, 2194, 6850, 11166, 9780, 20954, 276, 1292, 339, 25946, 13, 50982], "temperature": 0.0, "avg_logprob": -0.1763423283894857, "compression_ratio": 1.642570281124498, "no_speech_prob": 0.0031230461318045855}, {"id": 224, "seek": 126788, "start": 1281.3200000000002, "end": 1285.4, "text": " Die kann beliebig kompliziert sein, aber es muss durch eine Funktion approximierbar sein.", "tokens": [51036, 3229, 4028, 1351, 37660, 24526, 43590, 6195, 11, 4340, 785, 6425, 7131, 3018, 11166, 9780, 8542, 811, 5356, 6195, 13, 51240], "temperature": 0.0, "avg_logprob": -0.1763423283894857, "compression_ratio": 1.642570281124498, "no_speech_prob": 0.0031230461318045855}, {"id": 225, "seek": 126788, "start": 1286.0400000000002, "end": 1291.48, "text": " Das bedeutet, dass alles mit einem Deep Learning Netz, also mit einem neuronalen Netz beschreibbar ist.", "tokens": [51272, 2846, 27018, 11, 2658, 7874, 2194, 6827, 14895, 15205, 38889, 11, 611, 2194, 6827, 12087, 21523, 268, 38889, 17498, 38606, 5356, 1418, 13, 51544], "temperature": 0.0, "avg_logprob": -0.1763423283894857, "compression_ratio": 1.642570281124498, "no_speech_prob": 0.0031230461318045855}, {"id": 226, "seek": 126788, "start": 1291.96, "end": 1293.0, "text": " Das ist die Voraussetzung.", "tokens": [51568, 2846, 1418, 978, 691, 3252, 2023, 38584, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1763423283894857, "compression_ratio": 1.642570281124498, "no_speech_prob": 0.0031230461318045855}, {"id": 227, "seek": 129300, "start": 1293.4, "end": 1296.84, "text": " Und jetzt ist nur die Frage, wenn wir beliebig viele Knoten haben, dann ist das klar.", "tokens": [50384, 2719, 4354, 1418, 4343, 978, 13685, 11, 4797, 1987, 1351, 37660, 9693, 591, 2247, 268, 3084, 11, 3594, 1418, 1482, 14743, 13, 50556], "temperature": 0.0, "avg_logprob": -0.11351818508572048, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.06093586981296539}, {"id": 228, "seek": 129300, "start": 1297.2, "end": 1298.8, "text": " Was man dann einfach macht, ist Overfitting.", "tokens": [50574, 3027, 587, 3594, 7281, 10857, 11, 1418, 4886, 69, 2414, 13, 50654], "temperature": 0.0, "avg_logprob": -0.11351818508572048, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.06093586981296539}, {"id": 229, "seek": 129300, "start": 1298.96, "end": 1306.16, "text": " Also ihr k\u00f6nnt euch quasi vorstellen, ich habe eine Aufgabenstellung und ich verstehe die Aufgabenstellung nicht, sondern ich lerne sie einfach auswendig.", "tokens": [50662, 2743, 5553, 22541, 10403, 20954, 34346, 11, 1893, 6015, 3018, 29648, 25071, 30016, 674, 1893, 22442, 675, 978, 29648, 25071, 30016, 1979, 11, 11465, 1893, 32068, 716, 2804, 7281, 3437, 20128, 328, 13, 51022], "temperature": 0.0, "avg_logprob": -0.11351818508572048, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.06093586981296539}, {"id": 230, "seek": 129300, "start": 1306.48, "end": 1316.6, "text": " Sagen wir mal, mein Konfigurationsraum hat 10 hoch 32 viele M\u00f6glichkeiten und 10 hoch 32 viele Outputs zu einem gegebenen Problem, eins aus diesen 10 hoch 32.", "tokens": [51038, 318, 4698, 1987, 2806, 11, 10777, 12718, 20646, 374, 763, 31502, 2385, 1266, 19783, 8858, 9693, 42627, 674, 1266, 19783, 8858, 9693, 5925, 2582, 82, 2164, 6827, 32572, 268, 11676, 11, 21889, 3437, 12862, 1266, 19783, 8858, 13, 51544], "temperature": 0.0, "avg_logprob": -0.11351818508572048, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.06093586981296539}, {"id": 231, "seek": 129300, "start": 1316.92, "end": 1322.2, "text": " Dann kann ich doch, wenn ich genug Knoten, also Speicherpunkte habe, kann ich die einfach auswendig lernen.", "tokens": [51560, 7455, 4028, 1893, 9243, 11, 4797, 1893, 33194, 591, 2247, 268, 11, 611, 3550, 14934, 27133, 975, 6015, 11, 4028, 1893, 978, 7281, 3437, 20128, 328, 36082, 13, 51824], "temperature": 0.0, "avg_logprob": -0.11351818508572048, "compression_ratio": 1.7421383647798743, "no_speech_prob": 0.06093586981296539}, {"id": 232, "seek": 132220, "start": 1322.2, "end": 1324.88, "text": " Ich kann alle Antworten auf alle Fragen auswendig lernen.", "tokens": [50364, 3141, 4028, 5430, 34693, 268, 2501, 5430, 25588, 3437, 20128, 328, 36082, 13, 50498], "temperature": 0.0, "avg_logprob": -0.14534834811561986, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.004263016860932112}, {"id": 233, "seek": 132220, "start": 1326.76, "end": 1332.28, "text": " Also stellt euch einfach nur vor, ihr wollt eine Software schreiben, die Bilder erkennt und ihr gebt der Software vor.", "tokens": [50592, 2743, 38582, 10403, 7281, 4343, 4245, 11, 5553, 45826, 3018, 27428, 48546, 11, 978, 44719, 1189, 41838, 674, 5553, 1519, 4517, 1163, 27428, 4245, 13, 50868], "temperature": 0.0, "avg_logprob": -0.14534834811561986, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.004263016860932112}, {"id": 234, "seek": 132220, "start": 1332.96, "end": 1336.52, "text": " Man gibt ein Bild rein, das ist 256 mal 256 Pixel gro\u00df.", "tokens": [50902, 2458, 6089, 1343, 15746, 6561, 11, 1482, 1418, 38882, 2806, 38882, 28323, 17253, 13, 51080], "temperature": 0.0, "avg_logprob": -0.14534834811561986, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.004263016860932112}, {"id": 235, "seek": 132220, "start": 1337.72, "end": 1347.16, "text": " Und jetzt macht ihr eine feine Absch\u00e4tzung und steckt sozusagen einfach alle m\u00f6glichen Bilder, die \u00fcberhaupt existieren k\u00f6nnen im Konfigurationsraum.", "tokens": [51140, 2719, 4354, 10857, 5553, 3018, 579, 533, 5813, 339, 3628, 27667, 674, 2126, 19951, 33762, 7281, 5430, 16294, 268, 44719, 11, 978, 20023, 2514, 5695, 6310, 566, 12718, 20646, 374, 763, 31502, 13, 51612], "temperature": 0.0, "avg_logprob": -0.14534834811561986, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.004263016860932112}, {"id": 236, "seek": 134716, "start": 1347.3600000000001, "end": 1359.1200000000001, "text": " Also alle Kombinationen von Pixelwerten, die 256 mal 256 Bilder haben k\u00f6nnen \u00fcberhaupt, die steckt ihr rein und lernt sie auswendig mit dem Satz darunter, was sieht man auf dem Bild.", "tokens": [50374, 2743, 5430, 34678, 2486, 268, 2957, 28323, 26521, 268, 11, 978, 38882, 2806, 38882, 44719, 3084, 6310, 20023, 11, 978, 2126, 19951, 5553, 6561, 674, 32068, 580, 2804, 3437, 20128, 328, 2194, 1371, 5344, 89, 4072, 21777, 11, 390, 14289, 587, 2501, 1371, 15746, 13, 50962], "temperature": 0.0, "avg_logprob": -0.16385301813348993, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.0913100615143776}, {"id": 237, "seek": 134716, "start": 1359.52, "end": 1360.52, "text": " Das lernt ihr auswendig.", "tokens": [50982, 2846, 32068, 580, 5553, 3437, 20128, 328, 13, 51032], "temperature": 0.0, "avg_logprob": -0.16385301813348993, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.0913100615143776}, {"id": 238, "seek": 134716, "start": 1361.3600000000001, "end": 1366.0800000000002, "text": " Wenn ihr unendlich viel Speicherplatz habt und unendlich gro\u00df sozusagen das Lehr machen k\u00f6nnt, ist das nat\u00fcrlich kein Problem.", "tokens": [51074, 7899, 5553, 517, 521, 1739, 5891, 3550, 14934, 34755, 23660, 674, 517, 521, 1739, 17253, 33762, 1482, 29943, 7069, 22541, 11, 1418, 1482, 8762, 13424, 11676, 13, 51310], "temperature": 0.0, "avg_logprob": -0.16385301813348993, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.0913100615143776}, {"id": 239, "seek": 134716, "start": 1366.3600000000001, "end": 1371.72, "text": " Dabei habt ihr aber sozusagen, ihr habt das Problem nur gelernt, aber ihr habt es nicht verstanden.", "tokens": [51324, 39606, 23660, 5553, 4340, 33762, 11, 5553, 23660, 1482, 11676, 4343, 49224, 11, 4340, 5553, 23660, 785, 1979, 1306, 33946, 13, 51592], "temperature": 0.0, "avg_logprob": -0.16385301813348993, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.0913100615143776}, {"id": 240, "seek": 137172, "start": 1372.44, "end": 1382.28, "text": " Und die Idee ist sozusagen, kann man das runter kochen auf irgendetwas, was weniger ist als Overfitting und sozusagen die Datenmenge besser abbildet,", "tokens": [50400, 2719, 978, 32651, 1418, 33762, 11, 4028, 587, 1482, 33295, 8384, 2470, 2501, 11093, 302, 6569, 11, 390, 23224, 1418, 3907, 4886, 69, 2414, 674, 33762, 978, 31126, 2558, 432, 18021, 410, 16248, 302, 11, 50892], "temperature": 0.0, "avg_logprob": -0.14185071624485793, "compression_ratio": 1.5733788395904438, "no_speech_prob": 0.39897775650024414}, {"id": 241, "seek": 137172, "start": 1382.48, "end": 1390.44, "text": " sozusagen eine sehr gro\u00dfe Datenmenge, zum Beispiel dieser riesige Konfigurationsraum von allen m\u00f6glichen Bildern, die 256 Quadratviele Pixel haben,", "tokens": [50902, 33762, 3018, 5499, 19691, 31126, 2558, 432, 11, 5919, 13772, 9053, 23932, 3969, 12718, 20646, 374, 763, 31502, 2957, 18440, 16294, 268, 15746, 1248, 11, 978, 38882, 29619, 4481, 12702, 306, 28323, 3084, 11, 51300], "temperature": 0.0, "avg_logprob": -0.14185071624485793, "compression_ratio": 1.5733788395904438, "no_speech_prob": 0.39897775650024414}, {"id": 242, "seek": 137172, "start": 1391.4, "end": 1398.04, "text": " irgendwie abzubilden auf irgendwas, wo wir trotzdem das Problem l\u00f6sen k\u00f6nnen, n\u00e4mlich eine Software, die uns am Ende sagt, was sieht man auf dem Bild oder so.", "tokens": [51348, 20759, 410, 40566, 793, 268, 2501, 47090, 11, 6020, 1987, 28325, 1482, 11676, 25209, 6748, 6310, 11, 21219, 3018, 27428, 11, 978, 2693, 669, 15152, 15764, 11, 390, 14289, 587, 2501, 1371, 15746, 4513, 370, 13, 51680], "temperature": 0.0, "avg_logprob": -0.14185071624485793, "compression_ratio": 1.5733788395904438, "no_speech_prob": 0.39897775650024414}, {"id": 243, "seek": 139804, "start": 1398.36, "end": 1404.96, "text": " Und das ist quasi das, was ich hier sozusagen jetzt mit Overfitting auch erkl\u00e4ren wollte, aber wie gesagt, das Video ist ein bisschen heftig, ich wei\u00df schon.", "tokens": [50380, 2719, 1482, 1418, 20954, 1482, 11, 390, 1893, 3296, 33762, 4354, 2194, 4886, 69, 2414, 2168, 46528, 24509, 11, 4340, 3355, 12260, 11, 1482, 9777, 1418, 1343, 10763, 415, 34765, 11, 1893, 13385, 4981, 13, 50710], "temperature": 0.0, "avg_logprob": -0.17215245769869897, "compression_ratio": 1.5157232704402517, "no_speech_prob": 0.09131387621164322}, {"id": 244, "seek": 139804, "start": 1405.96, "end": 1407.04, "text": " Das nennt man Overfitting.", "tokens": [50760, 2846, 16399, 580, 587, 4886, 69, 2414, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17215245769869897, "compression_ratio": 1.5157232704402517, "no_speech_prob": 0.09131387621164322}, {"id": 245, "seek": 139804, "start": 1407.48, "end": 1413.8, "text": " Viel besser w\u00e4re es doch, wenn ich runter gehe mit meiner Komplexit\u00e4t in meinem Deep Learning Netzwerk, also in diesem Zwischenlayer,", "tokens": [50836, 35931, 18021, 14558, 785, 9243, 11, 4797, 1893, 33295, 34252, 2194, 20529, 14286, 18945, 14053, 294, 24171, 14895, 15205, 38889, 26833, 11, 611, 294, 10975, 29385, 6282, 8376, 260, 11, 51152], "temperature": 0.0, "avg_logprob": -0.17215245769869897, "compression_ratio": 1.5157232704402517, "no_speech_prob": 0.09131387621164322}, {"id": 246, "seek": 139804, "start": 1415.12, "end": 1422.96, "text": " auf viel weniger Dimension und trotzdem noch eine sehr hohe Approximationsg\u00fcte kriege, also 99,999 Prozent oder so.", "tokens": [51218, 2501, 5891, 23224, 20975, 3378, 674, 28325, 3514, 3018, 5499, 1106, 675, 29551, 3081, 763, 70, 774, 975, 25766, 432, 11, 611, 11803, 11, 49017, 29726, 4513, 370, 13, 51610], "temperature": 0.0, "avg_logprob": -0.17215245769869897, "compression_ratio": 1.5157232704402517, "no_speech_prob": 0.09131387621164322}, {"id": 247, "seek": 139804, "start": 1423.76, "end": 1425.28, "text": " Denn dann habe ich ja Folgendes geschafft.", "tokens": [51650, 19027, 3594, 6015, 1893, 2784, 15255, 9395, 279, 45215, 13, 51726], "temperature": 0.0, "avg_logprob": -0.17215245769869897, "compression_ratio": 1.5157232704402517, "no_speech_prob": 0.09131387621164322}, {"id": 248, "seek": 142528, "start": 1425.68, "end": 1432.12, "text": " Ich habe mein Problem komprimiert, was konkret, ich habe aus dem Auswendiglernen Verstehen gemacht.", "tokens": [50384, 3141, 6015, 10777, 11676, 5207, 1424, 332, 4859, 11, 390, 36500, 11, 1893, 6015, 3437, 1371, 48500, 521, 328, 75, 25657, 4281, 2941, 2932, 12293, 13, 50706], "temperature": 0.0, "avg_logprob": -0.1753424021823347, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.08877076208591461}, {"id": 249, "seek": 142528, "start": 1432.6399999999999, "end": 1439.72, "text": " Ich habe sozusagen, und das ist ja das, was grunds\u00e4tzlich irgendwie nie jemand mal rafft, was ist jetzt der Unterschied zwischen lehnen und lernen und verstehen?", "tokens": [50732, 3141, 6015, 33762, 11, 674, 1482, 1418, 2784, 1482, 11, 390, 30886, 82, 33373, 20759, 2838, 21717, 2806, 367, 29445, 11, 390, 1418, 4354, 1163, 41414, 19875, 476, 71, 2866, 674, 36082, 674, 37352, 30, 51086], "temperature": 0.0, "avg_logprob": -0.1753424021823347, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.08877076208591461}, {"id": 250, "seek": 142528, "start": 1440.16, "end": 1451.56, "text": " Verstehen bedeutet, mit dem minimalen Satz an Handlungsanweisungen das Problem vollst\u00e4ndig zu l\u00f6sen und aus verschiedensten Wissenssachen, aus Kombinationen von Wissen, neues Wissen zu erzeugen.", "tokens": [51108, 4281, 2941, 2932, 27018, 11, 2194, 1371, 13206, 268, 5344, 89, 364, 8854, 49876, 282, 35033, 5084, 1482, 11676, 15593, 16913, 328, 2164, 25209, 6748, 674, 3437, 22263, 268, 6266, 343, 891, 694, 82, 11646, 11, 3437, 34678, 2486, 268, 2957, 343, 10987, 11, 43979, 343, 10987, 2164, 1189, 19303, 268, 13, 51678], "temperature": 0.0, "avg_logprob": -0.1753424021823347, "compression_ratio": 1.6392857142857142, "no_speech_prob": 0.08877076208591461}, {"id": 251, "seek": 145156, "start": 1452.36, "end": 1459.0, "text": " Ja, eine Anwendung zu finden, die sozusagen nur darauf basiert, dass ich hier das eine schon mal verstanden habe und ich direkt auf das andere anwenden kann.", "tokens": [50404, 3530, 11, 3018, 1107, 20128, 1063, 2164, 20734, 11, 978, 33762, 4343, 18654, 987, 4859, 11, 2658, 1893, 3296, 1482, 3018, 4981, 2806, 1306, 33946, 6015, 674, 1893, 20315, 2501, 1482, 10490, 364, 86, 8896, 4028, 13, 50736], "temperature": 0.0, "avg_logprob": -0.1389680862426758, "compression_ratio": 1.6088709677419355, "no_speech_prob": 0.058328110724687576}, {"id": 252, "seek": 145156, "start": 1460.08, "end": 1461.6, "text": " So, und das ist auch da der Fall.", "tokens": [50790, 407, 11, 674, 1482, 1418, 2168, 1120, 1163, 7465, 13, 50866], "temperature": 0.0, "avg_logprob": -0.1389680862426758, "compression_ratio": 1.6088709677419355, "no_speech_prob": 0.058328110724687576}, {"id": 253, "seek": 145156, "start": 1461.9199999999998, "end": 1466.48, "text": " Wenn man also den minimalen Satz quasi die beste Architektur von Layern erzeugen kann,", "tokens": [50882, 7899, 587, 611, 1441, 13206, 268, 5344, 89, 20954, 978, 22245, 10984, 642, 2320, 374, 2957, 20084, 1248, 1189, 19303, 268, 4028, 11, 51110], "temperature": 0.0, "avg_logprob": -0.1389680862426758, "compression_ratio": 1.6088709677419355, "no_speech_prob": 0.058328110724687576}, {"id": 254, "seek": 145156, "start": 1467.72, "end": 1472.8, "text": " um zum Beispiel Energie zu sparen, Speicher zu sparen, dann komprimiert das sozusagen. Ich mache eine Kompression dabei.", "tokens": [51172, 1105, 5919, 13772, 35309, 2164, 637, 4484, 11, 3550, 14934, 2164, 637, 4484, 11, 3594, 5207, 1424, 332, 4859, 1482, 33762, 13, 3141, 28289, 3018, 591, 8586, 2775, 14967, 13, 51426], "temperature": 0.0, "avg_logprob": -0.1389680862426758, "compression_ratio": 1.6088709677419355, "no_speech_prob": 0.058328110724687576}, {"id": 255, "seek": 147280, "start": 1473.6399999999999, "end": 1482.84, "text": " Und da kommen dann sozusagen die mehreren Layer dann typischerweise im Deep Learning dazu. Es ist nicht einer, sondern es sind viele verschachtelte Layer, die runter und hoch gehen in ihrer Dimension.", "tokens": [50406, 2719, 1120, 11729, 3594, 33762, 978, 5417, 5170, 35166, 3594, 2125, 19674, 13109, 566, 14895, 15205, 13034, 13, 2313, 1418, 1979, 6850, 11, 11465, 785, 3290, 9693, 20563, 3589, 338, 975, 35166, 11, 978, 33295, 674, 19783, 13230, 294, 23990, 20975, 3378, 13, 50866], "temperature": 0.0, "avg_logprob": -0.25633275168282643, "compression_ratio": 1.6715116279069768, "no_speech_prob": 0.085029236972332}, {"id": 256, "seek": 147280, "start": 1482.84, "end": 1488.96, "text": " Da gibt es sozusagen Konvolutiones und was auch immer. Es gibt \u00fcbelst viele krasse Architekturen mittlerweile, die alle sehr tricky sind.", "tokens": [50866, 3933, 6089, 785, 33762, 12718, 85, 3386, 279, 674, 390, 2168, 5578, 13, 2313, 6089, 3304, 5390, 372, 9693, 350, 3906, 405, 10984, 642, 2320, 9873, 41999, 11, 978, 5430, 5499, 12414, 3290, 13, 51172], "temperature": 0.0, "avg_logprob": -0.25633275168282643, "compression_ratio": 1.6715116279069768, "no_speech_prob": 0.085029236972332}, {"id": 257, "seek": 147280, "start": 1489.76, "end": 1501.9199999999998, "text": " Aber das ist sozusagen erstmal die Idee. Es gibt diesen universellen Funktions-Approximator und man kann mit einem Neuronalen jetzt jede beliebige Funktion, insbesondere alles, was wir in unserer Realit\u00e4t beobachten, wenn es dem geht.", "tokens": [51212, 5992, 1482, 1418, 33762, 38607, 978, 32651, 13, 2313, 6089, 12862, 6445, 19191, 11166, 2320, 626, 12, 9132, 340, 3081, 1639, 674, 587, 4028, 2194, 6827, 1734, 374, 21523, 268, 4354, 34039, 1351, 65, 3969, 11166, 9780, 11, 48694, 7874, 11, 390, 1987, 294, 20965, 8467, 14053, 312, 996, 20806, 11, 4797, 785, 1371, 7095, 13, 51820], "temperature": 0.0, "avg_logprob": -0.25633275168282643, "compression_ratio": 1.6715116279069768, "no_speech_prob": 0.085029236972332}, {"id": 258, "seek": 150280, "start": 1503.76, "end": 1514.32, "text": " Das ist die Annahme. Und warum sollte man also nicht das Gehirn damit beschreiben k\u00f6nnen? Und warum sollte man nicht die gesamte Realit\u00e4t und das gesamte Universum damit beschreiben k\u00f6nnen?", "tokens": [50412, 2846, 1418, 978, 8860, 545, 1398, 13, 2719, 24331, 18042, 587, 611, 1979, 1482, 2876, 24118, 77, 9479, 17498, 25946, 6310, 30, 2719, 24331, 18042, 587, 1979, 978, 39746, 975, 8467, 14053, 674, 1482, 39746, 975, 14052, 449, 9479, 17498, 25946, 6310, 30, 50940], "temperature": 0.0, "avg_logprob": -0.2772525958160856, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.1966511458158493}, {"id": 259, "seek": 150280, "start": 1521.68, "end": 1525.24, "text": " Ich habe jetzt schon erkl\u00e4rt in meinem anderen Video, was das Bewusstsein ist.", "tokens": [51308, 3141, 6015, 4354, 4981, 27570, 24802, 294, 24171, 11122, 9777, 11, 390, 1482, 40512, 26340, 33042, 1418, 13, 51486], "temperature": 0.0, "avg_logprob": -0.2772525958160856, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.1966511458158493}, {"id": 260, "seek": 152524, "start": 1526.1200000000001, "end": 1534.68, "text": " Und Joshua Bach hat es ziemlich kompakt nochmal ausgedr\u00fcckt und hat gesagt, das Bewusstsein ist ein Modell f\u00fcr den Inhalt unseres Aufmerksamkeitsmodells.", "tokens": [50408, 2719, 24005, 30920, 2385, 785, 28901, 5207, 79, 5886, 26509, 3437, 3004, 81, 37532, 674, 2385, 12260, 11, 1482, 40512, 26340, 33042, 1418, 1343, 6583, 898, 2959, 1441, 682, 20731, 2693, 9931, 9462, 936, 28665, 330, 1208, 8014, 13677, 13, 50836], "temperature": 0.0, "avg_logprob": -0.15845719405582973, "compression_ratio": 1.6088709677419355, "no_speech_prob": 0.3133799731731415}, {"id": 261, "seek": 152524, "start": 1534.68, "end": 1541.84, "text": " Und was meint ihr damit? Na ja, es ist so eine Art Meta-Aufmerksamkeit. Ihr m\u00fcsst euch vorstellen, es ist so eine Art Meta-Learning.", "tokens": [50836, 2719, 390, 385, 686, 5553, 9479, 30, 6056, 2784, 11, 785, 1418, 370, 3018, 5735, 6377, 64, 12, 32, 2947, 936, 28665, 9238, 13, 14773, 49481, 10403, 34346, 11, 785, 1418, 370, 3018, 5735, 6377, 64, 12, 11020, 2341, 13, 51194], "temperature": 0.0, "avg_logprob": -0.15845719405582973, "compression_ratio": 1.6088709677419355, "no_speech_prob": 0.3133799731731415}, {"id": 262, "seek": 152524, "start": 1541.84, "end": 1547.32, "text": " Was meint jetzt eigentlich Aufmerksamkeit? Na ja, also erstmal gibt es ja mittlerweile schon Transformatoren.", "tokens": [51194, 3027, 385, 686, 4354, 10926, 9462, 936, 28665, 9238, 30, 6056, 2784, 11, 611, 38607, 6089, 785, 2784, 41999, 4981, 27938, 267, 10948, 13, 51468], "temperature": 0.0, "avg_logprob": -0.15845719405582973, "compression_ratio": 1.6088709677419355, "no_speech_prob": 0.3133799731731415}, {"id": 263, "seek": 154732, "start": 1547.32, "end": 1558.96, "text": " Also es gibt schon Attention-Networks im Deep Learning. Diese Sprache wurde schon etabliert und das macht auch ziemlich genau das, was man sich eigentlich unter dem Konzept von Aufmerksamkeit vorstellt.", "tokens": [50364, 2743, 785, 6089, 4981, 31858, 12, 31890, 18357, 566, 14895, 15205, 13, 18993, 7702, 6000, 11191, 4981, 1030, 455, 2753, 83, 674, 1482, 10857, 2168, 28901, 12535, 1482, 11, 390, 587, 3041, 10926, 8662, 1371, 12718, 32082, 2957, 9462, 936, 28665, 9238, 4245, 372, 12783, 13, 50946], "temperature": 0.0, "avg_logprob": -0.1728616153492647, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.08261857181787491}, {"id": 264, "seek": 154732, "start": 1558.96, "end": 1566.6, "text": " Also erstmal Aufmerksamkeit und Bewusstheit, das h\u00e4ngt schon miteinander zusammen, auch vom Wort, sozusagen von der w\u00f6rtlichen Bedeutung.", "tokens": [50946, 2743, 38607, 9462, 936, 28665, 9238, 674, 40512, 26340, 8480, 11, 1482, 276, 29670, 4981, 43127, 14311, 11, 2168, 10135, 22748, 11, 33762, 2957, 1163, 261, 11454, 10193, 363, 4858, 325, 1063, 13, 51328], "temperature": 0.0, "avg_logprob": -0.1728616153492647, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.08261857181787491}, {"id": 265, "seek": 156660, "start": 1567.08, "end": 1577.52, "text": " Und im Moment ist es so, normale Deep Learning-Netzwerke machen Folgendes. Die haben einen Fehler-Signal, also sagen wir mal, wir haben eine Beobachtung und eine Prediktion von unserem Modell und das ist falsch.", "tokens": [50388, 2719, 566, 19093, 1418, 785, 370, 11, 43646, 14895, 15205, 12, 45, 10074, 1554, 330, 7069, 15255, 9395, 279, 13, 3229, 3084, 4891, 48101, 12, 50, 788, 304, 11, 611, 8360, 1987, 2806, 11, 1987, 3084, 3018, 879, 996, 3589, 1063, 674, 3018, 32969, 9874, 313, 2957, 26792, 6583, 898, 674, 1482, 1418, 43340, 13, 50910], "temperature": 0.0, "avg_logprob": -0.166767332288954, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.32375669479370117}, {"id": 266, "seek": 156660, "start": 1577.52, "end": 1584.52, "text": " Und dieses Fehler-Signal, die Differenz davon, die versuchen wir zur\u00fcckzuf\u00fchren, in unser Modell wegzutrecken. Wo kommt sozusagen der Fehler her?", "tokens": [50910, 2719, 12113, 48101, 12, 50, 788, 304, 11, 978, 413, 12612, 11368, 18574, 11, 978, 34749, 1987, 15089, 39467, 29540, 11, 294, 12977, 6583, 898, 15565, 89, 325, 265, 13029, 13, 6622, 10047, 33762, 1163, 48101, 720, 30, 51260], "temperature": 0.0, "avg_logprob": -0.166767332288954, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.32375669479370117}, {"id": 267, "seek": 156660, "start": 1584.52, "end": 1590.8799999999999, "text": " Und dabei sozusagen bei vielen Daten entstehen nat\u00fcrlich eine ganze Menge in diesen statistischen Gewichten, wo irgendwas sich \u00e4ndert.", "tokens": [51260, 2719, 14967, 33762, 4643, 19885, 31126, 35955, 2932, 8762, 3018, 18898, 40723, 294, 12862, 16012, 6282, 19063, 24681, 11, 6020, 47090, 3041, 24981, 911, 13, 51578], "temperature": 0.0, "avg_logprob": -0.166767332288954, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.32375669479370117}, {"id": 268, "seek": 159088, "start": 1590.88, "end": 1603.44, "text": " Wenn man jetzt mit vielen Daten lernt, mitteln sich sozusagen die unwichtigen Gewichte raus und was \u00fcberbleibt ist das, wo der wirkliche Fehler herkommt, also das sind dann meistens nur noch weniger Gewichte.", "tokens": [50364, 7899, 587, 4354, 2194, 19885, 31126, 32068, 580, 11, 19130, 9878, 3041, 33762, 978, 14853, 1405, 3213, 19063, 18972, 17202, 674, 390, 4502, 638, 13651, 1418, 1482, 11, 6020, 1163, 9696, 68, 48101, 720, 74, 22230, 11, 611, 1482, 3290, 3594, 36894, 694, 4343, 3514, 23224, 19063, 18972, 13, 50992], "temperature": 0.0, "avg_logprob": -0.16711054042894014, "compression_ratio": 1.5450819672131149, "no_speech_prob": 0.06006363779306412}, {"id": 269, "seek": 159088, "start": 1603.44, "end": 1613.16, "text": " So, das ist aber ein Prozess, der super ineffizient ist und langsam, der ist super langsam, weil man immer wieder den kompletten Backpropagating-Step durchmachen muss.", "tokens": [50992, 407, 11, 1482, 1418, 4340, 1343, 1705, 37575, 11, 1163, 1687, 7167, 602, 590, 1196, 1418, 674, 39597, 11, 1163, 1418, 1687, 39597, 11, 7689, 587, 5578, 6216, 1441, 5207, 14657, 1147, 5833, 79, 1513, 559, 990, 12, 23624, 7131, 43981, 6425, 13, 51478], "temperature": 0.0, "avg_logprob": -0.16711054042894014, "compression_ratio": 1.5450819672131149, "no_speech_prob": 0.06006363779306412}, {"id": 270, "seek": 161316, "start": 1614.0400000000002, "end": 1629.52, "text": " Viel geiler w\u00e4re, wenn man ein Netzwerk hat, was oben da bei diesem Lernprozess draufguckt und feststellt, wo sind die gr\u00f6\u00dften Gewichte im Gradienten, also quasi in dem Backpropagating und kann ich das beobachten, wenn ich das sozusagen \u00f6fter mache.", "tokens": [50408, 35931, 1519, 5441, 14558, 11, 4797, 587, 1343, 38889, 26833, 2385, 11, 390, 21279, 1120, 4643, 10975, 441, 1248, 4318, 37575, 22763, 70, 47800, 674, 6633, 372, 12783, 11, 6020, 3290, 978, 20691, 1147, 19063, 18972, 566, 16710, 1196, 268, 11, 611, 20954, 294, 1371, 5833, 79, 1513, 559, 990, 674, 4028, 1893, 1482, 312, 996, 20806, 11, 4797, 1893, 1482, 33762, 4044, 828, 28289, 13, 51182], "temperature": 0.0, "avg_logprob": -0.13656554090867348, "compression_ratio": 1.5647058823529412, "no_speech_prob": 0.621540367603302}, {"id": 271, "seek": 161316, "start": 1629.52, "end": 1639.28, "text": " Und dann kann man Statistiken betreiben und sozusagen das abk\u00fcrzen. Man kann diesen Lernprozess abk\u00fcrzen, weil man sozusagen den Shortcut geht.", "tokens": [51182, 2719, 3594, 4028, 587, 16249, 468, 19640, 778, 25946, 674, 33762, 1482, 410, 74, 1655, 2904, 13, 2458, 4028, 12862, 441, 1248, 4318, 37575, 410, 74, 1655, 2904, 11, 7689, 587, 33762, 1441, 16881, 6672, 7095, 13, 51670], "temperature": 0.0, "avg_logprob": -0.13656554090867348, "compression_ratio": 1.5647058823529412, "no_speech_prob": 0.621540367603302}, {"id": 272, "seek": 163928, "start": 1639.44, "end": 1648.3999999999999, "text": " Das hat so ein bisschen was von, wenn ihr euch mit iterativen L\u00f6sungsverfahren von algebraischen Gleichungen auskennt, das hat ein bisschen was von Order-Zapsets, finde ich.", "tokens": [50372, 2846, 2385, 370, 1343, 10763, 390, 2957, 11, 4797, 5553, 10403, 2194, 17138, 267, 5709, 34642, 5846, 331, 34394, 2957, 21989, 6282, 33858, 5084, 3437, 41838, 11, 1482, 2385, 1343, 10763, 390, 2957, 16321, 12, 57, 2382, 1385, 11, 17841, 1893, 13, 50820], "temperature": 0.0, "avg_logprob": -0.21295685517160515, "compression_ratio": 1.4292682926829268, "no_speech_prob": 0.49095919728279114}, {"id": 273, "seek": 163928, "start": 1648.3999999999999, "end": 1654.96, "text": " Aber das ist wirklich nur meine Physiker-Intubation, da bin ich sozusagen mit gef\u00e4hrlichem Halbwissen hier unterwegs.", "tokens": [50820, 5992, 1482, 1418, 9696, 4343, 10946, 15542, 17314, 12, 25597, 836, 399, 11, 1120, 5171, 1893, 33762, 2194, 41484, 1739, 443, 13896, 65, 86, 10987, 3296, 36258, 13, 51148], "temperature": 0.0, "avg_logprob": -0.21295685517160515, "compression_ratio": 1.4292682926829268, "no_speech_prob": 0.49095919728279114}, {"id": 274, "seek": 165496, "start": 1654.96, "end": 1680.8400000000001, "text": " Jedenfalls, das k\u00fcrzt den Lernprozess ab und das erkl\u00e4rt nat\u00fcrlich auch, warum sozusagen heutzutage so ein Alpha Star, also die KI, die StarCraft gewinnt, die hat ja so was wie umgerechnet 200 Jahre StarCraft gespielt, bevor sie auf einem Level ist, wie ein normaler Master-Spieler im High MMR Blizzard Elo.", "tokens": [50364, 508, 6876, 18542, 11, 1482, 350, 1655, 2682, 1441, 441, 1248, 4318, 37575, 410, 674, 1482, 27570, 24802, 8762, 2168, 11, 24331, 33762, 415, 12950, 325, 609, 370, 1343, 20588, 5705, 11, 611, 978, 47261, 11, 978, 5705, 34, 4469, 6906, 259, 580, 11, 978, 2385, 2784, 370, 390, 3355, 1105, 34899, 46248, 2331, 15557, 5705, 34, 4469, 5019, 43929, 11, 37591, 2804, 2501, 6827, 16872, 1418, 11, 3355, 1343, 2710, 260, 6140, 12, 14014, 1187, 260, 566, 5229, 376, 21173, 40976, 41784, 13, 51658], "temperature": 0.0, "avg_logprob": -0.21352436325766824, "compression_ratio": 1.390134529147982, "no_speech_prob": 0.4911520779132843}, {"id": 275, "seek": 168084, "start": 1680.84, "end": 1690.6799999999998, "text": " Und ein Mensch braucht nat\u00fcrlich viel weniger Daten. Warum? Na ja, weil unsere Hirnarchitektur krasser ist. Wir leben schon auf dem n\u00e4chsten Level von Deep Learning.", "tokens": [50364, 2719, 1343, 27773, 22623, 8762, 5891, 23224, 31126, 13, 25541, 30, 6056, 2784, 11, 7689, 14339, 23192, 77, 1178, 642, 2320, 374, 15913, 12129, 1418, 13, 4347, 26392, 4981, 2501, 1371, 19101, 16872, 2957, 14895, 15205, 13, 50856], "temperature": 0.0, "avg_logprob": -0.1670832712142194, "compression_ratio": 1.6539682539682539, "no_speech_prob": 0.24452488124370575}, {"id": 276, "seek": 168084, "start": 1690.6799999999998, "end": 1702.8, "text": " Was sozusagen die State of the Art ist im Moment, das erste Level war quasi, wir bringen ein Programm bei Schach zu spielen und das kriegen wir einfach hin.", "tokens": [50856, 3027, 33762, 978, 4533, 295, 264, 5735, 1418, 566, 19093, 11, 1482, 20951, 16872, 1516, 20954, 11, 1987, 27519, 1343, 48244, 4643, 2065, 608, 2164, 30950, 674, 1482, 46882, 1987, 7281, 14102, 13, 51462], "temperature": 0.0, "avg_logprob": -0.1670832712142194, "compression_ratio": 1.6539682539682539, "no_speech_prob": 0.24452488124370575}, {"id": 277, "seek": 168084, "start": 1702.8, "end": 1710.6799999999998, "text": " Das n\u00e4chste Level war, wir schreiben eine Architektur, zum Beispiel bei Go, da geht das nicht mehr, das Spiel ist zu kompliziert, man kann nicht in einem Programm einfach schreiben, was das kann.", "tokens": [51462, 2846, 30661, 16872, 1516, 11, 1987, 48546, 3018, 10984, 642, 2320, 374, 11, 5919, 13772, 4643, 1037, 11, 1120, 7095, 1482, 1979, 5417, 11, 1482, 14266, 1418, 2164, 24526, 43590, 11, 587, 4028, 1979, 294, 6827, 48244, 7281, 48546, 11, 390, 1482, 4028, 13, 51856], "temperature": 0.0, "avg_logprob": -0.1670832712142194, "compression_ratio": 1.6539682539682539, "no_speech_prob": 0.24452488124370575}, {"id": 278, "seek": 171068, "start": 1710.76, "end": 1716.1200000000001, "text": " Sondern wir schreiben ein Netzwerk, was selber lernt, sich Go beizubringen.", "tokens": [50368, 318, 10881, 1987, 48546, 1343, 38889, 26833, 11, 390, 23888, 32068, 580, 11, 3041, 1037, 312, 590, 836, 2937, 268, 13, 50636], "temperature": 0.0, "avg_logprob": -0.1914147181683276, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.09526122361421585}, {"id": 279, "seek": 171068, "start": 1717.1200000000001, "end": 1723.04, "text": " Und Joshua Bach sagt jetzt, das ist aber immer noch nicht das, was wir k\u00f6nnen, wir Menschen sind noch ein Level krasser.", "tokens": [50686, 2719, 24005, 30920, 15764, 4354, 11, 1482, 1418, 4340, 5578, 3514, 1979, 1482, 11, 390, 1987, 6310, 11, 1987, 8397, 3290, 3514, 1343, 16872, 15913, 12129, 13, 50982], "temperature": 0.0, "avg_logprob": -0.1914147181683276, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.09526122361421585}, {"id": 280, "seek": 171068, "start": 1723.52, "end": 1731.92, "text": " Wir k\u00f6nnen Meta-Learning machen, wir k\u00f6nnen also ein Programm schreiben, was ein Programm findet, um ein Problem zu lernen.", "tokens": [51006, 4347, 6310, 6377, 64, 12, 11020, 2341, 7069, 11, 1987, 6310, 611, 1343, 48244, 48546, 11, 390, 1343, 48244, 27752, 11, 1105, 1343, 11676, 2164, 36082, 13, 51426], "temperature": 0.0, "avg_logprob": -0.1914147181683276, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.09526122361421585}, {"id": 281, "seek": 173192, "start": 1732.6000000000001, "end": 1740.4, "text": " Das ist sozusagen das, von dem er sagt, was wir als n\u00e4chsten Schritt brauchen. Und ich nehme an, das passiert erst nach dem n\u00e4chsten KI-Winter.", "tokens": [50398, 2846, 1418, 33762, 1482, 11, 2957, 1371, 1189, 15764, 11, 390, 1987, 3907, 19101, 33062, 19543, 13, 2719, 1893, 48276, 364, 11, 1482, 21671, 11301, 5168, 1371, 19101, 47261, 12, 54, 5106, 13, 50788], "temperature": 0.0, "avg_logprob": -0.19722017252220297, "compression_ratio": 1.518796992481203, "no_speech_prob": 0.2810136079788208}, {"id": 282, "seek": 173192, "start": 1740.4, "end": 1747.1200000000001, "text": " Also wenn sozusagen nur Slow reinkickt und wir noch mehr Rechnerkapazit\u00e4ten haben, um das zu machen.", "tokens": [50788, 2743, 4797, 33762, 4343, 17703, 319, 475, 40522, 674, 1987, 3514, 5417, 1300, 339, 1193, 34334, 921, 49289, 3084, 11, 1105, 1482, 2164, 7069, 13, 51124], "temperature": 0.0, "avg_logprob": -0.19722017252220297, "compression_ratio": 1.518796992481203, "no_speech_prob": 0.2810136079788208}, {"id": 283, "seek": 173192, "start": 1747.1200000000001, "end": 1752.68, "text": " Weil ihr d\u00fcrft mich vergessen, wie unfassbar viele Neuronen wir in unserem Kopf drin haben.", "tokens": [51124, 18665, 5553, 23637, 844, 6031, 42418, 11, 3355, 3971, 640, 5356, 9693, 1734, 374, 32923, 1987, 294, 26792, 28231, 24534, 3084, 13, 51402], "temperature": 0.0, "avg_logprob": -0.19722017252220297, "compression_ratio": 1.518796992481203, "no_speech_prob": 0.2810136079788208}, {"id": 284, "seek": 173192, "start": 1752.68, "end": 1755.3600000000001, "text": " Also die Komplexit\u00e4t unseres Gehirns ist schon ziemlich krass.", "tokens": [51402, 2743, 978, 14286, 18945, 14053, 2693, 9931, 2876, 24118, 3695, 1418, 4981, 28901, 15913, 640, 13, 51536], "temperature": 0.0, "avg_logprob": -0.19722017252220297, "compression_ratio": 1.518796992481203, "no_speech_prob": 0.2810136079788208}, {"id": 285, "seek": 175536, "start": 1756.32, "end": 1762.7199999999998, "text": " Und man kann sozusagen sagen, da ist eine ganze Menge krasser, universeller Funktions-Approximator mit drin.", "tokens": [50412, 2719, 587, 4028, 33762, 8360, 11, 1120, 1418, 3018, 18898, 40723, 15913, 12129, 11, 6445, 4658, 11166, 2320, 626, 12, 9132, 340, 3081, 1639, 2194, 24534, 13, 50732], "temperature": 0.0, "avg_logprob": -0.21606759102113784, "compression_ratio": 1.4753363228699552, "no_speech_prob": 0.1941666454076767}, {"id": 286, "seek": 175536, "start": 1762.7199999999998, "end": 1769.56, "text": " Und das Gehirn hat halt wirklich jetzt, das beobachtet halt, wo die meiste Performance herkommt beim Lernen.", "tokens": [50732, 2719, 1482, 2876, 24118, 77, 2385, 12479, 9696, 4354, 11, 1482, 312, 996, 48833, 12479, 11, 6020, 978, 385, 8375, 25047, 720, 74, 22230, 13922, 441, 25657, 13, 51074], "temperature": 0.0, "avg_logprob": -0.21606759102113784, "compression_ratio": 1.4753363228699552, "no_speech_prob": 0.1941666454076767}, {"id": 287, "seek": 175536, "start": 1769.56, "end": 1776.9599999999998, "text": " Und speichert quasi diese Aufmerksamkeit, das speichert es zusammen mit dem Kontext, in dem es gelernt hat, ab.", "tokens": [51074, 2719, 768, 480, 911, 20954, 6705, 9462, 936, 28665, 9238, 11, 1482, 768, 480, 911, 785, 14311, 2194, 1371, 20629, 3828, 11, 294, 1371, 785, 49224, 2385, 11, 410, 13, 51444], "temperature": 0.0, "avg_logprob": -0.21606759102113784, "compression_ratio": 1.4753363228699552, "no_speech_prob": 0.1941666454076767}, {"id": 288, "seek": 177696, "start": 1777.4, "end": 1781.72, "text": " Und das ist sozusagen der Inhalt der Aufmerksamkeit.", "tokens": [50386, 2719, 1482, 1418, 33762, 1163, 682, 20731, 1163, 9462, 936, 28665, 9238, 13, 50602], "temperature": 0.0, "avg_logprob": -0.2174265972979657, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.07802663743495941}, {"id": 289, "seek": 177696, "start": 1781.72, "end": 1788.6000000000001, "text": " Also das ist der Inhalt der Aufmerksamkeit, wo haben wir wann, was, wie am besten gelernt.", "tokens": [50602, 2743, 1482, 1418, 1163, 682, 20731, 1163, 9462, 936, 28665, 9238, 11, 6020, 3084, 1987, 38064, 11, 390, 11, 3355, 669, 30930, 49224, 13, 50946], "temperature": 0.0, "avg_logprob": -0.2174265972979657, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.07802663743495941}, {"id": 290, "seek": 177696, "start": 1788.6000000000001, "end": 1797.0, "text": " Was sozusagen hat die meisten Performance Boost in unserer Performance gebracht, wenn wir an dem einen statistischen Gewicht oder an dem einen Neuronen wackeln.", "tokens": [50946, 3027, 33762, 2385, 978, 29708, 25047, 43902, 294, 20965, 25047, 40744, 11, 4797, 1987, 364, 1371, 4891, 16012, 6282, 19063, 1405, 4513, 364, 1371, 4891, 1734, 374, 32923, 261, 326, 32099, 13, 51366], "temperature": 0.0, "avg_logprob": -0.2174265972979657, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.07802663743495941}, {"id": 291, "seek": 179700, "start": 1797.84, "end": 1807.6, "text": " Und der Witz ist, das ist so eine Art Meta-Learning, weil es sozusagen eine Aufmerksamkeit \u00fcber die Aufmerksamkeit macht.", "tokens": [50406, 2719, 1163, 343, 6862, 1418, 11, 1482, 1418, 370, 3018, 5735, 6377, 64, 12, 11020, 2341, 11, 7689, 785, 33762, 3018, 9462, 936, 28665, 9238, 4502, 978, 9462, 936, 28665, 9238, 10857, 13, 50894], "temperature": 0.0, "avg_logprob": -0.13558844597108902, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.14989274740219116}, {"id": 292, "seek": 179700, "start": 1807.6, "end": 1812.04, "text": " Es speichert sozusagen den Kontext und wo die Aufmerksamkeit war.", "tokens": [50894, 2313, 768, 480, 911, 33762, 1441, 20629, 3828, 674, 6020, 978, 9462, 936, 28665, 9238, 1516, 13, 51116], "temperature": 0.0, "avg_logprob": -0.13558844597108902, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.14989274740219116}, {"id": 293, "seek": 179700, "start": 1812.04, "end": 1815.72, "text": " Und das muss es beobachten, das Gehirn. Und das tut es.", "tokens": [51116, 2719, 1482, 6425, 785, 312, 996, 20806, 11, 1482, 2876, 24118, 77, 13, 2719, 1482, 3672, 785, 13, 51300], "temperature": 0.0, "avg_logprob": -0.13558844597108902, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.14989274740219116}, {"id": 294, "seek": 179700, "start": 1815.72, "end": 1822.2, "text": " Das ist sozusagen Meta-Learning, weil Aufmerksamkeit \u00fcber die Aufmerksamkeit quasi oder der Inhalt der Aufmerksamkeit zusammen mit dem Kontext.", "tokens": [51300, 2846, 1418, 33762, 6377, 64, 12, 11020, 2341, 11, 7689, 9462, 936, 28665, 9238, 4502, 978, 9462, 936, 28665, 9238, 20954, 4513, 1163, 682, 20731, 1163, 9462, 936, 28665, 9238, 14311, 2194, 1371, 20629, 3828, 13, 51624], "temperature": 0.0, "avg_logprob": -0.13558844597108902, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.14989274740219116}, {"id": 295, "seek": 179700, "start": 1822.2, "end": 1824.08, "text": " So ungef\u00e4hr muss man sich das vorstellen.", "tokens": [51624, 407, 41285, 6425, 587, 3041, 1482, 34346, 13, 51718], "temperature": 0.0, "avg_logprob": -0.13558844597108902, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.14989274740219116}, {"id": 296, "seek": 182408, "start": 1824.08, "end": 1829.3999999999999, "text": " Und ich finde, an diesem Punkt ist sozusagen diese maximale Selbstbez\u00fcglichkeit auch klar.", "tokens": [50364, 2719, 1893, 17841, 11, 364, 10975, 25487, 1418, 33762, 6705, 5138, 1220, 29712, 650, 89, 774, 8856, 9238, 2168, 14743, 13, 50630], "temperature": 0.0, "avg_logprob": -0.19046378553959362, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.030198540538549423}, {"id": 297, "seek": 182408, "start": 1829.3999999999999, "end": 1834.3999999999999, "text": " Und das ist auch das, was die Leute so schizophren finden an der Frage, habe ich einen freien Willen oder nicht.", "tokens": [50630, 2719, 1482, 1418, 2168, 1482, 11, 390, 978, 13495, 370, 41532, 20734, 364, 1163, 13685, 11, 6015, 1893, 4891, 2130, 1053, 3099, 268, 4513, 1979, 13, 50880], "temperature": 0.0, "avg_logprob": -0.19046378553959362, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.030198540538549423}, {"id": 298, "seek": 182408, "start": 1834.3999999999999, "end": 1835.8, "text": " Oder was ist das Bewusstsein?", "tokens": [50880, 20988, 390, 1418, 1482, 40512, 26340, 33042, 30, 50950], "temperature": 0.0, "avg_logprob": -0.19046378553959362, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.030198540538549423}, {"id": 299, "seek": 182408, "start": 1835.8, "end": 1840.04, "text": " Das ist halt sozusagen, das ist das, was Douglas Hofstadter wahrscheinlich meinte, mit einem Estrange Loop.", "tokens": [50950, 2846, 1418, 12479, 33762, 11, 1482, 1418, 1482, 11, 390, 23010, 37379, 48299, 391, 30957, 385, 12401, 11, 2194, 6827, 4410, 14521, 45660, 13, 51162], "temperature": 0.0, "avg_logprob": -0.19046378553959362, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.030198540538549423}, {"id": 300, "seek": 182408, "start": 1840.04, "end": 1844.6, "text": " Und das Buch habe ich bis jetzt noch nicht gelesen, aber das steht auch auf meinem Pile of Shame.", "tokens": [51162, 2719, 1482, 25818, 6015, 1893, 7393, 4354, 3514, 1979, 4087, 17403, 11, 4340, 1482, 16361, 2168, 2501, 24171, 430, 794, 295, 46835, 13, 51390], "temperature": 0.0, "avg_logprob": -0.19046378553959362, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.030198540538549423}, {"id": 301, "seek": 184460, "start": 1844.6, "end": 1857.8, "text": " Und das H\u00f6ren muss sozusagen auch genau darauf achten, also es muss ich selbst beobachten, worauf ich jetzt genau Aufmerksamkeit lege.", "tokens": [50364, 2719, 1482, 389, 26377, 6425, 33762, 2168, 12535, 18654, 2800, 1147, 11, 611, 785, 6425, 1893, 13053, 312, 996, 20806, 11, 469, 9507, 1893, 4354, 12535, 9462, 936, 28665, 9238, 476, 432, 13, 51024], "temperature": 0.0, "avg_logprob": -0.14884725638798305, "compression_ratio": 1.5547169811320756, "no_speech_prob": 0.21171382069587708}, {"id": 302, "seek": 184460, "start": 1857.8, "end": 1859.04, "text": " Also was gucke ich mir gerade an?", "tokens": [51024, 2743, 390, 695, 18627, 1893, 3149, 12117, 364, 30, 51086], "temperature": 0.0, "avg_logprob": -0.14884725638798305, "compression_ratio": 1.5547169811320756, "no_speech_prob": 0.21171382069587708}, {"id": 303, "seek": 184460, "start": 1859.04, "end": 1863.36, "text": " Und der Punkt ist irgendwie diese maximale Selbstbez\u00fcglichkeit, die macht es halt.", "tokens": [51086, 2719, 1163, 25487, 1418, 20759, 6705, 5138, 1220, 29712, 650, 89, 774, 8856, 9238, 11, 978, 10857, 785, 12479, 13, 51302], "temperature": 0.0, "avg_logprob": -0.14884725638798305, "compression_ratio": 1.5547169811320756, "no_speech_prob": 0.21171382069587708}, {"id": 304, "seek": 184460, "start": 1863.36, "end": 1871.6399999999999, "text": " Und falls ihr das kennt, also bei mir war es so, wenn ich mal, ich hatte mal so einen Moment, ich war auf einer LAN-Party und war wirklich komplett im Tunnel.", "tokens": [51302, 2719, 8804, 5553, 1482, 37682, 11, 611, 4643, 3149, 1516, 785, 370, 11, 4797, 1893, 2806, 11, 1893, 13299, 2806, 370, 4891, 19093, 11, 1893, 1516, 2501, 6850, 37387, 12, 37012, 88, 674, 1516, 9696, 32261, 566, 21363, 6396, 13, 51716], "temperature": 0.0, "avg_logprob": -0.14884725638798305, "compression_ratio": 1.5547169811320756, "no_speech_prob": 0.21171382069587708}, {"id": 305, "seek": 187164, "start": 1871.64, "end": 1878.24, "text": " Also ich hatte das nicht oft in meinem Leben, aber manchmal war ich vollst\u00e4ndig im Tunnel, zum Beispiel beim Zocken oder auch mal beim Schlagzeugspielen.", "tokens": [50364, 2743, 1893, 13299, 1482, 1979, 11649, 294, 24171, 15399, 11, 4340, 32092, 1516, 1893, 15593, 16913, 328, 566, 21363, 6396, 11, 5919, 13772, 13922, 1176, 1560, 268, 4513, 2168, 2806, 13922, 16420, 559, 19303, 4952, 12844, 13, 50694], "temperature": 0.0, "avg_logprob": -0.11951144641598328, "compression_ratio": 1.6955223880597015, "no_speech_prob": 0.6499567627906799}, {"id": 306, "seek": 187164, "start": 1878.24, "end": 1885.96, "text": " Und wenn man einmal so vollst\u00e4ndig in diesem Modus drin ist, dass man seine eigene Aufmerksamkeit overfittet,", "tokens": [50694, 2719, 4797, 587, 11078, 370, 15593, 16913, 328, 294, 10975, 6583, 301, 24534, 1418, 11, 2658, 587, 15925, 38549, 9462, 936, 28665, 9238, 670, 69, 593, 302, 11, 51080], "temperature": 0.0, "avg_logprob": -0.11951144641598328, "compression_ratio": 1.6955223880597015, "no_speech_prob": 0.6499567627906799}, {"id": 307, "seek": 187164, "start": 1885.96, "end": 1893.8400000000001, "text": " wenn man irgendwie so krass sich selber dabei beobachtet, wie heftig man unterwegs ist, dann ist man wie bei Matrix, wie bei Neo und sieht alles in Slow-Motion.", "tokens": [51080, 4797, 587, 20759, 370, 15913, 640, 3041, 23888, 14967, 312, 996, 48833, 11, 3355, 415, 34765, 587, 36258, 1418, 11, 3594, 1418, 587, 3355, 4643, 36274, 11, 3355, 4643, 24458, 674, 14289, 7874, 294, 17703, 12, 44, 19228, 13, 51474], "temperature": 0.0, "avg_logprob": -0.11951144641598328, "compression_ratio": 1.6955223880597015, "no_speech_prob": 0.6499567627906799}, {"id": 308, "seek": 187164, "start": 1893.8400000000001, "end": 1895.76, "text": " Man ist auf einmal superschnell.", "tokens": [51474, 2458, 1418, 2501, 11078, 9331, 22300, 8903, 13, 51570], "temperature": 0.0, "avg_logprob": -0.11951144641598328, "compression_ratio": 1.6955223880597015, "no_speech_prob": 0.6499567627906799}, {"id": 309, "seek": 187164, "start": 1895.76, "end": 1900.72, "text": " Und ich hatte das, ich war normalerweise so ein Spieler, ich hatte so 250 APM vielleicht, wenn es hochkommt.", "tokens": [51570, 2719, 1893, 13299, 1482, 11, 1893, 1516, 2710, 44071, 370, 1343, 44053, 11, 1893, 13299, 370, 11650, 5372, 44, 12547, 11, 4797, 785, 19783, 74, 22230, 13, 51818], "temperature": 0.0, "avg_logprob": -0.11951144641598328, "compression_ratio": 1.6955223880597015, "no_speech_prob": 0.6499567627906799}, {"id": 310, "seek": 190072, "start": 1900.76, "end": 1904.24, "text": " Und ich hatte mal auf einer LAN-Party so 320 bis 350.", "tokens": [50366, 2719, 1893, 13299, 2806, 2501, 6850, 37387, 12, 37012, 88, 370, 42429, 7393, 18065, 13, 50540], "temperature": 0.0, "avg_logprob": -0.1189376749890916, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.027989935129880905}, {"id": 311, "seek": 190072, "start": 1904.24, "end": 1907.4, "text": " Und ich kann mich auch im Nachhinein noch erinnern, das ging runter wie Butter.", "tokens": [50540, 2719, 1893, 4028, 6031, 2168, 566, 11815, 71, 533, 259, 3514, 1189, 19166, 77, 11, 1482, 21924, 33295, 3355, 22646, 13, 50698], "temperature": 0.0, "avg_logprob": -0.1189376749890916, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.027989935129880905}, {"id": 312, "seek": 190072, "start": 1907.4, "end": 1913.84, "text": " Meine Finger, das hat sich angef\u00fchlt wie das geschmeidigste, konstante Tippen der Welt.", "tokens": [50698, 22258, 37318, 11, 1482, 2385, 3041, 43907, 7254, 2282, 3355, 1482, 13511, 1398, 327, 328, 2941, 11, 34208, 2879, 42102, 268, 1163, 14761, 13, 51020], "temperature": 0.0, "avg_logprob": -0.1189376749890916, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.027989935129880905}, {"id": 313, "seek": 190072, "start": 1913.84, "end": 1918.1200000000001, "text": " Ich konnte nie wieder diese Performance erlangen wie in diesem einen Game, was ich mal hatte in meinem Leben.", "tokens": [51020, 3141, 24058, 2838, 6216, 6705, 25047, 1189, 75, 10784, 3355, 294, 10975, 4891, 7522, 11, 390, 1893, 2806, 13299, 294, 24171, 15399, 13, 51234], "temperature": 0.0, "avg_logprob": -0.1189376749890916, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.027989935129880905}, {"id": 314, "seek": 190072, "start": 1918.1200000000001, "end": 1922.8, "text": " Und ich glaube, das war so ein Zustand, der hat sich schon fast so ein bisschen transcendent angef\u00fchlt.", "tokens": [51234, 2719, 1893, 13756, 11, 1482, 1516, 370, 1343, 46322, 474, 11, 1163, 2385, 3041, 4981, 2370, 370, 1343, 10763, 28535, 317, 43907, 7254, 2282, 13, 51468], "temperature": 0.0, "avg_logprob": -0.1189376749890916, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.027989935129880905}, {"id": 315, "seek": 190072, "start": 1922.8, "end": 1926.56, "text": " Und ich glaube, das war, wo meine Aufmerksamkeit diesen Zustand erreicht hat,", "tokens": [51468, 2719, 1893, 13756, 11, 1482, 1516, 11, 6020, 10946, 9462, 936, 28665, 9238, 12862, 46322, 474, 46250, 2385, 11, 51656], "temperature": 0.0, "avg_logprob": -0.1189376749890916, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.027989935129880905}, {"id": 316, "seek": 192656, "start": 1926.56, "end": 1933.32, "text": " dass sie sich sozusagen selber vollst\u00e4ndig beobachtet und jederzeit eingreifen kann, v\u00f6llig problemlos.", "tokens": [50364, 2658, 2804, 3041, 33762, 23888, 15593, 16913, 328, 312, 996, 48833, 674, 19610, 13712, 17002, 265, 25076, 4028, 11, 35670, 1154, 9389, 13, 50702], "temperature": 0.0, "avg_logprob": -0.12688323071128443, "compression_ratio": 1.7, "no_speech_prob": 0.17290648818016052}, {"id": 317, "seek": 192656, "start": 1933.32, "end": 1938.08, "text": " Ich glaube, so einen Zustand kann man auch durch Meditation erreichen, wenn man irgendwie, wenn man das lang genug macht.", "tokens": [50702, 3141, 13756, 11, 370, 4891, 46322, 474, 4028, 587, 2168, 7131, 3982, 4614, 39464, 11, 4797, 587, 20759, 11, 4797, 587, 1482, 2265, 33194, 10857, 13, 50940], "temperature": 0.0, "avg_logprob": -0.12688323071128443, "compression_ratio": 1.7, "no_speech_prob": 0.17290648818016052}, {"id": 318, "seek": 192656, "start": 1938.08, "end": 1943.08, "text": " Jedenfalls, das ist irgendwie krass und das merkt ihr, wenn ihr irgendwann mal, wenn ihr irgendwas habt, wo ihr euch krass konzentrieren m\u00fcsst.", "tokens": [50940, 508, 6876, 18542, 11, 1482, 1418, 20759, 15913, 640, 674, 1482, 3551, 2320, 5553, 11, 4797, 5553, 34313, 2806, 11, 4797, 5553, 47090, 23660, 11, 6020, 5553, 10403, 15913, 640, 5897, 14185, 470, 5170, 49481, 13, 51190], "temperature": 0.0, "avg_logprob": -0.12688323071128443, "compression_ratio": 1.7, "no_speech_prob": 0.17290648818016052}, {"id": 319, "seek": 192656, "start": 1943.08, "end": 1947.96, "text": " Und das genaue Gegenteil ist sowas wie mit dem Fahrrad irgendwo langfahren, zur Arbeit oder so.", "tokens": [51190, 2719, 1482, 1049, 64, 622, 27826, 1576, 388, 1418, 19766, 296, 3355, 2194, 1371, 19843, 6206, 40865, 2265, 34394, 11, 7147, 18604, 4513, 370, 13, 51434], "temperature": 0.0, "avg_logprob": -0.12688323071128443, "compression_ratio": 1.7, "no_speech_prob": 0.17290648818016052}, {"id": 320, "seek": 192656, "start": 1947.96, "end": 1952.1599999999999, "text": " Da denkt man gar nicht mehr dr\u00fcber nach, muss ich jetzt hier abbiegen, muss ich hier meinen rechten Fu\u00df nach unten dr\u00fccken,", "tokens": [51434, 3933, 38658, 587, 3691, 1979, 5417, 1224, 12670, 5168, 11, 6425, 1893, 4354, 3296, 410, 7392, 1766, 11, 6425, 1893, 3296, 22738, 319, 21043, 31419, 5168, 25693, 1224, 26037, 11, 51644], "temperature": 0.0, "avg_logprob": -0.12688323071128443, "compression_ratio": 1.7, "no_speech_prob": 0.17290648818016052}, {"id": 321, "seek": 195216, "start": 1952.16, "end": 1954.88, "text": " muss ich jetzt hier die Bremse dr\u00fccken, muss ich hier irgendwie lenken.", "tokens": [50364, 6425, 1893, 4354, 3296, 978, 363, 2579, 405, 1224, 26037, 11, 6425, 1893, 3296, 20759, 40116, 2653, 13, 50500], "temperature": 0.0, "avg_logprob": -0.1528105192546603, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.39147117733955383}, {"id": 322, "seek": 195216, "start": 1954.88, "end": 1956.92, "text": " Das ist alles Autopilot, ne?", "tokens": [50500, 2846, 1418, 7874, 6049, 404, 31516, 11, 408, 30, 50602], "temperature": 0.0, "avg_logprob": -0.1528105192546603, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.39147117733955383}, {"id": 323, "seek": 195216, "start": 1956.92, "end": 1959.8400000000001, "text": " Da ist genau das Gegenteil, keine Aufmerksamkeit mehr quasi.", "tokens": [50602, 3933, 1418, 12535, 1482, 27826, 1576, 388, 11, 9252, 9462, 936, 28665, 9238, 5417, 20954, 13, 50748], "temperature": 0.0, "avg_logprob": -0.1528105192546603, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.39147117733955383}, {"id": 324, "seek": 195216, "start": 1959.8400000000001, "end": 1966.96, "text": " Und \u00fcbrigens das wichtige, meilensteinartige Paper dazu zu den Transformers hei\u00dft Attention is all you need.", "tokens": [50748, 2719, 38215, 1482, 46276, 11, 385, 17471, 9089, 446, 3969, 24990, 13034, 2164, 1441, 27938, 433, 13139, 31858, 307, 439, 291, 643, 13, 51104], "temperature": 0.0, "avg_logprob": -0.1528105192546603, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.39147117733955383}, {"id": 325, "seek": 195216, "start": 1966.96, "end": 1968.0400000000002, "text": " Das ist schon so ein geiler Titel.", "tokens": [51104, 2846, 1418, 4981, 370, 1343, 1519, 5441, 14489, 338, 13, 51158], "temperature": 0.0, "avg_logprob": -0.1528105192546603, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.39147117733955383}, {"id": 326, "seek": 195216, "start": 1968.0400000000002, "end": 1972.0800000000002, "text": " Ich glaube, das war auch von so Boys, die bei Google in der AI-Abteilung sitzen.", "tokens": [51158, 3141, 13756, 11, 1482, 1516, 2168, 2957, 370, 21963, 11, 978, 4643, 3329, 294, 1163, 7318, 12, 23695, 30766, 1063, 44998, 13, 51360], "temperature": 0.0, "avg_logprob": -0.1528105192546603, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.39147117733955383}, {"id": 327, "seek": 195216, "start": 1972.0800000000002, "end": 1974.2, "text": " Das ist auch cool geschrieben, das kann man sich wirklich mal reinziehen.", "tokens": [51360, 2846, 1418, 2168, 1627, 47397, 11, 1482, 4028, 587, 3041, 9696, 2806, 6561, 28768, 13, 51466], "temperature": 0.0, "avg_logprob": -0.1528105192546603, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.39147117733955383}, {"id": 328, "seek": 195216, "start": 1974.2, "end": 1978.76, "text": " Also wenn ihr euch damit auskennt, wenn ihr euch damit auskennt, kennt ihr das Paper eh, was erz\u00e4hle ich hier.", "tokens": [51466, 2743, 4797, 5553, 10403, 9479, 3437, 41838, 11, 4797, 5553, 10403, 9479, 3437, 41838, 11, 37682, 5553, 1482, 24990, 7670, 11, 390, 28337, 306, 1893, 3296, 13, 51694], "temperature": 0.0, "avg_logprob": -0.1528105192546603, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.39147117733955383}, {"id": 329, "seek": 197876, "start": 1978.8, "end": 1986.68, "text": " Ich bin ja sozusagen der Laie, weil ich hab mir nur mal so einen w\u00f6chentlichen Crashkurs bei uns in der Informatikfakult\u00e4t mal reingeg\u00f6nnt dazu.", "tokens": [50366, 3141, 5171, 2784, 33762, 1163, 2369, 414, 11, 7689, 1893, 3025, 3149, 4343, 2806, 370, 4891, 261, 973, 339, 7698, 268, 31787, 74, 2156, 4643, 2693, 294, 1163, 34301, 267, 1035, 69, 514, 723, 3628, 2806, 319, 278, 1146, 3239, 580, 13034, 13, 50760], "temperature": 0.0, "avg_logprob": -0.13794578331104224, "compression_ratio": 1.6677018633540373, "no_speech_prob": 0.24477633833885193}, {"id": 330, "seek": 197876, "start": 1986.68, "end": 1991.72, "text": " Ja und da wird dann sozusagen auch klar, dass diese Sache mit dem Attention so geil ist,", "tokens": [50760, 3530, 674, 1120, 4578, 3594, 33762, 2168, 14743, 11, 2658, 6705, 31452, 2194, 1371, 31858, 370, 47165, 1418, 11, 51012], "temperature": 0.0, "avg_logprob": -0.13794578331104224, "compression_ratio": 1.6677018633540373, "no_speech_prob": 0.24477633833885193}, {"id": 331, "seek": 197876, "start": 1991.72, "end": 1998.72, "text": " weil wenn man jetzt zum Beispiel Text\u00fcbersetzung macht und man geht Brutforce von damals vor und \u00fcbersetzt Wort f\u00fcr Wort oder so,", "tokens": [51012, 7689, 4797, 587, 4354, 5919, 13772, 18643, 774, 1616, 38584, 10857, 674, 587, 7095, 1603, 325, 5156, 2957, 26067, 4245, 674, 45022, 3524, 22748, 2959, 22748, 4513, 370, 11, 51362], "temperature": 0.0, "avg_logprob": -0.13794578331104224, "compression_ratio": 1.6677018633540373, "no_speech_prob": 0.24477633833885193}, {"id": 332, "seek": 197876, "start": 1998.72, "end": 2001.4, "text": " dann geht das immer krachen, was man braucht, das Kontext.", "tokens": [51362, 3594, 7095, 1482, 5578, 15913, 11646, 11, 390, 587, 22623, 11, 1482, 20629, 3828, 13, 51496], "temperature": 0.0, "avg_logprob": -0.13794578331104224, "compression_ratio": 1.6677018633540373, "no_speech_prob": 0.24477633833885193}, {"id": 333, "seek": 197876, "start": 2001.4, "end": 2005.8, "text": " Und Attention ist richtig gut daf\u00fcr, weil Attention kann sozusagen den Finger auf ein Wort legen und sagen,", "tokens": [51496, 2719, 31858, 1418, 13129, 5228, 13747, 11, 7689, 31858, 4028, 33762, 1441, 37318, 2501, 1343, 22748, 48315, 674, 8360, 11, 51716], "temperature": 0.0, "avg_logprob": -0.13794578331104224, "compression_ratio": 1.6677018633540373, "no_speech_prob": 0.24477633833885193}, {"id": 334, "seek": 200580, "start": 2005.8, "end": 2013.04, "text": " also dieses Wort jetzt hier, zum Beispiel der Arzt oder der K\u00fcnstler, zum Beispiel Cain West.", "tokens": [50364, 611, 12113, 22748, 4354, 3296, 11, 5919, 13772, 1163, 1587, 2682, 4513, 1163, 591, 36656, 1918, 11, 5919, 13772, 383, 491, 4055, 13, 50726], "temperature": 0.0, "avg_logprob": -0.23575098580176676, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.05832129344344139}, {"id": 335, "seek": 200580, "start": 2013.04, "end": 2018.56, "text": " Sp\u00e4ter in einem anderen Satz steht dann nur noch R und dieses R muss bez\u00fcglich auf Cain West sein.", "tokens": [50726, 1738, 20230, 294, 6827, 11122, 5344, 89, 16361, 3594, 4343, 3514, 497, 674, 12113, 497, 6425, 10782, 774, 8856, 2501, 383, 491, 4055, 6195, 13, 51002], "temperature": 0.0, "avg_logprob": -0.23575098580176676, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.05832129344344139}, {"id": 336, "seek": 200580, "start": 2018.56, "end": 2022.36, "text": " Dieser Kontext, das kann Attention richtig gut lernen.", "tokens": [51002, 39609, 20629, 3828, 11, 1482, 4028, 31858, 13129, 5228, 36082, 13, 51192], "temperature": 0.0, "avg_logprob": -0.23575098580176676, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.05832129344344139}, {"id": 337, "seek": 200580, "start": 2022.36, "end": 2027.72, "text": " Und das konnten sozusagen die Learning-Strukturen vor den Transformern nicht so gut.", "tokens": [51192, 2719, 1482, 38216, 33762, 978, 15205, 12, 4520, 19977, 9873, 4245, 1441, 27938, 1248, 1979, 370, 5228, 13, 51460], "temperature": 0.0, "avg_logprob": -0.23575098580176676, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.05832129344344139}, {"id": 338, "seek": 200580, "start": 2027.72, "end": 2033.96, "text": " Und das ist sozusagen der erste Schritt in Sachen jetzt mal Attention machen und das ist halt krass.", "tokens": [51460, 2719, 1482, 1418, 33762, 1163, 20951, 33062, 294, 26074, 4354, 2806, 31858, 7069, 674, 1482, 1418, 12479, 15913, 640, 13, 51772], "temperature": 0.0, "avg_logprob": -0.23575098580176676, "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.05832129344344139}, {"id": 339, "seek": 203396, "start": 2033.96, "end": 2037.88, "text": " Der n\u00e4chste Schritt, also das ist jetzt sozusagen State of the Art, das ist das, was alle mittlerweile k\u00f6nnen", "tokens": [50364, 5618, 30661, 33062, 11, 611, 1482, 1418, 4354, 33762, 4533, 295, 264, 5735, 11, 1482, 1418, 1482, 11, 390, 5430, 41999, 6310, 50560], "temperature": 0.0, "avg_logprob": -0.18898449650517216, "compression_ratio": 1.562691131498471, "no_speech_prob": 0.0035376097075641155}, {"id": 340, "seek": 203396, "start": 2037.88, "end": 2040.28, "text": " und das ist auch das, was GPT-3 maximal krass macht.", "tokens": [50560, 674, 1482, 1418, 2168, 1482, 11, 390, 26039, 51, 12, 18, 49336, 15913, 640, 10857, 13, 50680], "temperature": 0.0, "avg_logprob": -0.18898449650517216, "compression_ratio": 1.562691131498471, "no_speech_prob": 0.0035376097075641155}, {"id": 341, "seek": 203396, "start": 2040.28, "end": 2048.44, "text": " Und man, also Ben G\u00fcrtel hat den Tag auch so was gesagt wie, naja, GPT-3 hat halt auch so unendlich viele Knoten,", "tokens": [50680, 2719, 587, 11, 611, 3964, 460, 1655, 83, 338, 2385, 1441, 11204, 2168, 370, 390, 12260, 3355, 11, 1667, 2938, 11, 26039, 51, 12, 18, 2385, 12479, 2168, 370, 517, 521, 1739, 9693, 591, 2247, 268, 11, 51088], "temperature": 0.0, "avg_logprob": -0.18898449650517216, "compression_ratio": 1.562691131498471, "no_speech_prob": 0.0035376097075641155}, {"id": 342, "seek": 203396, "start": 2048.44, "end": 2050.8, "text": " dass wahrscheinlich auch Overfitting schon dabei ist.", "tokens": [51088, 2658, 30957, 2168, 4886, 69, 2414, 4981, 14967, 1418, 13, 51206], "temperature": 0.0, "avg_logprob": -0.18898449650517216, "compression_ratio": 1.562691131498471, "no_speech_prob": 0.0035376097075641155}, {"id": 343, "seek": 203396, "start": 2050.8, "end": 2056.0, "text": " Das ist einfach nur ein v\u00f6llig \u00fcbertrieben gro\u00dfes neuronales Netzwerk, was das komplette Internet gelernt hat einmal", "tokens": [51206, 2846, 1418, 7281, 4343, 1343, 35670, 3304, 4290, 24027, 48875, 34090, 4229, 38889, 26833, 11, 390, 1482, 24526, 3007, 7703, 49224, 2385, 11078, 51466], "temperature": 0.0, "avg_logprob": -0.18898449650517216, "compression_ratio": 1.562691131498471, "no_speech_prob": 0.0035376097075641155}, {"id": 344, "seek": 203396, "start": 2056.0, "end": 2059.12, "text": " und deswegen mit dir reden kann wie ein Mensch, kurz mal.", "tokens": [51466, 674, 26482, 2194, 4746, 26447, 4028, 3355, 1343, 27773, 11, 20465, 2806, 13, 51622], "temperature": 0.0, "avg_logprob": -0.18898449650517216, "compression_ratio": 1.562691131498471, "no_speech_prob": 0.0035376097075641155}, {"id": 345, "seek": 205912, "start": 2059.16, "end": 2065.0, "text": " Das habe ich in meinem Live-Talk dann \u00fcber K\u00fcnstliche Intelligenz auch schon mal gezeigt, so ein Beispiel.", "tokens": [50366, 2846, 6015, 1893, 294, 24171, 10385, 12, 33210, 3594, 4502, 591, 36656, 10185, 18762, 3213, 89, 2168, 4981, 2806, 48661, 11, 370, 1343, 13772, 13, 50658], "temperature": 0.0, "avg_logprob": -0.1498766581217448, "compression_ratio": 1.5597269624573378, "no_speech_prob": 0.16223914921283722}, {"id": 346, "seek": 205912, "start": 2065.0, "end": 2071.08, "text": " Und jetzt ist es halt so, die normalen Transformer-Networks, die haben halt einen begrenzten Arbeitsspeicher,", "tokens": [50658, 2719, 4354, 1418, 785, 12479, 370, 11, 978, 2710, 268, 27938, 260, 12, 31890, 18357, 11, 978, 3084, 12479, 4891, 4612, 1095, 89, 1147, 23262, 7053, 14934, 11, 50962], "temperature": 0.0, "avg_logprob": -0.1498766581217448, "compression_ratio": 1.5597269624573378, "no_speech_prob": 0.16223914921283722}, {"id": 347, "seek": 205912, "start": 2071.08, "end": 2075.12, "text": " das hei\u00dft, sie kriegen auch nur einen begrenzten Text in sich rein.", "tokens": [50962, 1482, 13139, 11, 2804, 46882, 2168, 4343, 4891, 4612, 1095, 89, 1147, 18643, 294, 3041, 6561, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1498766581217448, "compression_ratio": 1.5597269624573378, "no_speech_prob": 0.16223914921283722}, {"id": 348, "seek": 205912, "start": 2075.12, "end": 2080.7999999999997, "text": " Was wir aber machen ist, wir haben sozusagen eine besseren Komprimierungsalgorithmus", "tokens": [51164, 3027, 1987, 4340, 7069, 1418, 11, 1987, 3084, 33762, 3018, 42410, 5170, 14286, 1424, 332, 40908, 20422, 6819, 18761, 51448], "temperature": 0.0, "avg_logprob": -0.1498766581217448, "compression_ratio": 1.5597269624573378, "no_speech_prob": 0.16223914921283722}, {"id": 349, "seek": 205912, "start": 2080.7999999999997, "end": 2086.6, "text": " und k\u00f6nnen deswegen viel mehr Daten in Attention, also in Kontext zueinandersetzen.", "tokens": [51448, 674, 6310, 26482, 5891, 5417, 31126, 294, 31858, 11, 611, 294, 20629, 3828, 710, 622, 259, 41430, 24797, 13, 51738], "temperature": 0.0, "avg_logprob": -0.1498766581217448, "compression_ratio": 1.5597269624573378, "no_speech_prob": 0.16223914921283722}, {"id": 350, "seek": 208660, "start": 2086.64, "end": 2092.08, "text": " Und das, was jetzt sozusagen Cain West und er und sozusagen der Kontext f\u00fcr so ein Deep Learning Netzwerk ist,", "tokens": [50366, 2719, 1482, 11, 390, 4354, 33762, 383, 491, 4055, 674, 1189, 674, 33762, 1163, 20629, 3828, 2959, 370, 1343, 14895, 15205, 38889, 26833, 1418, 11, 50638], "temperature": 0.0, "avg_logprob": -0.14413538987074442, "compression_ratio": 1.6305084745762712, "no_speech_prob": 0.013633723370730877}, {"id": 351, "seek": 208660, "start": 2092.08, "end": 2094.2799999999997, "text": " ist f\u00fcr uns das gesamte Universum.", "tokens": [50638, 1418, 2959, 2693, 1482, 39746, 975, 14052, 449, 13, 50748], "temperature": 0.0, "avg_logprob": -0.14413538987074442, "compression_ratio": 1.6305084745762712, "no_speech_prob": 0.013633723370730877}, {"id": 352, "seek": 208660, "start": 2094.2799999999997, "end": 2098.52, "text": " Das hei\u00dft erst, wenn wir auf diesem Level sind, quasi ein Level h\u00f6her in der Attention,", "tokens": [50748, 2846, 13139, 11301, 11, 4797, 1987, 2501, 10975, 16872, 3290, 11, 20954, 1343, 16872, 48045, 294, 1163, 31858, 11, 50960], "temperature": 0.0, "avg_logprob": -0.14413538987074442, "compression_ratio": 1.6305084745762712, "no_speech_prob": 0.013633723370730877}, {"id": 353, "seek": 208660, "start": 2098.52, "end": 2103.0, "text": " dann k\u00f6nnen wir auch so was wie k\u00fcnstliches Bewusstsein konstruieren.", "tokens": [50960, 3594, 6310, 1987, 2168, 370, 390, 3355, 350, 36656, 45502, 40512, 26340, 33042, 34208, 894, 5695, 13, 51184], "temperature": 0.0, "avg_logprob": -0.14413538987074442, "compression_ratio": 1.6305084745762712, "no_speech_prob": 0.013633723370730877}, {"id": 354, "seek": 208660, "start": 2103.0, "end": 2106.2799999999997, "text": " Also das ist zumindest das, was Joshua Bach sagt und was soll ich sagen,", "tokens": [51184, 2743, 1482, 1418, 38082, 1482, 11, 390, 24005, 30920, 15764, 674, 390, 7114, 1893, 8360, 11, 51348], "temperature": 0.0, "avg_logprob": -0.14413538987074442, "compression_ratio": 1.6305084745762712, "no_speech_prob": 0.013633723370730877}, {"id": 355, "seek": 208660, "start": 2106.2799999999997, "end": 2111.24, "text": " alles, was Joshua Bach sagt, klingt extrem plausibel und hier muss ich auf jeden Fall eingestehen,", "tokens": [51348, 7874, 11, 390, 24005, 30920, 15764, 11, 350, 45219, 4040, 34946, 44685, 674, 3296, 6425, 1893, 2501, 12906, 7465, 17002, 8887, 2932, 11, 51596], "temperature": 0.0, "avg_logprob": -0.14413538987074442, "compression_ratio": 1.6305084745762712, "no_speech_prob": 0.013633723370730877}, {"id": 356, "seek": 211124, "start": 2111.24, "end": 2117.24, "text": " dass ich nicht so krass bin wie Stanislav Lemb und ich finde im Prinzip fast alles, was Joshua Bach sagt,", "tokens": [50364, 2658, 1893, 1979, 370, 15913, 640, 5171, 3355, 10061, 5788, 706, 16905, 65, 674, 1893, 17841, 566, 47572, 2370, 7874, 11, 390, 24005, 30920, 15764, 11, 50664], "temperature": 0.0, "avg_logprob": -0.2196669578552246, "compression_ratio": 1.6219931271477663, "no_speech_prob": 0.08749188482761383}, {"id": 357, "seek": 211124, "start": 2117.24, "end": 2120.2, "text": " ist des Todes legit des Todes.", "tokens": [50664, 1418, 730, 2465, 279, 10275, 730, 2465, 279, 13, 50812], "temperature": 0.0, "avg_logprob": -0.2196669578552246, "compression_ratio": 1.6219931271477663, "no_speech_prob": 0.08749188482761383}, {"id": 358, "seek": 211124, "start": 2120.2, "end": 2122.7599999999998, "text": " Der Typ ist einfach nur zu geil.", "tokens": [50812, 5618, 17722, 1418, 7281, 4343, 2164, 47165, 13, 50940], "temperature": 0.0, "avg_logprob": -0.2196669578552246, "compression_ratio": 1.6219931271477663, "no_speech_prob": 0.08749188482761383}, {"id": 359, "seek": 211124, "start": 2122.7599999999998, "end": 2128.52, "text": " Und auf der Ebene der Neuronen \u00fcbrigens kann man das Bewusstsein auch noch mal genauso erkl\u00e4ren,", "tokens": [50940, 2719, 2501, 1163, 20418, 1450, 1163, 1734, 374, 32923, 38215, 4028, 587, 1482, 40512, 26340, 33042, 2168, 3514, 2806, 37694, 46528, 11, 51228], "temperature": 0.0, "avg_logprob": -0.2196669578552246, "compression_ratio": 1.6219931271477663, "no_speech_prob": 0.08749188482761383}, {"id": 360, "seek": 211124, "start": 2128.52, "end": 2133.8799999999997, "text": " n\u00e4mlich mit Emergenz, da kann man einfach sagen, naja, so ein Neuron muss jetzt sozusagen eine Funktion lernen,", "tokens": [51228, 21219, 2194, 18477, 1766, 89, 11, 1120, 4028, 587, 7281, 8360, 11, 1667, 2938, 11, 370, 1343, 1734, 374, 266, 6425, 4354, 33762, 3018, 11166, 9780, 36082, 11, 51496], "temperature": 0.0, "avg_logprob": -0.2196669578552246, "compression_ratio": 1.6219931271477663, "no_speech_prob": 0.08749188482761383}, {"id": 361, "seek": 211124, "start": 2133.8799999999997, "end": 2135.12, "text": " wann es feuert.", "tokens": [51496, 38064, 785, 29539, 911, 13, 51558], "temperature": 0.0, "avg_logprob": -0.2196669578552246, "compression_ratio": 1.6219931271477663, "no_speech_prob": 0.08749188482761383}, {"id": 362, "seek": 211124, "start": 2135.12, "end": 2138.8399999999997, "text": " Ein Neuron hat sozusagen, kann eine nicht-linearer Funktion approximieren.", "tokens": [51558, 6391, 1734, 374, 266, 2385, 33762, 11, 4028, 3018, 1979, 12, 28263, 260, 11166, 9780, 8542, 5695, 13, 51744], "temperature": 0.0, "avg_logprob": -0.2196669578552246, "compression_ratio": 1.6219931271477663, "no_speech_prob": 0.08749188482761383}, {"id": 363, "seek": 213884, "start": 2138.84, "end": 2143.0, "text": " Es hat ja ein multidimensionales Input, die ganzen Daten, die reinkommen.", "tokens": [50364, 2313, 2385, 2784, 1343, 2120, 327, 332, 11075, 279, 682, 2582, 11, 978, 23966, 31126, 11, 978, 319, 475, 5132, 13, 50572], "temperature": 0.0, "avg_logprob": -0.10929248332977295, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.22233395278453827}, {"id": 364, "seek": 213884, "start": 2143.0, "end": 2145.1600000000003, "text": " Wann feuert es? Das ist der Output.", "tokens": [50572, 343, 969, 29539, 911, 785, 30, 2846, 1418, 1163, 5925, 2582, 13, 50680], "temperature": 0.0, "avg_logprob": -0.10929248332977295, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.22233395278453827}, {"id": 365, "seek": 213884, "start": 2145.1600000000003, "end": 2149.76, "text": " Und das Neuron muss jetzt sozusagen das Output an das Universum liefern.", "tokens": [50680, 2719, 1482, 1734, 374, 266, 6425, 4354, 33762, 1482, 5925, 2582, 364, 1482, 14052, 449, 4544, 28958, 13, 50910], "temperature": 0.0, "avg_logprob": -0.10929248332977295, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.22233395278453827}, {"id": 366, "seek": 213884, "start": 2149.76, "end": 2153.36, "text": " Also es hat ja nur diesen Ausgang, es wei\u00df ja nichts von der \u00e4u\u00dferen Welt.", "tokens": [50910, 2743, 785, 2385, 2784, 4343, 12862, 9039, 19619, 11, 785, 13385, 2784, 13004, 2957, 1163, 3078, 43796, 5170, 14761, 13, 51090], "temperature": 0.0, "avg_logprob": -0.10929248332977295, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.22233395278453827}, {"id": 367, "seek": 213884, "start": 2153.36, "end": 2157.28, "text": " Irgendwann kommt wieder Input rein und dieser Input f\u00fcttert das Neuron ja.", "tokens": [51090, 9151, 9395, 86, 969, 10047, 6216, 682, 2582, 6561, 674, 9053, 682, 2582, 283, 7695, 391, 83, 1482, 1734, 374, 266, 2784, 13, 51286], "temperature": 0.0, "avg_logprob": -0.10929248332977295, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.22233395278453827}, {"id": 368, "seek": 213884, "start": 2157.28, "end": 2160.56, "text": " Wenn kein Input mehr kommt, dann stirbt das Neuron ja ab.", "tokens": [51286, 7899, 13424, 682, 2582, 5417, 10047, 11, 3594, 8946, 4517, 1482, 1734, 374, 266, 2784, 410, 13, 51450], "temperature": 0.0, "avg_logprob": -0.10929248332977295, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.22233395278453827}, {"id": 369, "seek": 213884, "start": 2160.56, "end": 2165.32, "text": " Das hei\u00dft, Neuronen haben ein Interesse daran, diese Funktion sehr gut zu approximieren,", "tokens": [51450, 2846, 13139, 11, 1734, 374, 32923, 3084, 1343, 5751, 7357, 24520, 11, 6705, 11166, 9780, 5499, 5228, 2164, 8542, 5695, 11, 51688], "temperature": 0.0, "avg_logprob": -0.10929248332977295, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.22233395278453827}, {"id": 370, "seek": 213884, "start": 2165.32, "end": 2168.0, "text": " um lange zu leben und fetter zu werden.", "tokens": [51688, 1105, 18131, 2164, 26392, 674, 15136, 391, 2164, 4604, 13, 51822], "temperature": 0.0, "avg_logprob": -0.10929248332977295, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.22233395278453827}, {"id": 371, "seek": 216800, "start": 2168.0, "end": 2169.76, "text": " Und das ist genau das, was wir haben.", "tokens": [50364, 2719, 1482, 1418, 12535, 1482, 11, 390, 1987, 3084, 13, 50452], "temperature": 0.0, "avg_logprob": -0.20020965638199473, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0031719948165118694}, {"id": 372, "seek": 216800, "start": 2169.76, "end": 2172.4, "text": " Und in unserem Hirn gibt es sozusagen diese goldene Regel,", "tokens": [50452, 2719, 294, 26792, 23192, 77, 6089, 785, 33762, 6705, 3821, 1450, 33139, 11, 50584], "temperature": 0.0, "avg_logprob": -0.20020965638199473, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0031719948165118694}, {"id": 373, "seek": 216800, "start": 2172.4, "end": 2174.84, "text": " Wires Together, Fires Together oder so.", "tokens": [50584, 343, 3145, 15911, 11, 479, 3145, 15911, 4513, 370, 13, 50706], "temperature": 0.0, "avg_logprob": -0.20020965638199473, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0031719948165118694}, {"id": 374, "seek": 216800, "start": 2174.84, "end": 2179.56, "text": " Das ist so eine Grundregel, die wird um Deep Learning sozusagen, diese Intuition wird da auch verwendet,", "tokens": [50706, 2846, 1418, 370, 3018, 13941, 3375, 338, 11, 978, 4578, 1105, 14895, 15205, 33762, 11, 6705, 5681, 19080, 4578, 1120, 2168, 1306, 20128, 302, 11, 50942], "temperature": 0.0, "avg_logprob": -0.20020965638199473, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0031719948165118694}, {"id": 375, "seek": 216800, "start": 2179.56, "end": 2186.36, "text": " weil sozusagen die ganze Struktur der neuronalen Netze auf dieser Analogie funktionieren,", "tokens": [50942, 7689, 33762, 978, 18898, 745, 31543, 1163, 12087, 21523, 268, 6188, 1381, 2501, 9053, 16128, 39031, 20454, 5695, 11, 51282], "temperature": 0.0, "avg_logprob": -0.20020965638199473, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0031719948165118694}, {"id": 376, "seek": 216800, "start": 2186.36, "end": 2190.16, "text": " von den Axionen und den Neuronen und so.", "tokens": [51282, 2957, 1441, 20118, 17068, 674, 1441, 1734, 374, 32923, 674, 370, 13, 51472], "temperature": 0.0, "avg_logprob": -0.20020965638199473, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0031719948165118694}, {"id": 377, "seek": 216800, "start": 2190.16, "end": 2196.72, "text": " Dem Schei\u00df, den wir sozusagen auf der biologischen Ebene schon sehen k\u00f6nnen,", "tokens": [51472, 4686, 25321, 6230, 11, 1441, 1987, 33762, 2501, 1163, 3228, 1132, 6282, 20418, 1450, 4981, 11333, 6310, 11, 51800], "temperature": 0.0, "avg_logprob": -0.20020965638199473, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0031719948165118694}, {"id": 378, "seek": 219672, "start": 2196.72, "end": 2198.9199999999996, "text": " wie unser Gehirn im Prinzip funktionieren muss.", "tokens": [50364, 3355, 12977, 2876, 24118, 77, 566, 47572, 20454, 5695, 6425, 13, 50474], "temperature": 0.0, "avg_logprob": -0.20037841796875, "compression_ratio": 1.6015325670498084, "no_speech_prob": 0.013016140088438988}, {"id": 379, "seek": 219672, "start": 2198.9199999999996, "end": 2203.6, "text": " Wir sehen sozusagen nur diese unendlich komplizierten Netzwerke und checken nicht, wie die gehen, so ungef\u00e4hr.", "tokens": [50474, 4347, 11333, 33762, 4343, 6705, 517, 521, 1739, 24526, 590, 29632, 38889, 1554, 330, 674, 1520, 268, 1979, 11, 3355, 978, 13230, 11, 370, 41285, 13, 50708], "temperature": 0.0, "avg_logprob": -0.20037841796875, "compression_ratio": 1.6015325670498084, "no_speech_prob": 0.013016140088438988}, {"id": 380, "seek": 219672, "start": 2208.68, "end": 2214.0, "text": " So und nur die Neuronen, die das richtig machen, die \u00fcberleben und die anderen sterben halt aus,", "tokens": [50962, 407, 674, 4343, 978, 1734, 374, 32923, 11, 978, 1482, 13129, 7069, 11, 978, 4502, 41467, 674, 978, 11122, 18924, 1799, 12479, 3437, 11, 51228], "temperature": 0.0, "avg_logprob": -0.20037841796875, "compression_ratio": 1.6015325670498084, "no_speech_prob": 0.013016140088438988}, {"id": 381, "seek": 219672, "start": 2214.0, "end": 2218.2, "text": " die das nicht machen, also da ist sozusagen wieder evolution\u00e4rer Selektionsdruck.", "tokens": [51228, 978, 1482, 1979, 7069, 11, 611, 1120, 1418, 33762, 6216, 9303, 2713, 260, 1100, 306, 2320, 626, 67, 8161, 13, 51438], "temperature": 0.0, "avg_logprob": -0.20037841796875, "compression_ratio": 1.6015325670498084, "no_speech_prob": 0.013016140088438988}, {"id": 382, "seek": 219672, "start": 2218.2, "end": 2221.72, "text": " Und Joshua Bach sagt halt jetzt, und das finde ich auch schon wieder so geil,", "tokens": [51438, 2719, 24005, 30920, 15764, 12479, 4354, 11, 674, 1482, 17841, 1893, 2168, 4981, 6216, 370, 47165, 11, 51614], "temperature": 0.0, "avg_logprob": -0.20037841796875, "compression_ratio": 1.6015325670498084, "no_speech_prob": 0.013016140088438988}, {"id": 383, "seek": 222172, "start": 2221.72, "end": 2227.6, "text": " ein Neuron ist halt einfach nur ein normaler kleiner Reinforcement Learning Agent.", "tokens": [50364, 1343, 1734, 374, 266, 1418, 12479, 7281, 4343, 1343, 2710, 260, 39496, 42116, 9382, 15205, 27174, 13, 50658], "temperature": 0.0, "avg_logprob": -0.1377288008158186, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.25910040736198425}, {"id": 384, "seek": 222172, "start": 2227.6, "end": 2236.3199999999997, "text": " Das ist ein kleines Reinforcement Agent Konzept, der sozusagen sich selbst immer verbessert.", "tokens": [50658, 2846, 1418, 1343, 9318, 1652, 42116, 9382, 27174, 12718, 32082, 11, 1163, 33762, 3041, 13053, 5578, 49112, 911, 13, 51094], "temperature": 0.0, "avg_logprob": -0.1377288008158186, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.25910040736198425}, {"id": 385, "seek": 222172, "start": 2236.3199999999997, "end": 2240.16, "text": " Und ja, es sendet halt einfach diese Signale in das Universum, also nach drau\u00dfen.", "tokens": [51094, 2719, 2784, 11, 785, 2845, 302, 12479, 7281, 6705, 13515, 1220, 294, 1482, 14052, 449, 11, 611, 5168, 44602, 13, 51286], "temperature": 0.0, "avg_logprob": -0.1377288008158186, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.25910040736198425}, {"id": 386, "seek": 222172, "start": 2240.16, "end": 2242.2, "text": " Das ist zum Beispiel das, was wir hier machen.", "tokens": [51286, 2846, 1418, 5919, 13772, 1482, 11, 390, 1987, 3296, 7069, 13, 51388], "temperature": 0.0, "avg_logprob": -0.1377288008158186, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.25910040736198425}, {"id": 387, "seek": 222172, "start": 2242.2, "end": 2246.2799999999997, "text": " Das beeinflusst unsere Handlung und den Willen und den freien Willen und diesen ganzen Schei\u00df,", "tokens": [51388, 2846, 17479, 19920, 3063, 372, 14339, 8854, 17850, 674, 1441, 3099, 268, 674, 1441, 2130, 1053, 3099, 268, 674, 12862, 23966, 25321, 6230, 11, 51592], "temperature": 0.0, "avg_logprob": -0.1377288008158186, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.25910040736198425}, {"id": 388, "seek": 222172, "start": 2246.2799999999997, "end": 2248.3599999999997, "text": " nur um eine positive Antwort zu bekommen.", "tokens": [51592, 4343, 1105, 3018, 3353, 34693, 2164, 19256, 13, 51696], "temperature": 0.0, "avg_logprob": -0.1377288008158186, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.25910040736198425}, {"id": 389, "seek": 224836, "start": 2248.36, "end": 2253.1200000000003, "text": " Und das Konglomerat von den ganzen Neuronen, die wir halt im Kopf haben, ist halt so angelegt,", "tokens": [50364, 2719, 1482, 9832, 75, 14301, 267, 2957, 1441, 23966, 1734, 374, 32923, 11, 978, 1987, 12479, 566, 28231, 3084, 11, 1418, 12479, 370, 15495, 22745, 11, 50602], "temperature": 0.0, "avg_logprob": -0.14835008893694196, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.01770154759287834}, {"id": 390, "seek": 224836, "start": 2253.1200000000003, "end": 2257.6, "text": " von der Architektur her auch, aber auch von r\u00e4umlich getrennten Kostenfunktionen,", "tokens": [50602, 2957, 1163, 10984, 642, 2320, 374, 720, 2168, 11, 4340, 2168, 2957, 39442, 449, 1739, 483, 1095, 14970, 47391, 15930, 9780, 268, 11, 50826], "temperature": 0.0, "avg_logprob": -0.14835008893694196, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.01770154759287834}, {"id": 391, "seek": 224836, "start": 2257.6, "end": 2264.6400000000003, "text": " also bestimmte Hirnareale machen bestimmte Sachen, dass es robust w\u00e4chst in der meisten,", "tokens": [50826, 611, 35180, 975, 23192, 77, 543, 1220, 7069, 35180, 975, 26074, 11, 2658, 785, 13956, 261, 28679, 294, 1163, 29708, 11, 51178], "temperature": 0.0, "avg_logprob": -0.14835008893694196, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.01770154759287834}, {"id": 392, "seek": 224836, "start": 2264.6400000000003, "end": 2266.6400000000003, "text": " bei fast allen Menschen, die geboren werden.", "tokens": [51178, 4643, 2370, 18440, 8397, 11, 978, 21125, 10948, 4604, 13, 51278], "temperature": 0.0, "avg_logprob": -0.14835008893694196, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.01770154759287834}, {"id": 393, "seek": 224836, "start": 2266.6400000000003, "end": 2269.36, "text": " Und die Gesamtheit davon nennen wir halt Gehirn.", "tokens": [51278, 2719, 978, 6761, 335, 3322, 270, 18574, 297, 16043, 1987, 12479, 2876, 24118, 77, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14835008893694196, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.01770154759287834}, {"id": 394, "seek": 224836, "start": 2269.36, "end": 2270.76, "text": " Das ist das.", "tokens": [51414, 2846, 1418, 1482, 13, 51484], "temperature": 0.0, "avg_logprob": -0.14835008893694196, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.01770154759287834}, {"id": 395, "seek": 224836, "start": 2270.76, "end": 2272.6, "text": " Und das ist krass.", "tokens": [51484, 2719, 1482, 1418, 15913, 640, 13, 51576], "temperature": 0.0, "avg_logprob": -0.14835008893694196, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.01770154759287834}, {"id": 396, "seek": 224836, "start": 2272.6, "end": 2276.2000000000003, "text": " Das ist wirklich erstaunlich, dass die Evolution das herausgebracht hat.", "tokens": [51576, 2846, 1418, 9696, 1189, 9140, 409, 1739, 11, 2658, 978, 40800, 1482, 25089, 432, 23404, 2385, 13, 51756], "temperature": 0.0, "avg_logprob": -0.14835008893694196, "compression_ratio": 1.6583629893238434, "no_speech_prob": 0.01770154759287834}, {"id": 397, "seek": 227620, "start": 2276.24, "end": 2280.04, "text": " Aber der Gehirn, habe ich ja auch glaube ich schon gesagt in dem anderen Video,", "tokens": [50366, 5992, 1163, 2876, 24118, 77, 11, 6015, 1893, 2784, 2168, 13756, 1893, 4981, 12260, 294, 1371, 11122, 9777, 11, 50556], "temperature": 0.0, "avg_logprob": -0.19628136140063293, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.23062030971050262}, {"id": 398, "seek": 227620, "start": 2280.04, "end": 2282.96, "text": " was f\u00fcr krasse Spr\u00fcnge da dazu geh\u00f6ren.", "tokens": [50556, 390, 2959, 350, 3906, 405, 7702, 3412, 432, 1120, 13034, 13218, 26377, 13, 50702], "temperature": 0.0, "avg_logprob": -0.19628136140063293, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.23062030971050262}, {"id": 399, "seek": 227620, "start": 2282.96, "end": 2288.3199999999997, "text": " Weil es ein super schlechter Trade-off war, f\u00fcr so eine Spezies wie den Menschenaffen", "tokens": [50702, 18665, 785, 1343, 1687, 22664, 26690, 23923, 12, 4506, 1516, 11, 2959, 370, 3018, 3550, 89, 530, 3355, 1441, 8397, 19182, 50970], "temperature": 0.0, "avg_logprob": -0.19628136140063293, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.23062030971050262}, {"id": 400, "seek": 227620, "start": 2288.3199999999997, "end": 2293.56, "text": " mehr in seinen Gehirn zu investieren und daf\u00fcr mehr Ressourcen irgendwo anders einzub\u00fc\u00dfen.", "tokens": [50970, 5417, 294, 24427, 2876, 24118, 77, 2164, 1963, 5695, 674, 13747, 5417, 497, 442, 396, 13037, 40865, 17999, 21586, 836, 774, 8989, 13, 51232], "temperature": 0.0, "avg_logprob": -0.19628136140063293, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.23062030971050262}, {"id": 401, "seek": 227620, "start": 2293.56, "end": 2296.64, "text": " Zum Beispiel war er nicht mehr so stark.", "tokens": [51232, 23906, 13772, 1516, 1189, 1979, 5417, 370, 17417, 13, 51386], "temperature": 0.0, "avg_logprob": -0.19628136140063293, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.23062030971050262}, {"id": 402, "seek": 227620, "start": 2296.64, "end": 2301.56, "text": " Das musste sozusagen in einer Zeit der Erbgeschichte auch entstehen, wo das gerade so ging,", "tokens": [51386, 2846, 34497, 33762, 294, 6850, 9394, 1163, 3300, 65, 23378, 18972, 2168, 35955, 2932, 11, 6020, 1482, 12117, 370, 21924, 11, 51632], "temperature": 0.0, "avg_logprob": -0.19628136140063293, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.23062030971050262}, {"id": 403, "seek": 227620, "start": 2301.56, "end": 2305.3599999999997, "text": " damit wir \u00fcber diesen Potenzialwald r\u00fcberkommen", "tokens": [51632, 9479, 1987, 4502, 12862, 9145, 11368, 831, 33262, 367, 12670, 13675, 51822], "temperature": 0.0, "avg_logprob": -0.19628136140063293, "compression_ratio": 1.542857142857143, "no_speech_prob": 0.23062030971050262}, {"id": 404, "seek": 230536, "start": 2305.36, "end": 2309.08, "text": " und dann uns als Menschen \u00fcberhaupt als Homo sapiens etablieren konnten.", "tokens": [50364, 674, 3594, 2693, 3907, 8397, 20023, 3907, 389, 13395, 18985, 24594, 1030, 455, 2753, 268, 38216, 13, 50550], "temperature": 0.0, "avg_logprob": -0.17482416680518617, "compression_ratio": 1.625, "no_speech_prob": 0.003649590304121375}, {"id": 405, "seek": 230536, "start": 2309.08, "end": 2312.6400000000003, "text": " Das war ein super Heft, da sind mehrere Sachen zusammengefallen,", "tokens": [50550, 2846, 1516, 1343, 1687, 634, 844, 11, 1120, 3290, 44677, 26074, 14311, 432, 24425, 11, 50728], "temperature": 0.0, "avg_logprob": -0.17482416680518617, "compression_ratio": 1.625, "no_speech_prob": 0.003649590304121375}, {"id": 406, "seek": 230536, "start": 2312.6400000000003, "end": 2317.2400000000002, "text": " dass das durch Selektionsdruck dann passieren konnte und der Mensch daraus entstehen konnte.", "tokens": [50728, 2658, 1482, 7131, 1100, 306, 2320, 626, 67, 8161, 3594, 46223, 24058, 674, 1163, 27773, 274, 46483, 35955, 2932, 24058, 13, 50958], "temperature": 0.0, "avg_logprob": -0.17482416680518617, "compression_ratio": 1.625, "no_speech_prob": 0.003649590304121375}, {"id": 407, "seek": 230536, "start": 2317.2400000000002, "end": 2322.2400000000002, "text": " Weil sowas wie Augen zum Beispiel, das wurde glaube ich drei, viermal separat voneinander", "tokens": [50958, 18665, 19766, 296, 3355, 29692, 5919, 13772, 11, 1482, 11191, 13756, 1893, 16809, 11, 17634, 5579, 3128, 267, 371, 546, 20553, 51208], "temperature": 0.0, "avg_logprob": -0.17482416680518617, "compression_ratio": 1.625, "no_speech_prob": 0.003649590304121375}, {"id": 408, "seek": 230536, "start": 2322.2400000000002, "end": 2325.84, "text": " in der Evolution festgestellt, dass sich Augen gebildet haben.", "tokens": [51208, 294, 1163, 40800, 6633, 26293, 11, 2658, 3041, 29692, 1519, 16248, 302, 3084, 13, 51388], "temperature": 0.0, "avg_logprob": -0.17482416680518617, "compression_ratio": 1.625, "no_speech_prob": 0.003649590304121375}, {"id": 409, "seek": 230536, "start": 2325.84, "end": 2327.0, "text": " Zuf\u00e4llig.", "tokens": [51388, 1176, 2947, 10053, 328, 13, 51446], "temperature": 0.0, "avg_logprob": -0.17482416680518617, "compression_ratio": 1.625, "no_speech_prob": 0.003649590304121375}, {"id": 410, "seek": 230536, "start": 2327.0, "end": 2328.8, "text": " Durch Evolutionsdruck.", "tokens": [51446, 28557, 5689, 15892, 67, 8161, 13, 51536], "temperature": 0.0, "avg_logprob": -0.17482416680518617, "compression_ratio": 1.625, "no_speech_prob": 0.003649590304121375}, {"id": 411, "seek": 230536, "start": 2328.8, "end": 2333.2000000000003, "text": " Das ist zum Beispiel was, was man \u00f6fter beobachtet, aber so ein Shit wie krasses Gehirn", "tokens": [51536, 2846, 1418, 5919, 13772, 390, 11, 390, 587, 4044, 828, 312, 996, 48833, 11, 4340, 370, 1343, 19593, 3355, 15913, 26615, 2876, 24118, 77, 51756], "temperature": 0.0, "avg_logprob": -0.17482416680518617, "compression_ratio": 1.625, "no_speech_prob": 0.003649590304121375}, {"id": 412, "seek": 233320, "start": 2333.2, "end": 2339.4399999999996, "text": " und daf\u00fcr nat\u00fcrlich weniger Robustheit, k\u00f6rperliche, physische Robustheit, das ist selten.", "tokens": [50364, 674, 13747, 8762, 23224, 5424, 381, 8480, 11, 350, 25533, 10185, 11, 2529, 7864, 5424, 381, 8480, 11, 1482, 1418, 5851, 1147, 13, 50676], "temperature": 0.0, "avg_logprob": -0.20582499466543122, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.3035786747932434}, {"id": 413, "seek": 233320, "start": 2339.4399999999996, "end": 2342.68, "text": " Und ja, was soll ich dazu sagen?", "tokens": [50676, 2719, 2784, 11, 390, 7114, 1893, 13034, 8360, 30, 50838], "temperature": 0.0, "avg_logprob": -0.20582499466543122, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.3035786747932434}, {"id": 414, "seek": 233320, "start": 2342.68, "end": 2347.3999999999996, "text": " Die emergente Struktur von dem, was ich sozusagen gerade als Gehirn beschrieben habe,", "tokens": [50838, 3229, 4345, 70, 1576, 745, 31543, 2957, 1371, 11, 390, 1893, 33762, 12117, 3907, 2876, 24118, 77, 17498, 24027, 6015, 11, 51074], "temperature": 0.0, "avg_logprob": -0.20582499466543122, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.3035786747932434}, {"id": 415, "seek": 233320, "start": 2347.3999999999996, "end": 2348.96, "text": " das ist das Bewusstsein.", "tokens": [51074, 1482, 1418, 1482, 40512, 26340, 33042, 13, 51152], "temperature": 0.0, "avg_logprob": -0.20582499466543122, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.3035786747932434}, {"id": 416, "seek": 233320, "start": 2348.96, "end": 2350.3999999999996, "text": " So kann man es auch artikulieren.", "tokens": [51152, 407, 4028, 587, 785, 2168, 1523, 1035, 425, 5695, 13, 51224], "temperature": 0.0, "avg_logprob": -0.20582499466543122, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.3035786747932434}, {"id": 417, "seek": 233320, "start": 2350.3999999999996, "end": 2356.3199999999997, "text": " Und ich wollte jetzt wirklich nur mal dieses Video machen, weil ich finde das so krass.", "tokens": [51224, 2719, 1893, 24509, 4354, 9696, 4343, 2806, 12113, 9777, 7069, 11, 7689, 1893, 17841, 1482, 370, 15913, 640, 13, 51520], "temperature": 0.0, "avg_logprob": -0.20582499466543122, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.3035786747932434}, {"id": 418, "seek": 233320, "start": 2356.3199999999997, "end": 2360.2, "text": " Immer wenn ich mir wieder was von Erscha Bach einziehe, denke ich mir so, ja,", "tokens": [51520, 42676, 4797, 1893, 3149, 6216, 390, 2957, 3300, 82, 4413, 30920, 1343, 3283, 675, 11, 27245, 1893, 3149, 370, 11, 2784, 11, 51714], "temperature": 0.0, "avg_logprob": -0.20582499466543122, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.3035786747932434}, {"id": 419, "seek": 236020, "start": 2360.24, "end": 2363.48, "text": " Dieser Typ, den jeder muss wissen, wer das ist.", "tokens": [50366, 39609, 17722, 11, 1441, 19610, 6425, 16331, 11, 2612, 1482, 1418, 13, 50528], "temperature": 0.0, "avg_logprob": -0.20845519171820748, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.19415807723999023}, {"id": 420, "seek": 236020, "start": 2363.48, "end": 2366.24, "text": " Ich finde, das ist der krasseste Dude, den es gibt.", "tokens": [50528, 3141, 17841, 11, 1482, 1418, 1163, 15913, 640, 8887, 12042, 11, 1441, 785, 6089, 13, 50666], "temperature": 0.0, "avg_logprob": -0.20845519171820748, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.19415807723999023}, {"id": 421, "seek": 236020, "start": 2366.24, "end": 2369.3999999999996, "text": " Der basht halt auch die anderen Leute, also er basht die nat\u00fcrlich nicht weg,", "tokens": [50666, 5618, 987, 357, 12479, 2168, 978, 11122, 13495, 11, 611, 1189, 987, 357, 978, 8762, 1979, 15565, 11, 50824], "temperature": 0.0, "avg_logprob": -0.20845519171820748, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.19415807723999023}, {"id": 422, "seek": 236020, "start": 2369.3999999999996, "end": 2372.2, "text": " macht kein Reaction-Video oder ratet irgendwie auf irgendwen.", "tokens": [50824, 10857, 13424, 1300, 2894, 12, 46287, 4513, 5937, 302, 20759, 2501, 11093, 15615, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20845519171820748, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.19415807723999023}, {"id": 423, "seek": 236020, "start": 2372.2, "end": 2376.12, "text": " Aber er macht sie alle platt, er macht, also meiner Meinung nach ist er v\u00f6llig \u00fcberlegen", "tokens": [50964, 5992, 1189, 10857, 2804, 5430, 499, 1591, 11, 1189, 10857, 11, 611, 20529, 36519, 5168, 1418, 1189, 35670, 4502, 22936, 51160], "temperature": 0.0, "avg_logprob": -0.20845519171820748, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.19415807723999023}, {"id": 424, "seek": 236020, "start": 2376.12, "end": 2381.64, "text": " gegen\u00fcber zum Beispiel Roger Penrose oder so anderen Leuten, die sich nat\u00fcrlich auch den ganzen Tag,", "tokens": [51160, 41830, 5919, 13772, 17666, 10571, 37841, 4513, 370, 11122, 42301, 11, 978, 3041, 8762, 2168, 1441, 23966, 11204, 11, 51436], "temperature": 0.0, "avg_logprob": -0.20845519171820748, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.19415807723999023}, {"id": 425, "seek": 236020, "start": 2381.64, "end": 2386.04, "text": " auch Douglas Hofstadter, die sich den ganzen Tag mit Bewusstsein und diesen Fragen besch\u00e4ftigen.", "tokens": [51436, 2168, 23010, 37379, 48299, 391, 11, 978, 3041, 1441, 23966, 11204, 2194, 40512, 26340, 33042, 674, 12862, 25588, 38768, 3213, 13, 51656], "temperature": 0.0, "avg_logprob": -0.20845519171820748, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.19415807723999023}, {"id": 426, "seek": 238604, "start": 2386.04, "end": 2393.96, "text": " Ich finde, Erscha Bach hat die perfekte, sozusagen die bestm\u00f6gliche verf\u00fcgbare Komprimierung", "tokens": [50364, 3141, 17841, 11, 3300, 82, 4413, 30920, 2385, 978, 13826, 916, 975, 11, 33762, 978, 1151, 76, 16277, 68, 40660, 25644, 30682, 14286, 1424, 332, 11651, 50760], "temperature": 0.0, "avg_logprob": -0.13765275478363037, "compression_ratio": 1.6125461254612545, "no_speech_prob": 0.07364760339260101}, {"id": 427, "seek": 238604, "start": 2393.96, "end": 2396.12, "text": " schon da in seiner Sprache.", "tokens": [50760, 4981, 1120, 294, 23114, 7702, 6000, 13, 50868], "temperature": 0.0, "avg_logprob": -0.13765275478363037, "compression_ratio": 1.6125461254612545, "no_speech_prob": 0.07364760339260101}, {"id": 428, "seek": 238604, "start": 2396.12, "end": 2401.56, "text": " Seine Sprache ist so kompakt, da steckt so viel drin, weil er sozusagen diese ganze Terminologie", "tokens": [50868, 1100, 533, 7702, 6000, 1418, 370, 5207, 79, 5886, 11, 1120, 2126, 19951, 370, 5891, 24534, 11, 7689, 1189, 33762, 6705, 18898, 19835, 259, 20121, 51140], "temperature": 0.0, "avg_logprob": -0.13765275478363037, "compression_ratio": 1.6125461254612545, "no_speech_prob": 0.07364760339260101}, {"id": 429, "seek": 238604, "start": 2401.56, "end": 2406.44, "text": " von den Deep Learning Netzwerken so krass gefressen hat und diese ganzen Konzepte auch", "tokens": [51140, 2957, 1441, 14895, 15205, 38889, 1554, 2653, 370, 15913, 640, 11271, 735, 268, 2385, 674, 6705, 23966, 12718, 46342, 975, 2168, 51384], "temperature": 0.0, "avg_logprob": -0.13765275478363037, "compression_ratio": 1.6125461254612545, "no_speech_prob": 0.07364760339260101}, {"id": 430, "seek": 238604, "start": 2406.44, "end": 2410.12, "text": " problemlos auf alles M\u00f6gliche anwenden kann, weil er die Intelligenz da hat,", "tokens": [51384, 1154, 9389, 2501, 7874, 21467, 68, 364, 86, 8896, 4028, 11, 7689, 1189, 978, 18762, 3213, 89, 1120, 2385, 11, 51568], "temperature": 0.0, "avg_logprob": -0.13765275478363037, "compression_ratio": 1.6125461254612545, "no_speech_prob": 0.07364760339260101}, {"id": 431, "seek": 238604, "start": 2410.12, "end": 2412.92, "text": " um sozusagen die Analogien zu bilden, um das zu tun.", "tokens": [51568, 1105, 33762, 978, 16128, 664, 1053, 2164, 22105, 268, 11, 1105, 1482, 2164, 4267, 13, 51708], "temperature": 0.0, "avg_logprob": -0.13765275478363037, "compression_ratio": 1.6125461254612545, "no_speech_prob": 0.07364760339260101}, {"id": 432, "seek": 241292, "start": 2413.0, "end": 2417.64, "text": " Es ist einfach nur ein Genuss, dem Typen zuzuh\u00f6ren, auch wenn er extrem schnell redet,", "tokens": [50368, 2313, 1418, 7281, 4343, 1343, 3632, 2023, 11, 1371, 5569, 5200, 2164, 89, 3232, 26377, 11, 2168, 4797, 1189, 4040, 17589, 2182, 302, 11, 50600], "temperature": 0.0, "avg_logprob": -0.2554906533688915, "compression_ratio": 1.475, "no_speech_prob": 0.3064012825489044}, {"id": 433, "seek": 241292, "start": 2417.64, "end": 2419.16, "text": " so wie Sheldon Cooper \u00fcbrigens auch.", "tokens": [50600, 370, 3355, 1160, 5957, 266, 20355, 38215, 2168, 13, 50676], "temperature": 0.0, "avg_logprob": -0.2554906533688915, "compression_ratio": 1.475, "no_speech_prob": 0.3064012825489044}, {"id": 434, "seek": 241292, "start": 2419.16, "end": 2423.32, "text": " Aber ist kein Problem, man kann es einfach mehrmals h\u00f6ren und super.", "tokens": [50676, 5992, 1418, 13424, 11676, 11, 587, 4028, 785, 7281, 5417, 34978, 38681, 674, 1687, 13, 50884], "temperature": 0.0, "avg_logprob": -0.2554906533688915, "compression_ratio": 1.475, "no_speech_prob": 0.3064012825489044}, {"id": 435, "seek": 241292, "start": 2423.32, "end": 2427.16, "text": " Und Lex Friedman macht auch immer einen guten Job, wenn er nochmal nachfragt und so.", "tokens": [50884, 2719, 24086, 17605, 1601, 10857, 2168, 5578, 4891, 31277, 18602, 11, 4797, 1189, 26509, 5168, 69, 32243, 674, 370, 13, 51076], "temperature": 0.0, "avg_logprob": -0.2554906533688915, "compression_ratio": 1.475, "no_speech_prob": 0.3064012825489044}, {"id": 436, "seek": 241292, "start": 2427.16, "end": 2431.32, "text": " Also, ich wollte einfach ein Video dar\u00fcber machen.", "tokens": [51076, 2743, 11, 1893, 24509, 7281, 1343, 9777, 21737, 7069, 13, 51284], "temperature": 0.0, "avg_logprob": -0.2554906533688915, "compression_ratio": 1.475, "no_speech_prob": 0.3064012825489044}, {"id": 437, "seek": 241292, "start": 2431.32, "end": 2433.32, "text": " Reingehauen, YouTube.", "tokens": [51284, 1300, 8735, 71, 11715, 11, 3088, 13, 51384], "temperature": 0.0, "avg_logprob": -0.2554906533688915, "compression_ratio": 1.475, "no_speech_prob": 0.3064012825489044}, {"id": 438, "seek": 262292, "start": 2622.92, "end": 2624.92, "text": " Das war's f\u00fcr heute.", "tokens": [50364, 2846, 1516, 311, 2959, 9801, 13, 50464], "temperature": 0.0, "avg_logprob": -0.8699123382568359, "compression_ratio": 0.961038961038961, "no_speech_prob": 0.8920029401779175}, {"id": 439, "seek": 262292, "start": 2624.92, "end": 2626.92, "text": " Vielen Dank f\u00fcr's Zuschauen.", "tokens": [50464, 22502, 14148, 2959, 311, 48333, 11715, 13, 50564], "temperature": 0.0, "avg_logprob": -0.8699123382568359, "compression_ratio": 0.961038961038961, "no_speech_prob": 0.8920029401779175}, {"id": 440, "seek": 262292, "start": 2626.92, "end": 2628.92, "text": " Bis zum n\u00e4chsten Mal.", "tokens": [50564, 25271, 5919, 19101, 5746, 13, 50664], "temperature": 0.0, "avg_logprob": -0.8699123382568359, "compression_ratio": 0.961038961038961, "no_speech_prob": 0.8920029401779175}, {"id": 441, "seek": 265292, "start": 2652.92, "end": 2654.92, "text": " Der Kanal ist wie eine sch\u00f6ne Vorlesung aufgebaut.", "tokens": [50364, 5618, 38643, 1418, 3355, 3018, 41152, 12231, 904, 1063, 2501, 42858, 13, 50464], "temperature": 0.0, "avg_logprob": -0.16707195855874932, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.9840338230133057}, {"id": 442, "seek": 265292, "start": 2654.92, "end": 2660.92, "text": " Wenn ich unten Videos verlinke, dann w\u00e4re es angebracht, sich die auch reinzuziehen,", "tokens": [50464, 7899, 1893, 25693, 25903, 1306, 5045, 330, 11, 3594, 14558, 785, 15495, 23404, 11, 3041, 978, 2168, 6561, 11728, 28768, 11, 50764], "temperature": 0.0, "avg_logprob": -0.16707195855874932, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.9840338230133057}, {"id": 443, "seek": 265292, "start": 2660.92, "end": 2662.44, "text": " weil das aufeinander aufbaut.", "tokens": [50764, 7689, 1482, 1609, 2106, 20553, 2501, 65, 1375, 13, 50840], "temperature": 0.0, "avg_logprob": -0.16707195855874932, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.9840338230133057}, {"id": 444, "seek": 265292, "start": 2662.44, "end": 2664.92, "text": " Bestimmte Begriffe werden definiert, viele Beispiele werden genannt.", "tokens": [50840, 9752, 6753, 975, 879, 861, 31387, 4604, 1561, 4859, 11, 9693, 879, 7631, 15949, 4604, 1049, 39878, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16707195855874932, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.9840338230133057}, {"id": 445, "seek": 265292, "start": 2664.92, "end": 2670.2000000000003, "text": " Und ja, ich denke mir schon was dabei, weil es sozusagen meinen eigenen Erkenntnisprozess", "tokens": [50964, 2719, 2784, 11, 1893, 27245, 3149, 4981, 390, 14967, 11, 7689, 785, 33762, 22738, 28702, 3300, 41838, 10661, 4318, 37575, 51228], "temperature": 0.0, "avg_logprob": -0.16707195855874932, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.9840338230133057}, {"id": 446, "seek": 265292, "start": 2670.2000000000003, "end": 2672.92, "text": " abbildet, die Reihenfolge, in denen ich die Videos hier hochlade.", "tokens": [51228, 410, 16248, 302, 11, 978, 1300, 29842, 7082, 432, 11, 294, 19998, 1893, 978, 25903, 3296, 19783, 75, 762, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16707195855874932, "compression_ratio": 1.5252918287937742, "no_speech_prob": 0.9840338230133057}], "language": "de"}