When a child is killed in Gaza, it's because somebody made a decision that this killing
was worth it to hit another target, and this is according to five sources that I've spoken
with in Israeli intelligence.
In all of the target fires that Israel is bombing right now, the amount of civilians
that are likely to be killed is written down.
After October 7th, there is a total disregard for Palestinian civilian life, even when hitting
targets that are either not distinctly military in nature.
And I think that to look at the civilian devastation that is happening right now in Gaza, you have
to understand that it's a consequence of a particular Israeli war policy.
It is a war policy that has a very loose interpretation of what a military target is, also attacking
people in civilian spaces.
It is a war policy that centers on deterrents and hitting these power targets that are intended
to place civilian pressure on Hamas, and it is a war policy that is increasingly being
helped by the use of big data, automation software, and AI.
My evidence suggests that many, many of the of the civilians who are being killed in Gaza
are being killed as a result of these policies that I do not think are justifiable policies.
International law experts would call them war crimes.
Hello comrades.
It's episode 303 of This Machine Kills.
I'm Jaythin, joined by Ed and producer Jeremy in the future.
It's your premium episode for this week, but I'm just going to say right up off the top
because we are going to get into a topic, into a very long investigative report that
is fucking stunning and shocking to me, stunning in its content, but also as Ed and I were
yelling about before recording, stunning in the fact that nobody else is fucking talking
about it and everyone needs to be talking about it.
While this is a premium episode, largely just because that's how the schedule of our recordings
worked out, we are going to pretty quickly unlock this one for the free feed because
this is something that deserves much wider discussion than it's receiving, especially
among people who like us are constantly crowing about the real material and imminent consequences
of AI and its use by people in power, whether that's corporations or governments, militaries,
whatever it might be.
And yet a lot of people ain't talking about this topic that we got for you all today.
And so we need to, yeah, we need to do our part to at least try to bring some more attention
to it.
So let's set the scene here, Ed.
So about a week ago as a recording for us, 972 Magazine in partnership with Local Call,
So 972 Magazine is a left-wing politics magazine, politics and news magazine based in Tel Aviv
and Local Call is a Hebrew language publication.
So 972 publishes a lot of investigative reporting from a left-wing perspective about Israel
and Palestine.
They're based in Tel Aviv, although they publish a lot of their stuff in English because they
want a big international audience to read it.
They want to draw attention to it.
And they've published in partnership with Local Call, which is a Hebrew language based
outlet in Israel, this just long and stunning report titled A Mass Assassination Factory
Inside Israel's Calculated Bombing of Gaza.
And then the deck here is permissive airstrikes on non-military targets and the use of an
artificial intelligence system have enabled the Israeli army to carry out its deadliest
war on Gaza.
So this is a really long piece on the systems, the technological systems and the military
strategies that are underlying Gaza's current and previous military campaigns, the genocide
that they're waging, the mass destruction that they are committing on Gaza, this AI
system that is really being revealed in this report that we're going to talk about and
strategy underlying that this is what is a really kind of massive foundational pillar
behind everything that we see happening in Gaza right now.
And of course, when there's an AI angle, TMK is going to be on the case.
But this is striking me as not just a little bit of AI sauce on something else.
This is really striking me as a case where Israel and the IDF is really, you know, and
proudly putting AI at the front and center of their strategies, their target making.
They've got this, we'll get into it, but there's like a target administrative system or whatever
it is that is really focused on creating in a really mass produced way targets for these
like one ton bombs that Israel is dropping on high rise buildings, on universities, on
hospitals, on mosque, on open air, you know, flea markets, on schools, wherever they can.
They are claiming at least that this AI system that we'll get into is what's spitting out
all of these targets, right, is giving them, as they claim, 100 targets a day, more targets
than they can keep up with with their bombing campaign, giving out of a total database of
something like 30 to 40,000 individual targets for Israel to airstrike.
There's obviously a lot here, but we'll get into the details.
There's a lot to discuss around the kind of the politics of this.
There's a lot to discuss around the technology of this as well and what what we should or
should not believe in terms of what IDF spokespeople or the IDF chief of staff is saying
and claiming versus what might be the actual fact.
But there is also a lot to be said about how, at the end of the day, there is a really clear,
you know, AI, whether it's an AI powered or an AI alibi, right, justified by AI, there
is a really clear kill chain happening here.
There's a really clear kill matrix happening here.
You know, as we get into this, I think astute listeners might recall some of the reporting
from 10 years ago, you know, in 2013 around the Obama administration's secret kill
matrix, right, or the disposition matrix, the kill list that they had created.
So crazy. They call it the dispossession matrix.
It's like it's some fucking spin off of the dispossessed from Ursula Le Guin, you know.
Oh, my God.
But, you know, this kill list that the Obama administration created to guide their drone
strikes, right, that and so astute listeners might find some really obvious connections
here from the from 10 years ago with the Obama administration's kill list or or I should
say it's the disposition matrix, but it did dispossess a lot of people of their lives.
That's for sure. And then upgrade.
And then what we're talking about now with this AI system that Israel has is very much
a in the direct lineage of the disposition matrix, but is one that, you know, is powered
by machine learning, is taking it to another level, is handing over a lot more decision
making to an automated system, whereas if the kill list, the disposition matrix was
a database, this AI system that Israel has is very much at least in the way that the
IDF is talking about it quite explicitly from both current and former soldiers who spoke
to 972 and intelligence officials.
They are talking about it as a automated system that that that that title mass assassination
factory is not a bit of a stylistic verve that the magazine came up with.
That is a direct quote from an IDF source who called it a mass assassination factory.
Right. And so I think let's get into what this system is, what the underlying strategy,
the military strategy is for this.
But it's it's it's stunning.
It's shocking. And it's even to people like me and Ed, who like our whole job is to not
only know about this stuff, but to like think about it in the most cynical and pessimistic
terms possible.
And so it is always a real a real blow when we read stuff that is beyond even our our
imagination of what's actually going on.
You know, this article and we were talking a little bit about this before this article
sickening on two accounts, right on the first account, right, is that this makes it this
kind of makes me think about all the death and destruction in a slightly more darker
way and that like, you know, these people are, you know, whether or not, you know, because
we still need reporting done on what the AI, what the actual artificial intelligence is.
But like you like you said off the top, it doesn't really matter whether what the
configuration is. Right. This is one of the risk of AI being promulgated like this, which
is that it sanitizes monstrous crimes because data is neutral.
Right. Because a collation via algorithm is neutral because automation is neutral.
These are not political things.
These are categories that are outside of human cognition.
And as thus we can refer to them, we can embody them, we can reinforce them because
they're not flawed, biased, so on and so forth.
So sickening because it probably also reveals an even like greater pathology about the
children that are being murdered, about the, you know, the young men and women that are
being murdered, about the civilians that are being murdered, right, that in one way or
another, not only is there already the racism, incumbent and Zionism, and especially this
genocide of Palestinians, but then there's also this insistence that might emerge even
from people who might not have that or might take pause with that, that, well, you know,
the mass murder is reciprocal and neutral, you know, because the data is just leading
us in that direction. And then on the second count, also sickening in a way that we'll
talk more about later in the episode, just like, you know, this, this is, this is the
fucking fire this time, you know, and what are people doing?
What are people in the AI ethics and the AI risk and the AI safety community saying
about this, about a system where whether or not AI is actually in a role of it, a military
is proudly saying, we named this thing the gospel. It's job, find people to kill. What
does it do? Help us kill as many people as possible. Seems like the worst possible scenario
and the perfect one for all of these industries and cottage factories and all these fuckers
to speak up and say anything. How many have a handful? If that, right, you can, you can
guess if you're sitting at home, you can probably guess who has said literally anything about
this and deals with those fields. And you'd be almost certainly right on those instances.
I guess who would say this? And you'd be wrong because a lot of those people have not said
anything. That is also true. That's another thing to say that there also are some notable
notable signs. Yes. Um, and so it's like, okay, well, what's going on here? You know,
because I find it hard to believe it's either they don't know about it or, you know, as
we have talked about, and as other people have talked about, you know, for example,
Khadija over at logic, my editor, you know, has talked about, there's just like a moral
failure in the core framework about how we think about AI ethics and safety and risk
that makes the framework almost useless, right? Like maybe from an outside in perspective,
we can all agree and understand that things should not be developed mindlessly and that
there are real consequences to taking data that's constructed in biased means and racist
means from a society that has its own politics and its own social relations and its own economics
and its own history and its own culture, that things will bleed through and, you know,
result in the creation of data sets or result in the deployment of weapons and surveillance in
ways that discriminate and hurt certain people. But the industries as they actually are,
AI safety, AI risk, AI ethics, they're corporate captured, you know, industries,
and what are, you know, what are, that are doing what? Soaking up all the academic talent
and critical thinkers and critics, hoarding all the resources and the access to the data and to
the computational infrastructure, right? And sucking the air out of any real critique
of how artificial intelligence is talked about, thought about, deployed,
and integrated into things that leaves us within a moment where these motherfuckers are building a
Death Star, you know, like a literal fucking Death Star aimed at civilians in a place that
they occupy and are doing a genocide in, everybody shuts the fuck up. You know, is this, it's useless.
It's useless. I don't, we don't even need, we don't even need, what more do we need to say about it
other than that? If in the moment where there's a literal genocide happening,
most of them are silent. It's so fucking frustrating. And it's even more frustrating
when you read the piece, right? Because the piece makes it very clear, whatever the system is,
it has massively expanded the capabilities, right? Where in previous operations,
they were happy to get maybe 100 targets a year. And now they're generating as many targets as
they did in a year inside of a day because what they're doing or what they claim to be doing with
this AI system is that they will take, hey, this is, this is the address of a spokesperson for Hamas.
Hey, this is a militant that we know is in this group. Hey, this is like, you know, someone who
was spotted at some sort of demonstration or resistance. This is someone we arrested or picked
up and saying, okay, this is where they live. This is where they work. Are these places affiliated
with Hamas? Probably not. But because this person lives there or works there or has been there,
we can blow that shit to shit, blow it to shit. You know, there's just one really striking
paragraph. I think it's pretty demonstrative both of the danger in this AI system existing
and then also in the fact that we can say that and not get caught up on how much of it is AI or not
because at the end of the day, it's a digital system or piece of technology that's being used
by people who are already looking for a certain problem to be served in a certain way.
Solved in a certain way and they have the solution in the way that they deploy it. So we have,
at one point, this guy says, you know, first I'll step back and kind of explain one of the
insights of the report. In case you might not have known, you know, and as 972 and local call kind
of talk about, there are four categories and targets that are struck by Israeli aircraft.
The first are tactical targets, right? And so, you know, these are supposed to be standard military
targets. These are arm cells. These are places where weapons are being held. These are actual
weapons themselves, rocket launchers, anti-tank missile launchers, launch pits, mortar bombs,
military headquarters, observation posts, and so on. The second are hardened underground targets,
the tunnels that Hamas uses as well as the tunnels that span through Gaza and underneath
Gaza's neighborhoods that are part of the network that brings in goods since Israel's
maintaining a blockade. Then, you know, strikes of these targets are not as clear as the first one
because a lot of these tunnels are under civilians, right? They're homes. They're actual
infrastructure, the centers of their life, universities, hospitals. You know, you strike
that, you're going to kill a massive amount of people, right? Not that that's going to stop them.
The third are power targets, as they call them, and these are high rises, residential towers,
public buildings, right? These are things that where you hit them, you're hitting not just a place,
you know, not just a neighborhood, not just a place where people work, but like a center
of a section of the life, of the rhythm of a city, of a community, right? You're killing the
university, you're killing the banks, you're killing the government offices, you're killing
hospitals, you're killing office buildings and commercial areas. You're crippling the city,
essentially, right? And you're crippling the ability of people to do anything other than
cower in their homes, which you're going to bomb anyway, right? And the idea here,
as self-admitted by the Israeli intelligence sources, is that if we hit these power targets,
we place pressure on Hamas, that Palestinian society will reject Hamas. Sidebar, war crime.
You know, you can't bomb civilians collectively punishing them for something that groups,
in this case, Hamas is done, right? Not that that matters, but that will come back to be relevant
later. And then the final category are family homes or operatives homes, right? And so these
are homes where, these are targets where you suspect one person in that residence is in Hamas
or the Islamic Jihad, right? And you bomb it so that you might be able to get that one person,
even though as other, and as we'll talk about other reporting reveals, a lot of the times,
the houses that people say are, you know, that houses that the IDF says we're holding, one
militant or one operative of the Islamic Jihad actually didn't. And in fact, they just killed
huge amount of civilians, right? And so 972 writes that, you know,
the Israeli army had really been focusing on the third and fourth categories this time
in response to October 7th, in response to Hamas' operation on October 7th. And that,
they write, according to statements on October 11th by the IDF spokesperson,
during the first five days of fighting, half the targets bombed, 1,329 out of a total 2,687
were deemed power targets. And so here's the quote. One source said, we are asked to look
for high rise buildings with half a floor that can be attributed to Hamas. Sometimes it is a
militant group spokesperson office or a point where operatives meet. I understood that the
floor is an excuse that allows the army to cause a lot of destruction in Gaza. This is what they
told us. If they would tell the whole world that the Islamic Jihad offices on the 10th floor are
not important as a target, but that its existence is a justification to bring down the entire high
rise with the aim of pressuring civilian families who live in it in order to put pressure on terrorist
organizations, this would itself be seen as terrorism. So they do not say it. Yeah,
because that's terrorism. That is literally the textbook definition of terrorism. When we went
into the fucking Middle East and the phase of the invasions premised on the idea that we have
evidence. This is when we were still trying to figure out what the war aims were going to be.
We have evidence that Osama bin Laden was responsible for 9-11. Give him over to us
in senior leadership who's involved in planning it and we won't invade. Okay. Do you have evidence?
Yeah. Will you give us the evidence? No. Okay. Then we're not going to hand him over. It was
more or less the exchange in the early years. So we invaded. There are very clear guidelines
in international law on what you're supposed to do. You don't have to follow them when you
have the largest military in the world. An insistence to follow them is deemed as
unpatriotic, naive, simple-minded, apologism for terrorism when in reality it's to present
situations like this, where you have Israel bombing, flying anything that moves on anything
that moves so that they can do what? Clear out Gaza as part of what is the very transparent
genocide of Palestinians so that they can come in and eventually settle the northern and the
southern strips of Gaza. So what I was saying earlier about whether or not the role of AI here
as it gets fleshed out in future reporting, we see here the thing that we've talked about as an
actual risk of artificial intelligence. There's a problem. Israel wants justification for killing
as many civilians as possible for clearing out buildings in Gaza, for terrorizing Palestinian
society. Solution, argue that the reason you're doing the thing that you've been planning on
doing that you have actually been doing for the past few decades is because of a new technology
that allows you to collate disparate threads of intelligence and pinpoint with computational
sanitation or cleanliness exactly what parts of Palestinian society you need to press on to make
Hamas screen because that's really what it comes down to. Whether or not
AI, there's an actual real automated system here or it's a bunch of people who still have to review
the system and are still doing the final say on everything or it's a bunch of people just signing
off. Whatever it is, it's very clear what's happening here, which is one of the things
we should be concerned about as an AI risk, which is that people are going to use it to justify some
incredibly immoral thing that they're eager to do by saying that, well, the computer is just turning
out the numbers. I'm just in the Chinese box. I'm just following orders. I'm just translating
reality based on what this thing is finding out there. And that, again, it's just like it's
so insane. This is like one of the earliest parts of the article, right? This is one of the earliest
and such unambiguous war crime material, AI risk material things, silence, silence from that sector.
Yeah, the silence is deafening. Really, truly, the silence is deafening. And you're exactly
right here. When we get into, and we will very shortly, more into Habsora, which is the name
of this system. And as you said, that translates to English from Hebrew as the gospel, which,
talk about the direct link to the episode we just released as well.
Yeah, about making God.
About making God, using AI to make God, and using AI to justify the techno fascism through
religiosity. You name your mass assassination factory, the gospel, as if it is the voice of
God telling you where to drop tens of thousands of tons of bombs on all these
quote unquote power targets. It's so unambiguous here. It's so unambiguous. And like you said,
it doesn't matter if it's truly some AI automated system. Even though, I mean,
it is telling, and we'll get to this later as well, that IDF people are explicitly saying
that they don't have the ability to closely review all of the outputs from the system
because there are just so many, but their whole mandate is quantity. The more targets, the better.
And so they are explicitly, which is a total reversal of everybody else who is always talking
about how, no, no, no, we have careful human review of the outputs from automated systems.
And here the IDF is being like, we don't have time to do all that human review of this shit.
We got bombs to drop.
There was some guy who was just talking the other day on Twitter about how artificial
intelligence is like the printing press, and that it's an inherent good, and everybody is
made more intelligent by the proliferation of AI products, goods, and services.
And when you mentioned and talk about that quantity over quality element,
in some ways it is like the printing press. And in this specific scenario,
getting flooded with junk, with bullshit, with bad data. I don't even want to call it bad data
because that suggests there's good data that the IDF could use. But getting flooded with
information and then using that to just be like, well, we are going to kill them all anyway.
So now at least we have nicely tabulated formal categories and zones of attack and targets
versus having to ad hoc justify why we did a war crime again.
Yeah. And talk about the links to the Obama kill list as well.
The IDF is explicitly saying, they explicitly said, we are not being surgical.
They are just loving to say the loud part, the quiet part loud right now on all of this.
But that is also, I think, really telling here as well where the disposition matrix
and drone warfare was designed to be sanitized through its surgicalness. It was supposed to be
precise, surgical. We have a kill list of individual targets. We have drones that are
dropping smart missiles on people. It's all surgical. It's all precise. It's sanitized.
Here, what we have is an AI system, Haspera, which is cranking out more targets than IDF
intelligence can keep up with. Those targets are leading. They are the justification.
Ultimately, it doesn't really matter if the AI is leading you to the power targets or the AI is
the justification for bombing power targets you already want to bomb. At the end of the day,
it's the same output where the IDF is claiming that the AI is leading them through this network
analysis to these targets. And then all they do is if a target is spit out, then they bomb it.
Targets spit out, they bomb it. Targets spit out, they bomb it.
It does seem to be a really nice coincidence for the IDF that so many of the targets that are being
spit out by Hapsora are targets that their military strategy is focused on. As you said
from the 972 reporting power targets, so the high-rise buildings, the universities,
the hospitals, the infrastructure. It's not even shock and awe. It's shock and massacre
because also the fourth target that you mentioned is the family home.
So Hapsora is pointing them to power targets and family homes. It explains why if you are
paying attention to what's happening in Gaza, the reporting is constantly talking about,
well, Gaza has no more universities. Gaza has no more hospitals. Oh, here's yet another person
who has had more than 30 of their family members killed by Israel. The reporting here as well
around some of those numbers are mind-boggling. It really puts a material human cost on these
systems and strategies. For the IDF for Israel, the technological system and the military strategy
are intimately linked together. They are not able to be disentangled. They are serving each other.
It's not a unidirectional causal flow where the technology leads to the strategy or the strategy
leads. It's bidirectional. It's a relationship. They are mutually reinforcing each other.
We can see this in the material human cost that is being reported. We're recording this. It's
December 6. I'll just put a time date on it because this is all moving so fast.
This is two months after October 7, after everything started. The report that we're seeing
and that 972 reports on some of the death toll numbers even just from a month into the war,
the massacre, the genocide. There's numbers from November 11 talking about that even by that time,
even a month into it, there were over 300 people who had more than 10 family members killed.
Then there were hundreds of people who had seven to nine family members killed,
hundreds more that had five family members. The death tolls here are only able to be
comprehended, I think, as incomprehensible as they are by understanding it as an outcome of
the bombing power targets in family homes. It is a military strategy bolstered by, powered by,
justified by, sanitized by a technological system that is designed to not just cause
mass destruction but wipe out entire family lineages. This is ethnic cleansing.
This is Nakba. We have to understand that sanitizing nature of the technology is also
a cleansing. It's an ethnic cleansing that we see here. A cleansing by wiping away whole lineages
of families, of Palestinian families. You can even hear it when they talk about,
you know, for example, they talk about bombing entire neighborhoods. Like you said,
how they're not surgical. When pressed about it, they're just like, well, you know,
and they've always been, and IDF has always been consistent on this. Neighborhoods are
just tamas recruiting grounds. They're terror nests. They're operational headquarters.
Civilians are operational assets. Residents inside of these buildings are assets that might
be used by the terrorist buildings. Like no one's a person. Everyone's a potential terrorist
or a subhuman that's waiting to be purged. And now we have this technological implementation
that just so happens to agree with us, right? Exactly. Exactly. And, you know, I want to get
into the technology a bit more too. But before, you know, I'll do that right now. But before I do,
you know, I mentioned the kill, the Obama kill list. The way this technology works and the way
it's being used also really strikes me as being extremely similar to Palantir, right? The network
analysis that Palantir has created, which was as we should always remember and always mention,
you know, Palantir was created with funding from the, in QTEL, the CIA's venture capital
arm. And it was created originally, despite its widespread use now in public health,
in finance and policing and mineral extraction, you know, despite all of that,
it was originally created for the war on terror. It was created as a tool for the CIA. It was
created as a tool for the Pentagon. It was created to do this kind of network analysis,
as they call it, to find terror cells, right? And how do they do that? Well, the whole idea
is that they were going to create networks of associations with people, right? You create a
network of all of the people associated with somebody, all of the addresses associated with
them, the locations, the vehicles, everything that you possibly can, every object or relationship
that you can associate with somebody, you map that into this big, giant network. And the idea
is that through that, you are able to map these terror cells, right? You're able to see how,
oh, these people have a vehicle in common with each other. We've got data that at independent
times, these three people have used this same vehicle. Okay, maybe that's an Al-Qaeda vehicle.
That was the whole idea of Palantir, is to create this network analysis, specifically to map out
these terror cells, to map out their associations with addresses, with vehicles, with families,
with friends, with whoever, with whatever, with wherever, and then as a way to then create targets
to do an enhanced interrogation of, to assassinate, right? Whatever it is that you're doing,
all of the terrible things, that is also, it sounds to me like the way Hapsara works is very,
very similar to that kind of network analysis, right? I think it's also important to keep in
mind all of these technological lineages that we see here, that as absolutely fucking terrifying,
horrendous and shocking as this is, it is also not unprecedented. It is very much in a, which
means it's not unpredictable either, or unpredicted. It is in a long precedent and lineage of how these
technologies are designed, how they're created, and what they are used for.
So, let's talk a little bit about 972's reporting on Hapsara so that we can understand as well.
We talked about the military strategy here of focusing on power targets and family homes.
Let's talk about the technological system that is powering that and justifying that, right?
I'm actually just going to quote some from 972 because I think they really clearly
lay it out and I don't want to mince their careful wording here.
According to intelligence sources, Hapsara, which, as a side note, remember, this translates as
the gospel. So, always keep that in mind too. According to intelligence sources, Hapsara generates,
among other things, automatic recommendations for attacking private residences where people suspected
of being Hamas or Islamic Jihad operatives live. Israel then carries out large-scale assassination
operations through the heavy shelling of these residential homes.
Side note as well, this is not our common imagination of assassination.
We now think of assassination as Assassin's Creed or as Hitman or as Navy SEALs or James Bond.
As this thing that happens in secret and with surgical precision, no. For Israel, assassination
means heavy shelling of residential homes. It is literally shooting a passenger plane out of the
sky not because there's a Hamas operative on the plane, but because a Hamas operative once sat on
a seat in the plane. That is what's happening here. Hapsara, as one of the sources explained,
processes enormous amounts of data that, quote, tens of thousands of intelligence officers could
not process and recommends bombing sites in real time because most senior Hamas officials head into
underground tunnels with the start of any military operation, the sources say. The use
of a system like Hapsara makes it possible to locate and attack the homes of relatively junior
operatives. One former intelligence officer explained that the Hapsara system enables the
army to run a, quote, unquote, mass assassination factory in which the, quote, emphasis is on
quantity and not on quality. A human eye will go over the targets before each attack,
but it need not spend a lot of time on them, said the former intelligence officer.
Since Israel estimates that there are approximately 30,000 Hamas members in Gaza and they are all
marked for death, the number of potential targets is enormous. In 2019, the Israeli
army created a new center aimed at using AI to accelerate target generation. As the former IDF
chief of staff said in an in-depth interview with YNET earlier this year, quote, the target's
administrative division is a unit that includes hundreds of officers and soldiers and is based on
AI capabilities. The IDF chief of staff went on to say, quote, this is a machine that with the help
of AI processes a lot of data better and faster than any human and translates it into targets
for attack. The result was that in Operation Guardian of the Walls in 2021, from the moment
this machine was activated, it generated 100 new targets every day. You see, in the past,
there was times in Gaza when we would create 50 targets per year, and here the machine produced
100 targets in one day. We prepare the targets automatically and work according to a checklist,
one of the sources who worked in the new targets administrative division told 972 and Local Call.
Quote, it really is like a factory. We work quickly and there is no time to delve deep into
the target. The view is that we are judged according to how many targets we manage to generate.
So, I mean, that's Hapsara in a nutshell. That's the gospel in a nutshell. It is about
maximizing. Calling it a mass assassination factory is actually the most apt description you have
because it is truly about mass production. It is Fordism for genocide, right? It is about maximizing
the quantity of target generation and then responding to those targets in an automated way
of, you know, as I said before, target generated, bomb dropped, target generated, bomb dropped,
and you just, and the whole job is to keep up with the factory line, spitting out targets,
no review, no oversight for quality, no concerns about the morality or legality of it, right?
But instead, the AI provides justification. You know, the AI is giving you targets of
her mass operatives. It's not our fault that they use human shields, whether it be
their own families or hospitals or schools or mosque, you know, it's not our fault,
but we got to kill the Hamas operative, you know, and so if we have to, you know, hey,
we're not the one holding the human shield, right? But we are blowing it apart. I mean,
this is what's happening here, right? I think, you know, to go back as well to some of the real,
the deafening silence of the fact that people are, like, why are the people that we follow,
the people that are in our networks, the people we know, who are, like, always, who are so concerned
about the ethics and politics and impacts of AI, why is this not the thing everyone is screaming
their head off about right now? Or at least linking to the article, right? I've not even seen
people link to the article, you know, like, why? It doesn't make sense to me, especially when,
you know, if you consider how much attention it receives, how loudly people crow
when, and rightfully so, whenever there's a similar kind of automated system that is
targeting people for welfare fraud, right? And like, as we've seen, as we see in recent reporting
in France, as we talked about with the Dutch algorithm, as we've talked about with Australia,
as we talked about in the US, right? Like, as we've talked about in the UK, right? These systems
exist all over the place, right? These AI systems or automated systems that target people
with risk scores, label them as welfare frauds, and then deny them their welfare benefits,
trap them in these labyrinthine bureaucratic systems, deprive them of the resources and aid
that they not only need, but are due, right? That they deserve. Anytime that happens,
there is so much attention to it, right? There is so much writing about it, you know,
yelling about it, and rightfully so. It's truly awful. And yet, when we see a system that is
doing much the same, but not to deprive you of welfare benefits, but to deprive you of your life,
to deprive you of your entire family, to deprive you of your communities, to deprive you of your
land, to deprive you of everything that you have, and everything that you will ever have.
When we see a system doing that, people just are suddenly clamped up, you know? Are they too
shocked to talk? Why is it that it seems to be more morally reprehensible to deprive a Palestinian
of their welfare benefits than it is to deprive them of their life and their whole family? Why?
I don't understand. Have we reached the limits of politics where it is so horrendous that we
can't even speak of it? Have we reached the limits of morality and legality where actually,
this is justified, that the people deserve it, that, hey, I can see both sides of the issue,
and it's a land of contrast? None of that holds water for me. I don't understand.
I mean, it's cowardice, right? I mean, it's cowardice. Khadijah wrote a long time about
this in an article that I think got a lot of people to pay attention to. It was called
on the moral collapse of AI ethics, right? And it was talking about how a lot of people are
focusing on the firing of Timnit Gebru and the firing shows that Google is fucked, right? But
there are also other questions and debates and discussions we should have, like the fact that AI
ethics is a fundamentally limited thing, especially if we're talking about AI ethics in Google.
We've already lost the plot if we think that a firing at Google is the problem, because the
problem is even larger than the fact that Google has a specific engagement with the AI ethics
community, right? It scales up to the fact that even our ideas about what AI ethics looks like
are just nonsense, right? Because they don't begin with a fundamentally antagonistic relationship
or engagement towards how capitalism and specifically racial capitalism has created
and spurred along the development of deeply discriminatory and exploitative and extractive
technologies. We talk about it all the time. Most of the technology we have in this world
that people crow about is just an exotic way to surveil on another person or to kill another
person. That's most of the shit. That's most of the shit that everyone is excited about all the
fucking time, right? And so when we are over here talking about AI ethics, a lot of the time it's
like, well, how do we make a more humane way of killing other human beings? Or how do we make a
more humane way of surveilling other people, right? Fundamentally. And to lose sight of that is to have
lost the plot. And most people in that field or industry in one way or another have lost the plot,
which is not to say there are a lot of people I know and respect who are in these fields,
right? But the ones whose work I really love and look to and read and think about are also grappling
with the fact that there are fundamental limits here, right? And that the real problem is how do
we get out of it? What's the off ramp? How do you get out of this stupid fucking discussion?
Well, the question is, do we do a one-year moratorium on Apple's facial recognition software
inside of its ring surveillance cameras that it gives to police departments for free or a two-year
moratorium instead of why the fuck is a delivery company giving surveillance cameras to police
departments and giving them facial recognition software when it's misidentifying black and
brown people at a ridiculous rate? Why is that? What's going on there? Well,
good luck really having a great discussion with that. And even, as Skedija points out,
even the engagement that we do get with that in the form of surveillance capitalism
falls short. The idea of surveillance as a service is still fundamentally blunted once
she writes that the larger point is even our language is failing us because we're too often
fighting to save a democracy that modernity has foreclosed. We need a radical reframing away from
the lone researcher against Goliath to reckon with the failures of fairness, accountability,
transparency, and broader ethics frameworks that have allowed opportunists to build their brand,
taking up space required to address the core issues of big tax hegemonic violence as described
in Timnit's paper. I'll also quote at length because I think that I really encourage you to
check out the writings from a few years ago, but it's still poignant. I think about it,
think about it especially with this piece. She writes, if you were shocked by the firing
of Timnit, you haven't been paying attention. We need to fucking take responsibility for the
present because while you're immobilized, debating whether if you're the one who should mention the
corporate diversity, equity, and inclusion lexicon inherently decoupled from political
economic analysis is half the problem. The most opportunist and or mediocre are defining the
discourse on a global stage. We're ruminating about Jeff Deans' feelings instead of building
a cross-class labor movement that defines tech workers broadly, i.e. researchers, engineers,
Uber drivers, Amazon warehouse workers, content moderators, etc. We should cry out,
not because Timnit is a brilliant scientist who shaped her field, but because her firing
is a symptom of a broken society we've constructed. She isn't fucked, we're all fucked.
The academics and computer scientists claiming they will no longer volunteer their free labor
when it comes time to reviewing Google academic submissions should be commended. Whoever they
should ask themselves, what is it about the high-minded ideals of unimpeded academic freedom
that were so divorced from the praxis that research was enabling? Saying that phrenology
was conducted rigorously is insufficient. That issue isn't Google or language models or Jeff
Dean, rather it is a field that has sold itself to industries that are killing people. That is the
crux of the thing. That's where it comes down home to. Israel is a perfect either representation or
mirror in our own tech industry. I'll start with Israel first. As we had Anthony Lowenstein come
on to talk about the Palestine laboratory, one of the indispensable roles that Israel plays
in the world is that it battle tests surveillance tech and murder tech on Palestinians and then
exports it to autocratic regimes around the world to suppress minority populations.
As the climate begins to get worse on climate refugees, silence on that as well as a refusal
to acknowledge that axis of oppression that's going on is a lot of things, but we should also
understand fundamentally is moral cowardice. Part of that is also because the same fucking
thing happens here. The largest sources of legitimacy in the tech industries and in
various fields attached to the tech industry come from how close people are to the money
that's being generated from selling yourself to an industry that kills people. How much money
are you going to be making if you're helping train these drones or do the network analysis
that helps with this targeted assassination or help militarize this or that technology and
application in the field? What kind of innovations to surveillance as a service can you offer
both in terms of the advertising model and also in terms of also just surveillance as a product
that will allow you to tap into other aspects of people's daily lives? We can talk about the
surveillance products that Amazon rolls out without dipping into the limited surveillance
capitalism thing because we can talk about the fact that, again, the ring cameras.
What is a ring camera? A ring camera is not simply like a little surveillance camera
that is offered as a product divorced from reality that you get to buy. A ring camera is a pretty
smart move that ends up playing roles intentional and growing to fill unintentional ones with a
little nudging. When you have a ring camera, you're part of a process where this company gives them
to other people, departments for free, and gins them up in marketing as, look, you live in an
unsafe place. You should have surveillance so that you can protect your property, you can protect
your packages, you can protect your family, ignoring that, for example, a lot of the people
who get them are in places that are already relatively safe and also that a lot of people
who use them are using them to surveil packages that, if you lose them, Amazon refunds you
and you can get them, again, no questions asked. You could get the thing, take it into your home,
say you never got it, and they give it to you, no questions asked. But besides the point on that,
also, you're getting then ingratiated into this network where the idea is we should all be
encouraged in one way or another to sign on to these forms of surveillance as they help out
Amazon. Sometimes that means disciplining or surveilling delivery workers who are already
overworked and underpaid and over-exploited. But then Amazon has created a Byzantine legal
structure to keep hands distance at, so maybe it doesn't have as much control as it might want,
but has more control than it should. Well, the surveillance camera has now allowed people inside
of their homes with the Ring cameras to act as another boss that they have to deal with
while they're still being kept away from Amazon as not being an actual employee of Amazon.
The Ring camera ends up growing and expanding because Ring is going here and saying,
look, Amazon is going here and saying, we have a piece of technology.
And the thing that we're interested in doing is growing it out, one, as surveillance products so
that we can fill our other needs inside of the ecosystem, and two, because we're interested in
exploring whether we can plug it into other goods and services, whether we can garner insights from
there that will get us into maybe creating more network devices in your home, or maybe in figuring
out if we can try to push on to larger and larger groups of people, larger and larger, smaller and
more niche products. You see that the technology quickly grows and expands until it becomes
an all-consuming, it becomes plugged into part of this ever-consuming expansionary goal.
So we can note all that and then also talk about the racial elements that come with racial capitalism
and talk about how a lot of the times the surveillance technology is used to raise its
ends, it's biased against black and brown faces, and even if you correct the bias,
it's still going to be used by institutions, by the police departments, as an example,
by welfare agencies, as we've talked about in previous episodes, to discriminate against
people. Because in the real world, even outside of the corrected data, if you made it neutral
somehow, you still have a racist politics at the end of the day. The idea that you can just create
a piece of tech and debate about the rules that are inscribed on it inside of this narrow
application, ignoring the actual society in which it's being deployed in and the people who are
deploying it is stupid. And it's also cowardice and it's coming usually from people who are
compensated well enough to ignore that, as it is in the United States, as it is in Israel.
Because if you were in Israel and someone was like, oh, what's going on with all the things
that we're testing out on Palestinians? We would find it a little stupid if people were debating
about how high the apartheid border fences should be, or what kind of cameras should be put on them,
or whether the camera should have algorithmic facial recognition services or not, or what types
of labor regimes settlers should be subjecting Palestinians to, what type of schedules of work
they should be subjected to. All of this would be considered quibbling and over a fundamentally
immoral thing. And yet, or I would hope so, because some people don't actually even think
that there's anything wrong going on with the apartheid system. But nonetheless,
when we look here and then when some people look out there, they don't see anything wrong.
There's nothing wrong to them. And whenever you come across that as a case, it's usually because
of moral cowardice. It's because people who really think that when we say the word AI ethics, AI safety,
AI risk, in their heads, they're talking about the sci-fi shit. They're talking about God going
in a mech and killing everybody. They're talking about Skynet. They're not talking about here and
now where there's a fucking genocide going on, or where there's an apartheid going on, where there's
a welfare system going on. And it's a bit insane. It's a bit insane. It speaks to such a deep
moral collapse and rot that the nicest thing you can say about such an industry is that it's useless.
Because how do you have that much access to capital? You're soaked by that many corporations.
You're isolated and given enough access to academic resources and time and space to develop
them intellectually or synthesize them into some framework. And the best that we come up with
a lot of the times from these ethics, safety, and risks people is checking the boxes on diversity.
I don't know. It's just a fundamental failure. It's disgusting.
It is. And I think Khadijah is also dead on here as well, where she says in her essay,
if the room taken up with building individual brands lends itself to researching with those
most impacted, developing tactical initiatives like a social justice war room, we'd be in a
very different place. I think that's also a big part of it too. A lot of this is, unfortunately,
about building up your brand as the more ethical person in the room. It's not to say that people
don't care about this stuff. I'm not saying that. I don't think people are cynical and they don't
actually care beyond seeing it as an opportunity to build a brand. But we are reaching some limits
here where people have decided the trade off of speaking versus not speaking.
I can't take that trade off because it might irreparably damage my brand. It might damage
my ability to do more good in the future. We've got to think effectively about these altruistic
actions. What's the most good I can do as an activist in the future?
It's cowardice. It's a cowardice. I think it is also based in an ironic fetishism of the technology
itself. We might bang on about those people who fetishize the technology because they think it's
going to make God. It is God. It's going to end the world. It's going to bring utopia. They
fetishize the technology and say, tis, tis, tis. Don't fetishize the technology. At the end of the
day as well, they fetishize the technology. They're concerned about the ethics or morality
or politics or responsibility of AI. Hey, we've got to make it ethical. We've got to make the
technology responsible. Our whole analysis forever has always been a materialist one,
which is focused on not the technology itself but the systems that the technology emerges out of,
plugs into, plays a role in. We talk about the technology because we want to talk about
capital. We talk about the technology because we want to talk about fascism. We talk about
the technology because we want to talk about genocide. Those are the things we're actually
talking about. We're talking about the ways that the technology gets enrolled into perpetuating,
intensifying, justifying, covering up those other things that actually matter in the world.
I guess when the genocide is too out in your face, it's hard to talk about the AI.
Mm-hmm.
It's too much for me. This story, it hits so close to us as well because it is talking about
how these systems are being used. Ultimately, it doesn't matter, as we've talked about,
if the AI system is actually being used as an artificial intelligence to automate this,
or if it's being used as a movie prop to justify actions that the IDF already wants to take. It
doesn't matter because the material consequences at the end of the day are the same. Part of those
material consequences are getting a lot of people to not talk about it, to not pay attention to it.
It's not just justifying the action. It's not just directing the action. It is also getting a
lot of people to not pay attention to the action as well in whatever reason. If anything, a story
like this about Hapsara, about the gospel, if anything, this should be a clarion call for all
those people who are like, hey, Israel-Palestine conflict, that's not my job description. I study
technology. Well, here you go. Here's a technology to pay attention to. It should be bringing people
into the fold to be like, okay, fuck, all right, I study technology. Okay, I got to pay attention.
This matters, but it's done the opposite.
There's this line he has. It's in his The Taming of Tech Criticism piece, which every year
that passes by, his really condemnation and disgust in a very dry, Laconian way that reminds me of
Chomsky's own disgust with public intellectuals feels more and more right when he says that
disconnected from actual political struggles and social criticism, technology criticism is
just an elaborate but affirmative footnote to the status quo. In more than one way,
I think that has been the case, right? Because one of the things that I think has dominated,
and it's hard to even figure out, well, it's not hard. I mean, it's easy to talk about,
but it's also like, I think conservatives and right-wingers are pretty cynical in which it's
easy when you start talking about DEI, for example, and the ways in which a lot of people
have kind of slotted that or assumed that to be a social criticism. When in reality, it doesn't
matter what color the other faces at the table are at the end of the day, right? If the fundamental
system, well, take whoever's at the table and plug them into a system that still exploits
everyone else who looks a certain way outside of the table, right? You can have an AI ethics,
this call for, for example, for the OpenAI board to be half men and half women, to have POC on it.
What the fuck will that, what would that actually do though if we were being tech critics about
what OpenAI's designs and plans for artificial intelligence are, right?
What would that do to change Sam Altman's attempt to create a firm that has AI hardware
that every firm has to then rely on if they're interested in developing software?
And similarly, what would that mean or do for criticisms of Israel's apartheid state and
genocide? I mean, they let everyone serve in their military. Does that change the degree to which
they do war crimes? No. Would the AI ethics lens and focusing on this sort of diversity and inclusion
have any real possibility of impact? Would Elbit Systems be more humane in the weapons that it uses
against Palestinians if it had some Palestinians on the board? These seem a bit silly and maybe
unfair examples to use, but that is one of the places or one of the only places you end up going
for example, focusing on that DI element, right? If you expand it a little bit more broadly and
quibble about what's going on, as you said, a lot of people insist, well, it's a complicated issue,
that's political. I'm not really concerned about that. I'm just concerned about making sure there's
some sort of equity and outcome, right? But how are you going to achieve that if there's not an
actual political vision and struggle? I mean, part of the reason why Silicon Valley has been
so ascendant is because nobody seems to have another fucking vision, right? For what to
actually do with technology in our daily lives and how to integrate and organize our societies
around it or not around it, right? Where should technology be plugged in and where should it
shouldn't be? What types of weapons, what types of artificial intelligences, what types of systems
are we going to put a hard lock on and what types of systems will we ensure that other people can't
bypass when we have that hard lock on? What types of capital flows are we going to prohibit? What
types of business activity are we going to prohibit, right? None of these things really
bubble up, except in, of course, relatively radical segments, largely academics, right?
And activists who are able to articulate these visions, but will have them ignored by the
mainstream, right? Which is largely concerned with just build it perpetually until there's a
problem. And then when there's a problem, let the people who caused the problem write the rules
about how the problem should be handled, you know? And I worry, my greatest fucking worry with this
is that one, either people in that industry will see it in shrug, right? Or we'll use it
to do the brand building, right? That Khadija kind of talked about where they say, whoa, whoa,
you know, this is horrible. Not much we can do about it, but, you know, if we listen to my 10
steps, then we'd be able to prevent that happening in the United States. When the thing is,
you know, it's happening somewhere right now, you know? What, if anything, can we do about that?
I will not, I'm not going to pretend like I would even begin to really know, but the people who do
insist they know, and who pretend to know, and who create their whole identities and brands around
this have been silent and left us without anything. Yeah. And it is always that,
the exact what you just said there as well of like, you know, if we do these 10 steps,
then we can prevent this happening in the U.S., right? Like there is, so often there is this,
like, whether it's explicitly or it's just kind of beneath the surface, there is this kind of idea of
like, man, it'd be wild if that happened to us, you know? Like, I'm glad I can do some remote
criticism here. You know, I can take up my telescope and look abroad into exotic places
or foreign time periods and be like, man, it'd be wild if this happened, huh? But it is already
happening. Like, there is so much of that, that is the, like, it doesn't matter until you come.
Buy my mixtape.
But there is so much of that, like, of that, that like moral immediacy as well, where it doesn't
matter unless it's happening to you, or it doesn't matter unless you can conceive of it happening to
you. I mean, it's something we've talked about a ton before around like all of the AI doomerism
stuff. And, you know, it's the, you know, Sam Altman, Elon Musk, whoever, whatever, whatever,
right? The whole idea they have to create like an evil AI God that's going to exterminate the world
is because they are in such powerful and privileged positions that it's like literally
the only threat they can conceive of to themselves is an evil AI God who destroys everything in the
world, not just them, right? Everything in the world. And, you know, I think you see it a lot
in the way that people do talk about, like, AI ethics or the politics of AI or the consequences
of AI is, you know, so often it is about, like, the bad, hey, bad stuff could happen to other
people, or rather, what if the bad stuff happened to me, you know? And I think it is, you know,
this might be a good place to wrap it up. But I think it's also really telling as well where
you have a system like Hapsara, which its whole design is to generate Hamas targets for
assassination, right? And it has apparently an extremely broad definition of who is classified
as a Hamas operative or a relationship to Hamas. You know, it's like we were talking about before
where it's like, you know, as long as you can justify that there's like, you know, half of a
floor of a 10-story building was at one point related to Hamas, well, then you can take down
the whole building. I think a lot of people might be in for a real rude awakening, too, that, like,
you know, their idea that this activism is, you know, that they are doing a good thing
because they are caring about other, like, foreign people, you know, but except when they reach the
limits of care for foreign people or other time periods or other places or whatever it is that,
like, you know, they can't imagine that this stuff might come for them one day. And so that kind of
limits their ability to speak out about it, to rally against it, to draw attention to its,
you know, horrendous and atrocious actuality, right? Like, they can't imagine it might come
for them one day. Hey, I'm not a Hamas. I'm not a terrorist. I'm not Palestinian. I'm not a climate
refugee. You know, I'm not any of these things. I don't have to worry about that as much, you know?
But consider as well, it don't take long for all of a sudden those boundaries to get real loose
and real fluid and to start capturing a lot more people than you can think of. You know, I just
think of, like, all of the governments around the world, the UK, Australia, the US, like, all the
governments talking about these massive rallies for Palestinian rights, rallies for ceasefire,
rallies against the Israeli-led genocide. You know, just think about, for example, like,
high up people in the UK government, they're crying, the half million people marching,
the millions of people marching as all Hamas sympathizers, you know? Got a whole lot of Hamas
sympathizers all of a sudden, all these, you know, and it ain't just, you know,
cross-punk leftist out there. You know, if you've been out on the marches, it's just a lot of normal
middle-class families with their kids, you know, out there marching for Palestinian rights,
marching for a ceasefire, marching against genocide. You know, all of those people are
being painted as Hamas operatives or Hamas sympathizers. I mean, listen, it makes them,
it makes them a legitimate target for Habsara to spit out one day in that regard.
I have some, yeah, I should tweet out to the IDF. I have information about Donald Trump that might
make him a power target for you guys. May Allah awaken the people and help them to see the
evildoings of Israel and the United States. But yeah, no, I think you're right. All the data
Habsara needs to spit out Trump's name next. Trump Tower. It revealed to us that on the,
on the half of, in one room, in one corner office on the 67th floor, Trump sent out
a tweet that said, may Allah awaken the people, may Allah open the people's eyes and awaken them
to the evildoings of Israel and the United States. We got to take the whole thing down guys. I'm sorry.
Fair is fair. Yeah, I mean, this is, you know, we don't need to talk about like,
this is the danger of these systems, because the danger of these systems is already readily
apparent. It is clear and present, right? That like, you know, these systems,
and you know, are causing, right, whether they are causing it directly, they are causing it
in some kind of, you know, giving cover for it, whatever it might be, right? These systems are
already leading to the mass assassination, mass destruction, right? But it doesn't end there,
right? If you can't, if what it takes for someone to relate to this, to empathize, to sympathize,
to speak out against, to scream their head off about if what it takes is for you to conceive
that one day, it might not only affect exotic brown people, or it might not only be some,
you know, sci fi near future. If what it takes is for you to conceive of how you yourself might
get caught up in the system. It don't take a lot of imagination to see how that's not only possible,
but increasingly likely as we see governments act in more and more authoritarian ways,
use technology to justify and expand the powers that they have, use fascist policies and propaganda
to create targets to call people human animals. As the defensemen, as the Israeli defense minister
said after October 7th, quote, we are fighting human animals and we act accordingly.
To consider how that broad categorization of human animals is a fluid and dynamic category
that can begin to include a lot of people in a lot of different places under a lot of certain
circumstances. You don't have to think hard and you don't have to think long to understand how
these systems may not only come for you one day, they are already coming for you.
They are already coming for people who, you know, all around the world, they are already causing
and wreaking death and destruction. No, you're telling me that the IDF who trains with American
police departments might one day export the gospel, which is kind of, you know,
if it weren't so grim, it is kind of funny for them to export the gospel to the United States.
But, you know, that's besides the point. It's dark and grim, actually, right? You're telling
me that they would export that to America? Oh, man, unprecedented. Surely not something that,
you know, journalists like Anthony Loewenstein have written a whole book about. Surely not
something that we did a long episode with them about this whole very lucrative economy of exporting
the technologies of apartheid and genocide to the rest of the world. No, no, I can't imagine it,
Ed, which is why Aaron, I cannot imagine there and I do not speak. Right. Right.
Fucking hell. Well, I think I'm going to.
Don't worry, guys, the next episode will also be depressing.
Well, as long as the world keeps beating us down, we have to pass on those beatings to our dear
listeners. The negative project continues. You're absolutely right.
Thank you to our dear subscribers. And like I said, this episode will be released very shortly
after it goes up on the Patreon feed. So I will speak to you, all of our listeners who are
non-subscribers, please sign up and support the work that we do. It really does keep us going.
And doing these kinds of episodes, having these discussions. And so we appreciate that support,
as always. And more importantly, go fucking talk to people about this stuff. Help spread some
awareness, some outrage, some attention on these things that really need to be. I mean,
this shit is clearly, I think, just these are these are war crimes. They go against even our
completely inadequate liberal system of international law. These things violate that
to such an extreme degree in such an obvious way. Right. Like these are technologies that
do not deserve to exist and indeed should be outlawed, smashed in every way. These are weapons
of mass destruction, weapons of mass assassination. And as such, we need to talk about it. We need to
bring attention to it. We need to fight against it. We need to advocate against it, whatever it
takes. You know, but so with that, thank you everybody for listening. Thanks for your support.
And we will catch you next time later.
So
it's
you
