start	end	text
0	2720	The following is a conversation with Josje Bach,
2720	4960	his second time on the podcast.
4960	8560	Josje is one of the most fascinating minds in the world
8560	10640	exploring the nature of intelligence,
10640	14520	cognition, computation, and consciousness.
14520	17720	To support this podcast, please check out our sponsors,
17720	22720	Coinbase, Codecademy, Linode, NetSuite, and ExpressVPN.
23960	26760	Their links are in the description.
26760	29160	As usual, I'll do a few minutes of ads now,
29200	30720	no ads in the middle.
30720	32120	I try to make these interesting,
32120	35300	so hopefully you don't skip, but if you do,
35300	38880	please still check out the sponsors in the description.
38880	41400	It's the best way to support this podcast.
41400	45600	I use their stuff and enjoy it, so maybe you will too.
45600	48000	This show is brought to you by Coinbase,
48000	51120	which is a trusted and easy to use platform
51120	54220	to buy, sell, and spend cryptocurrency.
54220	56460	I use it and love it.
56500	60980	You can buy Bitcoin, Ethereum, Cardano, Dogecoin,
60980	62100	the list goes on.
62100	64620	All the most popular digital currencies.
64620	66940	It's the place I recommend to people all the time
66940	68820	if they're kind of curious about cryptocurrency,
68820	71660	so Coinbase just makes it super easy
71660	73940	to buy whatever cryptocurrency you want.
73940	78140	I think cryptocurrency space is very exciting,
78140	79820	so if you're missing out on it,
79820	81620	I think some percent of your portfolio
81620	84200	should be in cryptocurrency.
84560	86840	Investing in a little bit of cryptocurrency,
86840	91840	I think puts your money where your mind is,
92160	95960	so it forces you to kind of explore the space more and more.
95960	98200	If you find it interesting, you can always, of course,
98200	101440	invest more, but I do recommend people at least try it out.
101440	105680	Anyway, go to Coinbase.com slash Lex for a limited time.
105680	108860	New users can get $5 and free Bitcoin
108860	113860	when you, they sign up at Coinbase.com slash Lex.
114060	117580	That's Coinbase.com slash Lex.
117580	120700	This show is brought to you by Codecademy,
120700	123460	the website I highly recommend you go to
123460	125720	if you want to learn how to code.
125720	128180	It doesn't matter if you're totally new
128180	131580	or somewhat experienced, there's courses there for you.
131580	134660	For the beginner, for the intermediate, for the advanced,
134660	136380	I recommend you sign up
136380	139180	and take their Learn Python 3 course.
139180	142100	They say it takes 25 hours to complete,
142100	145420	but it is so clear, accessible, even fun,
145420	148260	that I think it actually will take less time,
148260	151580	but even if it takes that long, it'll just fly by.
151580	154300	It gives you the most important basics
154300	157820	and it delivers them in this very clear, concise way
157820	159540	that I really enjoy.
159540	162540	I think that's what a course on Python should be,
162540	164600	on the fundamentals of Python should be,
164600	167780	so if you're curious to learn how to program,
167780	170360	I think Python is the right language to start with.
170360	172480	If you're curious to learn Python,
172480	175240	learn Python 3 at Codecademy.
175240	178340	That's definitely the go-to course I would recommend.
179360	182600	You can get 15% off your Codecademy Pro membership
182600	187120	when you go to codecademy.com and use promo code lex.
187120	191280	That's promo code lex at codecademy.com
191280	194060	to get 15% off Codecademy Pro,
194060	196940	the best way to learn to code.
196940	199560	This episode is sponsored by Linode,
199560	201480	Linux Virtual Machines.
201480	204160	It's an awesome compute infrastructure
204160	206560	that lets you develop, deploy, and scale
206560	209720	what applications you build faster and easier.
209720	214120	This is both for small personal projects and huge systems.
214120	216780	Lower cost than AWS,
216780	220360	but more important to me is the simplicity
220360	223380	and the quality of the customer service
223380	228380	with real human beings 24-7 every single day,
228460	230720	365 days a year.
230720	233140	I could, of course, talk a little bit about
234180	236740	the things I don't like about some of the big players
236740	238500	that Linode is competing with,
238500	241400	but it's probably better to focus on the good things
241400	244100	that Linode is doing in contrast to them.
244100	245740	Again, like I said, customer service
245740	249600	and just the simplicity of the interface
249600	252700	and how to set everything up and how to use it.
252700	254820	Their motto, of course, I love Linux.
254820	258260	Their motto is if it runs on Linux, it runs on Linode.
259220	261600	Visit linode.com slash Lex
261600	265420	and click on the Create Free Account button to get started.
265420	269340	And if you do, you get $100 in free credit.
269340	272140	That's linode.com slash Lex.
273100	275920	This show is sponsored by NetSuite.
275920	277780	Running a business is hard.
277780	280740	If you own a business, don't let QuickBooks and spreadsheets
280740	283160	make it even harder than it needs to.
283160	285460	You should consider upgrading to NetSuite.
285460	289060	It allows you to manage financials, HR, inventory,
289060	292640	e-commerce, and many more business-related details
292640	294000	all in one place.
295480	297040	I've probably talked about this before,
297040	299080	but I dislike bureaucracy
299080	301760	that companies sometimes build up around this.
301760	303440	NetSuite can probably help.
304320	305280	It's always really important
305280	307120	to have the right tool for the job.
307120	308960	Anyway, whether you're doing a million
308960	311280	or hundreds of millions in revenue,
311280	314440	save time and money with NetSuite.
314440	316940	24,000 companies use it.
316940	320280	Let NetSuite show you how they'll benefit your business
320280	323700	with a free product tour at netsuite.com slash Lex.
323700	326320	If you own a business, try them out.
326320	328440	Schedule your free product tour right now
328440	333440	at netsuite.com slash Lex, netsuite.com slash Lex.
333880	337200	This show is sponsored by ExpressVPN.
337200	339360	I use them, have used them for many years
339360	342160	to protect my privacy on the internet.
342160	345000	Obviously, as I've mentioned many times before,
345000	348200	ISPs don't necessarily have your best interest in mind
348200	349760	when it comes to your data.
349760	353320	So VPN can really help protect your data.
353320	355200	Another aspect that's very useful
355200	357760	is basically with a VPN,
357760	361300	you can specify your location to be anywhere.
361300	364960	So from the perspective of services online,
364960	367480	it can appear as if you're anywhere in the world.
367480	368720	And this is useful
368720	372040	like if you go to Netflix to watch shows
372040	375360	that are only available in certain parts of the world.
375360	378320	It's a way to travel without actually having to get up
378320	379960	from your computer desk.
379960	381040	It's brilliant.
381040	384000	Speaking of computer desk, it's not just about computers.
384000	388920	It's all devices, all operating systems, including Linux.
388920	392480	Plus the ExpressVPN design itself is really nice.
392480	395200	There's a one power on button that says not connected
395200	396840	when you're not connected and connected
396840	398360	when you're connected.
398360	400160	Super easy to use, super fast.
400160	404320	Like I said, go to expressvpn.com slash lexpod
404320	406680	to get an extra three months free.
406680	410540	That's expressvpn.com slash lexpod.
410540	412760	This is the Lex Friedman podcast.
412760	416160	And here is my conversation with Josje Bach.
417040	441120	Thank you for once again coming on to this particular Russian program and sticking to
441120	443120	the theme of a Russian program.
443120	445320	Let's start with the darkest of topics.
445400	447400	Privet.
447400	450600	So this is inspired by one of your tweets.
450600	455920	You wrote that, quote, when life feels unbearable,
455920	458760	I remind myself that I'm not a person.
458760	462240	I am a piece of software running on the brain of a random ape
462240	463640	for a few decades.
463640	466720	It's not the worst brain to run on.
466720	469840	Have you experienced low points in your life?
469840	471920	Have you experienced depression?
471920	474280	Of course, we all experience low points in our life.
474280	479200	And we get appalled by the things, by the ugliness of stuff around us.
479200	483400	We might get desperate about our lack of self-regulation.
483400	486680	And sometimes life is hard.
486680	490040	And I suspect you don't get to your life.
490040	495000	Nobody does to get through their life without low points and without moments where they
495000	495840	are despairing.
495840	502280	And I thought that let's capture this state and how to deal with that state.
502280	507040	And I found that very often you realise that when you stop taking things personally,
507040	512840	when you realise that this notion of a person is a fiction, similar as it is in Westworld,
512840	517080	where the robots realise that their memories and desires are just stuff that keeps them
517080	521280	in the loop and they don't have to act on those memories and desires.
521280	524680	That our memories and expectations is what make us unhappy.
524680	526360	And the present rarely does.
526360	530600	The day in which we are, for the most part, it's OK, right?
530760	535400	When we are sitting here, right here, right now, we can choose how we feel.
535400	541080	And the thing that affects us is the expectation that something is going to be different from
541080	546520	what we wanted to be or the memory that something was different from what you wanted it to be.
546520	551320	And once we basically zoom out from all this, what's left is not a person.
551320	555920	What's left is this state of being conscious, which is the software state.
555920	558000	And software doesn't have an identity.
558000	560080	It's a physical law.
560080	564560	And it's a law that acts in all of us and it's embedded in a suitable substrate.
564560	566040	And we didn't pick that substrate, right?
566040	569200	We are mostly randomly instantiated on it.
569200	574040	And they're all these individuals and everybody has to be one of them.
574040	578680	And eventually you're stuck on one of them and have to deal with that.
578680	581480	So you're like a leaf floating down the river.
581480	586160	You just have to accept that there's a river and you just float wherever it takes you.
586160	587280	You don't have to do this.
587280	591920	The thing is that the illusion that you are an agent is a construct.
591920	595640	What part of that is actually under your control?
595640	600860	And I think that our consciousness is largely a control model for our own attention.
600860	605320	So we notice where we are looking and we can influence what we are looking, how we are
605320	609120	disambiguating things, how we put things together in our mind.
609120	613280	And the whole system that runs us is this big cybernetic motivational system.
613280	617320	So we're basically like a little monkey sitting on top of an elephant.
617320	621820	And we can prod this elephant here and there to go this way or that way.
621820	625280	And we might have the illusion that we are the elephant or that we are telling it what
625280	626280	to do.
626280	629820	And sometimes we notice that it walks into a completely different direction.
629820	634840	And we didn't set this thing up, it just is the situation that we find ourselves in.
634840	638680	How much prodding can we actually do of the elephant?
638680	639760	A lot.
639760	645320	But I think that our consciousness cannot create the motive force.
645320	647640	Is the elephant consciousness in this metaphor?
647640	650200	No, the monkey is the consciousness.
650200	652600	The monkey is the attentional system that is observing things.
652600	657640	There is a large perceptual system combined with the motivational system that is actually
657640	659640	providing the interface to everything.
659640	664760	And our own consciousness, I think, is the tool that directs the attention of that system,
664760	669780	which means it singles out features and performs conditional operations for which it needs
669780	671260	an index memory.
671260	675040	But this index memory is what we perceive as our stream of consciousness.
675040	677240	But the consciousness is not in charge.
677240	678240	That's an illusion.
678240	683680	So everything outside of that consciousness is the elephant.
683680	688320	So it's the physics of the universe, but it's also society that's outside of your...
688320	690620	I would say the elephant is the agent.
690620	695620	So there is an environment in which the agent is stomping, and you are influencing a little
695620	697380	part of that agent.
697380	702340	So is the agent a single human being?
702340	704700	Which object has agency?
704700	705980	That's an interesting question.
705980	712780	I think a way to think about an agent is that it's a controller with a set point generator.
712780	717220	The notion of a controller comes from cybernetics and control theory.
717220	724460	The control system consists out of a system that is regulating some value and the deviation
724460	726300	of that value from a set point.
726300	732260	And it has a sensor that measures the system's deviation from that set point and an effector
732260	734920	that can be parameterized by the controller.
734920	737880	So the controller tells the effector to do a certain thing.
737880	742660	And the goal is to reduce the distance between the set point and the current value of the
742660	743660	system.
743660	746560	And there's an environment which disturbs the regulated system, which brings it away
746560	747900	from that set point.
747900	750160	So simplest case is the thermostat.
750160	752520	The thermostat is really simple because it doesn't have a model.
752520	758280	The thermostat is only trying to minimize the set point deviation in the next moment.
758280	762360	And if you want to minimize the set point deviation over a longer time span, you need
762360	763360	to integrate it.
763360	766000	You need to model what is going to happen.
766000	770680	So for instance, when you think about that your set point is to be comfortable in life,
770680	773680	maybe you need to make yourself uncomfortable first.
773680	776240	So you need to make a model of what's going to happen when.
776240	781680	And this is the task of the controller is to use its sensors to measure the state of
781680	787320	the environment and the system that is being regulated and figure out what to do.
787320	793440	And if the task is complex enough, the set points are complicated enough, and if the
793440	798520	controller has enough capacity and enough sensor feedback, then the task of the controller
798520	802200	is to make a model of the entire universe that it's in, the conditions under which
802320	804680	it exists and of itself.
804680	808160	And this is a very complex agent, and we are in that category.
808160	811760	And an agent is not necessarily a thing in the universe.
811760	816880	It's a class of models that we use to interpret aspects of the universe.
816880	821760	And when we notice the environment around us, a lot of things only make sense at the
821760	825760	level that you're entangled with them if we interpret them as control systems that make
825760	829320	models of the world and try to minimize their own set points.
829480	832680	So the models are the agents.
832680	836800	The agent is a class of model, and we notice that we are an agent ourself.
836800	840960	We are the agent that is using our own control model to perform actions.
840960	845600	We notice we would use a change in the model and things in the world change.
845600	849840	And this is how we discover the idea that we have a body, that we are situated
849840	853280	environment and that we have a first person perspective.
853280	855480	Still don't understand.
855560	862280	What's the best way to think of which object has agency with respect to human beings?
862280	863760	Is it the body?
863760	865640	Is it the brain?
865640	868200	Is it the contents of the brain as agency?
868200	871240	Like, what's the actuators that you're referring to?
871240	874320	What is the controller and where does it reside?
874320	876240	Or is it these impossible things?
876240	881960	Because I keep trying to ground it to space time, the three dimension of space and the
881960	883800	one dimension of time.
883800	886760	What's the agent in that for humans?
886760	888240	There is not just one.
888240	892280	It depends on the way in which you're looking at this thing and which you're framing it.
892280	898920	Imagine that you are, say, Angela Merkel and you are acting on behalf of Germany.
898920	902040	Then you could say that Germany is the agent.
902040	907360	And in the mind of Angela Merkel, she is Germany to some extent, because in the way in which
907360	910280	she acts, the destiny of Germany changes.
910280	914880	There are things that she can change that basically affect the behavior of that nation
914880	915880	state.
915880	922480	OK, so it's hierarchies of to go to another one of your tweets with I think you were playfully
922480	928200	mocking Jeff Hawkins with saying his brains all the way down.
928200	930960	So it's like it's agents all the way down.
930960	934120	It's agents made up of agents, made up of agents.
934120	939440	Like if Angela Merkel is Germany and Germany's made up a bunch of people and the people are
939480	945760	themselves agents in some kind of context, and then people are made up of cells, each
945760	947120	individual.
947120	949400	So is it agents all the way down?
949400	955080	I suspect that has to be like this in a world where things are self-organizing.
955080	960680	Most of the complexity that we are looking at, everything in life is about self-organization.
960680	969240	So I think up from the level of life, you have agents and below life, you rarely have
969240	974240	agents because sometimes you have control systems that emerge randomly in nature and
974240	976080	try to achieve a set point.
976080	979000	But they're not that interesting agents that make models.
979000	983120	And because to make an interesting model of the world, you typically need a system that
983120	984120	is true and complete.
984120	988560	Can I ask you a personal question?
988560	991160	What's the line between life and non-life?
991160	994560	It's personal because you're a life form.
995080	999640	What do you think in this emerging complexity, at which point does the thing start being
999640	1001360	living and have agency?
1001360	1006920	Personally, I think that the simplest answer is that life is cells.
1006920	1007920	Life is what?
1007920	1008920	Cells.
1008920	1009920	Cells.
1009920	1010920	Biological cells.
1010920	1014520	So it's a particular kind of principle that we have discovered to exist in nature.
1014520	1021400	It's modular stuff that consists out of basically this DNA tape with a read-write head on top
1021400	1027960	of it that is able to perform arbitrary computations and state transitions within the cell.
1027960	1033280	And it's combined with a membrane that insulates the cell from its environment.
1033280	1039000	And there are chemical reactions inside of the cell that are in disequilibrium.
1039000	1044120	And the cell is running in such a way that this disequilibrium doesn't disappear.
1044120	1048880	And if the cell goes into an equilibrium state, it dies.
1048920	1054560	And it requires something like a neck entropy extractor to maintain this disequilibrium.
1054560	1060600	So it's able to harvest neck entropy from its environment and keep itself running.
1060600	1066760	So there's information and there's a wall to maintain this disequilibrium.
1066760	1069320	But isn't this very earth-centric?
1069320	1071160	Like what you're referring to as a...
1071160	1073360	I'm not making a normative notion.
1073360	1077560	You could say that there are probably other things in the universe that are cell-like
1077560	1079040	and lifelike.
1079040	1080480	And you could also call them life.
1080480	1086320	But eventually it's just a willingness to find an agreement of how to use the terms.
1086320	1091520	I like cells because it's completely co-extensional with the way that we use the word even before
1091520	1092920	we knew about cells.
1092920	1097240	So people were pointing at some stuff and saying, this is somehow animate and this is
1097240	1099120	very different from the non-animate stuff.
1099120	1102680	And what's the difference between the living and the dead stuff?
1102680	1105480	And it's mostly whether the cells are working or not.
1105480	1110320	And also this boundary of life where we say that, for instance, a virus is basically an
1110320	1115040	information packet that is subverting the cell and not life by itself.
1115040	1116800	That makes sense to me.
1116800	1118320	And it's somewhat arbitrary.
1118320	1122600	You could of course say that systems that permanently maintain a disequilibrium and
1122600	1125960	can self-replicate are always life.
1125960	1129000	And maybe that's a useful definition too.
1129000	1132880	But this is eventually just how you want to use the word.
1132880	1139520	Is it so useful for conversation, but is it somehow fundamental to the universe?
1139520	1143800	Do you think there's an actual line to eventually be drawn between life and non-life?
1143800	1146640	Or is it all a kind of continuum?
1146640	1150800	I don't think it's a continuum, but there's nothing magical that is happening.
1150800	1153640	Living systems are a certain type of machine.
1153640	1155440	What about non-living systems?
1155440	1156800	Is it also a machine?
1156800	1162600	There are non-living machines, but the question is at which point is the system able to perform
1162600	1167680	arbitrary state transitions to make representations?
1167680	1169280	And living things can do this.
1169280	1172600	And of course, we can also build non-living things that can do this.
1172600	1178040	But we don't know anything in nature that is not a cell and is not created by still
1178040	1182640	alive that is able to do that.
1182640	1185240	Not only do we not know.
1185240	1188320	I don't think we have the tools to see otherwise.
1188320	1194600	I always worry that we look at the world too narrowly.
1194600	1200440	There could be life of a very different kind right under our noses that we're just not
1200440	1207440	seeing because we're not either limitations of our cognitive capacity or we're just not
1207440	1213960	open-minded enough, either with the tools of science or just the tools of our mind.
1214360	1215360	Yeah, that's possible.
1215360	1217600	I find this thought very fascinating.
1217600	1222320	And I suspect that many of us ask ourselves since childhood, what are the things that
1222320	1223320	we are missing?
1223320	1230320	What kind of systems and interconnections exist that are outside of our gaze?
1230320	1240080	But we are looking for it and physics doesn't have much room at the moment for opening up
1240160	1245640	something that would not violate the conservation of information as we know it.
1245640	1253600	Yeah, but I wonder about time scale and scale, spatial scale, whether we just need to open
1253600	1257640	up our idea of how life presents itself.
1257640	1262360	It could be operating in a much slower time scale, a much faster time scale.
1262360	1267680	And it's almost sad to think that there's all this life around us that we're not seeing
1267680	1275160	because we're just not thinking in terms of the right of the right scale, both time
1275160	1276640	and space.
1276640	1278400	What is your definition of life?
1278400	1279640	What do you understand as life?
1283040	1288240	Entities of sufficiently high complexity that are full of surprises.
1288240	1294920	I don't know.
1294920	1297840	I don't have a free will, so that just came out of my mouth.
1297840	1299680	I'm not sure that even makes sense.
1299680	1301600	There's certain characteristics.
1301600	1306920	So complexity seems to be a necessary property of life.
1306920	1315680	And I almost want to say it has ability to do something unexpected.
1315680	1322000	It seems to me that life is the main source of complexity on earth.
1322000	1330020	And complexity is basically a bridgehead that order builds into chaos by modeling, by processing
1330020	1334960	information in such a way that you can perform reactions that would not be possible for dump
1334960	1335960	systems.
1335960	1340120	And this means that you can harvest neck entropy that dump systems cannot harvest.
1340120	1342520	And this is what complexity is mostly about.
1343480	1347200	In some sense, the purpose of life is to create complexity.
1347200	1348200	Yeah.
1348200	1349200	Increasing.
1349200	1358360	I mean, there seems to be some kind of universal drive towards increasing pockets of complexity.
1358360	1359920	I don't know what that is.
1359920	1364680	That seems to be like a fundamental, I don't know if it's a property of the universe or
1364680	1370840	it's just a consequence of the way the universe works, but there seems to be this small pockets
1370840	1375680	of emergent complexity that builds on top of each other and starts having like greater
1375680	1380440	and greater complexity by having like a hierarchy of complexity.
1380440	1385040	Little organisms building up a little society that then operates almost as an individual
1385040	1386400	organism itself.
1386400	1390120	And all of a sudden you have Germany and Merkel.
1390120	1391400	That's not obvious to me.
1391400	1394760	Everything that goes up has to come down at some point, right?
1394760	1400760	So if you see this big exponential curve somewhere, it's usually the beginning of an S curve.
1401480	1405520	Whereas something eventually reaches saturation and the S curve is the beginning of some kind
1405520	1407960	of bump that goes down again.
1407960	1416240	And there is just this thing that when you are inside of an evolution of life, you are
1416240	1421240	on top of a puddle of negentropy that is being sucked dry by life.
1421240	1426300	And during that happening, you see an increase in complexity because life forms are competing
1426300	1433420	with each other to get more and more and finer and finer corner of that negentropy extraction.
1433420	1439060	But I feel like that's a gradual, beautiful process that almost follows a process akin
1439060	1440500	to evolution.
1440500	1444740	And the way it comes down is not the same way it came up.
1444740	1449760	The way it comes down is usually harshly and quickly.
1449760	1452900	So usually there's some kind of catastrophic event.
1452900	1455460	The Roman Empire took a long time.
1456300	1461820	But would you classify that as a decrease in complexity though?
1461820	1462820	Yes.
1462820	1467140	I think that the size of the cities that could be fed has decreased dramatically.
1467140	1472460	And you could see that the quality of the art decreased and it did so gradually.
1472460	1478220	And maybe future generations, when they look at the history of the United States in the
1478220	1484140	21st century, will also talk about the gradual decline, not something that suddenly happens.
1484140	1490060	Do you have a sense of where we are?
1490060	1492060	Are we on the exponential rise?
1492060	1498140	Are we at the peak or are we at the downslope of the United States Empire?
1498140	1503860	It's very hard to say from a single human perspective, but it seems to me that we are
1503860	1507620	probably at the peak.
1507620	1511980	I think that's probably the definition of optimism and cynicism.
1511980	1518820	So my nature of optimism is, I think we're on the rise.
1518820	1521860	I think that's just all a matter of perspective.
1521860	1522860	Nobody knows.
1522860	1529620	But I do think that erring on the side of optimism, you need a minimum number of optimists
1529620	1533260	in order to make that up thing actually work.
1533260	1535820	And so I tend to be on the side of the optimists.
1535820	1541180	I think that we are basically a species of grasshoppers that have turned into locusts.
1541380	1546500	And when you are in the locust mode, you see an amazing rise of population numbers and
1546500	1551140	of the complexity of the interactions between the individuals.
1551140	1554780	But ultimately, the question is, is it sustainable?
1554780	1562660	See, I think we're a bunch of lions and tigers that have become domesticated cats, to use
1562660	1563660	a different metaphor.
1563660	1566580	And so I'm not exactly sure we're so destructive.
1566580	1570100	We're just softer and nicer and lazier.
1570220	1572180	I think we have monkeys and not the cats.
1572180	1575820	And if you look at the monkeys, they are very busy.
1575820	1578300	The ones that have a lot of sex, those monkeys?
1578300	1579300	Not just the bonobos.
1579300	1582900	I think that all the monkeys are basically a discontent species that always needs to
1582900	1583900	meddle.
1583900	1589500	Well, the gorillas seem to have a little bit more of a structure, but it's a different
1589500	1592020	part of the tree.
1592020	1598180	OK, you mentioned the elephant and the monkey riding the elephant.
1598980	1602700	And consciousness is the monkey.
1602700	1605460	And there's some prodding that the monkey gets to do.
1605460	1608340	And sometimes the elephant listens.
1608340	1612820	I heard you got into some contentious, maybe you can correct me, but I heard you got into
1612820	1616180	some contentious free will discussions.
1616180	1618540	Is this with Sam Harris or something like that?
1618540	1621020	Not that I know of.
1621020	1627820	Some people on Clubhouse told me you made a bunch of big debate points about free will.
1627820	1634580	Well, let me just then ask you, where in terms of the monkey and the elephant, do you think
1634580	1637620	we land in terms of the illusion of free will?
1637620	1640780	How much control does the monkey have?
1640780	1645500	We have to think about what the free will is in the first place.
1645500	1646500	We are not the machine.
1646500	1648820	We are not the thing that is making the decisions.
1648900	1652300	We are a model of that decision-making process.
1652300	1658460	And there is a difference between making your own decisions and predicting your own decisions.
1658460	1662180	And that difference is the first-person perspective.
1662180	1669420	And what basically makes decision-making and the conditions of free will distinct from
1669420	1675020	just automatically doing the best thing is that we often don't know what the best thing
1675020	1676020	is.
1676020	1677380	We make decisions under uncertainty.
1677540	1682100	We make informed bets using a betting algorithm that we don't yet understand because we haven't
1682100	1684340	reverse engineered our own minds sufficiently.
1684340	1685940	We don't know the expected rewards.
1685940	1689220	We don't know the mechanism by which we estimate the rewards and so on.
1689220	1690340	But there is an algorithm.
1690340	1697860	We observe ourselves performing where we see that we weight facts and factors and the future,
1697860	1703620	and then some kind of possibility, some motive gets raised to an intention.
1703620	1705860	And that's informed bet that the system is making.
1706580	1710820	And that making of the informed bet, the representation of that is what we call free will.
1711540	1716420	And it seems to be paradoxical because we think that the crucial thing is that it's
1716420	1717620	somehow indeterministic.
1718580	1721140	And yet, if it was indeterministic, it would be random.
1722420	1726900	And it cannot be random because if it was random, if just dice were being thrown and
1726900	1730340	the universe randomly forces you to do things, it would be meaningless.
1730340	1734020	So the important part of the decisions is always the deterministic stuff.
1734740	1738900	But it appears to be indeterministic to you because it's unpredictable.
1738900	1743460	Because if it was predictable, you wouldn't experience it as a free will decision.
1743460	1746740	You would experience it as just doing the necessary right thing.
1747620	1753220	And you see this continuum between the free will and the execution of automatic behavior
1753780	1755220	when you're observing other people.
1755220	1759700	So for instance, when you are observing your own children, if you don't understand them,
1759780	1765380	you will abuse this agent model where you have an agent with a set point generator.
1765380	1769460	And the agent is doing the best it can to minimize the difference to the set point.
1769460	1774820	And it might be confused and sometimes impulsive or whatever, but it's acting on its own free will.
1775380	1780660	And when you understand what happens in the mind of the child, you see that it's automatic.
1780660	1782420	And you can outmodel the child.
1782420	1786740	You can build things around the child that will lead the child to making exactly the
1786740	1788180	decision that you are predicting.
1788820	1792500	And under these circumstances, like when you were a stage magician
1792500	1797300	or somebody who is dealing with people that you sell a car to,
1797300	1801300	and you completely understand the psychology and the impulses and the space of
1801300	1803620	thoughts that this individual can have at that moment.
1803620	1806420	Under these circumstances, it makes no sense to attribute free will
1808100	1810340	because it's no longer decision-making under uncertainty.
1810340	1811300	You are already certain.
1811300	1813940	For them, there's uncertainty, but you already know what they're doing.
1815780	1816820	But what about for you?
1818180	1825780	So is this akin to systems like cellular automata where it's deterministic,
1826340	1828740	but when you squint your eyes a little bit,
1829940	1834340	it starts to look like there's agents making decisions at the higher,
1835940	1840980	when you zoom out and look at the entities that are composed by the individual cells,
1841540	1850340	even though there's underlying simple rules that make the system evolve in deterministic ways,
1850340	1853700	it looks like there's organisms making decisions.
1853700	1858420	Is that where the illusion of free will emerges, that jump in scale?
1859540	1863460	It's a particular type of model, but this jump in scale is crucial.
1863460	1866580	The jump in scale happens whenever you have too many parts to count
1866580	1868580	and you cannot make a model at that level.
1868660	1873620	You try to find some higher-level regularity, and the higher-level regularity is a pattern
1873620	1879060	that you project into the world to make sense of it, and agency is one of these patterns.
1879060	1884260	You have all these cells that interact with each other, and the cells in our body are set up in
1884260	1889620	such a way that they benefit if their behavior is coherent, which means that they act as if
1889620	1891060	they were serving a common goal.
1892340	1896980	That means that they will evolve regulation mechanisms that act as if they were serving
1896980	1898020	a common goal.
1898020	1902180	Now you can make sense of all these cells by projecting the common goal into them.
1903140	1904900	For you, then, free will is an illusion?
1905940	1908180	No, it's a model, and it's a construct.
1908900	1912100	It's basically a model that the system is making of its own behavior.
1912100	1915300	It's the best model that it can come up with under the circumstances,
1915300	1918900	and it can get replaced by a different model, which is automatic behavior,
1918900	1921700	when you fully understand the mechanism under which you are acting.
1921700	1925060	Yeah, but another word for model is what?
1925140	1925620	Story.
1926420	1927860	So it's the story you're telling.
1927860	1929940	I mean, do you actually have control?
1929940	1935300	Is there such a thing as a you, and is there such a thing as you having control?
1936820	1944100	Are you manifesting your evolution as an entity?
1944100	1947700	In some sense, the you is the model of the system that is in control.
1947700	1951940	It's a story that the system tells itself about somebody who is in control.
1952420	1952980	Yeah.
1952980	1956740	And the contents of that model are being used to inform the behavior of the system.
1958980	1959780	Okay.
1959780	1964740	So the system is completely mechanical, and the system creates that story like a loom,
1965300	1969540	and then it uses the contents of that story to inform its actions
1969540	1973220	and writes the results of that actions into the story.
1973220	1974740	So how is that not an illusion?
1975380	1981540	The story is written, then, or rather, we're not the writers of the story.
1982100	1984660	Yes, but we always knew that.
1986100	1987380	No, we don't know that.
1987380	1988740	When did we know that?
1988740	1991300	I think that's mostly a confusion about concepts.
1991300	1997140	The conceptual illusion in our culture comes from the idea that we live in physical reality
1997700	2000820	and that we experience physical reality and that we have ideas about it.
2001540	2004180	And then you have this dualist interpretation where you have
2004820	2010980	two substances, res extensa, the world that you can touch and that is made of extended things,
2010980	2013140	and res cogitans, which is the world of ideas.
2013700	2016020	And in fact, both of them are mental representations.
2016660	2019940	One is the representations of the world as a game engine
2019940	2023140	that your mind generates to make sense of the perceptual data.
2023140	2024340	That's the physical world?
2024340	2026500	Yes, that's what we perceive as the physical world.
2026500	2029060	But we already know that the physical world is nothing like that, right?
2029060	2033300	Quantum mechanics is very different from what you and me perceive as the world.
2033300	2036180	The world that you and me perceive is a game engine.
2036180	2036820	Yeah.
2036820	2039220	And there are no colors and sounds in the physical world.
2039220	2042100	They only exist in the game engine generated by your brain.
2042100	2046820	And then you have ideas that cannot be mapped onto extended regions, right?
2046820	2051540	So the objects that have a spatial extension in the game engine are res extensa,
2051540	2056020	and the objects that don't have a physical extension in the game engine are ideas.
2056580	2060260	And they both interact in our mind to produce models of the world.
2060260	2068340	Yep. But when you play video games, I understand that what's actually happening is zeros and ones
2068340	2079380	inside of a computer, inside of a CPU and a GPU, but you're still seeing the rendering of that.
2080180	2085780	And you're still making decisions whether to shoot, to turn left, or to turn right,
2085780	2086740	if you're playing a shooter.
2087540	2090420	Every time I start thinking about Skyrim and Elder Scrolls
2090420	2092980	and walking around in beautiful nature and swinging a sword.
2092980	2096260	But it feels like you're making decisions inside that video game.
2097140	2103300	So even though you don't have direct access in terms of perception to the bits,
2103300	2107060	to the zeros and ones, it still feels like you're making decisions.
2107060	2111060	And your decisions are actually feels like they're being applied
2112260	2114180	all the way down to the zeros and ones.
2114180	2114420	Yes.
2114420	2118660	It feels like you have control even though you don't have direct access to reality.
2118660	2122660	So there is basically a special character in the video game that is being created by
2122660	2123620	the video game engine.
2124180	2126740	And this character is serving the aesthetics of the video game.
2127700	2128420	And that is you.
2129220	2132100	Yes. But I feel like I have control inside the video game.
2133620	2136900	All those 12-year-olds that kick my ass on the internet.
2137540	2142020	So when you play the video game, it doesn't really matter that there's zeros and ones.
2142020	2143780	You don't care about the bits of the bus.
2143780	2146500	You don't care about the nature of the CPU that it runs on.
2146500	2149860	What you care about are the properties of the game that you're playing.
2149860	2151540	And you hope that the CPU is good enough.
2152180	2152500	Yes.
2152500	2155380	And a similar thing happens when we interact with physics.
2155380	2158020	The world that you and me are in is not the physical world.
2158020	2160260	The world that you and me are in is a dream world.
2161700	2163620	How close is it to the real world though?
2165540	2169620	We know that it's not very close, but we know that the dynamics of the dream world
2169620	2173060	match the dynamics of the physical world to a certain degree of resolution.
2173060	2175300	But the causal structure of the dream world is different.
2177300	2180180	So you see, for instance, waves crashing on your feet, right?
2180180	2181460	But there are no waves in the ocean.
2181460	2183540	There's only water molecules that have
2183540	2189460	tangents between the molecules that are the result of electrons
2189460	2192100	in the molecules interacting with each other.
2192100	2194100	Aren't they very consistent?
2194660	2197780	You're seeing a very crude approximation.
2197780	2204980	Isn't our dream world very consistent to the point of being mapped directly one-to-one
2204980	2209700	to the actual physical world as opposed to us being completely tricked?
2210020	2211300	This is like where you have Donald-
2211300	2212980	It's not a trick. That's my point.
2212980	2215780	It's not an illusion. It's a form of data compression.
2215780	2218980	It's an attempt to deal with the dynamics of too many parts to count
2218980	2222740	at the level at which we're entangled with the best model that you can find.
2222740	2228180	Yeah. So we can act in that dream world and our actions have impact in the real world,
2228180	2230660	in the physical world to which we don't have access.
2230660	2235220	Yes. But it's basically like accepting the fact that the software that we live in,
2235220	2239220	the dream that we live in is generated by something outside of this world that
2239220	2240100	you and me are in.
2240100	2243540	So is the software deterministic and do we not have any control?
2244260	2250580	Do we have? Free will is having a conscious being.
2251700	2254660	Free will is the monkey being able to steer the elephant.
2257460	2262580	No, it's slightly different. Basically, in the same way as you are modeling
2262580	2267140	the water molecules in the ocean that engulf your feet when you are walking on the beach
2267140	2271860	as waves and there are no waves, but only the atoms and more complicated stuff
2271860	2277300	underneath the atoms and so on. And you know that, right? You would accept, yes,
2277300	2281460	there is a certain abstraction that happens here. It's a simplification of what happens
2281460	2286340	and the simplification that is designed in such a way that your brain can deal with it
2286340	2291780	temporarily and spatially in terms of resources and tuned for the predictive value. So you can
2291780	2295380	predict with some accuracy whether your feet are going to get wet or not.
2295380	2299620	But it's a really good interface and approximation.
2302420	2306580	Equations are a good approximation or they're a much better approximation.
2307940	2313940	So to me, waves is a really nice approximation of what all the complexity is happening underneath.
2313940	2317940	Basically, it's a machine learning model that is constantly tuned to minimize surprises. So it
2317940	2321780	basically tries to predict as well as it can what you're going to perceive next.
2321780	2327140	Are we talking about which is the machine learning? Our perception system or the dream world?
2328820	2333220	Dream world is the result of the machine learning process of the perception system.
2333220	2334340	That's doing the compression.
2334340	2341540	Yes. And the model of you as an agent is not a different type of model. It's a different type,
2341540	2348740	but not different in its model-like nature from the model of the ocean. Some things are oceans,
2348740	2353700	some things are agents. And one of these agents is using your own control model,
2353700	2359380	the output of your model, the things that you perceive yourself as doing. And that is you.
2360340	2363940	What about the fact that when you're standing
2366180	2374180	with the water on your feet and you're looking out into the vast open water of the ocean and
2374260	2379700	then there's a beautiful sunset and the fact that it's beautiful and then maybe you have
2379700	2384820	friends or a loved one with you and you feel love. What is that? As the dream world? What is that?
2384820	2387700	Yes. It's all happening inside of the dream.
2387700	2392500	Okay. But see, the word dream makes it seem like it's not real.
2393540	2399860	Of course it's not real. The physical universe is real, but the physical universe is
2399860	2404500	incomprehensible and it doesn't have any feeling of realness. The feeling of realness that you
2404500	2409620	experience gets attached to certain representations where your brain assesses, this is the best model
2409620	2414420	of reality that I have. So the only thing that's real to you is the thing that's happening at the
2414420	2420980	very base of reality. Yeah. For something to be real, it needs to be implemented.
2422180	2428820	So the model that you have of reality is real in as far as it is a model. It's an appropriate
2428820	2435700	description of the world to say that there are models that are being experienced, but the world
2435700	2441540	that you experience is not necessarily implemented. There is a difference between a reality, a
2441540	2448180	simulation, and a simulacrum. The reality that we are talking about is something that fully emerges
2448180	2453380	over a causally closed lowest layer. And the idea of physicalism is that we are in that layer,
2453380	2458180	that basically our world emerges over that. Every alternative to physicalism is a simulation theory,
2458180	2462660	which basically says that we are in some kind of simulation universe and the real world needs to
2462660	2468020	be an apparent universe of that where the actual causal structure is. And when you look at the
2468020	2472500	ocean and your own mind, you are looking at a simulation that explains what you're going to see
2472500	2477220	next. So we are living in a simulation. Yes, but a simulation generated by our own brains.
2478340	2483620	And this simulation is different from the physical reality because the causal structure that is being
2483620	2487860	produced, what you are seeing is different from the causal structure of physics. But consistent.
2488980	2493860	Hopefully. If not, then you are going to end up in some kind of institution where people will take
2493860	2498820	care of you because your behavior will be inconsistent. Your behavior needs to work in
2498820	2504340	such a way that it's interacting with an accurately predictive model of reality. And if your brain is
2504340	2510020	unable to make your model of reality predictive, you will need help. So what do you think about
2510020	2517860	Donald Hoffman's argument that it doesn't have to be consistent, the dream world to what he calls
2517860	2522820	like the interface to the actual physical reality, where there could be evolution.
2522820	2528580	I think he makes an evolutionary argument, which is like it could be an evolutionary advantage
2528580	2534340	to have the dream world drift away from physical reality. I think that only works if you have
2534340	2538980	tenure. As long as you're still interacting with the ground whose model needs to be somewhat
2538980	2545860	predictive. Well, in some sense, humans have achieved a kind of tenure in the animal kingdom.
2545860	2551220	Yeah. And at some point we became too big to fail. So we became postmodernist.
2553540	2556260	It all makes sense now. There's no reality that we like.
2557140	2564580	Oh man. Okay. Yeah. But basically you can do magic. You can change your assessment of reality,
2564580	2568900	but eventually reality is going to come bite you in the ass if it's not predictive.
2568900	2577940	Do you have a sense of what is that base layer physical reality? You have these attempts at the
2577940	2586100	theories of everything. The very, very small like string theory or what Stephen Wolfram talks about
2586100	2591940	with a hyper grass. These are these tiny, tiny, tiny, tiny objects. And then there is more like
2592500	2597940	quantum mechanics that's talking about objects that are much larger, but still very, very,
2597940	2604980	very tiny. Do you have a sense of where the tiniest thing is that is like at the lowest level?
2604980	2608100	The turtle at the very bottom. Do you have a sense what that turtle is?
2608100	2612820	I don't think that you can talk about where it is because space is emergent over the activity
2612820	2621700	of these things. So the coordinates only exist in relation to other things. And so you could,
2621700	2626340	in some sense, abstract it into locations that can hold information and trajectories
2626340	2630020	that the information can take between the different locations. And this is how we
2630020	2637060	construct our notion of space. And physicists usually have a notion of space that is continuous.
2637780	2644020	And this is a point where I tend to agree with people like Stephen Wolfram who are very skeptical
2644020	2649540	of the geometric notions. I think that geometry is the dynamics of too many parts to count. And
2651780	2655540	there are no infinities. If there were two infinities, you would be running into
2655540	2659140	contradictions, which is in some sense what Gdel and Turing discovered
2659860	2663380	in response to Hilbert's call. There are no infinities.
2663380	2668260	There are no infinities. There is unboundedness, but if you have a language that talks about
2668260	2672900	infinity, at some point the language is going to contradict itself, which means it's no longer
2672900	2677700	valid. In order to deal with infinities and mathematics, you have to postulate the existence
2678660	2683380	initially. You cannot construct the infinities. And that's an issue. You cannot build up an
2683380	2688100	infinity from zero. But in practice, you never do this, right? When you perform calculations,
2688100	2694900	you only look at the dynamics of too many parts to count. And usually these numbers are not that
2694900	2700580	large. They're not Googles or something. The infinities that we are dealing with in our universe
2700580	2708100	are mathematically speaking relatively small integers. And still what we're looking at is
2708100	2714980	dynamics where a trillion things behave similar to a hundred trillion things or
2718100	2722660	something that is very, very large because they're converging. And these convergent dynamics,
2722660	2726340	these operators, this is what we deal with when we are doing the geometry.
2727140	2732900	Geometry is stuff where we can pretend that it's continuous because if we subdivide the space
2732900	2739700	sufficiently fine-grained, these things approach a certain dynamic. And this approached dynamic,
2739700	2745620	that is what we mean by it. But I don't think that infinity would work, so to speak, that you
2745620	2750180	would know the last digit of pi and that you have a physical process that rests on knowing the last
2750180	2757140	digit of pi. Yeah, that could be just a peculiar quirk of human cognition that we like discrete.
2757140	2761940	Discrete makes sense to us. Infinity doesn't in terms of our intuitions.
2761940	2766980	No, the issue is that everything that we think about needs to be expressed in some kind of mental
2766980	2772260	language, not necessarily a natural language, but some kind of mathematical language that your
2772260	2778100	neurons can speak that refers to something in the world. And what we have discovered is that
2778980	2783300	we cannot construct a notion of infinity without running into contradictions, which means that
2783300	2789140	such a language is no longer valid. And I suspect this is what made Pythagoras so unhappy when
2789140	2793460	somebody came up with the notion of irrational numbers before it was time. There's this myth
2793460	2797860	that he had this person killed when he blabbed out the secret that not everything can be expressed
2797860	2802500	as a ratio between two numbers, but there are numbers between the ratios. The world was not
2802500	2808420	ready for this. And I think he was right. That has confused mathematicians very seriously because
2808420	2813860	these numbers are not values, they are functions. And so you can calculate these functions to a
2813860	2818260	certain degree of approximation, but you cannot pretend that Pi has actually a value.
2819140	2825060	Pi is a function that would approach this value to some degree, but nothing in the world rests
2825060	2833940	on knowing Pi. How important is this distinction between discrete and continuous for you to get to
2833940	2841860	the bottom? Because in discussion of your favorite flavor of the theory of everything, there's a few
2841860	2851860	on the table. So there's string theory, there's loop quantum gravity, which focused on one
2851860	2859700	particular unification. There's just a bunch of favorite flavors of different people trying to
2861060	2866820	propose a theory of everything. Eric Weinstein and a bunch of people throughout history,
2866820	2871940	and then of course, Stephen Wolfram, who I think is one of the only people doing a discrete-
2871940	2879700	No, no. There's a bunch of physicists who do this right now, like Topholi and Tomasello.
2882820	2887860	Digital physics is something that is, I think, growing in popularity. But the
2887860	2897620	main reason why this is interesting is because it's important sometimes to settle disagreements.
2897620	2900980	I don't think that you need infinities at all, and you never needed them.
2901860	2906180	You can always deal with very large numbers, and you can deal with limits. We are fine with doing
2906180	2911220	that. You don't need any kind of affinity. You can build your computer algebra systems just as well
2911220	2914260	without believing in infinity in the first place. So you're okay with limits?
2914900	2921060	Yeah. Basically, a limit means that something is behaving pretty much the same if you make the
2921060	2925860	number larger, because it's converging to a certain value, and at some point, the difference
2925860	2930980	becomes negligible, and you can no longer measure it. In this sense, you have things that,
2932180	2936740	if you have an n-gon which has enough corners, then it's going to behave like a circle at some
2936740	2942340	point. It's only going to be in some kind of esoteric thing that cannot exist in the physical
2942340	2947540	universe that you would be talking about this perfect circle. Now it turns out that it also
2947540	2952100	wouldn't work in mathematics, because you cannot construct mathematics that has infinite resolution
2952100	2957940	without running into contradictions. That is itself not that important, because we never did
2957940	2963540	that. It's just a thing that some people thought we could. This leads to confusion. For instance,
2963540	2969540	Roger Penrose uses this as an argument to say that there are certain things that mathematicians can
2969540	2977220	do dealing with infinities, and by extension, our mind can do that computers cannot do.
2977220	2984340	Yeah. He talks about that the human mind can do certain mathematical things that the computer,
2984340	2990980	as defined by the universal Turing machine, cannot. So it has to do with infinity.
2990980	2996420	Yes. It's one of the things. So he is basically pointing at the fact that there are things that
2996420	3005140	are possible in the mathematical mind and in pure mathematics that are not possible in
3005700	3011700	machines that can be constructed in the physical universe. Because he is an honest guy, he thinks
3011700	3016900	this means that present physics cannot explain operations that happen in our mind.
3016900	3022820	Do you think he's right? Let's leave his discussion of consciousness aside for the moment.
3022820	3027380	Do you think he's right about just what he's basically referring to as intelligence?
3028180	3035460	So is the human mind fundamentally more capable as a thinking machine than a universal Turing
3035460	3043060	machine? No. So he's suggesting that, right? So our mind is actually less than a Turing machine.
3043060	3048340	There can be no Turing machine because it's defined as having an infinite tape. And we always only
3048420	3053380	have a finite tape. Our minds can only perform finitely many operations. Yes, he thinks so.
3053380	3056740	He can do the kind of computation the Turing machine cannot.
3056740	3061700	That's because he thinks that our minds can do operations that have infinite resolution in
3061700	3067780	some sense. And I don't think that's the case. Our minds are just able to discover these limit
3067780	3074900	operators over too many parts to count. I see. What about his idea that consciousness
3074900	3084660	is more than a computation? So it's more than something that a Turing machine can do. So again,
3085220	3089700	saying that there's something special about our mind, they cannot be replicated in the machine.
3091860	3097380	The issue is that I don't even know how to construct a language to express this statement
3097380	3110260	correctly. Well, the basic statement is there's a human experience that includes intelligence,
3110260	3115780	that includes self-awareness, that includes the hard problem of consciousness. And the question
3115780	3123380	is, can that be fully simulated in the computer, in the mathematical model of the computer as we
3123380	3133380	understand it today? Roger Pernero says no. So the universal Turing machine cannot simulate the
3133380	3139060	universe. So the interesting question is, and you have to ask him this, is why not? What is the
3139060	3145380	specific thing that cannot be modeled? And when I looked at his writings, and I haven't read all of
3145380	3151940	it, but when I read, for instance, the section that he writes in the introduction and wrote to
3151940	3157860	infinity, the thing that he specifically refers to is the way in which human minds deal with
3157860	3167060	infinities. And that itself can, I think, easily be deconstructed. A lot of people feel that our
3167060	3172500	experience cannot be explained in a mechanical way, and therefore, it needs to be different.
3173140	3179700	And I concur. Our experience is not mechanical. Our experience is simulated. It exists only in
3179700	3184020	a simulation. Only a simulation can be conscious. Physical systems cannot be conscious because they
3184020	3189140	are only mechanical. Cells cannot be conscious. Neurons cannot be conscious. Brains cannot be
3189140	3193220	conscious. People cannot be conscious if you understand them as physical systems.
3193940	3200260	What can be conscious is the story of a system in the world where you write all these things into
3200260	3205300	the story. You have experiences for the same reason that a character in a novel has experiences,
3205300	3210580	because it's written into the story. And now the system is acting on that story. And it's
3210580	3215060	not a story that is written in a natural language. It's written in a perceptual language, in this
3215060	3221060	multimedia language of the game engine. And in there, you write in what kind of experience
3221060	3224980	you have and what this means for the behavior of the system, for your behavior tendencies,
3224980	3228500	for your focus, for your attention, for your experience of valence, and so on.
3228500	3232820	And this is being used to inform the behavior of the system in the next step.
3232820	3239860	And then the story updates with the reactions of the system and the changes in the world and so on.
3239860	3243460	And you live inside of that model. You don't live inside of the physical reality.
3247700	3254740	Just to linger on it, it's in the perceptual language, the multimodal perceptual language.
3255540	3262260	That's the experience. That's what consciousness is within that model, within that story.
3262900	3268900	But do you have agency? When you play a video game, you can turn left and you can turn right
3269620	3277700	in that story. So in that dream world, how much control do you have? Is there such a thing as you
3277700	3285060	in that story? Is it right to say the main character? Everybody's NPCs, and then there's
3285060	3291060	the main character, and you're controlling the main character? Or is that an illusion? Is there
3291060	3296020	a main character that you're controlling? I'm getting to the point of the free will point.
3296580	3301940	Imagine that you are building a robot that plays soccer, and you've been to MIT computer science.
3301940	3307780	You basically know how to do that. And so you would say the robot is an agent that solves a
3307780	3313140	control problem, how to get the ball into the goal. And it needs to perceive the world, and the world
3313140	3317860	is disturbing him in trying to do this. So he has to control many variables to make that happen and
3317860	3323140	to project itself and the ball into the future and understand its position on the field relative
3323140	3329220	to the ball and so on, and the position of its limbs in the space around it and so on. So it
3329220	3335620	needs to have an adequate model that is directing reality in a useful way. And you could say that
3335620	3342740	this robot does have agency over what it's doing in some sense. And the model is going to be a
3342740	3348340	control model. And inside of that control model, you can possibly get to a point where this thing
3348340	3352980	is sufficiently abstract to discover its own agency. Our current robots don't do that. They
3352980	3358020	don't have a unified model of the universe, but there's not a reason why we shouldn't be getting
3358020	3363140	there at some point in the not too distant future. And once that happens, you will notice that the
3363700	3371460	robot tells a story about a robot playing soccer. So the robot will experience itself playing soccer
3371460	3377540	in a simulation of the world that it uses to construct a model of the locations of its legs
3377540	3382660	and limbs in space on the field with relationship to the ball. And it's not going to be at the level
3382660	3388500	of the molecules. It will be an abstraction that is exactly at the level that is most suitable for
3388500	3393380	past planning of the movements of the robot. It's going to be a high-level abstraction,
3393380	3398580	but a very useful one that is as predictive as we can make it. And in that side of that story,
3398660	3405380	there is a model of the agency of that system. So this model can accurately predict that the
3405380	3410900	contents of the model are going to be driving the behavior of the robot in the immediate future.
3410900	3417700	But there's the hard problem of consciousness, which I would also, there's a subjective
3417700	3424100	experience of free will as well, that I'm not sure where the robot gets that, where that little leap
3424100	3429460	is. Because for me right now, everything I imagine with that robot, as it gets more and more and more
3429460	3437300	sophisticated, the agency comes from the programmer of the robot still, of what was programmed in.
3437940	3442340	You could probably do an end-to-end learning system. You maybe need to give it a few prayers
3442340	3446340	so you nudge the architecture in the right direction that it converges more quickly.
3446340	3451140	But ultimately, discovering the suitable hyperparameters of the architecture is also
3451140	3456660	only a search process. And as the search process was evolution, it has informed our brain
3456660	3461540	architecture so we can converge in a single lifetime on useful interaction with the world.
3462340	3468420	The problem is if we define hyperparameters broadly, so it's not just the parameters that
3468420	3472820	control this end-to-end learning system, but the entirety of the design of the robot.
3475460	3479380	You have to remove the human completely from the picture. And then in order to build the robot,
3479380	3484660	you have to create an entire universe. Because you have to go, you can't just shortcut evolution,
3484660	3488900	you have to go from the very beginning in order for it to have, because I feel like there's always
3488900	3496500	a human pulling the strings. And that makes it seem like the robot is cheating, it's getting a
3496500	3500740	shortcut to consciousness. When you are looking at the current Boston Dynamics robots, it doesn't
3500740	3504740	look as if there is somebody pulling the strings. It doesn't look like cheating anymore. Okay,
3504740	3508580	so let's go there because I got to talk to you about this. So obviously with the case of Boston
3508580	3516100	Dynamics, as you may or may not know, it's always either hard-coded or remote controlled.
3516100	3521140	There's no intelligence. I don't know how the current generation of Boston Dynamics robots works,
3521140	3526580	but what I've been told about the previous ones was that it's basically all cybernetic control,
3527300	3532900	which means you still have feedback mechanisms and so on. But it's not deep learning for the
3532900	3539060	most part as it's currently done. It's for the most part just identifying a control hierarchy
3539060	3543780	that is congruent to the limbs that exist and the parameters that need to be optimized for the
3543780	3547700	movement of these limbs. And then there is a convergence progress. So it's basically just
3547700	3551540	regression that you would need to control this. But again, I don't know whether that's true.
3551540	3554900	That's just what I've been told about how that works. We have to separate several
3555540	3561780	levels of discussions here. So the only thing they do is pretty sophisticated control with no
3561780	3570180	machine learning in order to maintain balance or to write itself. It's a control problem in terms of
3570180	3575700	using the actuators to when it's pushed or when it steps on a thing that's uneven,
3575700	3580180	how to always maintain balance. And there's a tricky set of heuristics around that, but
3581220	3586900	that's the only goal. Everything you see Boston Dynamics doing in terms of that to us humans is
3586900	3594020	compelling, which is any kind of higher order movement like turning, wiggling its butt,
3597700	3603780	jumping back on its two feet, dancing. Dancing is even worse because dancing is hard
3603780	3612100	coded in. It's choreographed by humans. There's choreography software. So of all that high level
3612100	3621540	movement, there's no anything that you certainly can't call AI, but there's no even basic heuristics.
3621540	3630580	It's all hard coded in. And yet we humans immediately project agency onto them, which is
3630580	3636420	fascinating. So the gap here is it doesn't necessarily have agency. What it has is cybernetic
3636420	3641220	control. And the cybernetic control means you have a hierarchy of feedback loops that keep the
3641220	3645700	behavior in certain boundaries so the robot doesn't fall over and it's able to perform the
3645700	3650660	movements. And the choreography cannot really happen with motion capture because the robot
3650660	3654740	would fall over because the physics of the robot, the weight distribution and so on is different
3654740	3661140	from the weight distribution in the human body. So if you were using the directly motion captured
3661140	3665140	movements of the human body to project it into this robot, it wouldn't work. You can do this
3665140	3670100	with a computer animation. It would look a little bit off, but who cares? But if you want to correct
3670100	3676580	for the physics, you need to basically tell the robot where it should move its limbs. And then
3676580	3681780	the control algorithm is going to approximate a solution that makes it possible within the physics
3681780	3687140	of the robot. And you have to find the basic solution for making that happen. And there's
3687140	3692820	probably going to be some regression necessary to get the control architecture to make these
3692820	3698420	movements. But those two layers are separate. So the thing, the higher level instruction of
3699060	3701540	how you should move and where you should move is the higher level.
3701540	3706420	I expect that the control level of these robots at some level is dumb. This is just the
3707140	3712420	physical control movement, the motor architecture. But it's a relatively smart motor architecture.
3712420	3716420	It's just that there is no high level deliberation about what decisions to make necessarily.
3716420	3720500	Yeah. But see, it doesn't feel like free will or consciousness.
3720500	3726660	No, that was not where I was trying to get to. I think that in our own body, we have that too.
3726660	3731860	So we have a certain thing that is basically just a cybernetic control architecture that is
3731860	3737700	moving our limbs. And deep learning can help in discovering such an architecture if you don't
3737700	3742900	have it in the first place. If you already know your hardware, you can maybe handcraft it. But
3742900	3746740	if you don't know your hardware, you can search for such an architecture. And this work already
3746740	3752340	existed in the 80s and 90s. People were starting to search for control architectures by motor
3752340	3757140	babbling and so on, and just use reinforcement learning architectures to discover such a thing.
3757700	3763060	And now imagine that you have the cybernetic control architecture already inside of you.
3763620	3769700	And you extend this a little bit. So you are seeking out food, for instance, or rest,
3769700	3776660	or so on. And you get to have a baby at some point. And now you add more and more control
3776660	3781620	layers to this. And the system is reverse engineering its own control architecture
3781620	3787220	and builds a high level model to synchronize the pursuit of very different conflicting goals.
3788340	3792740	And this is how I think you get to purposes. Purposes are models of your goals. The goals
3792740	3797700	may be intrinsic as a result of the different set point violations that you have. Hunger and thirst
3797700	3802740	were very different things. And rest and pain avoidance and so on. And you put all these things
3802740	3807380	together. And eventually, you need to come up with a strategy to synchronize them all.
3808100	3812740	And you don't need just to do this alone by yourself because you are state building
3812740	3818820	organisms. We cannot function as isolation the way that homo sapiens is set up. So our own
3818820	3825220	behavior only makes sense when you zoom out very far into a society or even into ecosystemic
3825220	3830660	intelligence on the planet and our place in it. So the individual behavior only makes sense in
3830660	3836020	these larger contexts. And we have a number of priors built into us. So we are behaving as if
3836020	3840900	we were acting on these high level goals pretty much right from the start. And eventually,
3840900	3844740	in the course of our life, we can reverse engineer the goals that we are acting on,
3844740	3849780	what actually are our higher level purposes. And the more we understand that, the more our
3849780	3854500	behavior makes sense. But this is all, at this point, complex stories within stories
3854500	3861300	that are driving our behavior. Yeah. I just don't know how big of a leap it is to start
3862260	3864740	create a system that's able to tell stories within stories.
3866340	3869700	Like how big of a leap that is from where currently Boston Dynamics is
3870260	3879140	or any robot that's operating in the physical space. And that leap might be big if it requires
3879140	3883060	to solve the hard problem of consciousness, which is telling a hell of a good story.
3883620	3888340	I suspect that consciousness itself is relatively simple. What's hard is perception
3889300	3891460	and the interface between perception and reasoning.
3893060	3897700	There's, for instance, the idea of the consciousness prior that would be built into
3897700	3905380	such a system by Yoshua Bengio. And what he describes, and I think that's accurate, is that
3906500	3911780	our own model of the world can be described through something like an energy function.
3911780	3916900	The energy function is modeling the contradictions that exist within the model at any given point.
3916900	3921140	And you try to minimize these contradictions, the tangents in the model. And to do this,
3921140	3925860	you need to sometimes test things. You need to conditionally disambiguate figure and ground.
3925860	3930500	You need to distinguish whether this is true or that is true, and so on. Eventually,
3930500	3935300	you get to an interpretation. But you will need to manually depress a few points in your model to
3935300	3939860	let it snap into a state that makes sense. And this function that tries to get the biggest dip
3939860	3944420	in the energy function in your model, according to Yoshua Bengio, is related to consciousness.
3944420	3950260	It's a low-dimensional, discrete function that tries to maximize this dip in the energy function.
3951780	3957060	Yeah. I think I would need to dig into details because I think the way he uses the word
3957060	3962020	consciousness is more akin to self-awareness, like modeling yourself within the world,
3962900	3965540	as opposed to the subjective experience, the hard problem.
3966420	3970420	No, it's not even the self within the world. The self is the agent, and you don't need to
3970420	3975860	be aware of yourself in order to be conscious. The self is just a particular content that you
3975860	3981700	can have, but you don't have to have. But you can be conscious in, for instance, a dream at night
3981700	3987220	or during a meditation state, but you don't have a self. You're just aware of the fact that you
3987220	3993620	are aware. And what we mean by consciousness in the colloquial sense is largely this
3993620	3998340	reflexive self-awareness, that you become aware of the fact that you're paying attention,
3999300	4003140	that we are the thing that pays attention. We are the thing that pays attention, right?
4004020	4012580	I don't see the awareness that we're aware. The hard problem doesn't feel like it's solved.
4014580	4020500	It's called a hard problem for a reason, because it seems like there needs to be a major leap.
4021380	4026500	Yeah. I think the major leap is to understand how it is possible that a machine can dream,
4027300	4032740	that a physical system is able to create a representation that the physical system is
4032740	4038020	acting on, and that is spun force and so on. But once you accept the fact that you are not
4038020	4043300	in physics, but that you exist inside of the story, I think the mystery disappears. Everything is
4043300	4047380	possible in a story. You exist inside the story. Okay. Your consciousness is being written into
4047380	4051940	the story. The fact that you experience things is written to the side of the story. You ask yourself,
4051940	4055380	is this real what I'm seeing? Your brain writes into the story. Yes, it's real.
4056420	4060660	What about the perception of consciousness? To me, you look conscious.
4063220	4069860	The illusion of consciousness, the demonstration of consciousness. I ask for the legged robot.
4069860	4076420	How do we make this legged robot conscious? There's two things, and maybe you can tell me if they're
4076420	4082580	neighboring ideas. One is actually make it conscious, and the other is make it appear
4082580	4090260	conscious to others. Are those related? Let's ask it from the other direction. What would it
4090260	4096660	take to make you not conscious? When you are thinking about how you perceive the world,
4097300	4103700	can you decide to switch from looking at qualia to looking at representational states?
4103860	4110020	And it turns out you can. There is a particular way in which you can look at the world and recognize
4110020	4115220	its machine nature, including your own. And in that state, you don't have that conscious experience
4115220	4122020	in this way anymore. It becomes apparent as a representation. Everything becomes opaque.
4122020	4127220	And I think this thing that you recognize everything as a representation, this is typically
4127220	4132820	what we mean with enlightenment states. And you can have it on the motivational level,
4133780	4138260	but you can also do this on the experiential level, on the perceptual level.
4138260	4146740	See, but then I can come back to a conscious state. Okay. I'm referring to the
4147860	4154180	social aspect that the demonstration of consciousness is a really nice thing at a party
4154180	4160820	when you're trying to meet a new person. It's a nice thing to know that they're conscious and
4160900	4165940	they can, I don't know how fundamental consciousness is in human interaction,
4165940	4172660	but it seems like to be at least an important part. And I asked that in the same kind of way
4173780	4181460	for robots, in order to create a rich, compelling human robot interaction, it feels like there needs
4181460	4186340	to be elements of consciousness within that interaction. My cat is obviously conscious.
4186980	4191300	And so my cat can do this party trick. She also knows that I am conscious,
4191300	4196900	be able to have feedback about the fact that we are both acting on models of our own awareness.
4196900	4204180	The question is how hard is it for the robot, artificially created robot to achieve cat level
4204180	4211140	and party tricks? Yes. So the issue for me is currently not so much on how to build a system
4211220	4216980	that creates a story about a robot that lives in the world, but to make an adequate representation
4216980	4224580	of the world. And the model that you and me have is a unified one. It's one where you basically
4224580	4229140	make sense of everything that you can perceive. Every feature in the world that enters your
4229140	4235220	perception can be relationally mapped to a unified model of everything. And we don't have an AI that
4235220	4240900	is able to construct such a unified model yet. So you need that unified model to do the party trick?
4240900	4246420	Yes. I think that it doesn't make sense if this thing is conscious, but not in the same universe
4246420	4251060	as you, because you could not relate to each other. So what's the process, would you say,
4251060	4256020	of engineering consciousness in a machine? What are the ideas here?
4256740	4262340	So you probably want to have some kind of perceptual system. This perceptual system is
4262340	4268580	a processing agent that is able to track sensory data and predict the next frame and the sensory
4268580	4273860	data from the previous frames of the sensory data and the current state of the system.
4273860	4278900	So the current state of the system is perception instrumental to predicting what happens next.
4279700	4283300	And this means you build lots and lots of functions that take all the blips that you
4283300	4289220	feel on your skin and that you see on your retina or that you hear and puts them into
4289220	4294260	a set of relationships that allows you to predict what kind of sensory data, what kind of sensor of
4294980	4299060	vector of blips you're going to perceive in the next frame. This is
4299060	4303060	tuned and it's constantly tuned until it gets as accurate as it can.
4304020	4310580	You build a very accurate prediction mechanism that is step one of the perception. So at first
4310580	4313860	you predict, then you perceive and see the error in your prediction.
4313860	4317860	And you have to do two things to make that happen. One is you have to build a network
4317860	4323860	of relationships that are constraints that take all the variants in the world, put each of the
4324340	4330020	variances into a variable that is connected with relationships to other variables.
4330020	4334420	And these relationships are computable functions that constrain each other. So when you see a nose
4334420	4338660	that points in a certain direction in space, you have a constraint that says there should be a
4338660	4343060	face nearby that has the same direction. And if that is not the case, you have some kind
4343060	4346260	of contradiction that you need to resolve because it's probably not a nose what you're
4346260	4352260	looking at. It just looks like one. So you have to reinterpret the data until you get to a point
4352260	4358340	where your model converges. And this process of making the sensory data fit into your model
4358340	4366100	structure is what Piaget calls the assimilation. And accommodation is the change of the models
4366100	4369140	where you change your model in such a way that you can assimilate everything.
4370260	4374420	So you're talking about building a hell of an awesome perception system
4374420	4378100	that's able to do prediction and perception and correct and keep improving.
4378100	4384260	Wait, there's more. Yes, there's more. So the first thing that we wanted to do is we want to
4384260	4388980	minimize the contradictions in the model. And of course, it's very easy to make a model in
4388980	4392820	which you minimize the contradictions just by allowing that it can be in many, many possible
4392820	4398500	states. So if you increase degrees of freedom, you will have fewer contradictions. But you also
4398500	4402740	want to reduce the degrees of freedom because degrees of freedom mean uncertainty. You want
4402740	4408740	your model to reduce uncertainty as much as possible. But reducing uncertainty is expensive.
4408740	4414900	So you have to have a trade-off between minimizing contradictions and reducing uncertainty. And you
4414900	4420820	have only a finite amount of compute and experimental time and effort available to reduce uncertainty in
4420820	4426420	the world. So you need to assign value to what you observe. So you need some kind of motivational
4426420	4430980	system that is estimating what you should be looking at and what you should be thinking
4430980	4434100	about it, how you should be applying your resources to model what that is.
4435140	4439540	So you need to have something like convergence links that tell you how to get from the present
4439540	4443540	state of the model to the next one. You need to have these compatibility links that tell you
4443540	4449940	which constraints exist and which constraint violations exist. And you need to have some kind
4449940	4454340	of motivational system that tells you what to pay attention to. So now we have a second agent next
4454340	4459220	to the perceptual agent. We have a motivational agent. This is a cybernetic system that is
4459220	4463540	modeling what the system needs, what's important for the system, and that interacts with the
4463540	4468580	perceptual system to maximize the expected reward. And you're saying the motivational system is some
4468580	4475860	kind of like, what is it, a higher level narrative over some lower level? It's just your brainstem
4475860	4480100	stuff, the limbic system stuff that tells you, okay, now you should get something to eat because
4480100	4483940	I've just measured your blood sugar. So you mean like motivational system, like the lower level
4483940	4488900	stuff, like hungry? Yes. There's basically physiological needs and some cognitive needs
4488900	4492820	and some social needs, and they all interact. And they all implemented different parts in your
4492820	4496820	nervous system as the motivational system. But they're basically cybernetic feedback loops.
4496820	4503540	It's not that complicated. It's just a lot of code. And so you now have a motivational agent
4503540	4510100	that makes your robot go for the ball or that makes your worm go to eat food and so on. And
4510100	4513780	you have the perceptual system that lets it predict the environment so it's able to solve
4513780	4518420	that control problem to some degree. And now what we learned is that it's very hard to build a
4518420	4523380	machine learning system that looks at all the data simultaneously to see what kind of relationships
4523380	4528900	could exist between them. So you need to selectively model the world. You need to figure out,
4528900	4532420	where can I make the biggest difference if I would put the following things together?
4533060	4537540	Sometimes you find a gradient for that. When you have a gradient, you don't need to remember where
4537540	4542500	you came from. You just follow the gradient until it doesn't get any better. But if you have a world
4542500	4547220	where the problems are discontinuous and the search spaces are discontinuous, you need to
4547300	4551700	retain memory of what you explored. You need to construct a plan of what to explore next.
4552660	4557460	And this thing means that you have next to this perceptual construction system
4557460	4563940	and the motivational cybernetics, an agent that is paying attention to what it should select at
4563940	4568820	any given moment to maximize reward. And this scanning system, this attention agent,
4569620	4573380	is required for consciousness. And consciousness is its control model.
4574340	4580420	So it's the index memories that this thing retains when it manipulates the perceptual representations
4580980	4587380	to maximize the value and minimize the conflicts in it to increase coherence. So the purpose of
4587380	4592500	consciousness is to create coherence in your perceptual representations, remove conflicts,
4592500	4597380	predict the future, construct counterfactual representations so you can coordinate your actions
4597700	4603300	and so on. And in order to do this, it needs to form memories. These memories are partial
4603300	4609220	binding states of the working memory contents that are being revisited later on to backtrack,
4609220	4615060	to undo certain states, to look for alternatives. And these index memories that you can recall,
4615060	4619460	that is what you perceive as your stream of consciousness. And being able to recall these
4619460	4623780	memories, this is what makes you conscious. If you could not remember what you paid attention to,
4623780	4631620	you wouldn't be conscious. So consciousness is the index in the memory database. Okay.
4633460	4638340	But let me sneak up to the questions of consciousness a little further.
4639300	4646260	So we usually relate suffering to consciousness. So the capacity to suffer.
4648340	4653060	I think to me that's a really strong sign of consciousness, is a thing that can suffer.
4654580	4662580	How is that useful? Suffering. And in your model where you just described which is indexing of
4662580	4669460	memories and what is the coherence with the perception, with this predictive thing that's
4669460	4676260	going on in the perception, how does suffering relate to any of that? The higher level suffering
4676260	4685060	that humans do. Basically pain is a reinforcement signal. Pain is a signal that one part of your
4685060	4690660	brain sends to another part of your brain or an abstract sense, part of your mind sends to another
4690660	4695620	part of the mind to regulate its behavior, to tell it the behavior that you're currently exhibiting
4695620	4701940	should be improved. And this is the signal that I tell you to move away from what you're currently
4702020	4708500	doing and push into a different direction. So pain gives you a part of you an impulse to do
4708500	4714420	something differently. But sometimes this doesn't work because the training part of your brain is
4714420	4719300	talking to the wrong region or because it has the wrong model of the relationships in the world.
4719300	4723140	Maybe you're mismodeling yourself or you're mismodeling the relationship of yourself to
4723140	4727060	the world or you're mismodeling the dynamics of the world. So you're trying to improve something
4727060	4731940	that cannot be improved by generating more pain, but the system doesn't have any alternative.
4732500	4736820	So it doesn't get better. What do you do if something doesn't get better and you want it
4736820	4741780	to get better? You increase the strengths of the signal. And then the signal becomes chronic when
4741780	4747780	it becomes permanent without a change inside. This is what we call suffering. And the purpose
4747780	4753140	of consciousness is to deal with contradictions, with things that cannot be resolved. The purpose
4753140	4757780	of consciousness, I think, is similar to a conductor in an orchestra. When everything
4757780	4762340	works well, the orchestra doesn't need much of a conductor as long as it's coherent.
4762340	4766420	But when there is a lack of coherence or something is consistently producing
4766420	4772180	disharmony and mismatches, then the conductor becomes alert and interacts with it. So suffering
4772180	4778020	attracts the activity of our consciousness. And the purpose of that is ideally that we bring new
4778020	4784660	layers online, new layers of modeling that are able to create a model of the dysregulation so
4784660	4790020	we can deal with it. And this means that we typically get higher level consciousness,
4790020	4794500	so to speak. We get some consciousness above our pay grade maybe if we have some suffering
4794500	4799140	early in our life. Most of the interesting people had trauma early on in their childhood.
4799140	4805300	And trauma means that you are suffering an injury for which the system is not prepared,
4805300	4809220	which it cannot deal with, which it cannot insulate itself from. So something breaks.
4809940	4817380	And this means that the behavior of the system is permanently disturbed in the way that some
4817380	4822580	mismatch exists now in the regulation, that just by following your impulses, by following the pain
4822580	4827460	and the direction which it hurts, the situation doesn't improve but get worse. And so what needs
4827460	4833220	to happen is that you grow up. And that's part that is grown up is able to deal with the part
4833220	4836660	that is stuck in this earlier phase. Yeah, so it leads to growth,
4836660	4843220	you're adding extra layers to your cognition. Let me ask you then, because I gotta stick on
4843220	4850980	suffering, the ethics of the whole thing. So not our consciousness, but the consciousness of others.
4850980	4859220	You've tweeted, one of my biggest fears is that insects could be conscious. The amount of suffering
4859220	4868660	on earth would be unthinkable. So when we think of other conscious beings, it's suffering
4870500	4874900	a property of consciousness that we're most concerned about. So
4878900	4884820	I'm still thinking about robots, how to make sense of other non-human things
4884820	4894100	that appear to have the depth of experience that humans have. And to me, that means consciousness
4894100	4901860	and the darkest side of that, which is suffering, the capacity to suffer. And so I started thinking,
4902580	4908260	how much responsibility do we have for those other conscious beings? That's where the
4909220	4917140	definition of consciousness becomes most urgent. Having to come up with the definition of consciousness
4917140	4923220	becomes most urgent, is who should we and should we not be torturing?
4926740	4930100	There's no general answer to this. Was Genghis Khan doing anything wrong?
4931140	4937220	It depends on how you look at it. Well, he drew a line somewhere.
4938340	4946020	Where this is us and that's them. It's the circle of empathy. You don't have to use the word
4946020	4952580	consciousness, but these are the things that matter to me if they suffer or not. And these
4952580	4956900	are the things that don't matter to me. Yeah. But when one of his commanders failed him, he
4957780	4964660	broke his spine and let him die in a horrible way. And so in some sense, I think he was indifferent
4964660	4970900	to suffering. He was not different in the sense that he didn't see it as useful if he inflicted
4970900	4977540	suffering, but he did not see it as something that had to be avoided. That was not the goal.
4977540	4983380	The question was, how can I use suffering and the infliction of suffering to reach my goals
4983380	4990420	from his perspective? I see. So like different societies throughout history put different value
4990420	4993700	on the- Different individuals, different psyches, right?
4993700	4999540	But also even the objective of avoiding suffering. Like some societies probably,
4999540	5006740	I mean, this is where like religious belief really helps. That afterlife, it doesn't matter
5006740	5013060	that you suffer or die. What matters is you suffer honorably, right? So that you enter the afterlife.
5014020	5020740	It seems to be superstitious to me. Basically beliefs that assert things for which no evidence
5020740	5025860	exists are incompatible with sound epistemology. And I don't think that religion has to be
5025860	5030020	superstitious. Otherwise it should be condemned in all cases. You're somebody who's saying,
5030020	5036340	we live in a dream world. We have zero evidence for anything. That's not the case. There are limits
5036340	5041540	to what languages can be constructed. Mathematics brings solid evidence for its own structure.
5041540	5047140	And once we have some idea of what languages exist and how a system can learn and what learning
5047140	5055060	itself is in the first place, we can begin to realize that our intuitions, that we are able
5055060	5060180	to learn about the regularities of the world and minimize surprise and understand the nature of our
5060180	5066180	own agency to some degree of abstraction. That's not an illusion. It's a useful approximation.
5066180	5070340	Just because we live in a dream world doesn't mean mathematics can't
5071300	5076980	give us a consistent glimpse of objective reality.
5076980	5084260	We can basically distinguish useful encodings from useless encodings. And when we apply our
5084260	5088900	truth seeking to the world, we know we usually cannot find out whether a certain thing is true.
5089460	5094180	What we typically do is we take the state vector of the universe, separate it into separate objects
5094180	5098500	that interact with each other, so interfaces. And this distinction that we are making is not
5098500	5104900	completely arbitrary. It's done to optimize the compression that we can apply to our models of
5104900	5109940	the universe so we can predict what's happening with our limited resources. In this sense,
5109940	5115380	it's not arbitrary. But the separation of the world into objects that are somehow discrete
5115380	5121060	and interacting with each other is not the true reality. The boundaries between the objects are
5121060	5124260	projected into the world, not arbitrarily projected. But still,
5124820	5127220	it's only an approximation of what's actually the case.
5128580	5132340	And we sometimes notice that we run into contradictions when we try to understand
5132340	5138020	high level things like economic aspects of the world and so on, or political aspects or
5138020	5142500	psychological aspects where we make simplifications. And the objects that we are using to separate the
5142500	5148260	world are just one of many possible projections of what's going on. And so it's not in this
5148260	5152340	postmodernist sense completely arbitrary and you're free to pick what you want or dismiss
5152340	5157380	what you don't like because it's all stories. No, that's not true. You have to show for every model
5157380	5161860	of how well it predicts the world. So the confidence that you should have in the entities
5161860	5165140	of your models should correspond to the evidence that you have.
5165220	5175460	Can I ask you on a small tangent to talk about your favorite set of ideas and people,
5175460	5187860	which is postmodernism? What is postmodernism? How would you define it? And why to you is it not
5187860	5194580	a useful framework of thought? Postmodernism is something that I'm really not an expert on.
5195140	5201540	And postmodernism is a set of philosophical ideas that it's difficult to lump together
5203540	5208980	that is characterized by some useful thinkers, some of them post structuralist and so on.
5208980	5213860	And I'm mostly not interested in it because I think that it's not leading me anywhere that I
5213860	5220020	find particularly useful. It's mostly, I think, born out of the insight that the ontologies
5220020	5225140	that we impose on the world are not literally true and that we can often get to a different
5225140	5229780	interpretation by the world by using a different ontology that is different separation of the world
5229780	5237060	into interacting objects. But the idea that this makes the world a set of stories that are arbitrary,
5237060	5244500	I think is wrong. And the people that are engaging in this type of philosophy are working in an area
5244500	5249060	that I largely don't find productive. There's nothing useful coming out of this. So this idea
5249060	5254100	that truth is relative is not something that has, in some sense, informed physics or theory
5254100	5260020	of relativity, and there is no feedback between those. There is no meaningful influence of this
5260020	5265380	type of philosophy on the sciences or on engineering or in politics. But there is a very strong
5265380	5275220	information on ideology because it basically has become an ideology that is justifying itself
5275220	5281540	by the notion that truth is a relative concept. And it's not being used in such a way that the
5281540	5287540	philosophers or sociologists that take up these ideas say, oh, I should doubt my own ideas because
5287540	5291700	maybe my separation of the world into objects is not completely valid and I should maybe use
5291700	5298020	a different one and be open to a pluralism of ideas. But it mostly exists to dismiss the ideas
5298020	5304180	of other people. Yeah, it becomes a political weapon of sorts to achieve power.
5304180	5311620	Basically, there's nothing wrong, I think, with developing a philosophy around this. But
5311620	5317460	to develop norms around the idea that truth is something that is completely negotiable
5317460	5324980	is incompatible with the scientific project. And I think if the academia has no defense against
5324980	5329780	the ideological parts of the postmodernist movement, it's doomed.
5330740	5337300	Right. You have to acknowledge the ideological part of any movement actually, including postmodernism.
5337300	5342020	Well, the question is what an ideology is. And to me, an ideology is basically a viral
5342020	5348660	memeplex that is changing your mind in such a way that reality gets warped. It gets warped
5348660	5352500	in such a way that you're being cut off from the rest of human thought space. And you cannot
5352500	5357780	consider things outside of the range of ideas of your own ideology as possibly true.
5358580	5362580	I mean, there's certain properties to an ideology that make it harmful. One of them is that
5362580	5370180	dogmatism of just certainty, dogged certainty in that you're right, you have the truth,
5370180	5372420	and nobody else does. Yeah, but what is creating the certainty?
5372420	5377700	It's very interesting to look at the type of model that is being produced. Is it basically just a
5377700	5382660	strong prior? And you tell people, oh, this idea that you consider to be very true, the evidence
5382660	5386660	for this is actually just much weaker than you thought. And look here at some studies. No,
5386660	5392420	this is not how it works. It's usually normative, which means some thoughts are unthinkable
5392980	5398340	because they would change your identity into something that is no longer acceptable.
5399300	5404580	And this cuts you off from considering an alternative. And many de facto religions
5404580	5409220	use this trick to lock people into a certain mode of thought. And this removes agency over
5409220	5414740	your own thoughts. And it's very ugly to me. It's basically not just a process of domestication,
5414740	5421300	but it's actually an intellectual castration that happens. It's an inability to think creatively
5421300	5429380	and to bring forth new thoughts. I can ask you about substances, chemical
5429380	5438980	substances that affect the video game, the dream world. So psychedelics that increasingly have been
5438980	5444660	getting a lot of research done on them. So in general, psychedelics, psilocybin, MDMA,
5444660	5452100	but also a really interesting one, the big one, which is DMT. What and where are the places that
5452100	5459620	these substances take the mind that is operating in the dream world? Do you have an interesting
5459620	5466580	sense how this throws a wrinkle into the prediction model? Is it just some weird little quirk,
5466580	5470820	or is there some fundamental expansion of the mind going on?
5473700	5479380	I suspect that a way to look at psychedelics is that they induce particular types of lucid dreaming
5479380	5485860	states. So it's a state in which certain connections are being severed in your mind,
5485860	5491140	they're no longer active. Your mind basically gets free to move in a certain direction because
5492100	5497380	some particular inhibition doesn't work anymore. And as a result, you might stop having a self,
5497380	5505620	or you might stop perceiving the world as three-dimensional. And you can explore that
5505620	5510820	state. And I suppose that for every state that can be induced with psychedelics, there are people
5510820	5516340	that are naturally in that state. So sometimes psychedelics that shift you through a range of
5516340	5521140	possible mental states, and they can also shift you out of the range of permissible mental states
5521780	5528660	where you can make predictive models of reality. And what I observe in people that use psychedelics
5528660	5536660	a lot is that they tend to be overfitting. Overfitting means that you are using more bits
5536660	5542740	for modeling the dynamics of a function than you should. And so you can fit your curve to
5542740	5547220	extremely detailed things in the past, but this model is no longer predictive for the future.
5548180	5553780	What is it about psychedelics that forces that? I thought it would be the opposite. I thought
5555460	5564340	it's a good mechanism for generalization, for regularization. So it feels like psychedelics
5564340	5571620	expansion of the mind, like forcing your model to be non-predictive is a good thing.
5573380	5579460	Meaning like, it's almost like, okay, what I would say psychedelics are akin to is traveling
5579460	5584500	to a totally different environment. Like going, if you've never been to like India or something
5584500	5589140	like that from the United States, very different set of people, different culture, different
5589140	5593460	food, different roads and values and all those kinds of things.
5593460	5598980	Yeah. So psychedelics can, for instance, teleport people into a universe that is
5598980	5605700	hyperbolic, which means that if you imagine a room that you're in, you can turn around 360 degrees
5605700	5609380	and you didn't go full circle. You need to go 20 degrees to go full circle.
5609380	5610180	Exactly.
5610180	5615220	So the things that people learn in that state cannot be easily transferred in this universe that
5615220	5620340	we are in. It could be that if they're able to abstract and understand what happened to them,
5620340	5625860	that they understand that some part of their spatial cognition has been desynchronized and
5625860	5629380	has found a different synchronization. And this different synchronization happens
5629380	5633060	to be a hyperbolic one, right? So you learn something interesting about your brain.
5633060	5636740	It's difficult to understand what exactly happened, but we get a pretty good idea
5636740	5639860	once we understand how the brain is representing geometry.
5639860	5643620	Yeah, but doesn't give you a fresh perspective on the physical reality.
5648180	5652100	Who's making that sound? Is it inside my head or is it external?
5653060	5659780	Well, there is no sound outside of your mind, but it's making sense. Phenomena in physics.
5661780	5668340	Yeah, in the physical reality, there's sound waves traveling through air. Okay.
5669060	5670660	That's our model of what happened.
5670660	5672580	That's our model of what happened. Right.
5677220	5681140	Don't psychedelics give you a fresh perspective on this physical reality?
5681220	5684740	Like, not this physical reality, but this more...
5688340	5691940	What do you call the dream world that's mapped directly to...
5691940	5695780	The purpose of dreaming at night, I think, is data augmentation.
5695780	5698340	Well, exactly. So that's very different.
5698340	5699540	That's very similar to psychedelics.
5699540	5702740	So you basically exchange parameters about the things that you have learned.
5703700	5708100	And for instance, when you are young, you have seen things from certain perspectives,
5708100	5713540	but not from others. So your brain is generating new perspectives of objects that you already know,
5713540	5717220	which means you can learn to recognize them later from different perspectives.
5717220	5721780	And I suspect that's the reason that many of us remember to have flying dreams as children,
5721780	5725060	because it's just different perspectives of the world that you already know.
5725060	5729940	And that it starts to generate these different perspective changes.
5729940	5734100	And then it fluidly turns this into a flying dream to make sense of what's happening.
5734100	5737140	Right. So you fill in the gaps and suddenly you see yourself flying.
5737940	5740900	And similar things can happen with semantic relationships.
5740900	5745460	So it's not just spatial relationships, but it can also be the relationships between ideas
5745460	5750980	that are being changed. And it seems that the mechanisms that make that happen during dreaming
5752340	5758580	are interacting with these same receptors that are being simulated by psychedelics.
5759300	5763540	So I suspect that there is a thing that I haven't read really about.
5764020	5769940	The way in which dreams are induced in the brain is not just that the activity of the brain gets
5769940	5775940	tuned down because your eyes are closed and you no longer get enough data from your eyes,
5775940	5781780	but there is a particular type of neurotransmitter that is saturating your brain during these
5781780	5786180	phases, during the RM phases, and you produce controlled hallucinations.
5786820	5791380	And psychedelics are linking into these mechanisms, I suspect.
5792100	5796100	So isn't that another trickier form of data augmentation?
5796100	5801860	Yes. But it's also data augmentation that can happen outside of the specification
5801860	5805460	that your brain is tuned to. So basically people are overclocking their brains
5805460	5810580	and that produces states that are subjectively extremely interesting.
5812580	5814580	But from the outside, very suspicious.
5815780	5819300	I think I'm over-applying the metaphor of a neural network in my own mind,
5820020	5823780	which I just think that doesn't lead to overfitting.
5826580	5830740	But you were just sort of anecdotally saying my experiences with people that have no psychedelics
5830740	5833620	are that kind of quality. I think it typically happens.
5833620	5839060	So if you look at people like Timothy Leary, and he has written beautiful manifestos about
5839780	5844900	the effect of LSD on people, he genuinely believed, he writes in these manifestos,
5844900	5849300	that in the future, science and art will only be done on psychedelics because it's so much
5849300	5855620	more efficient and so much better. And he gave LSD to children in this community of a few thousand
5855620	5863220	people that he had near San Francisco. Basically, he was losing touch with reality. He did not
5863220	5868660	understand the effects that the things that he was doing would have on the reception of psychedelics
5868660	5873140	by society, because he was unable to think critically about what happened. What happened
5873140	5878660	was that he got in a euphoric state. That euphoric state happened because he was overfitting.
5878660	5884660	He was taking this sense of euphoria and translating it into a model of actual success
5884660	5891540	in the world. He was feeling better. Limitations had disappeared that he experienced to be existing,
5891540	5894500	but he didn't get superpowers. I understand what you mean by overfitting now.
5895940	5899540	There's a lot of interpretation to the term overfitting in this case,
5899540	5906500	but I got you. He was getting positive rewards from a lot of actions that he shouldn't have.
5906500	5912260	Not just this. If you take, for instance, John Lilly, who was studying dolphin languages and
5912260	5916900	aliens and so on, a lot of people that use psychedelics became very loopy.
5918500	5922980	The typical thing that you notice when people are on psychedelics is that they are in a state
5922980	5927940	where they feel that everything can be explained now. Everything is clear. Everything is obvious.
5929540	5934660	Sometimes they have indeed discovered a useful connection, but not always. Very often,
5934660	5941540	these connections are overinterpretations. I wonder, there's a question of correlation
5941540	5947140	versus causation. Also, I wonder if it's the psychedelics or if it's more the social,
5947860	5955220	being the outsider and having a strong community of outsider and having a leadership position in
5955220	5960500	and outside a cult-like community that could have a much stronger effect of overfitting than
5960500	5966340	do psychedelics themselves, the actual substances, because it's a counterculture thing. It could be
5966340	5972340	that as opposed to the actual substance. If you're a boring person who wears a suit and tie and
5972340	5979140	works at a bank and takes psychedelics, that could be a very different effect of psychedelics on your
5979140	5984900	mind. I'm just raising the point that the people you referenced are already weirdos.
5984900	5991300	I'm not sure exactly. No, not necessarily. A lot of the people that tell me that they use psychedelics
5991300	5997460	in a useful way started out as squares and were liberating themselves because they were stuck.
5998020	6002180	They were basically stuck in local optimum of their own self-model, of their relationship to
6002180	6008100	the world, and suddenly they had data augmentation. They basically saw and experienced a space of
6008100	6011300	possibilities. They experienced what it would be like to be another person.
6011940	6015700	That took important lessons from that experience back home.
6018820	6027300	Yeah. I love the metaphor of data augmentation because that's been the primary driver of
6027300	6033220	self-supervised learning in the computer vision domain, is data augmentation. It's funny to think
6033220	6040420	of chemically induced data augmentation in the human mind.
6041300	6048820	There's also a very interesting effect that I noticed. I know several people who are severe
6048820	6057300	to me that LSD has cured their migraines. Severe cluster headaches or migraines that didn't respond
6057300	6063860	to standard medication that disappeared after a single dose. I don't recommend anybody doing this,
6063860	6069460	especially not in the US where it's illegal. There are no studies on this for that reason,
6069460	6077300	but it seems anecdotally that it basically can reset the serotonergic system. It's basically
6078580	6084260	pushing them outside of their normal boundaries. As a result, it needs to find a new equilibrium.
6084260	6088980	Some people's equilibrium is better, but it also follows that in other people it might be worse.
6089940	6094340	If you have a brain that is already teetering on the boundary to psychosis,
6094980	6099220	it can be permanently pushed over that boundary. That's why you have to do good science,
6099220	6102660	which they're starting to do on all these different substances of how it actually works
6102660	6108180	for the different conditions like MDMA seems to help with PTSD, same with psilocybin.
6109060	6113140	You need to do good science, meaning large studies of large N.
6114020	6120180	Based on the existing studies with MDMA, it seems that if you look at Rick Doblin's work
6120180	6125300	and what he has published about this and talks about, MDMA seems to be a psychologically
6125300	6132740	relatively safe drug, but it's physiologically not very safe. That is, there is neurotoxicity if you
6132740	6139060	use too large dose. If you combine this with alcohol, which a lot of kids do in party settings
6139060	6145540	during raves and so on, it's very hepatotoxic. Basically, you can kill your liver. This means
6145540	6149940	that it's probably something that is best and most productively used in a clinical setting
6150500	6154660	by people who really know what they're doing. I suspect that's also true for
6154660	6159700	the other psychedelics. That is, while the other psychedelics are probably not as toxic
6159700	6164900	as, say, alcohol, the effects on the psyche can be much more profound and lasting.
6165620	6172180	Yeah. Well, as far as I know psilocybin, so mushrooms, magic mushrooms, as far as I know
6172180	6177780	in terms of the studies they're running, I think they're allowed to do what they're calling
6177780	6183780	heroic doses. That one does not have a toxicity, so they can do huge doses in a clinical setting
6183780	6186500	when they're doing study on psilocybin, which is kind of fun.
6187220	6192900	Yeah. It seems that most of the psychedelics work in extremely small doses, which means that the
6192900	6198980	effect on the rest of the body is relatively low. MDMA is probably the exception. Maybe ketamine
6198980	6205700	can be dangerous in larger doses because it can depress breathing and so on. But the LSD and
6205700	6211540	psilocybin work in very, very small doses, at least the active part of psilocybin LSD is only
6211540	6218660	the active part. But the effect that it can have on your mental wiring can be very dangerous,
6218660	6225860	I think. Let's talk about AI a little bit. What are your thoughts about GPT-3 and
6226580	6229220	language models trained with self-supervised learning?
6232100	6235300	It came out quite a bit ago, but I wanted to get your thoughts on it.
6235300	6243220	Yeah. In the 90s, I was in New Zealand and I had an amazing professor, Ian Witten,
6243220	6249860	who realized I was bored in class and put me in his lab. He gave me the task to discover
6249860	6255860	grammatical structure in an unknown language. The unknown language that I picked was English
6255860	6261780	because it was the easiest one to find a corpus for, a construct one. He gave me the largest
6261780	6266740	computer at the whole university. It had two gigabytes of RAM, which was amazing. I wrote
6266740	6273460	everything in C with some in-memory compression to do statistics over the language. I first would
6274100	6279140	create a dictionary of all the words, which basically tokenizes everything and compresses
6279140	6285300	things so that I don't need to store the whole word, but just a code for every word. Then I was
6286260	6291220	taking this all apart in sentences and I was trying to find all the relationships
6291220	6297300	between all the words in the sentences and do statistics over them. That proved to be impossible
6297300	6302820	because the complexity is just too large. If you want to discover the relationship between
6302820	6306820	an article and a noun and there are three adjectives in between, you cannot do n-gram
6306820	6311380	statistics and look at all the possibilities that can exist, at least not with the resources
6311460	6316340	that we had back then. I realized I need to make some statistics over what I need to make
6316340	6323220	statistics over. I wrote something that was pretty much a hack that did this for at least
6323220	6327780	first-order relationships. I came up with some kind of mutual information graph that was indeed
6327780	6332580	discovering something that looks exactly like the grammatical structure of the sentence,
6332580	6336980	just by trying to encode the sentence in such a way that the words would be written in the
6336980	6344660	optimal order inside of the model. What I also found is that if we would be able to increase
6344660	6350980	the resolution of that and not just use this model to reproduce grammatically correct sentences,
6350980	6355620	we would also be able to correct stylistically correct sentences by just having more bits in
6355620	6359860	these relationships. If we wanted to have meaning, we would have to go much higher order.
6360900	6365460	I didn't know how to make higher order models back then without spending way more years in
6365460	6369540	research on how to make the statistics over what we need to make statistics over.
6371460	6374980	This thing that we cannot look at the relationships between all the bits in your
6374980	6379460	input is being solved in different domains in different ways. In computer graphics,
6380500	6385620	computer vision, standard methods for many years now is convolutional neural networks.
6385620	6390500	Convolutional neural networks are hierarchies of filters that exploit the fact that neighboring
6390500	6395220	pixels in images are usually semantically related and distance pixels in images are
6395220	6401140	usually not semantically related. You can just by grouping the pixels that are next to each other
6401140	6407300	hierarchically together reconstruct the shape of objects. This is an important prior that we build
6407300	6412660	into these models so they can converge quickly. But this doesn't work in language for the reason
6412660	6418500	that adjacent words are often but not always related and distant words are sometimes related,
6418500	6423860	while the words in between are not. How can you learn the topology of language?
6425220	6430980	I think for this reason that this difficulty existed, the transformer was invented in
6432100	6438100	natural language processing, not in vision. What the transformer is doing, it's a hierarchy of
6438100	6443620	layers where every layer learns what to pay attention to in the given context in the previous
6443620	6452420	layer. What to make the statistics over. The context is significantly larger than the
6452420	6460820	adjacent word. Yes. The context that GPT-3 has been using, the transformer itself is from 2017 and
6460820	6467780	it wasn't using that large of a context. OpenAI has basically scaled up this idea as far as they
6467780	6475780	could at the time. The context is about 2048 symbols, tokens in the language. These symbols
6475780	6482180	are not characters, but they take the words and project them into a vector space, where words
6482180	6486900	that are statistically co-occurring a lot are neighbors already. It's already a simplification
6486900	6492100	of the problem a little bit. Every word is basically a set of coordinates in a high-dimensional
6492100	6498420	space. Then they use some kind of trick to also encode the order of the words in a sentence,
6498420	6504340	or in the not just sentence, but 2048 tokens is about couple pages of text or two and a half
6504340	6510580	pages of text. They managed to do pretty exhaustive statistics over the potential relationships
6511220	6516580	between two pages of text, which is tremendous. I was just using a single sentence back then,
6517140	6522500	and I was only looking for first-order relationships, and they were really looking
6522500	6527540	for much, much higher-level relationships. What they discover after they fed this with
6527540	6532980	an enormous amount of training data, pretty much the written internet or the subset of it that
6532980	6539860	had some quality, but substantial portion of the common draw, that they're not only able to reproduce
6539860	6545860	style, but they're also able to reproduce some pretty detailed semantics like being able to add
6545860	6550420	three-digit numbers and multiply two-digit numbers, or to translate between pro-angular
6550420	6556180	languages and things like that. The results that GBT-3 got, I think, were amazing.
6556180	6561700	By the way, I actually didn't check carefully. It's funny you just mentioned
6562580	6568180	how you coupled semantics to the multiplication. Is it able to do some basic math on two-digit
6568180	6568980	numbers? Yes.
6569940	6574580	Okay. Interesting. I thought there's a lot of failure cases.
6575700	6579780	It basically fails if you take larger-digit numbers, so four-digit numbers and so on
6580580	6585940	makes carrying mistakes and so on. If you take larger numbers, you don't get useful results at
6585940	6594260	all. This could be an issue of the training set. There are not many examples of successful
6594260	6597700	long-form addition in standard human-written text.
6597700	6601460	Humans aren't very good at doing three-digit numbers either.
6601460	6607060	Yeah. You're not writing a lot about it. The other thing is that the loss function that is
6607060	6611700	being used is only minimizing surprises. It's predicting what comes next in a typical text.
6611700	6615060	It's not trying to go for causal closure first, as we do.
6615060	6623540	Yeah. But the fact that that kind of prediction works to generate text that's semantically
6624260	6627300	rich and consistent is interesting. Yeah.
6628180	6632980	It's amazing that it's able to generate semantically consistent text.
6632980	6636740	It's not consistent. The problem is that it loses coherence at some point,
6636740	6643380	but it's also, I think, not correct to say that GPT-3 is unable to deal with semantics at all
6643380	6647460	because you ask it to perform certain transformations in text, and it performs
6647460	6652260	these transformations in text. The kind of additions that it's able to perform are
6652820	6657380	transformations in text. There are proper semantics involved.
6657380	6663780	You can also do more. There was a paper that was generating lots and lots of mathematically
6663780	6670980	correct text and was feeding this into a transformer. As a result, it was able to
6671060	6675940	learn how to do differentiation integration in race that, according to the authors,
6675940	6682820	Mathematica could not. To which some of the people in Mathematica responded that they were
6682820	6687860	not using Mathematica in the right way and so on. I have not really followed the resolution of this
6687860	6693860	conflict. This part, as a small tangent, I really don't like in machine learning papers, which
6694500	6702580	they often do anecdotal evidence. They'll find one example in some specific use of Mathematica
6702580	6708500	and demonstrate, look, they'll show successes and failures, but they won't have a very clear
6708500	6713300	representation of how many cases this actually represents. But I think as a first paper,
6713300	6720500	this is a pretty good start. The take-home message, I think, is that the authors could
6720500	6725220	get better results from this and their experiments than they could get from the
6725220	6730420	way in which they were using computer algebra systems, which means that was not nothing.
6731860	6738180	It's able to perform substantially better than GPTSV can based on a much larger amount of training
6738180	6744980	data using the same underlying algorithm. Let me ask, again, so I'm using your tweets as if
6744980	6753060	this is like Play-Doh, as if this is well thought out novels that you've written.
6753860	6763860	You tweeted, GPT-4 is listening to us now. This is one way of asking, what are the limitations of
6763860	6772500	GPT-3 when it scales? What do you think will be the capabilities of GPT-4, GPT-5, and so on? What
6772500	6777620	are the limits of this approach? Obviously, when we are writing things right now, everything that
6777620	6782260	we are writing now is going to be training data for the next generation of machine learning models.
6782260	6787700	Yes, of course, GPT-4 is listening to us. I think the tweet is already a little bit older,
6787700	6794100	and we now have Voodoo, and we have a number of other systems that basically are placeholders
6794100	6801380	for GPT-4. I don't know what open AIS plans are in this regard. I read that tweet in several ways.
6801380	6806020	One is, obviously, everything you put on the internet has used this training data,
6806820	6816020	but in a second way I read it is we talked about agency. I read it as almost like GPT-4 is
6816020	6824100	intelligent enough to be choosing to listen. Not only did a programmer tell it to collect this data
6824100	6830820	and use it for training, I almost saw the humorous angle, which is like it has achieved AGI kind of
6830820	6839380	thing. Well, the thing is, could we already be living in GPT-5? GPT-4 is listening, and
6839380	6844020	GPT-5 is actually constructing the entirety of the reality. Of course, in some sense,
6844900	6849300	what everybody is trying to do right now in AI is to extend the transformer to be able to deal
6849300	6858020	with video. There are very promising extensions. There is a work by Google that is called Perceiver,
6858580	6864500	and that is overcoming some of the limitations of the transformer by letting it learn the topology
6864500	6872100	of the different modalities separately and by training it to find better input features.
6872100	6879700	So the specific feature abstractions that are being used by this successor to GPT-3 are chosen
6879700	6885380	in such a way that it's able to deal with video input. There is more to be done. One of the
6885460	6892180	limitations of GPT-3 is that it's amnesiac, so it forgets everything beyond the two pages that
6892180	6897780	it currently reads, also during generation, not just during learning. Do you think that's fixable
6898660	6903380	within the space of deep learning? Can you just make a bigger, bigger, bigger input?
6903380	6908900	No, I don't think that our own working memory is infinitely large. It's probably also just a few
6908900	6915140	thousand bits, but what you can do is you can structure this working memory. So instead of just
6915700	6921540	force-feeding a certain thing that it has to focus on, and it's not allowed to focus on anything else
6921540	6927860	with its network, you allow it to construct its own working memory, as we do. When we are reading
6927860	6933380	a book, it's not that we are focusing our attention in such a way that we can only remember the current
6933380	6939540	page. We will also try to remember other pages and try to undo what we learn from them or modify
6939540	6943940	what we learn from them. We might get up and take another book from the shelf. We might go out and
6943940	6950740	ask somebody. We can edit our working memory in any way that is useful to put a context together
6950740	6956820	that allows us to draw the right inferences and to learn the right things. So this ability to
6956820	6963140	perform experiments on the world based on an attempt to become fully coherent and to achieve
6963140	6968900	causal closure, to achieve a certain aesthetic of your modeling, that is something that eventually
6968900	6974580	needs to be done. At the moment, we are skirting this in some sense by building systems that are
6974580	6979380	larger and faster so they can use dramatically larger resources than human beings can do and
6979380	6984740	much more training data to get to models that in some sense are already very superhuman and in
6984740	6992100	other ways are laughingly incoherent. So do you think sort of making the systems like,
6992100	7000020	what would you say, multi-resolutional, so like some of the language models are focused on
7000980	7008660	two pages, some are focused on two books, some are focused on two years of reading,
7008660	7014020	some are focused on a lifetime. So it's like stacks of GPT-3s all the way down.
7014020	7019140	You want to have gaps in between them. So it's not necessarily two years, there's no gaps,
7019140	7024260	it's things out of two years or out of 20 years or 2,000 years or 2 billion years,
7024260	7029540	that you are just selecting those bits that are predicted to be the most useful ones
7029540	7034020	to understand what you're currently doing. And this prediction itself requires a very
7034020	7037940	complicated model and that's the actual model that you need to be making. It's not just that
7037940	7041940	you are trying to understand the relationships between things, but what you need to make
7042820	7047540	discover relationships over. I wonder what that thing looks like,
7047540	7051300	what the architecture for the thing that's able to have that kind of model.
7052180	7057860	I think it needs more degrees of freedom than the current models have. So it starts out with the
7057860	7063460	fact that you possibly don't just want to have a feedforward model, but you want it to be fully
7063460	7068900	recurrent. And to make it fully recurrent, you probably need to loop it back into itself and
7068980	7074420	allow it to skip connections. Once you do this, when you're predicting the next frame and your
7074420	7081220	internal next frame in every moment, and you are able to skip connection, it means that signals can
7081220	7088100	travel from the output of the network into the middle of the network faster than the inputs do.
7088100	7092820	Do you think it can still be differentiable? Do you think it still can be in your own network?
7092820	7097620	Sometimes it can and sometimes it cannot. So it can still be in your own network,
7097620	7102980	but not a fully differentiable one. And when you want to deal with non-differentiable ones,
7102980	7107620	you need to have an attention system that is discrete and two-dimensional and can perform
7107620	7112260	grammatical operations. You need to be able to perform program synthesis. You need to be able
7112260	7117460	to backtrack in these operations that you perform on this thing. This thing needs a model of what
7117460	7121780	it's currently doing. And I think this is exactly the purpose of our own consciousness.
7122180	7128340	Yeah. The program things that trickle in your own networks. So let me ask you, it's not quite
7128340	7135380	program synthesis, but the application of these language models to generation, to program synthesis,
7135380	7141460	but generation of programs. So if you look at GitHub OpenPilot, which is based on OpenAI's codecs,
7141460	7146260	I don't know if you got a chance to look at it, but it's the system that's able to generate code
7146260	7152260	once you prompt it with, what is it? Like the header of a function with some comments,
7152260	7159060	and it seems to do an incredibly good job or not a perfect job, which is very important,
7159060	7164580	but an incredibly good job of generating functions. What do you make of that? Are you,
7164580	7170100	is this exciting or is this just a party trick, a demo, or is this revolutionary?
7171060	7179220	I haven't worked with this yet, so it's difficult for me to judge it, but I would not be surprised
7179220	7184020	if it turns out to be revolutionary. And that's because the majority of programming tasks that
7184020	7189460	are being done in the industry right now are not creative. People are writing code that other
7189460	7193700	people have written, or they're putting things together from code fragments that others have had.
7193700	7199460	And a lot of the work that programmers do in practice is to figure out how to overcome
7199460	7203060	the gaps in their current knowledge in the things that people have already done.
7203060	7206180	How to copy and paste from Stack Overflow, that's right.
7206180	7208580	And so of course we can automate that.
7208580	7212980	Yeah, to make it much faster to copy and paste from Stack Overflow.
7212980	7217220	Yes, but it's not just copying and pasting, it's also basically learning which parts you need to
7217220	7222980	modify to make them fit together. Yeah, like literally sometimes as
7222980	7227140	simple as just changing the variable names so it fits into the rest of your code.
7227140	7230820	Yes, but this requires that you understand the semantics of what you're doing to some degree.
7230820	7235780	Yeah, and you can automate some of those things. The thing that makes people nervous of course is
7235780	7242900	that a little bit wrong in a program can have a dramatic effect on the actual
7243780	7249780	final operation of that program. So like one little error, which in the space of language
7249780	7254100	doesn't really matter, but in the space of programs can matter a lot.
7254100	7257940	Yes, but this is already what is happening when humans program code.
7257940	7261620	Yeah. So we have a technology to deal with this.
7262340	7268180	Somehow it becomes scarier when you know that a program generated code that's running a nuclear
7268180	7272900	power plant. It becomes scarier. You know humans have errors too.
7273460	7277620	Exactly. But it's scarier when a program is doing it because
7278580	7290100	why? I mean there's a fear that a program may not be as good as humans
7290100	7300340	to know when stuff is important to not mess up. There's a misalignment of priorities,
7301460	7304900	of values, that's potential. Maybe that's the source of the worry.
7305540	7314580	I mean, okay, if I give you code generated by GitHub OpenPilot and code generated by a human
7314580	7322420	and say, here, use one of these. How do you select today and in the next 10 years
7322420	7325700	which code to use? Wouldn't you still be comfortable with the human?
7327380	7334020	At the moment, when you go to Stanford to get an MRI, they will write a bill to the insurance
7334020	7341220	over $20,000. And of this, maybe half of that gets paid by the insurance and a quarter gets
7341220	7349140	paid by you. And the MRI costs them $600 to make maybe, probably less. And what are the values of
7349140	7356100	the person that writes the software and deploys this process? It's very difficult for me to say
7356100	7363700	whether I trust people. I think that what happens there is a mixture of proper Anglo-Saxon Protestant
7363700	7368340	values where somebody is trying to serve an abstract rate of whole and organized crime.
7368340	7379540	Well, that's a very harsh. I think that's a harsh view of humanity. There's a lot of bad people,
7379540	7385700	whether incompetent or just malevolent in this world, yes. But it feels like the more
7386660	7393540	malevolent, so the more damage you do to the world, the more resistance you have in your own
7394580	7400820	human heart. But don't explain with malevolence or stupidity what can be explained by just people
7400820	7406420	acting on their incentives. So what happens in Stanford is not that somebody is evil,
7407060	7409860	it's just that they do what they're being paid for.
7410820	7421780	No, it's not evil. I see that as malevolence. I see even being a good German, as I told you
7421780	7429860	offline, it's not absolute malevolence, but it's a small amount. It's cowardice. I mean,
7429860	7435380	when you see there's something wrong with the world, it's either incompetence that you're
7435380	7441700	not able to see it, or it's cowardice that you're not able to stand up, not necessarily in a big
7441700	7449060	way, but in a small way. So I do think that is a bit of malevolence. I'm not sure the example
7449060	7452660	you're describing is a good example of that. So the question is, what is it that you are aiming
7452660	7458820	for? And if you don't believe in the future, if you, for instance, think that the dollar is
7458820	7464660	going to crash by what you try to save dollars, if you don't think that humanity will be around
7464660	7470180	in 100 years from now because global warming will wipe out civilization, why would you need
7470180	7477620	to act as if it were? So the question is, is there an overarching aesthetics that is projecting you
7477620	7482420	and the world into the future, which I think is the basic idea of religion, that you understand
7482420	7487060	the interactions that we have with each other as some kind of civilization level agent that is
7487060	7494180	projecting itself into the future. If you don't have that shared purpose, what is there to be
7494180	7500980	ethical for? So I think when we talk about ethics and AI, we need to go beyond the insane bias
7500980	7506180	discussions and so on, where people are just measuring the distance between a statistic to
7506180	7512180	their preferred current world model. But the optimism... Wait, wait, wait. I was a little
7512180	7523540	confused by the previous thing, just to clarify. There is a kind of underlying morality to having
7523540	7530340	an optimism that human civilization will persist for longer than 100 years. I think a lot of people
7530340	7536980	believe that it's a good thing for us to keep living and thriving. Yeah, of course. This morality
7536980	7544260	itself is not an end to itself. It's instrumental to people living in 100 years from now or 500
7544260	7550740	years from now. So it's only justifiable if you actually think that it will lead to people or
7550740	7556100	increase the probability of people being around in that timeframe. And a lot of people don't
7556100	7562660	actually believe that, at least not actively. But believe what exactly? Most people don't believe
7562660	7567860	that they can afford to act on such a model. Basically, what happens in the US is I think
7567860	7572020	that the healthcare system is for a lot of people no longer sustainable, which means that if they
7572020	7576740	need the help of the healthcare system, they're often not able to afford it. And when they cannot
7576820	7582500	help it, they are often going bankrupt. I think the leading cause of personal bankruptcy in the
7582500	7588820	US is the healthcare system. And that would not be necessary. It's not because people are consuming
7588820	7594180	more and more medical services and are achieving a much, much longer life as a result. That's not
7594180	7597860	actually the story that is happening because you can compare it to other countries. And
7597860	7602020	life expectancy in the US is currently not increasing and it's not as high as in all the
7602020	7605860	other industrialized countries. So some industrialized countries are doing better
7605860	7611460	with a much cheaper healthcare system. And what you can see is, for instance, administrative
7611460	7619380	load. The healthcare system has maybe to some degree deliberately set up a job placement program
7619380	7628020	to allow people to continue living a middle-class existence despite not having a useful use case in
7628020	7633540	productivity. So they are being paid to push paper around. And the number of administrators
7633540	7637860	in the healthcare system has been increasing much faster than the number of practitioners.
7637860	7643300	And this is something that you have to pay for. And also the revenues that are being generated
7643300	7648020	in the healthcare system are relatively large and somebody has to pay for them. And the result why
7648020	7654340	they are so large is because market mechanisms are not working. The FDA is largely not protecting
7654340	7660900	people from malpractice of healthcare providers. The FDA is protecting healthcare providers from
7660900	7666980	competition. So this is a thing that has to do with values. And this is not because people are
7666980	7673140	malicious on all levels. It's because they are not incentivized to act on a greater whole on this
7673140	7677780	idea that you treat somebody who comes to you as a patient like you would treat a family member.
7677780	7683300	Yeah. But you're highlighting a lot of the flaws of the different institutions,
7683300	7687300	the systems we're operating under. But I think there's a continued throughout history
7688020	7693780	mechanism design of trying to design incentives in such a way that these systems behave better
7693780	7698980	and better and better. I mean, it's a very difficult thing to operate a society of hundreds
7698980	7704580	of millions of people effectively with- Yes. So do we live in a society that is ever correcting?
7705540	7712420	Do we observe that our models of what we are doing are predictive of the future and when they are not,
7712420	7718100	we improve them. Our laws are adjudicated with clauses that you put into every law,
7718100	7722100	what is meant to be achieved by that law, and the law will be automatically repealed
7722100	7726340	if it's not achieving that. If you are optimizing your own laws, if you're writing your own source
7726340	7731460	code, you probably make an estimate of what is this thing that's currently wrong in my life?
7731460	7735620	What is it that I should change about my own policies? What is the expected outcome?
7736180	7741220	And if that outcome doesn't manifest, I will change the policy back or I would change it
7741220	7744260	into something different. Are we doing this on a societal level?
7744260	7749700	I think so. I think it's easy to highlight. I think we're doing it in the way that
7751300	7756340	I operate my current life. I didn't sleep much last night. You would say that,
7756340	7759780	Lex, the way you need to operate your life is you need to always get sleep. The fact that you
7759780	7766740	didn't sleep last night is totally the wrong way to operate in your life. You should have gotten
7766740	7771780	all your shit done in time and gotten to sleep because sleep is very important for health,
7771780	7777780	and you're highlighting, look, this person is not sleeping. Look, the medical, the healthcare system
7777780	7783460	is operating poor. But the point is, it seems like this is the way, especially in the capitalist
7783460	7790500	society we operate, we keep running into trouble and last minute we try to get our way out through
7790500	7796100	innovation, and it seems to work. You have a lot of people that ultimately are trying to
7796100	7804340	build a better world and get urgency about them when the problem becomes more and more imminent,
7804980	7812740	and that's the way this operates. But if you look at the long arc of history, it seems like that
7813620	7819060	operating on deadlines produces progress and builds better and better systems.
7819060	7826100	You probably agree with me that the US should have engaged in mass production in January 2020
7826660	7833060	and that we should have shut down the airports early and that we should have made it mandatory
7833060	7840020	that the people that work in nursing homes live on campus rather than living at home
7840020	7845060	and then coming in and infecting people in the nursing homes that had no immune response to
7845060	7851380	COVID. That is something that was, I think, visible back then. The correct decisions haven't
7851380	7856660	been made. We would have the same situation again. How do we know that these wrong decisions are not
7856660	7861380	being made again? Have the people that made the decisions to not protect the nursing homes been
7861380	7867860	punished? Have the people that made the wrong decisions with respect to testing that prevented
7867860	7872980	the development of testing by startup companies and the importing of tests from countries that
7872980	7878260	already had them? Have these people been held responsible? First of all, what do you want to
7879220	7883860	put before the firing squad? No, just make sure that this doesn't happen again.
7885540	7889940	But it's not that, yes, they're being held responsible by many voices, by people being
7889940	7895940	frustrated. There's new leaders being born now that are going to see rise to the top in 10 years.
7896500	7903300	This moves slower than, there's obviously a lot of older incompetence in bureaucracy
7903300	7910580	and these systems move slowly. They move like science, one death at a time. So yes,
7910580	7917620	I think the pain that's been felt in the previous year is reverberating throughout the world.
7917620	7921220	Maybe I'm getting old. I suspect that every generation in the US
7921220	7925300	after the war has lost the plot even more. I don't see this development.
7925300	7928820	The war, World War II? Yeah. So basically, there was a time
7928820	7936020	when we were modernist. And in this modernist time, the US felt actively threatened by the
7936020	7940660	things that happened in the world. The US was worried about possibility of failure.
7941780	7949060	And this imminence of possible failure led to decisions. There was a time when the government
7949060	7954420	would listen to physicists about how to do things. And the physicists were actually concerned about
7954420	7958180	what the government should be doing. So they would be writing letters to the government.
7958180	7962340	And so for instance, the decision for the Manhattan Project was something that was
7962340	7966100	driven in a conversation between physicists and the government.
7966100	7968980	I don't think such a discussion would take place today.
7968980	7974740	I disagree. I think if the virus was much deadlier, we would see a very different response.
7974740	7979540	I think the virus was not sufficiently deadly. And instead, because it wasn't very deadly,
7979540	7986100	what happened is the current system started to politicize it. This is what I realized with
7986100	7992980	masks early on. They very quickly became not as a solution, but they became a thing
7993540	7998420	that politicians used to divide the country. So the same things happened with vaccines.
8000900	8004340	People weren't talking about solutions to this problem because I don't think the problem was
8004340	8009860	bad enough. When you talk about the war, I think our lives are too comfortable.
8010900	8016500	I think in the developed world, things are too good and we have not faced severe dangers.
8017060	8022900	When the severe dangers, existential threats are faced, that's when we step up,
8022900	8031540	on a small scale and a large scale. Now, that's sort of my argument here, but I did think the
8031540	8039300	virus... I was hoping that it was actually sufficiently dangerous for us to step up,
8039300	8044420	because especially in the early days, it was unclear. It still is unclear because of mutations
8045460	8056580	how bad it might be. So I thought we would step up. The masks point is a tricky one because
8057060	8062340	to me, the manufacture of masks isn't even the problem. I'm still to this day, and I was
8062340	8067860	involved with a bunch of this work, have not seen good signs done on whether masks work or not.
8069140	8074420	There still has not been a large scale study. To me, there should be large scale studies on every
8074420	8079780	possible solution, aggressive in the same way that the vaccine development was aggressive.
8079860	8087540	There should be masks, what kind of tests work really well. Even the question of how the virus
8087540	8094740	spreads. There should be aggressive studies on that to understand. As far as I know, there's still
8094740	8099620	a lot of uncertainty about that. Nobody wants to see this as an engineering problem that needs to
8099620	8106660	be solved. That I was surprised about. I find that our views are largely convergent,
8106660	8112660	but not completely. So I agree with the thing that because our society in some sense perceives
8112660	8118980	itself as too big to fail, and the virus did not alert people to the fact that we are facing
8118980	8124340	possible failure, that basically put us into the postmodernist mode. And I don't mean in
8124340	8130020	the philosophical sense, but in a societal sense. The difference between the postmodern society
8130020	8134420	and the modern society is that the modernist society has to deal with the ground truth,
8134420	8139380	and the postmodernist society has to deal with appearances. Politics becomes a performance,
8139940	8144340	and the performance is done for an audience, and the organized audience is the media,
8144340	8150100	and the media evaluates itself via other media. So you have an audience of critics that evaluate
8150100	8154500	themselves. And I don't think it's so much the failure of the politicians, because to get in
8154500	8159620	power and to stay in power, you need to be able to deal with the published opinion.
8159620	8165380	Well, I think it goes into cycles, because what's going to happen is all of the small
8165380	8170500	business owners, all the people who truly are suffering and will suffer more because of the
8170500	8177460	effects of the closure of the economy and the lack of solutions to the virus, they're going to
8177460	8184660	apprise. And hopefully, I mean, this is where charismatic leaders can get the world in trouble,
8184740	8193380	but hopefully will elect great leaders that will break through this postmodernist idea of
8195620	8199780	the media and the perception and the drama on Twitter and all that kind of stuff.
8199780	8204900	But you know, this can go either way. When the Weimar Republic was unable to
8205460	8211060	deal with the economic crisis that Germany was facing, there was an option to go back.
8211060	8216020	And there were people which thought, let's get back to a constitutional
8216020	8222020	monarchy and let's get this to work, because democracy doesn't work. And eventually,
8222660	8226580	there was no way back. People decided there was no way back. They needed to go forward.
8226580	8233940	And the only options for going forward was to become Stalinist communists, basically an
8233940	8240420	option to completely expropriate the factories and so on and nationalize them and to reorganize
8241060	8248660	Germany in communist terms and ally itself with Stalin and fascism. And both options were obviously
8248660	8255700	very bad. And the one that the Germans picked led to a catastrophe that devastated Europe.
8256500	8261380	And I'm not sure if the US has an immune response against that. I think that the
8261380	8264900	far right is currently very weak in the US, but this can easily change.
8264900	8272020	Do you think from a historical perspective, Hitler could have been stopped
8272980	8279940	from within Germany or from outside? Or this? Well, it depends on who you want to focus,
8279940	8283780	whether you want to focus on Stalin or Hitler. But it feels like Hitler was the one
8284500	8286420	as a political movement that could have been stopped.
8287300	8293620	I think that the point was that a lot of people wanted Hitler. So he got support from a lot of
8293620	8298340	quarters. There was a number of industrialists who supported him because they thought that the
8298340	8303940	democracy is obviously not working and unstable and you need a strongman. And he was willing to
8303940	8310580	play that part. There were also people in the US who thought that Hitler would stop Stalin and
8310580	8317700	would act as a bulwark against Bolshevism, which he probably would have done at which cost.
8318340	8325300	And then many of the things that he was going to do, like the Holocaust, was something where
8325300	8330660	people thought this is rhetoric. He's not actually going to do this, especially many of the Jews
8330660	8335380	themselves, which were humanists. And for them, this was outside of the scope that was thinkable.
8335380	8345060	Right. I wonder if Hitler is uniquely, I want to carefully use this term, but uniquely evil.
8345620	8354020	So if Hitler was never born, if somebody else would come in this place. So just thinking about
8354020	8361540	the progress of history, how important are those singular figures that lead to mass destruction
8361540	8371460	and cruelty? Because my sense is Hitler was unique. It wasn't just about the environment
8371540	8378580	and the context that gave him. Another person would not come in his place to do as destructive
8378580	8385700	of the things that he did. There was a combination of charisma, of madness, of
8385700	8392740	psychopathy, of just ego, all those things, which are very unlikely to come together in one person
8392740	8397860	in the right time. It also depends on the context of the country that you're operating in.
8398580	8405940	If you tell the Germans that they have a historical destiny in this romantic country,
8405940	8413940	the effect is probably different than it is in other countries. But Stalin has killed a few more
8413940	8419460	people than Hitler did. And if you look at the probability that you survived under Stalin,
8420100	8428900	Hitler killed people if he thought they were not worth living or if they were harmful to his
8429700	8435620	racist project. Basically, he felt that the Jews would be too cosmopolitan and would not be willing
8435620	8442500	to participate in the racist redefinition of society and the value of society and an ethno-state
8442500	8449540	in this way, as he wanted it to have it. So he saw them as harmful danger, especially
8449540	8454740	since they played such an important role in the economy and culture of Germany.
8458100	8464740	Basically, he had some radical but rational reason to murder them, and Stalin just killed everyone.
8465380	8469220	He basically, the Stalinist churches were such a random thing where he said that there's a
8472660	8478340	certain possibility that this particular part of the population has a number of German collaborators
8478340	8483700	or something, and we just killed them all. Or if you look at what Mao did, the number of
8483700	8488740	people that were killed in absolute numbers were much higher under Mao than they were under Stalin.
8489700	8495700	So it's super hard to say. The other thing is that you look at Genghis Khan and so on,
8495700	8502900	how many people he killed. There are a number of things that happen in human history that actually
8502900	8510500	really put a substantial dent in the existing population, or Napoleon. And it's very difficult
8510500	8517860	to eventually measure it because what's happening is basically evolution on a human scale for one
8517860	8525300	monkey figures out a way to become viral and is using this viral technology to change the
8525300	8532020	patterns of society at the very, very large scale. And what we find so abhorrent about these changes
8532020	8537060	is the complexity that is being destroyed by this. That it's basically like a big fire that burns out
8537060	8540020	a lot of the existing culture and structure that existed before.
8541300	8547460	Yeah. And it all just starts with one monkey, one charismatic ape, and there's a bunch of them
8547460	8552260	throughout history. Yeah. But it's in a given environment. It's basically similar to wildfires
8552260	8558420	in California, right? The temperature is rising. There is less rain falling. And then suddenly a
8558420	8565460	single spark can have an effect that in other times would be contained. Okay. Speaking of which,
8565460	8575700	I love how we went to Hitler and Stalin from 20, 30 minutes ago, GPT-3 doing program synthesis.
8575700	8587460	The argument was about morality of AI versus human. And specifically in the context of
8587460	8592580	writing programs, specifically in the context of programs that can be destructive. So running
8592580	8600980	nuclear power plants or autonomous weapons systems, for example. And I think your inclination was to
8600980	8607620	say that it's not so obvious that AI would be less moral than humans or less effective at making
8608260	8615220	a world that would make humans happy. So I'm not talking about self-directed systems that are
8616580	8622020	making their own goals at a global scale. If you just talk about the deployment of technological
8622020	8628500	systems that are able to see order and patterns and use this as control models to act on the
8628500	8634420	goals that we give them, then if we have the correct incentives to set the correct incentives
8634420	8640980	for these systems, I'm quite optimistic. So humans versus AI. Let me give you an example.
8642660	8649620	Autonomous weapons system. Let's say there's a city somewhere in the Middle East that has
8650420	8657060	a number of terrorists. And the question is what's currently done with drone technologies,
8657060	8662180	you have information about the location of a particular terrorist and you have a targeted
8662180	8668100	attack, you have a bombing of that particular building. And that's all directed by humans at
8668100	8675300	the high level strategy and also at the deployment of individual bombs and missiles. Everything is
8675300	8683380	done by human except the final targeting and the spot, similar thing like control the flight.
8683940	8691460	Okay. What if you give AI control and saying, write a program that says,
8692420	8696340	here's the best information I have available about the location of these five terrorists.
8696900	8702180	Here's the city. Make sure all the bombing you do is constrained to the city. Make sure it's
8702180	8708260	precision based, but you take care of it. So you do one level of abstraction out and saying,
8708900	8715220	take care of the terrorists in the city. Which are you more comfortable with, the humans or the
8715220	8724500	JavaScript GPT-3 generated code that's doing the deployment? This is the kind of question I'm asking
8724500	8731540	is the kind of bugs that we see in human nature, are they better or worse than the kind of bugs we
8731540	8738100	see in AI? They're different bugs. There is an issue that if people are creating
8738980	8744420	imperfect automation of a process that normally requires a moral judgment,
8744980	8751140	and this moral judgment is the reason why it cannot be automated often. It's not because
8751140	8757620	the computation is too expensive, but because the model that you give the AI is not an adequate
8757620	8762180	model of the dynamics of the world because the AI does not understand the context that it's
8762180	8766820	operating in the right way. And this is something that already happens with Excel,
8767060	8772420	you don't need to have an AI system to do this. You have an automated process in place
8772420	8778180	where humans decide using automated criteria whom to kill when and whom to target when,
8778180	8783700	which already happens. And you have no way to get off the kill list once that happens,
8783700	8788740	once you have been targeted according to some automatic criterion by people in a bureaucracy.
8790420	8793700	That is the issue. The issue is not the AI, it's the automation.
8794420	8799460	So there's something about, right, it's automation, but there's something about
8800900	8805780	there's a certain level of abstraction where you give control to AI to do the automation.
8806580	8812900	There's a scale that can be achieved that it feels like the scale of bug and scale mistake
8812900	8820100	and scale of destruction that can be achieved of the kind that humans cannot achieve. So AI is
8820100	8825940	much more able to destroy an entire country accidentally versus humans. It feels like the
8825940	8834100	more civilians die as a react or suffer as the consequences of your decisions, the more weight
8834100	8841700	there is on the human mind to make that decision. And so it becomes more and more unlikely to make
8841700	8848340	that decision for humans. For AI, it feels like it's harder to encode that kind of weight.
8849140	8855220	In a way, the AI that we're currently building is automating statistics. Intelligence is the ability
8855220	8861540	to make models so you can act on them. And AI is a tool to make better models. So in principle,
8861540	8868900	if you're using AI wisely, you're able to prevent more harm. And I think that the main issue is not
8868900	8873380	on the side of the AI, it's on the side of the human command hierarchy that is using technology
8873380	8880580	irresponsibly. So the question is how hard is it to properly encode the right incentives into the
8880580	8886580	AI? So for instance, there's this idea of what happens if we let our airplanes being flown
8886580	8892340	with AI systems and the neural network is a black box and so on. And it turns out our neural networks
8892340	8898740	are actually not black boxes anymore. There are function approximators using linear algebra,
8898740	8903380	and there are performing things that we can understand. But we can also,
8903380	8907780	instead of letting the neural network fly the airplane, use the neural network to generate
8907780	8912820	a proven to be correct program. There's a degree of accuracy of the proof that a human could not
8912820	8918820	achieve. And so we can use our AI by combining different technologies to build systems that are
8918820	8925380	much more reliable than the systems that a human being could create. And so in this sense, I would
8925380	8933460	say that if you use an early stage of technology to save labor and don't employ competent people,
8933460	8938020	but just to hack something together because you can, that is very dangerous. And if people are
8938020	8944100	acting under these incentives that they get away with delivering shoddy work more cheaply using AI
8944100	8947220	is less human oversight than before. That's very dangerous.
8947220	8952500	The thing is though, AI is still going to be unreliable, perhaps less so than humans,
8952500	8959300	but it'll be unreliable in novel ways. And- Yeah, but this is an empirical question
8959300	8965220	and it's something that we can figure out and work with. So the issue is, do we trust the systems,
8965220	8970100	the social systems that we have in place and the social systems that we can build and maintain
8970100	8976180	that they're able to use AI responsibly? If they can, then AI is good news. If they cannot,
8976180	8978260	then it's going to make the existing problems worse.
8979220	8983300	Well, and also who creates the AI, who controls it, who makes money from it,
8983300	8988980	because it's ultimately humans. And then you start talking about how much you trust the humans.
8988980	8994260	So the question is, what does who mean? I don't think that we have identity per se. I think that
8994260	9000180	the story of a human being is somewhat random. What happens is more or less that everybody is
9000180	9004820	acting on their local incentives, what they're perceived to be their incentives. And the question
9004900	9010740	is, what are the incentives that the one that is pressing the button is operating under?
9010740	9016740	Yeah. It's nice for those incentives to be transparent. So for example,
9018180	9026500	there seems to be a significant distrust of tech, like entrepreneurs in the tech space
9026500	9030900	or people that run, for example, social media companies like Mark Zuckerberg.
9031860	9038420	There is not a complete transparency of incentives under which that particular human being operates.
9040900	9044900	We can listen to the words he says or what the marketing team says for a company,
9044900	9051620	but we don't know. And that becomes a problem when the algorithms and the systems created by
9052580	9058420	him and other people in that company start having more and more impact on society.
9059300	9067460	And if the incentives were somehow, the definition and the explainability of the
9067460	9076340	incentives was decentralized such that nobody can manipulate it. No propaganda type manipulation
9076340	9084660	of how these systems actually operate could be done. Then yes, I think AI could achieve
9085220	9095860	a much fairer, much more effective solutions to difficult ethical problems. But when there's
9095860	9102660	like humans in the loop manipulating the dissemination, the communication of how the
9102660	9107620	system actually works, that feels like you can run into a lot of trouble. And that's why there's
9107620	9113380	currently a lot of distrust for people at the heads of companies that have increasingly powerful
9113780	9120980	AI systems. I suspect what happened traditionally in the US was that since our decision-making is
9120980	9126900	much more decentralized than in an authoritarian state, people are making decisions autonomously
9126900	9133060	at many, many levels in society. What happened that was we created coherence and cohesion in
9133060	9139380	society by controlling what people thought and what information they had. The media synchronized
9139380	9145380	public opinion and social media have disrupted this. It's not, I think, so much Russian influence
9145380	9150820	or something. It's everybody's influence. It's that a random person can come up with a conspiracy
9150820	9157940	theory and disrupt what people think. And if that conspiracy theory is more compelling or more
9157940	9164340	attractive than the standardized public conspiracy theory that we give people as a default, then it
9164340	9168900	might get more traction. You suddenly have the situation that a single individual somewhere
9168900	9175780	on a farm in Texas has more listeners than CNN. Which particular farmer you're referring to in Texas?
9181220	9185780	Yes, I had dinner with him a couple of times. Okay. Right. It's an interesting situation because
9185780	9192660	you cannot get to be an anchor in CNN if you don't go through a complicated gatekeeping process. And
9192660	9198500	suddenly you have random people without that gatekeeping process just optimizing for attention.
9199300	9204340	Not necessarily with a lot of responsibility for the long-term effects of projecting these
9204340	9210420	theories into the public. And now there is a push of making social media more like traditional media,
9210420	9215380	which means that the opinion that is being projected in social media is more limited to
9215380	9221540	an acceptable range with the goal of getting society into safe waters and increase the stability
9221540	9226500	and cohesion of society again, which I think is a laudable goal. But of course, it also is an
9226500	9232740	opportunity to seize the means of indoctrination. And the incentives that people are under when
9232740	9241300	they do this are in such a way that the AI ethics that we would need becomes very often something
9241300	9247540	like AI politics, which is basically partisan and ideological. And this means that whatever one
9247540	9253220	side says, another side is going to be disagreeing with. And the same way as when you turn masks or
9253220	9258740	the vaccine into a political issue, if you say that it is politically virtuous to get vaccinated,
9258740	9263860	it will mean that the people that don't like you will not want to get vaccinated. And as soon as
9263860	9269380	you have this partisan discourse, it's going to be very hard to make the right decisions because
9269380	9274100	the incentives get to be the wrong ones. AI ethics needs to be super boring. It needs to be done by
9274100	9280500	people who do statistics all the time and have extremely boring, long-winded discussions that
9280500	9284580	most people cannot follow because they are too complicated, but that are dead serious.
9284580	9289060	These people need to be able to be better at statistics than the leading machine learning
9289060	9295220	researchers. And at the moment, the AI ethics debate is the one where you don't have any barrier
9295220	9300500	to entry, right? Everybody who has a strong opinion and is able to signal that opinion in
9300500	9307940	the right way can enter it. To me, that is a very frustrating thing because the field is so crucially
9307940	9313700	important to our future. It's so crucially important, but the only qualification currently
9313700	9318820	need is to be outraged by the injustice in the world. It's more complicated, right? Everybody
9318820	9324340	seems to be outraged, but let's just say that the incentives are not always the right ones. So
9324340	9330660	basically, I suspect that a lot of people that enter this debate don't have a vision for what
9330660	9335620	society should be looking like in a way that is nonviolent, that we preserve liberal democracy,
9335620	9341940	where we make sure that we all get along and we are around in a few hundred years from now,
9342580	9346180	preferably with the comfortable technological civilization around us.
9346980	9354180	I generally have a very foggy view of that world, but I tend to try to follow,
9354180	9359780	and I think society should in some degree follow the gradient of love, increasing the amount of
9359780	9365540	love in the world. And whenever I see different policies or algorithms or ideas that are not
9365540	9372420	doing so, obviously, that's the ones that kind of resist. So the thing that terrifies me about this
9372420	9379300	notion is I think that German fascism was driven by love. It was just a very selective love.
9379940	9382820	It was a love that basically- Well, now you're just manipulating. I mean,
9383540	9392660	I mean, you have to be very careful. You're talking to the wrong person in this way about love.
9392660	9398100	So let's talk about what love is. And I think that love is the discovery of shared purpose.
9398100	9404020	It's the recognition of the sacred and the other. And this enables non-transactional interactions.
9404980	9414500	But the size of the other that you include needs to be maximized. So it's basically
9417060	9427700	deep appreciation of the world around you, including the people that are very different
9427700	9432820	than you, the people that disagree with you completely, including living creatures outside
9432820	9440500	of just people, including ideas and appreciation of the full mess of it. And also it has to do with
9440500	9449220	empathy, which is coupled with a lack of confidence, uncertainty of your own rightness.
9449220	9453220	It's like a radical open-mindedness to the way forward.
9453220	9458660	I agree with every part of what you said. And now if you scale it up, what you recognize is that
9459140	9465380	love is in some sense the service to a next level agency, to the highest level agency that you can
9465380	9472340	recognize. It could be, for instance, life on earth or beyond that. You could say intelligent
9472340	9477380	complexity in the universe that you try to maximize in a certain way. But when you think
9477380	9483060	it's true, it basically means a certain aesthetic. And there is not one possible aesthetic. There are
9483060	9487540	many possible aesthetics. And once you project an aesthetic into the future,
9487540	9492900	you can see that there are some which defect from it, which are in conflict with it,
9492900	9498420	that are corrupt, that are evil. You and me would probably agree that Hitler was evil
9499140	9504100	because the aesthetic of the world that he wanted is in conflict with the aesthetic of the world that
9504100	9512100	you and me have in mind. And so the thing that he destroyed, we want to keep them in the world.
9512660	9520180	There's kind of ways to deal. I mean, Hitler is an easier case, but perhaps he wasn't so easy
9520180	9523860	in the 30s to understand who is Hitler and who is not.
9524420	9529540	No, it was no consensus that the aesthetics that he had in mind were unacceptable.
9529540	9539860	Yeah. I mean, it's difficult. Love is complicated because you can't just be so open-minded that
9539940	9550340	you let evil walk into the door, but you can't be so self-assured that you can always identify evil
9550340	9556260	perfectly because that's what leads to Nazi Germany. Having a certainty of what is and
9556260	9565540	wasn't evil, like always drawing lines of good versus evil. There has to be a dance between
9570020	9577540	hard stances extending up against what is wrong, and at the same time, empathy and open-mindedness
9577540	9582980	of towards not knowing what is right and wrong, and a dance between those.
9582980	9587140	I found that when I watched the Miyazaki movies that there is nobody who captures
9587140	9593860	my spirituality as well as he does. It's very interesting and suspicious. There is something
9593860	9599780	going on in his movies that is very interesting. For instance, Mononoka is discussing not only
9599940	9607060	an answer to Disney's simplistic notion of Mowgli, the jungle boy who was raised by wolves,
9607060	9614820	and as soon as he sees people realizes that he's one of them. The way in which the moral life
9614820	9619940	and nature is simplified and romanticized and turned into kitsch is disgusting in the Disney
9619940	9625460	movie. He answers to this. You see, he's replaced by Mononoka, this wolf girl who was raised by
9625460	9631380	wolves and was fierce and dangerous and who cannot be socialized because she cannot be tamed,
9631380	9635460	cannot be part of human society. You see, human society, it's something that is very,
9635460	9641220	very complicated. You see people extracting resources and destroying nature, but the purpose
9641220	9648100	is not to be evil, but to be able to have a life that is free from, for instance, oppression and
9648180	9656020	violence and to curb death and disease. You basically see this conflict which cannot be
9656020	9661300	resolved in a certain way. You see this moment when nature is turned into a garden and it loses
9661300	9666020	most of what it actually is and humans no longer submitting to life and death and nature.
9666980	9672100	To these questions, there is no easy answer. It just turns it into something that is being observed
9672100	9677940	as a journey that happens and that happens with a certain degree of inevitability. The nice thing
9677940	9682500	about all his movies is there's a certain main character and it's the same in all movies.
9683300	9690660	It's this little girl that is basically Heidi and I suspect that happened because when he did
9692020	9696500	field work for working on the Heidi movies back then, the Heidi animations,
9696500	9702580	before he did his own movies, he traveled to Switzerland and southeastern Europe and
9702580	9707380	the Adriatic and so on and got an idea about a certain aesthetic and a certain way of life
9707380	9714260	that informed his future thinking. Heidi has a very interesting relationship to herself and to
9714260	9719860	the world. There is nothing that she takes for herself. She is in a way fearless because she is
9719860	9726100	committed to a service to a greater whole. Basically, she is completely committed to serving God.
9726100	9730820	It is not an institutionalized God. It has nothing to do with the Roman Catholic Church or
9731780	9736020	something like this, but in some sense, Heidi is an embodiment of the spirit of European
9736020	9741380	Protestantism. It's this idea of a being that is completely perfect and pure,
9742260	9749620	and it's not a feminist vision because she is not a girl boss or something like this.
9750740	9756500	She is the justification for the men in the audience to protect her, to build the civilization
9756500	9762740	around her that makes her possible. She is not just the sacrifice of Jesus who is innocent and
9762740	9767620	therefore nailed to the cross. She is not being sacrificed. She is being protected
9767620	9771860	by everybody around her who recognizes that she is sacred, and there are enough around her
9771860	9777300	to see that. This is a very interesting perspective. There is a certain notion of
9777300	9782180	innocence, and this notion of innocence is not universal. It's not in all cultures.
9782180	9788100	Hitler wasn't innocent. His idea of Germany was not that there is an innocence that is being
9788100	9793140	protected. There was a predator that was going to triumph, and it's also something that is not at
9793140	9796900	the core of every religion. There are many religions which don't care about innocence.
9796900	9805620	They might care about increasing the status of something, and that's a very interesting notion
9805620	9811060	that is quite unique, and I'm not claiming it's the optimal one. It's just a particular kind of
9811060	9817060	aesthetic, which I think makes Miyazaki into the most relevant Protestant philosopher today.
9817620	9822980	And you're saying in terms of all the ways that a society can operate,
9822980	9828500	perhaps the preservation of innocence might be one of the best.
9829220	9837060	No, it's just my aesthetic. It's a particular way in which I feel that I relate to the world that
9837060	9842740	is natural to my own civilization, and maybe it's not an accident that I have cultural roots in
9842740	9849700	Europe in a particular world. So maybe it's a natural convergence point, and it's not something
9849700	9856980	that you will find in all other times in history. So I'd like to ask you about Solzhenitsyn and our
9856980	9864420	individual role as ants in this very large society. So he says that some version of the line between
9864420	9868980	good and evil runs through the heart of every man. Do you think all of us are capable of good and
9868980	9879140	evil? What's our role in this play, in this game we're all playing? Is all of us capable to play
9879140	9888180	any role? Is there an ultimate responsibility to, you mentioned maintaining innocence or whatever
9888180	9894180	the highest ideal for a society you want, are all of us capable of living up to that? And that's our
9894180	9900660	responsibility, or is there significant limitations to what we're able to do in terms of good and evil?
9903460	9909540	So there is a certain way, if you are not parable, if you are committed to some kind of
9909540	9914980	civilizational agency, a next level agent that you are serving, some kind of transcendent principle,
9916260	9920500	in the eyes of that transcendental principle, you are able to discern good from evil. Otherwise,
9920500	9925460	you cannot. Otherwise, you have just individual aesthetics. The cat that is torturing a mouse is
9925460	9932100	not evil because the cat does not envision or not part of the world, of the cat is envisioning a
9932100	9937380	world where there is no violence and nobody is suffering. If you have an aesthetic where you
9937380	9943700	want to protect innocence, then torturing somebody needlessly is evil, but only then.
9944660	9951940	No, but I guess the question is within your sense of what is good and evil,
9954100	9958340	it seems like we're still able to commit evil.
9959220	9964020	Yes. So basically, if you are committing to this next level agent, you are not necessarily
9964020	9969220	are this next level agent. You are part of it. You have a relationship to it like the cell does
9969220	9974740	to its organism, its hyperorganism. It only exists to the degree that it's being implemented
9974740	9981620	by you and others. That means that you're not completely fully serving it. You have freedom
9981620	9985780	in what you decide, whether you are acting on your impulses and local incentives, on your
9985780	9991220	feral impulses, so to speak, or whether you're committing to it. What you perceive then is a
9991220	9997780	tension between what you would be doing with respect to the thing that you recognize as the
9997780	10002980	sacred, if you do, and what you're actually doing. This is the line between good and evil,
10003780	10008820	where you see, oh, I'm here acting on my local incentives or impulses, and here I'm acting on
10008820	10013460	what I consider to be sacred, and there's a tension between those. This is the line between
10013460	10018740	good and evil that might run through your heart. If you don't have that, if you don't have this
10018740	10023140	relationship to a transcendental agent, you could call this relationship to the next level agent
10023140	10028180	soul. It's not a thing. It's not an immortal thing that is intrinsically valuable. It's a
10028180	10032740	certain kind of relationship that you project to understand what's happening. Somebody is serving
10032740	10037460	this transcendental sacredness or they're not. If you don't have a soul, you cannot be evil.
10038020	10044900	You're just a complex natural phenomenon. If you look at life starting today or starting
10044900	10051860	tomorrow, when we leave here today, there's a bunch of trajectories that you can take through life.
10054020	10062020	Maybe countless. Do you think some of these trajectories in your own conception of yourself,
10062020	10070660	some of those trajectories are the ideal life? A life that if you were to be the hero of your life
10070660	10075940	story, you would want to be. Look, is there some Josh Abak that you're striving to be?
10076740	10080580	This is the question I ask myself as an individual trying to make
10081540	10086260	a better world in the best way that I could conceive of. What is my responsibility there?
10087380	10094740	How much am I responsible for the failure to do so? Because I'm lazy and incompetent
10094740	10099780	too often in my own perception. In my own worldview, I'm not very important.
10101780	10108020	I don't have place for me as a hero in my own world. I'm trying to do the best that I can,
10108020	10116900	which is often not very good. It's not important for me to have status or to be seen in a particular
10116900	10121780	way. It's helpful if others can see me or a few people can see me that can be my friends.
10121780	10127540	No, sorry. I want to clarify that here I didn't mean status or perception or
10130420	10135060	some kind of marketing thing, but more in private, in the quiet of your own mind.
10136020	10141860	Is there the kind of man you want to be and would consider it a failure if you don't become that?
10142420	10147940	That's what I meant by hero. Yeah, not really. I don't perceive myself as having such an identity.
10150580	10160420	And it's also sometimes frustrating, but it's basically a lack of having this notion of
10160660	10162500	father that I need to be emulating.
10166100	10171860	It's interesting. I mean, it's the leaf floating down the river. I worry that...
10173940	10175460	Sometimes it's more like being the river.
10175460	10188500	I'm just a fat frog sitting on a leaf on a dirty muddy lake.
10193220	10197940	Waiting for a princess to kiss me or the other way. I forgot which way it goes.
10197940	10204020	Somebody kisses somebody. Can I ask you, I don't know if you know who Michael Malice is, but
10205780	10212340	in terms of constructing systems of incentives, it's interesting to ask. I don't think I've talked
10212340	10220340	to you about this before. Malice espouses anarchism. So he sees all government as fundamentally
10222740	10225300	getting in the way or even being destructive to
10227620	10234100	collaborations between human beings thriving. What do you think? What's the role of government
10234100	10244340	in a society that thrives? Is anarchism at all compelling to you as a system? Not just
10244340	10249460	small government, but no government at all. Yeah, I don't see how this would work.
10251860	10257220	The government is an agent that imposes an offset on your reward function on your payout
10257220	10261460	metrics. So your behavior becomes compatible with the common good.
10263300	10269780	The argument there is that you can have collectives like governing organizations,
10269780	10275700	but not government. Where you're born on a particular set of land and therefore you must
10275700	10286100	follow this rule or else you're forced by what they call violence because there's an implied
10286100	10295380	violence here. The key aspect of government is it protects you from the rest of the world
10295380	10303700	with an army and with police. It has a monopoly on violence. It's the only one that's able to do
10303700	10308900	violence. There are many forms of government. Not all governments do that, but we find that
10308980	10316980	in successful countries, the government has a monopoly on violence. That means that you
10316980	10320820	cannot get ahead by starting your own army because the government will come down on you
10320820	10325380	and destroy you if you try to do that. In countries where you can build your own army
10325380	10330660	and get away with it, some people will do it. These countries is what we call failed countries
10330660	10338260	in a way. If you don't want to have violence, the point is not to appeal to the moral intentions of
10338260	10343700	people because some people will use strategies if they get ahead with them that feel a particular
10343700	10350260	kind of ecological niche. You need to destroy that ecological niche. If an effective government
10350260	10356260	has a monopoly on violence, it can create a world where nobody is able to use violence and get ahead.
10357060	10362180	You want to use that monopoly on violence not to exert violence, but to make violence impossible,
10362180	10367460	to raise the cost of violence so people need to get ahead with nonviolent means.
10368340	10374260	The idea is that you might be able to achieve that in an anarchist state with companies,
10374260	10381700	so with the forces of capitalism, is create security companies where the one that's most
10381700	10387300	ethically sound rises to the top. Basically, it would be a much better representative of the people
10387300	10395300	because there is less stickiness to the big military force sticking around,
10395300	10402100	even though it's long, overlived, outlived. You have groups of militants that are hopefully
10402100	10406660	efficiently organized because otherwise they're going to lose against the other groups of militants
10406660	10412740	and they are coordinating themselves with the rest of society until they are having a monopoly
10412740	10417860	on violence. How is that different from a government? It's basically converging to the
10417860	10424100	same thing. I was trying to argue with Malice. I feel like it always converges towards government
10424100	10430260	at scale, but I think the idea is you can have a lot of collectives that are ... You basically
10430260	10437540	never let anything scale too big. One of the problems with governments is it gets too big
10437540	10443380	in terms of the size of the group over which it has control.
10446100	10452660	My sense is that would happen anyway. A successful company like Amazon or Facebook,
10453380	10459620	it starts forming a monopoly over entire populations, not over just the hundreds of
10459620	10467140	millions but billions of people. I don't know, but there is something about the abuses of power,
10467140	10473780	the government can have when it has a monopoly on violence. That's a tension there.
10475140	10480180	The question is, how can you set the incentives for government correctly? This mostly applies
10480180	10484980	at the highest levels of government. Because we haven't found a way to set them correctly,
10484980	10490260	we made the highest levels of government relatively weak. This is, I think, part of the
10490260	10495700	reason why we had difficulty to coordinate the pandemic response. China didn't have that much
10495700	10502020	difficulty. There is, of course, a much higher risk of the abuse of power that exists in China
10502020	10508100	because the power is largely unchecked. That's basically what happens in the next generation,
10508100	10512500	for instance. Imagine that we would agree that the current government of China is largely correct
10512500	10518660	and benevolent, and maybe we don't agree on this. But if we did, how can we make sure that this
10518660	10524740	stays like this? If you don't have checks and balances and division of power, it's hard to
10524740	10529460	achieve. You don't have a solution for that problem. But the abolishment of government
10529460	10534820	basically would remove the control structure. From a cybernetic perspective, there is an optimal
10534820	10541140	point in the system that the regulation should be happening, that you can measure the current
10541140	10546180	incentives and the regulator would be properly incentivized to make the right decisions and
10546180	10550660	change the payout metrics of everything below it in such a way that the local prisoners' dilemmas
10550660	10556020	get resolved. You cannot resolve the prisoner's dilemma without some kind of eternal control
10557060	10565140	that emulates an infinite game in a way. There's a sense in which it seems like the
10565140	10571300	reason the parts of government that don't work well currently is because there's not good
10572900	10579300	mechanisms through which to interact, for the citizenry to interact with government.
10581380	10586900	It hasn't caught up in terms of technology. I think once you integrate some of the digital
10586900	10593220	revolution of being able to have a lot of access to data, be able to vote on different ideas
10593220	10599140	at a local level, at all levels, at the optimal level, like you're saying, that can resolve the
10599140	10607380	prisoner dilemmas and to integrate AI to help you automate things that don't require the human
10607380	10614740	ingenuity, I feel like that's where government could operate that well and can also break apart
10614740	10621700	the inefficient bureaucracies if needed. There'll be a strong incentive to be efficient and
10621700	10627700	successful. Throughout human history, we see an evolutionary competition of modes of government
10627700	10632820	and of individual governments is in these modes. Every nation state in some sense is some kind of
10632820	10638260	organism that has found different solutions for the problem of government. You could look
10638260	10643460	at all these different models and the different scales at which it exists as empirical attempts
10643460	10651300	to validate the idea of how to build a better government. I suspect that the idea of anarchism
10651300	10658020	similar to the idea of communism is the result of being disenchanted with the ugliness of the
10658020	10664660	real existing solutions and the attempt to get to an utopia. And I suspect that communism
10664660	10669060	originally was not a utopia. I think that in the same way as original Christianity,
10669620	10675940	it had a particular kind of vision. And this vision is a society, a mode of organization
10675940	10684340	within that society in which humans can coexist at scale without coercion, the same way as we do in
10684340	10690980	a healthy family. In a good family, you don't terrorize each other into compliance, but you
10690980	10695940	understand what everybody needs and what everybody is able to contribute and what the intended future
10695940	10701220	of the whole thing is. And everybody coordinates their behavior in the right way and informs
10701220	10705700	each other about how to do this. And all the interactions that happen are instrumental to
10705700	10711300	making that happen. Could this happen at scale? And I think this is the idea of communism.
10711300	10716900	Communism is opposed to the idea that we need economic terror or other forms of terror to make
10716900	10721780	that happen. But in practice, what happened is that the proto-communist countries, the real
10721780	10728100	existing socialism replaced a part of the economic terror with moral terror. So we were told to do
10728100	10732740	the right thing for moral reasons. And of course, it didn't really work and the economy eventually
10732740	10738500	collapsed. And the moral terror had actual real cost. People were in prison because they were
10738580	10747140	morally non-compliant. The other thing is that the idea of communism became a utopia. So it
10747140	10752740	basically was projected into the afterlife. We were told in my childhood that communism was
10752740	10757540	a hypothetical society to which we were in a permanent revolution that justified everything
10757540	10762500	that was presently wrong with society morally. But it was something that our grandchildren
10762500	10767460	probably would not ever see because it was too ideal and too far in the future to make it happen
10767460	10772500	right now. And people were just not there yet morally. And the same thing happened with
10772500	10777460	Christianity. This notion of heaven was mythologized and projected into an afterlife.
10777460	10782260	And I think this was just the idea of God's kingdom, of this world in which we instantiate
10782260	10787220	the next level transcendental agent in the perfect form. So everything goes smoothly and without
10787220	10792820	violence and without conflict and without this human messiness on this economic messiness and
10792820	10798180	the terror and coercion that existed in the present societies. And the idea of whether
10798180	10804660	humans can exist at scale in a harmonious way and non-coercively is untested. A lot of people
10804660	10809860	tested it, but didn't get it to work so far. And the utopia is a world in where you get all
10809860	10815860	the good things without any of the bad things. And you are, I think, very susceptible to believe in
10815860	10820660	utopias when you are very young and don't understand that everything has to happen in
10820660	10825060	causal patterns, that there is always feedback loops that ultimately are closed. There's nothing
10825060	10830260	that just happens because it's good or bad. Good or bad don't exist in isolation. They only exist
10830260	10840980	with respect to larger systems. So can you intuit why utopias fail as systems? So like having a
10840980	10848260	utopia that's out there beyond the horizon, is it because then... It's not only because it's
10848260	10856020	impossible to achieve utopias, but it's because what certain humans, certain small number of humans
10856020	10872740	start to sort of greedily attain power and money and control and influence as they see the power
10872820	10877860	in using this idea of a utopia for propaganda. That's a bit like saying,
10877860	10882740	why is my garden not perfect? It's because some evil weeds are overgrowing it and they always do.
10883780	10888740	But this is not how it works. A good garden is a system that is in balance and requires minimal
10888740	10896420	interactions by the gardener. And so you need to create a system that is designed to self-stabilize.
10896420	10900980	And the design of social systems requires not just the implementation of the desired functionality,
10900980	10905540	but the next level design, also in biological systems. You need to create a system that wants
10905540	10910980	to converge to the intended function. And so instead of just creating an institution like
10910980	10916900	the FDA that is performing a particular kind of role in society, you need to make sure that the
10916900	10921620	FDA is actually driven by a system that wants to do this optimally, that is incentivized to do it
10921620	10927460	optimally, and then makes the performance that is actually enacted in every generation instrumental
10927460	10932260	to that thing, that actual goal. And that is much harder to design and to achieve.
10932260	10938340	So you have to design a system where... Listen, communism also was, quote unquote, incentivized
10939060	10947940	to be a feedback loop system that achieves that utopia. It wasn't working given human nature.
10947940	10950100	The incentives were not correct given human nature.
10950100	10955380	So how do you incentivize people when they are getting coal off the ground to work as hard as
10955380	10961700	possible? Because it's a terrible job and it's very bad for your health. And how do you do this?
10961700	10967940	And you can give them prices and metals and status to some degree. There's only so much status to
10967940	10974500	give for that. And most people will not fall for this. Or you can pay them. And you probably have
10974500	10980500	to pay them in an asymmetric way, because if you pay everybody the same and you nationalize the coal
10980500	10983860	mines, eventually people will figure out that they can game the system.
10984100	10991300	Yes. So you're describing capitalism. Capitalism is the present solution to the system. And
10991300	10996500	what we also noticed that I think that Marx was correct in saying that capitalism is prone to
10996500	11002900	crisis. That capitalism is a system that in its dynamics is not convergent, but divergent.
11002900	11010500	It's not a stable system. And that eventually it produces an enormous potential for productivity,
11011060	11016100	but it also is systematically misallocating resources. So a lot of people cannot participate
11016820	11021620	in the production and consumption anymore. And this is what we observe. We observe that
11021620	11027620	the middle class in the US is tiny. A lot of people think that they're middle class,
11027620	11030420	but if you are still flying economy, you're not middle class.
11030900	11035220	Every class is a magnitude smaller than the previous class.
11035220	11036020	That's the Twitter quote.
11042020	11047380	The thing about classes is really like airline classes. There is the no-play class. A lot of
11047380	11052180	people are economy class, business class, and very few are first class, and some are budget.
11052180	11059540	I mean, I understand. I think there is, yeah, maybe some people, probably I would push back
11060260	11063540	against that definition of the middle class. It does feel like the middle class is pretty large,
11063540	11068980	but yes, there's a discrepancy in terms of wealth. There's a big wealth gap.
11068980	11074660	In terms of the productivity that our society could have, there is no reason for anybody to
11074660	11079140	fly economy. We would be able to let everybody travel in style.
11080180	11085700	But also some people like to be frugal even when they're billionaires. So let's take that into
11085700	11091060	account. I mean, you probably don't need to be a traveling lavish, but you also don't need to
11091060	11096260	be tortured, right? There is a difference between frugal and subjecting yourself to torture.
11096260	11101060	Listen, I love economy. I don't understand why you're comparing a flying economy to torture.
11102180	11108100	Although the flight here, there's two crying babies next to me. But that has nothing to do
11108100	11111460	with economy. It has to do with crying babies. They're very cute, though.
11112420	11116340	I have two kids, and sometimes I have to go back to visit the grandparents.
11119540	11124820	Back means going from the West Coast to Germany, and it's a long flight.
11124820	11130580	Is it true that when you're a father, you grow immune to the crying and all that kind of stuff?
11131540	11137380	Because me just not having kids, other people's kids can be quite annoying when they're crying
11137380	11141700	and screaming and all that kind of stuff. When you have children and you're wired up
11141700	11145780	in the default natural way, you're lucky in this regard, you fall in love with them.
11146500	11152020	And this falling in love with them means that you basically start to see the world through their
11152020	11159060	eyes and you understand that in a given situation, they cannot do anything but expressing despair.
11159860	11165300	And so it becomes more differentiated. I noticed that, for instance, my son is typically acting on
11165860	11172660	a pure experience of what things are like right now, and he has to do this right now. And you
11172660	11179060	have this small child when he was a baby and so on, where he was just immediately expressing
11179060	11183380	what he felt. And if you cannot regulate this from the outside, there's no point to be upset
11183380	11188580	about it. It's like dealing with weather or something like this. You all have to get through
11188580	11194980	it. And it's not easy for him either. But if you also have a daughter, maybe she is planning
11194980	11201220	for that. Maybe she understands that she's sitting in the car behind you and she's screaming at the
11201220	11205220	top of her lungs and you're almost doing an accident and you really don't know what to do.
11205860	11209860	What should I have done to make you stop screaming? You could have given me candy.
11209860	11216580	I think that's like a cat versus dog discussion. I love it. Because you said
11217620	11226580	a fundamental aspect of that is love that makes it all worth it. What in this monkey riding an
11226580	11232660	elephant in a dream world, what role does love play in the human condition?
11234420	11238100	I think that love is the facilitator of non-transactual interaction.
11240020	11245620	And you are observing your own purposes. Some of these purposes go beyond your ego. They go
11245620	11249620	beyond the particular organism that you are and your local interests.
11249620	11251380	That's what you mean by non-transactional.
11251380	11254900	Yes. So basically when you are acting in a transactional way, it means that you are
11254900	11259540	respecting something in return for you from the one that you're interacting with.
11260740	11264660	You are interacting with a random stranger. You buy something from them on eBay. You expect a
11264660	11269140	fair value for the money that you send them and vice versa. Because you don't know that person.
11269140	11273220	You don't have any kind of relationship to them. But when you know this person a little bit better
11273220	11277540	and you know the situation that they're in, you understand what they try to achieve in their life
11277540	11283460	and you approve because you realize that they're in some sense serving the same human sacredness
11283460	11288020	as you are. And they need to think that you have. Maybe you give it to them as a present.
11289300	11296020	But the feeling itself of joy is a kind of benefit, is a kind of transaction.
11296820	11301940	Yes, but the joy is not the point. The joy is the signal that you get. It's the reinforcement
11301940	11307060	signal that your brain sends to you because you are acting on the incentives of the agent that
11307060	11312180	you're part of. We are meant to be part of something larger. This is the way in which
11312180	11317940	we out-competed other hominids. Take that, Neanderthals.
11318980	11325620	Yeah. And also other humans. There was a population bottleneck for human society
11325620	11332420	that leads to an extreme lack of genetic diversity among humans. If you look at Bushmen
11332420	11337140	in the Kalahari, that basically tribes that are not that far distant to each other have more
11337140	11343460	genetic diversity than exists between Europeans and Chinese. And it's because basically the
11343460	11349300	out-of-Africa population at some point had a bottleneck of just a few thousand individuals.
11350260	11355780	And what probably happened is not that at any time the number of people shrunk below a few
11355780	11360900	hundred thousand. What probably happened is that there was a small group that had a decisive
11360900	11365220	mutation that produced an advantage. And this group multiplied and killed everybody else.
11366180	11372900	And we are descendants of that group. Yeah. I wonder what the peculiar characteristics
11372900	11377220	of that group. I mean, we can never know. And a lot of people do.
11378180	11383780	We can only just listen to the echoes in ours, like the ripples that are still within us.
11383780	11388740	So I suspect what eventually made a big difference was the ability to organize at scale,
11389700	11393460	to program each other. With ideas.
11393460	11399220	That we became programmable, that we are willing to work in lockstep, that we went above the tribal
11399220	11404180	level, that we no longer were groups of a few hundred individuals and acted on direct
11404180	11410180	reputation systems transactionally, but that we basically evolved an adaptation to become state
11410180	11416820	building. Yeah. To form collectives outside of
11416820	11419860	the direct collectives. Yes. And that basically a part of us
11419860	11424020	became committed to serving something outside of what we know.
11424020	11428020	Yeah. Then that's kind of what love is. And it's terrifying because it meant that
11428020	11434900	we eradicated the others. It's a force. It's an adaptive force that gets us ahead in evolution,
11434900	11437300	which means we displace something else that doesn't have that.
11438820	11441700	Oh, so we had to murder a lot of people that weren't about love.
11442420	11445460	So love led to destruction. We didn't have the same strong love as we did.
11446100	11452100	Right. That's why I mentioned this thing with fascism. When you see these speeches,
11452660	11457220	do you want total war? And everybody says, yes, right? This is this big,
11458180	11461700	oh my God, we are part of something that is more important than me, that gives meaning to my
11461700	11465620	existence. Fair enough.
11469060	11476260	Do you have advice for young people today in high school, in college, that are thinking about what
11476260	11482420	to do with their career, with their life, so that at the end of the whole thing,
11482420	11488820	they can be proud of what they did? Don't cheat. Have integrity.
11488820	11490500	A for integrity. Integrity.
11490500	11495300	So what does integrity look like when you're the river or the leaf or the fat frog in a lake?
11496580	11503300	It basically means that you try to figure out what the thing is that is the most right.
11504100	11508500	And this doesn't mean that you have to look for what other people tell you what's right,
11509220	11514820	but you have to aim for more autonomy. So things need to be right independently of what other
11514820	11523140	people say. I always felt that when people told me to listen to what others say, like
11523860	11529060	read the room, build your ideas of what's true based on the high status people of your in-group,
11529060	11535060	that does not protect me from fascism. The only way to protect yourself from fascism is to decide
11535060	11541700	it's the world that is being built here, the world that I want to be in. And so in some sense,
11541700	11546820	try to make your behavior sustainable. Act in such a way that you would feel comfortable on
11546820	11550980	all sides of the transaction. Realize that everybody is you in a different timeline,
11550980	11554580	but is seeing things differently and has reasons to do so.
11555300	11562900	Yeah, I've come to realize this recently. There is an inner voice that tells you what's right and
11563460	11570820	wrong. And speaking of reading the room, there's times what integrity looks like is there's times
11570820	11576660	when a lot of people are doing something wrong. And what integrity looks like is not going on
11576660	11584420	Twitter and tweeting about it, but not participating quietly, not doing. So it's not like signaling or
11585300	11591140	all this kind of stuff, but actually living what you think is right, like living it.
11591140	11596420	There's also sometimes this expectation that others are like us. So imagine the possibility
11596420	11602580	that some of the people around you are space aliens that only look human. So they don't have
11602580	11607540	the same prayers as you do. They don't have the same impulses of what's right and wrong. There's
11607540	11613860	a large diversity in these basic impulses that people can have in a given situation.
11613860	11618980	And now realize that you are a space alien. You are not actually human. You think that you're
11618980	11623940	human, but you don't know what it means, what it's like to be human. You just make it up as
11623940	11629780	you go along like everybody else. And you have to figure that out, what it means that you are
11630420	11634900	full human being, what it means to be human in the world and how to connect with others on that.
11635620	11643060	And there's also something, don't be afraid in the sense that if you do this, you're not good enough.
11643060	11647220	Because if you are acting on these incentives of integrity, you become trustworthy.
11647220	11652100	That's the way in which you can recognize each other. There is a particular place where you
11652100	11657780	can meet and you can figure out what that place is, where you will give support to people because
11657780	11663220	you realize that they act with integrity and they will also do that. So in some sense,
11663220	11669220	you are safe if you do that. You're not always protected. There are people which will abuse you
11669220	11675060	and that are bad actors in a way that it's hard to imagine before you meet them. But
11676100	11679060	there is also people which will try to protect you.
11679060	11687380	CB Yeah. Thank you for saying that. That's such a hopeful message that no matter what happens to
11687460	11696900	you, there'll be a place, there's people you'll meet that also have what you have
11697780	11702100	and you will find happiness there and safety there.
11702100	11707620	SRS Yeah. But it doesn't need to end well. It can also all go wrong. So there's no guarantees
11707620	11713220	in this life. So you can do everything right and you still can fail and you can still
11713220	11718660	horrible things happen to you that traumatize you and mutilate you and you have to be grateful
11718660	11724980	if it doesn't happen. CB And ultimately be grateful no matter what happens
11724980	11732420	because even just being alive is pretty damn nice. SRS Yeah. Even that, you know. The gratefulness
11732420	11739860	in some sense is also just generated by your brain to keep you going. It's all the trick.
11740820	11748420	CB Speaking of which, Camus said, I see many people die because they judge that life is not
11748420	11754900	worth living. I see others paradoxically getting killed for the ideas or illusions that give them
11754900	11760340	a reason for living. What is called the reason for living is also an excellent reason for dying.
11761380	11768580	I therefore conclude that the meaning of life is the most urgent of questions. So I have to ask
11768580	11775780	what, Jascha Bach, is the meaning of life? It is an urgent question according to Camus.
11775780	11782580	SRS I don't think that there's a single answer to this. Nothing makes sense unless the mind
11782580	11789380	makes it so. So you basically have to project a purpose. And if you zoom out far enough,
11789380	11793220	there's the heat test of the universe and everything is meaningless. Everything is just
11793220	11797220	a blip in between. And the question is, do you find meaning in this blip in between?
11797860	11804340	Do you find meaning in observing squirrels? Do you find meaning in raising children and
11804340	11809860	projecting a multi-generational organism into the future? Do you find meaning in projecting an
11809860	11814420	aesthetic of the world that you like into the future and trying to serve that aesthetic?
11814420	11819140	And if you do, then life has that meaning. And if you don't, then it doesn't.
11819140	11824420	SRS I kind of enjoy the idea that you just create the most
11825940	11832260	vibrant, the most weird, the most unique kind of blip you can, given your environment,
11832260	11843140	given your set of skills, just be the most weird set of like local pocket of complexity you can be.
11843780	11848820	So that like when people study the universe, they'll pause and be like, uh, that's weird.
11850260	11854500	It looks like a useful strategy, but of course, it's still motivated reasoning.
11857540	11859620	You're obviously acting on your incentives here.
11859620	11865540	SRS It's still a story we tell ourselves within a dream that's hardly in touch with the reality.
11865540	11868100	SRS It's definitely a good strategy if you are a podcaster.
11868340	11874820	And a human, which I'm still trying to figure out if I am.
11874820	11876900	SRS Yeah, there's a mutual relationship somehow.
11876900	11881940	SRS Somehow. Josh, you're one of the most incredible people I know.
11882900	11888100	I really love talking to you. I love talking to you again. And it's really an honor that you spend
11888100	11894420	your valuable time with me. I hope we get to talk many times throughout our short and meaningless
11894420	11900020	lives. Or meaningful. Or meaningful. Thank you, Alex. I enjoyed this conversation very much.
11901060	11907220	Thanks for listening to this conversation with Joshua Bach and thank you to Coinbase, Codecademy,
11907220	11913380	Linode, NetSuite, and ExpressVPN. Check them out in the description to support this podcast.
11914180	11916820	Now, let me leave you with some words from Carl Jung.
11917860	11923220	People will do anything, no matter how absurd, in order to avoid facing their own souls.
11923860	11929300	One does not become enlightened by imagining figures of light, but by making the darkness
11929860	11935060	conscious. Thank you for listening and hope to see you next time.
11953220	11954020	you
