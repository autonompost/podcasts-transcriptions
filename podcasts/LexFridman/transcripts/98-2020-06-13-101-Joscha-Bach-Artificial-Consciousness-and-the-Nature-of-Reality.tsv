start	end	text
0	5520	The following is a conversation with Yosha Bach, VP of Research at the AI Foundation
5520	12160	with a history of research positions at MIT and Harvard. Yosha is one of the most unique
12160	18080	and brilliant people in the artificial intelligence community, exploring the workings of human mind,
18080	24560	intelligence, consciousness, life on Earth, and the possibly simulated fabric of our universe.
25840	28640	I could see myself talking to Yosha many times in the future.
29600	36480	Quick summary of the ads. Two sponsors, ExpressVPN and CashApp. Please consider supporting the
36480	43840	podcast by signing up at expressvpn.com slash LexPod and downloading CashApp and using code
43840	51120	lexpodcast. This is the artificial intelligence podcast. If you enjoy it, subscribe on YouTube,
51120	56080	review it with 5 stars on Apple Podcast, support it on Patreon, or simply connect with me on
56160	63360	Twitter at Lex Friedman. Since this comes up more often than I ever would have imagined,
63360	68400	I challenge you to try to figure out how to spell my last name without using the letter E.
69520	74640	And it will probably be the correct way. As usual, I'll do a few minutes of ads now
74640	77760	and never any ads in the middle that can break the flow of the conversation.
78880	85760	This show is sponsored by ExpressVPN. Get it at expressvpn.com slash LexPod to support this
85760	92240	podcast and to get an extra three months free on a one-year package. I've been using ExpressVPN
92240	99520	for many years. I love it. I think ExpressVPN is the best VPN out there. They told me to say it,
100160	105840	but I think it actually happens to be true. It doesn't log your data, it's crazy fast,
105840	112080	and it's easy to use. Literally, just one big power on button. Again, for obvious reasons,
112080	116720	it's really important that they don't log your data. It works on Linux and everywhere else,
116720	123840	too. Shout out to my favorite flavor of Linux, Ubuntu Mate 2004. Once again,
123840	131200	get it at expressvpn.com slash LexPod to support this podcast and to get an extra three months
131200	138000	free on a one-year package. This show is presented by Cash App, the number one finance app in the
138000	144240	App Store. When you get it, use code LexPodcast. Cash App lets you send money to friends,
144240	149760	buy bitcoin, and invest in the stock market with as little as one dollar. Since Cash App does
149760	154800	fractional share trading, let me mention that the order execution algorithm that works behind the
154800	161120	scenes to create the abstraction of the fractional orders is an algorithmic marvel. Big props to the
161120	166240	Cash App engineers for taking a step up to the next layer of abstraction over the stock market,
166240	171120	making trading more accessible for new investors and diversification much easier.
171760	176320	So again, if you get Cash App from the App Store or Google Play and use the code LexPodcast,
177760	182320	you get ten dollars. Cash App will also donate ten dollars to FIRST,
182320	187600	an organization that is helping advance robotics and STEM education for young people around the
187600	192960	world. And now, here's my conversation with Joscha Bach.
210960	215920	As you've said, you grew up in a forest in East Germany, just as we were talking about off mic,
216720	222560	to parents who were artists. And now I think, at least to me, you've become one of the most unique
222560	227120	thinkers in the AI world. So can we try to reverse engineer your mind a little bit?
228080	234880	What were the key philosophers, scientists, ideas, maybe even movies, or just realizations
234880	240800	that had an impact on you when you were growing up that kind of led to the trajectory, or were the
240800	244560	key sort of crossroads in the trajectory of your intellectual development?
246240	253200	My father came from a long tradition of architects, a distant branch of the Bach family,
253760	260960	and so basically he was technically a nerd. And nerds need to interface in society with
260960	266480	non-standard ways. Sometimes I define a nerd as somebody who thinks that the purpose of
266480	273120	communication is to submit your ideas to peer review. And normal people understand that the
273120	278720	primary purpose of communication is to negotiate alignment. And these purposes tend to conflict,
279280	282560	which means that nerds have to learn how to interact with society at large.
283680	288720	Who is the reviewer in the nerd's view of communication?
288720	294320	Everybody who you consider to be a peer. So whatever hapless individual is around,
294640	297920	you would try to make him or her the gift of information.
301440	307680	Now, by the way, my research mal-informed me. So you're an architect or artist?
309200	314240	He did study architecture, but basically my grandfather made the wrong decision.
314240	323120	He married an aristocrat and was drawn into the war. And he came back after 15 years. So basically
323120	330320	my father was not parented by a nerd, but by somebody who tried to tell him what to do
330320	337040	and expected him to do what he was told. And he was unable to. He's unable to do things if he's
337040	342640	not intrinsically motivated. So in some sense, my grandmother broke her son and her son responded
343440	349120	when he became an architect to become an artist. So he built 100-wasser architecture. He built
349120	353920	houses without right angles. He built lots of things that didn't work in the more brutalist
353920	360000	traditions of Eastern Germany. And so he bought an old watermill, moved out of the countryside,
360000	365360	and did only what he wanted to do, which was art. Eastern Germany was perfect for Bohemia because
365920	370400	you had complete material safety, was heavily subsidized, healthcare was free,
370400	373120	you didn't have to worry about rent or pensions or anything.
373120	375520	So it's a socialized communist side of Germany.
375520	379760	And the other thing is it was almost impossible not to be in political disagreement with your
379760	383840	government, which is very productive for artists. So everything that you do is intrinsically
383840	388880	meaningful because it will always touch on the deeper currents of society, of culture,
388880	393440	and be in conflict with it and tension with it. And you will always have to define yourself
393440	396560	with respect to this. So what impact did your father, this
398480	403040	outside-of-the-box thinker against the government, against the world artists,
403280	408000	he was actually not a thinker. He was somebody who only got self-aware to the degree that he
408000	415920	needed to make himself functional. So in some sense, he was also late 1960s, and he was in
415920	421360	some sense a hippie. So he became a one-person cult. He lived out there in his kingdom. He built
421360	430240	big sculpture gardens and started many avenues of art and so on, and convinced a woman to live with
430240	435120	him. She was also an architect, and she adored him and decided to share her life with him.
435120	442160	And I basically grew up in a big cave full of books. I'm almost feral. And I was bored out there.
442160	448400	It was very, very beautiful, very quiet, and quite lonely. So I started to read. And by the time I
448400	453600	came to school, I've read everything until fourth grade and then some. And there was not a real way
453600	458880	for me to relate to the outside world. And I couldn't quite put my finger on why. And today
458880	464240	I know it was because I was a nerd, obviously. And I was the only nerd around. So there were no
464240	470560	other kids like me. And there was nobody interested in physics or computing or mathematics and so on.
471200	476480	And this village school that I went to was basically a nice school. Kids were nice to me.
476480	480960	I was not beaten up, but I also didn't make many friends or build deep relationships. They only
480960	485120	happened starting from ninth grade when I went into a school for mathematics and physics.
485840	487760	Do you remember any key books from this moment?
487760	493440	Yes. I basically read everything. So I went to the library and I worked my way through the
493440	498240	children's and young adult sections. And then I read a lot of science fiction. For instance,
498240	503680	Stanislav Lem, basically the great author of cybernetics, has influenced me. Back then,
503680	507280	I didn't see him as a big influence because everything that he wrote seemed to be so natural
507280	513200	to me. And it's only later that I contrasted it with what other people wrote. Another thing that
513200	518640	was very influential on me were the classical philosophers and also the teacher of romanticism.
518640	526240	So German poetry and art, Droste-Hilshoff and Heine and up to Hesse and so on.
526960	532400	Hesse, I love Hesse. So at which point do the classical philosophers end? At this point,
532400	537200	we're in the 21st century. What's the latest classical philosopher? Does this stretch through
538240	542480	even as far as Nietzsche or is this, are we talking about Plato and Aristotle?
542480	545680	I think that Nietzsche is the classical equivalent of a shit poster.
548160	554160	So he's very smart and easy to read. And yeah, but he's not so much trolling others. He's trolling
554160	558640	himself because he was at odds with the world. Largely his romantic relationships didn't work
558640	565360	out. He got angry and he basically became a nihilist. Isn't that a beautiful way to be
565360	570160	as an intellectual is to constantly be trolling yourself, to be in that conflict, in that,
570880	574800	no, in that tension. I think it's the lack of self-awareness. At some point, you have to
574800	579840	understand the comedy of your own situation. If you take yourself seriously and you are not
579840	584160	functional, it ends in tragedy as it did for Nietzsche. I think you think he took himself
584160	589680	too seriously in that tension. And as you find the same thing in Hesse and so on, the Steppenwolf
589680	594720	syndrome is classic adolescent where you basically feel misunderstood by the world and you don't
594720	599520	understand that all the misunderstandings are the result of your own lack of self-awareness.
599520	604160	Because you think that you are a prototypical human and the others around you should behave
604160	608400	the same way as you expect them based on your innate instincts and it doesn't work out. And
608400	613920	you become a transcendentalist to deal with that. So it's very, very understandable and have great
613920	618720	sympathies for this to the degree that I can have sympathy for my own intellectual history,
618720	624800	but you have to grow out of it. So as an intellectual, a life well-lived, a journey
624800	628720	well-traveled is one where you don't take yourself seriously. No, I think that you
629680	636800	neither serious or not serious yourself because you need to become unimportant as a subject. That is,
637840	642720	if you are a philosopher, belief is not a verb. You don't do this for the audience,
642720	647120	you don't do it for yourself. You have to submit to the things that are possibly true
647120	651680	and you have to follow wherever your inquiry leads, but it's not about you. It has nothing
651680	658160	to do with you. So do you think then people like Ayn Rand believed sort of in the idea of there's
658160	663680	objective truth. So what's your sense in the philosophical, if you remove yourself,
663680	666480	that's objective from the picture, you think it's possible to actually discover
667280	673120	ideas that are true or are we just in a mesh of relative concepts that are neither true nor false?
673120	678080	It's just a giant mess. You cannot define objective truth without understanding the
678080	683040	nature of truth in the first place. So what does the brain mean by saying that discover
683040	687280	something as truth? So for instance, a model can be predictive or not predictive.
688640	693280	Then there can be a sense in which a mathematical statement can be true because it's defined as
693280	700160	true under certain conditions. So it's basically a particular state that a variable can have in a
700160	705760	simple game. And then you can have a correspondence between systems and talk about truth, which is
705760	710480	again a type of model correspondence. And there also seems to be a particular kind of ground truth.
710480	715920	So for instance, you're confronted with the enormity of something existing at all, right?
715920	721920	That's stunning when you realize something exists rather than nothing. And this seems to be true,
721920	726480	right? There's an absolute truth in the fact that something seems to be happening.
726480	731600	Yeah, that to me is a showstopper. I could just think about that idea and be amazed by that idea
732320	736080	for the rest of my life and not go any farther because I don't even know the answer to that.
736080	739440	Why does anything exist at all? Well, the easiest answer is existence
739440	743440	is the default, right? So this is the lowest number of bits that you would need to encode this.
743440	744560	Whose answer? Who provides it?
744560	747920	The simplest answer to this is that existence is the default.
747920	749840	What about non-existence? I mean, that seems...
750800	754720	Non-existence might not be a meaningful notion in this sense. So in some sense,
754720	760080	if everything that can exist exists, for something to exist, it probably needs to be implementable.
760080	764880	The only thing that can be implemented is finite automata. So maybe the whole of existence is the
764880	769760	superposition of all finite automata. And we are in some region of the fractal that has the properties
769760	774960	that it can contain us. What does it mean to be a superposition of finite automata? So any
776640	782480	superposition of all possible rules? Imagine that every automaton is basically an operator
782480	787040	that acts on some substrate. And as a result, you get emergent patterns.
787040	790400	What's the substrate? I have no idea to know.
790400	793760	But some substrate. It's something that can store
793760	796240	information. Something that can store information,
796240	798320	there's a automaton. Something that can hold state.
798320	802400	Still, it doesn't make sense to me the why that exists at all. I could just sit there with a
803440	808080	beer or a vodka and just enjoy the fact, pondering the why.
808080	813040	May not have a why. This might be the wrong direction to ask into this. So there could be
813040	819440	no relation in the why direction without asking for a purpose or for a cause. It doesn't mean that
819440	825440	everything has to have a purpose or cause, right? So we mentioned some philosophers in that early,
825440	829600	just taking a brief step back into that. Okay, so we asked ourselves,
829600	833680	when did classical philosophy end? I think for Germany, it largely ended with the
833680	839040	first revolution. Which one was that? This was when we entered the monarchy
839760	845200	and started a democracy. And at this point, we basically came up with a new form of government
845200	850000	that didn't have a good sense of this new organism that society wanted to be.
850000	855040	And in a way, it decapitated the universities. So the universities went on through modernism
855040	859600	like a headless chicken. At the same time, democracy failed in Germany and we got fascism
859600	864800	as a result. And it burned down things in a similar way as Stalinism burned down intellectual
864800	870160	traditions in Russia. And Germany, both Germanies have not recovered from this. Eastern Germany had
870160	875680	this vulgar dialectic materialism and Western Germany didn't get much more edgy than Habermas.
876320	880640	So in some sense, both countries lost their intellectual traditions and killing off and
880640	888160	driving out the Jews didn't help. Yeah, so that was the end of really rigorous,
888160	893200	what you would say is classical philosophy. There's also this thing that in some sense,
894080	901680	the low-hanging fruits in philosophy were mostly wrapped. And the last big things that we discovered
901680	906800	was the constructivist turn in mathematics. So to understand that the parts of mathematics
906800	912320	that work are computation, there was a very significant discovery in the first half of the
912320	918720	20th century. And it hasn't fully permeated philosophy and even physics yet. So physicists
918720	923360	checked out the code libraries for mathematics before constructivism became universal.
923920	927440	What's constructivism? Are you referring to Gödel's incompleteness theorem?
929440	934640	Gödel himself, I think, didn't get it yet. Hilbert could get it. Hilbert saw that, for instance,
934640	939200	a country's set theoretic experiments in mathematics led into contradictions. And
939200	944560	he noticed that with the current semantics, we cannot build a computer in mathematics that
944560	950160	runs mathematics without crashing. And Gödel could prove this. And so what Gödel could show
950160	955440	is using classical mathematical semantics, you run into contradictions. And because Gödel strongly
955440	961040	believed in these semantics and more than in what he could observe and so on, he was shocked. It
961040	965200	basically shook his world to the core because in some sense he felt that the world has to be
965200	970720	implemented in classical mathematics. And for Turing, it wasn't quite so bad. I think that
970720	976080	Turing could see that the solution is to understand that mathematics was computation all along,
976080	983200	which means, for instance, pi in classical mathematics is a value. It's also a function,
983200	988320	but it's the same thing. And in computation, a function is only a value when you can compute
988320	992800	it. And if you cannot compute the last digit of pi, you only have a function. You can plug this
992800	997520	function into your local sun, let it run until the sun burns out. This is it. This is the last digit
997520	1002400	of pi you will know. But it also means that there can be no process in the physical universe or in
1002400	1008320	any physically realized computer that depends on having known the last digit of pi. Which means
1008320	1013280	there are parts of physics that are defined in such a way that cannot strictly be true because
1013280	1017520	assuming that this could be true leads into contradictions. So I think putting computation
1017520	1023760	at the center of the world view is actually the right way to think about it. Yes. And Wittgenstein
1023760	1028960	could see it. And Wittgenstein basically preempted the logitist program of AI that Minsky started
1028960	1034960	later, like 30 years later. Turing was actually a pupil of Wittgenstein. Really? I didn't know
1034960	1038320	there's any connection between Turing and Wittgenstein. Wittgenstein even canceled some
1038320	1042000	classes when Turing was not present because he thought it was not worth spending the time with
1042000	1046880	the others. Oh, interesting. Interesting. And if you read the Tractatus, it's a very beautiful book.
1046880	1051840	Basically, one thought on 75 pages. It's very non-typical for philosophy because it doesn't have
1051840	1058160	arguments in it and it doesn't have references in it. It's just one thought that is not intending
1058160	1062880	to convince anybody. It says it's mostly for people that had the same insight as me.
1062880	1068640	Just spell it out. And this insight is there is a way in which mathematics and philosophy
1068640	1073600	ought to meet. Mathematics tries to understand the domain of all languages by starting with those
1073600	1079120	that are so formalizable that you can prove all the properties of the statements that you make.
1079200	1083600	But the price that you pay is that your language is very, very simple. So it's very hard to say
1083600	1088320	something meaningful in mathematics. And it looks complicated to people, but it's far less
1088320	1092560	complicated than what our brain is casually doing all the time. And it makes sense of reality.
1093280	1098640	And philosophy is coming from the top. So it's mostly starting from natural languages with
1098640	1103360	vaguely defined concepts. And the hope is that mathematics and philosophy can meet at some point.
1103920	1107760	And Wittgenstein was trying to make them meet. And he already understood that, for instance,
1107760	1112240	you could express everything with the NAND calculus, that you could reduce the entire logic
1112240	1116960	to NAND gates as we do in our modern computers. So in some sense, he already understood Turing
1116960	1121040	universality before Turing spelled it out. I think when he wrote the track titles,
1121040	1124480	he didn't understand yet that the idea was so important and significant.
1124480	1130240	And I suspect then when Turing wrote it out, nobody cared that much. Turing was not that famous.
1130240	1136720	When he lived, it was mostly his work in decrypting the German codes that made him famous
1136720	1141920	or gave him some notoriety. But the saint status that he has to computer science right now in the AI
1141920	1145840	is something that I think he could acquire later. That's kind of interesting. Do you think of
1145840	1150800	computation and computer science? And you kind of represent that to me as maybe that's the modern
1150800	1159040	day. You, in a sense, are the new philosopher by sort of the computer scientist who dares to ask
1159040	1165040	the bigger questions that philosophy originally started. Is the new philosopher? Certainly not
1165040	1170320	me, I think. I'm mostly still this child that grows up in a very beautiful valley and looks
1170320	1174080	at the world from the outside and tries to understand what's going on. And my teachers
1174080	1178480	tell me things and they largely don't make sense. So I have to make my own models. I have to
1178480	1183120	discover the foundations of what the others are saying. I have to try to fix them to be charitable.
1183120	1187520	I try to understand what they must have thought originally or what their teachers or their
1187520	1191280	teachers' teachers must have thought until everything got lost in translation and how to
1191280	1196000	make sense of the reality that we are in. And whenever I have an original idea, I'm usually
1196000	1200080	late to the party by, say, 400 years. And the only thing that's good is that the parties get
1200080	1205920	smaller and smaller the older I get and the more I explore. The parties get smaller? And more
1205920	1212480	exclusive. And more exclusive. So it seems like one of the key qualities of your upbringing was
1212480	1218880	that you were not tethered, whether it's because of your parents or in general maybe you're something
1218960	1225280	within your mind, some genetic material, that you were not tethered to the ideas of the general
1225280	1232400	populace, which is actually a unique property or kind of the education system and whatever,
1232400	1237760	not education system, just existing in this world forces certain sets of ideas onto you.
1237760	1245120	Can you disentangle that? Why are you not so tethered? Even in your work today,
1245120	1255360	you seem to not care about perhaps a best paper in Europe. Being tethered to particular things
1255360	1262480	that current today in this year people seem to value as a thing you put on your CV and resume.
1262480	1266880	You're a little bit more outside of that world, outside of the world of ideas that people are
1266880	1272960	especially focused in, the benchmarks of today, the things. Can you disentangle that? Because I
1272960	1277360	think that's inspiring. And if there were more people like that, we might be able to solve some
1277360	1284400	of the bigger problems that sort of AI dreams to solve. And there's a big danger in this because
1284400	1290160	in a way you are expected to marry into an intellectual tradition and visit this tradition
1290160	1295200	into a particular school. If everybody comes up with their own paradigms, the whole thing is not
1295200	1300640	cumulative as an enterprise. So in some sense, you need a healthy balance, you need paradigmatic
1300640	1305200	thinkers, and you need people that work within given paradigms. Basically, scientists today
1305200	1310640	define themselves largely by methods. And it's almost a disease that we think as a scientist,
1310640	1316480	somebody who was convinced by their guidance counselor that they should join a particular
1316480	1320240	discipline and then they find a good mentor to learn the right methods. And then they are lucky
1320240	1326000	enough and privileged enough to join the right team. And then their name will show up on influential
1326000	1330240	papers. But we also see that there are diminishing returns with this approach.
1330800	1336400	And when our field, computer science and AI started, most of the people that joined this
1336400	1342080	field had interesting opinions. And today's thinkers in AI either don't have interesting
1342080	1345920	opinions at all, or these opinions are inconsequential for what they're actually doing.
1345920	1350160	Because what they're doing is they apply the state of the art methods with a small epsilon.
1350880	1358000	And this is often a good idea if you think that this is the best way to make progress. And for me,
1358000	1364000	it's first of all, very boring. If somebody else can do it, why should I do it? If the current
1364000	1369520	methods of machine learning lead to strong AI, why should I be doing it? I will just wait until
1369520	1374880	they're done and wait until they do this on the beach, or read interesting books or write some
1374880	1379920	and have fun. But if you don't think that you're currently doing the right thing,
1379920	1386800	if you are missing some perspectives, then it's required to think outside of the box.
1386800	1393680	It's also required to understand the boxes. But it's necessary to understand what worked
1393680	1398560	and what didn't work and for what reasons. So you have to be willing to ask new questions
1398560	1403840	and design new methods whenever you want to answer them. And you have to be willing to dismiss
1403840	1407600	the existing methods if you think that they're not going to yield the right answers.
1407680	1409600	It's very bad career advice to do that.
1410880	1419760	So maybe to briefly stay for one more time in the early days, when would you say for you was the
1419760	1426480	dream before we dive into the discussions that we just almost started? When was the dream to
1426480	1430640	understand or maybe to create human level intelligence born for you?
1430800	1435920	I think that you can see AI largely today as
1438320	1443040	advanced information processing. If you would change the acronym of AI into that,
1443040	1446720	most people in the field would be happy. It would not change anything what they're doing.
1446720	1452320	We're automating statistics and many of the statistical models are more advanced than what
1452320	1457760	statisticians had in the past. And it's pretty good work. It's very productive. And the other
1457760	1462240	aspect of AI is its philosophical project. And this philosophical project is very risky
1462960	1466480	and very few people work on it. And it's not clear if it succeeds.
1467120	1472640	First of all, you keep throwing a lot of really interesting ideas and I have to
1472640	1479120	pick which ones we go with. First of all, you use the term information processing,
1479920	1488960	just information processing, as if it's the muck of existence, as if it's the epitome,
1491120	1494240	that the entirety of the universe might be information processing, that consciousness,
1494240	1497440	the intelligence might be information processing. So maybe you can comment on
1499120	1505760	if the advanced information processing is a limiting kind of realm of ideas. And then the
1506000	1508880	other one is what do you mean by the philosophical project?
1508880	1515360	So I suspect that general intelligence is the result of trying to solve general problems.
1515920	1520240	So intelligence, I think, is the ability to model. It's not necessarily
1520240	1523920	goal-directed rationality or something. Many intelligent people are bad at this.
1524480	1529840	But it's the ability to be presented with a number of patterns and see a structure in
1529840	1534640	those patterns and be able to predict the next set of patterns to make sense of things.
1535040	1540720	And some problems are very general. Usually intelligence serves control. So you make these
1540720	1544480	models for a particular purpose of interacting as an agent with the world and getting certain
1544480	1550160	results. But the intelligence itself is in a sense instrumental to something. But by itself,
1550160	1554480	it's just the ability to make models. And some of the problems are so general that the system
1554480	1559040	that makes them needs to understand what itself is and how it relates to the environment.
1559840	1565040	So as a child, for instance, you notice you do certain things despite you perceiving
1565040	1568880	yourself as wanting different things. So you become aware of your own psychology.
1569760	1574160	You become aware of the fact that you have complex structure in yourself and you need to
1574160	1578640	model yourself to reverse engineer yourself to be able to predict how you will react to
1578640	1582880	certain situations and how you deal with yourself in relationship to your environment.
1582880	1588080	And this project, if you reverse engineer yourself in your relationship to reality
1588080	1593360	and the nature of a universe that can continue, if you go all the way, this is basically the
1593360	1597920	project of AI or you could say the project of AI is a very important component in it.
1597920	1604400	The true Turing test in a way is you ask a system what is intelligence. If that system is able to
1604400	1611440	explain what it is, how it works, then you should assign it the property of being intelligent in
1611440	1616560	this general sense. So the test that Turing was administering in a way, I don't think that
1616560	1621360	he couldn't see it, but he didn't express it yet in the original 1950 paper, is that
1622080	1626800	he was trying to find out whether he was generally intelligent. Because in order to take this test,
1626800	1630480	the rap is, of course, you need to be able to understand what the system is saying.
1630480	1634880	And we don't yet know if we can build an AI. We don't yet know if we are generally intelligent.
1634880	1638240	Basically, you win the Turing test by building an AI.
1638240	1643760	Yes. So in a sense, hidden within the Turing test is a kind of recursive test.
1643760	1648720	Yes. It's a test on us. The Turing test is basically a test of the conjecture whether
1648720	1651600	people are intelligent enough to understand themselves.
1652880	1657520	Okay. But you also mentioned a little bit of a self-awareness. And in the project of AI,
1657520	1663920	do you think this kind of emergent self-awareness is one of the fundamental aspects of intelligence?
1663920	1670960	So as opposed to goal oriented, as you said, kind of puzzle solving, is coming to
1671840	1675200	grips with the idea that you're an agent in the world?
1676160	1678800	I find that many highly intelligent people are not very self-aware.
1679920	1685120	So self-awareness and intelligence are not the same thing. And you can also be self-aware if
1685120	1690240	you have good priors, especially, without being especially intelligent. So you don't need to be
1690240	1694720	very good at solving puzzles if the system that you are already implements the solution.
1695360	1699600	But I do find intelligence. So you kind of mentioned children, right?
1701440	1707840	Is that the fundamental project of AI, is to create the learning system that's able to
1708480	1711920	exist in the world? So you kind of drew a difference between self-awareness
1712480	1719440	and intelligence. And yet you said that the self-awareness seems to be important for children.
1720160	1726480	So I call this ability to make sense of the world and your own place and so to make you able to
1726480	1730720	understand what you're doing in this world, sentience. And I would distinguish sentience
1730720	1735440	from intelligence because sentience is possessing certain classes of models.
1736160	1739600	And intelligence is a way to get to these models if you don't already have them.
1741200	1750560	I see. So can you maybe pause a bit and try to answer the question that we just said we
1750560	1756080	may not be able to answer? And it might be a recursive meta question of what is intelligence?
1757120	1759600	I think that intelligence is the ability to make models.
1760480	1768640	So models. I think it's useful as examples, very popular now, neural networks form representations
1770000	1777920	of large scale data set. They form models of those data sets. When you say models
1778480	1782400	and look at today's neural networks, what are the difference of how you're thinking about
1782400	1787440	what is intelligent in saying that intelligence is the process of making models?
1788080	1793680	Two aspects to this question. One is the representation. Is the representation adequate
1793680	1799920	for the domain that we want to represent? And the other one is the type of the model that you arrive
1799920	1806240	at adequate. So basically, are you modeling the correct domain? And I think in both of these
1806880	1811680	cases, modern AI is lacking still. And I think that I'm not saying anything new here. I'm not
1811680	1818000	criticizing the field. Most of the people that design our paradigms are aware of that.
1818000	1824000	And so one aspect that we're missing is unified learning. When we learn, we at some point discover
1824000	1829840	that everything that we sense is part of the same object, which means we learn it all into one model
1829840	1833680	and we call this model the universe. So the experience of the world that we are embedded on
1833680	1839120	is not a secret direct wire to physical reality. Physical reality is a weird quantum graph that we
1839120	1844480	can never experience or get access to. But it has these properties that it can create certain
1844480	1848720	patterns at our systemic interface to the world. And we make sense of these patterns and the
1848720	1853440	relationship between the patterns that we discover is what we call the physical universe. So at some
1853440	1860560	point in our development as a nervous system, we discover that everything that we relate to
1861440	1867440	in the world can be mapped to a region in the same three-dimensional space, by and large. We now know
1867440	1872000	in physics that this is not quite true. The world is not actually three-dimensional, but the world
1872000	1876960	that we are entangled with at the level which we are entangled with is largely a flat three-dimensional
1876960	1883040	space. And so this is the model that our brain is intuitively making. And this is, I think, what
1883040	1888240	gave rise to this intuition of res extensa of this material world, this material domain. It's one of
1888240	1893280	the mental domains, but it's just the class of all models that relate to this environment, this
1893280	1897760	three-dimensional physics engine in which we are embedded. Physics engine in which we're embedded.
1897760	1908960	I love that. Just slowly pause. So the quantum graph, I think you called it, which is the real
1908960	1913760	world, which you can never get access to, there's a bunch of questions I want to sort of disentangle
1913760	1919920	that. But maybe one useful one, one of your recent talks I looked at, can you just describe the
1919920	1926480	basics? Can you talk about what is dualism? What is idealism? What is materialism? What is functionalism
1926480	1930960	and what connects with you most? Because you just mentioned there's a reality we don't have access
1930960	1938720	to. What does that even mean? And why don't we get access to it? Aren't we part of that reality? Why
1938720	1944640	can't we access it? So the particular trajectory that mostly exists in the West is the result of
1944640	1948320	our indoctrination by a cult for 2000 years. A cult, which one?
1948320	1955120	Yes, the Catholic cult mostly. And for better or worse, it has created or defined many of the
1955120	1960000	modes of interaction that we have that has created this society, but it has also in some sense
1961680	1969120	scarred our rationality. And the intuition that exists, if you would translate the mythology of
1969120	1974240	the Catholic church into the modern world is that the world in which you and me interact
1974240	1979520	is something like a multiplayer role-playing adventure. And the money and the objects that
1979520	1985840	we have in this world, this is all not real. Or as Eastern philosophers would say, it's Maya. It's
1985840	1992160	just stuff that appears to be meaningful and this embedding in this meaning, if you believe in it,
1992160	1998320	it's samsara. It's basically the identification with the needs of the mundane secular everyday
1998320	2004720	existence. And the Catholics also introduced the notion of higher meaning, the sacred.
2005280	2010800	And this existed before, but eventually the natural shape of God is the platonic form of
2010800	2014160	the civilization that you are part of. It's basically the superorganism that is formed
2014160	2020880	by the individuals as an intentional agent. And basically, the Catholics used a relatively crude
2020880	2026000	mythology to implement software on the minds of people and get the software synchronized to make
2026000	2033120	them walk on lockstep to basically get this God online and to make it efficient and effective.
2033760	2039040	And I think God technically is just a self that spans multiple brains as opposed to your and
2039040	2044640	myself, which mostly exists just on one brain. And so in some sense, you can construct a self
2044640	2050320	functionally as a function that is implemented by brains that exists across brains. And this is a
2050320	2060240	God with a small g. This is one of the nice features of our brains. It seems that we can
2060240	2064320	all download the same piece of software like God in this case and share it.
2065120	2071920	You give everybody a spec and the mathematical constraints that are intrinsic to information
2071920	2076560	processing make sure that given the same spec, you come up with a compatible structure.
2076800	2081680	Okay. So there's the space of ideas that we all share. And we think that's kind of the mind,
2082480	2090160	but that's separate from the idea is from Christianity, from religion is that there's
2090160	2094880	a separate thing between the mind. There is a real world. And this real world is the world
2094880	2099440	in which God exists. God is the coder of the multiplayer adventure, so to speak. And
2100240	2103920	we are all players in this game. And that's dualism.
2104880	2110320	But the dualism aspect is because the mental realm exists in a different implementation
2110320	2115840	than the physical realm. And the mental realm is real. And a lot of people have this intuition
2115840	2121520	that there is this real room in which you and me talk and speak right now. Then comes a layer of
2121520	2126720	physics and abstract rules and so on. And then comes another real room where our souls are.
2126720	2130960	And our true form isn't a thing that gives us phenomenal experience. And this is, of course,
2130960	2137440	a very confused notion that you would get. And basically, it's the result of connecting
2137440	2144000	materialism and idealism in the wrong way. Okay. I apologize, but I think it's really
2144000	2149920	helpful if we just tried to define terms. What is dualism? What is idealism? What is
2149920	2154160	materialism for people who don't know? So the idea of dualism in our cultural
2154160	2159040	tradition is that there are two substances, a mental substance and a physical substance.
2159120	2165280	And they interact by different rules. And the physical world is basically causally closed and
2165280	2170160	is built on a low-level causal structure. So there's basically a bottom level that is
2170160	2174720	causally closed that's entirely mechanical. And mechanical in the widest sense. So it's
2174720	2178880	computational. There's basically a physical world in which information flows around.
2178880	2183120	And physics describes the laws of how information flows around in this world.
2183120	2186960	Would you compare it to a computer where you have hardware and software?
2186960	2190720	The computer is a generalization of information flowing around. Basically,
2191360	2195920	but you won't discover that there is a universal principle. You can define this universal machine
2196800	2202080	that is able to perform all the computations. So all these machines have the same power.
2202080	2205520	This means that you can always define a translation between them as long as they
2205520	2210880	have unlimited memory to be able to perform each other's computations.
2210880	2215440	So would you then say that materialism is this whole world is just the hardware
2215440	2218000	and idealism is this whole world is just the software?
2218800	2222560	Not quite. I think that most idealists don't have a notion of software yet
2222560	2225120	because software also comes down to information processing.
2226960	2231360	What you notice is the only thing that is real to you and me is this experiential world in which
2231360	2236080	things matter, in which things have taste, in which things have color, phenomenal content and so on.
2236640	2238080	You are bringing up consciousness.
2238800	2244400	And this is distinct from the physical world in which things have values only in an abstract
2244400	2252160	sense and you only look at cold patterns moving around. So how does anything feel like something?
2252160	2255760	And this connection between the two things is very puzzling to a lot of people,
2255760	2259200	and of course, too many philosophers. So idealism starts out with the notion that
2259200	2265440	mind is primary, materialism thinks that matter is primary. And so for the idealist,
2265440	2270640	the material patterns that we see playing out are part of the dream that the mind is dreaming.
2271200	2278880	And we exist in a mind on a higher plane of existence if you want. And for the materialist,
2278880	2284720	there is only this material thing and that generates some models and we are the result
2284720	2288800	of these models. And in some sense, I don't think that we should understand,
2288800	2294560	if you understand it properly, materialism and idealism is a dichotomy, but as two different
2294560	2299120	aspects of the same thing. So the weird thing is we don't exist in the physical world. We do
2299120	2301760	exist inside of a story that the brain tells itself.
2305600	2313760	Let my information processing take that in. We don't exist in the physical world. We exist
2313760	2318160	in the narrative. Basically, a brain cannot feel anything. A neuron cannot feel anything.
2318160	2322320	They're physical things. Physical systems are unable to experience anything, but it would be
2322320	2326960	very useful for the brain or for the organism to know what it would be like to be a person and to
2326960	2332880	feel something. So the brain creates a simulacrum of such a person that it uses to model the
2332880	2337040	interactions of the person. It's the best model of what that brain, this organism,
2337040	2341280	thinks it is in relationship to its environment. So it creates that model. It's a story,
2341280	2344480	a multimedia novel that the brain is continuously writing and updating.
2344480	2356240	But you also said that we exist in that story. What is real in any of this? So
2357600	2363920	there's a, again, these terms are, you kind of said there's a quantum graph. I mean, what is
2363920	2370320	this whole thing running on then? Is the story, and is it completely, fundamentally impossible
2370320	2377520	to get access to it? Because isn't the story supposed to, isn't the brain in something,
2378160	2384080	in existing, in some kind of context? So what we can identify as computer scientists,
2384080	2390480	we can engineer systems and test our theories this way that might have the necessary and
2390480	2395440	sufficient properties to produce the phenomena that we are observing, which is there is a self
2395440	2402240	in a virtual world that is generated in somebody's neocortex that is contained in the skull of this
2402240	2407600	primate here. And when I point at this, this indexicality is of course wrong, but I do create
2407600	2413600	something that is likely to give rise to patterns on your retina that allow you to interpret what
2413600	2418560	I'm saying. But we both know that the world that you and me are seeing is not the real
2418560	2422960	physical world. What we are seeing is a virtual reality generated in your brain
2422960	2427360	to explain the patterns on your retina. How close is it to the real world? That's kind of the
2427360	2435520	question. Is it, when you have people like Donald Hoffman that say that you're really far away,
2435520	2441120	the thing we're seeing you and I now, that interface we have is very far away from anything,
2441520	2445120	we don't even have anything close to the sense of what the real world is,
2445120	2451120	or is it a very surface piece of architecture? Imagine you look at the Mandelbrot fractal,
2451120	2457520	this famous thing that Bernard Mandelbrot discovered. If you see an overall shape in
2457520	2464080	there, but if you truly understand it, you know it's two lines of code. It's basically a series
2464080	2470080	that is being tested for complex numbers in the complex number plane for every point. And for
2470080	2478160	those where the series is diverging, you paint this black. And where it's converging, you don't.
2478160	2487920	And you get the intermediate colors by checking how far it diverges. This gives you this shape
2487920	2492000	of this fractal. But imagine you live inside of this fractal and you don't have access to
2492000	2496400	where you are in the fractal, or you have not discovered the generator function even.
2497040	2501520	So what you see is, all I can see right now is a spiral. And the spiral moves a little bit to
2501520	2506320	the right. Is this an accurate model of reality? Yes, it is. It is an adequate description.
2507040	2511920	You know that there is actually no spiral in the Mandelbrot fractal. It only appears like this to
2511920	2517200	an observer that is interpreting things as a two-dimensional space and then defines certain
2517200	2521200	regularities in there at a certain scale that it currently observes. Because if you zoom in,
2521200	2524880	the spiral might disappear and turn out to be something different at a different resolution.
2525840	2529200	At this level, you have the spiral. And then you discover the spiral moves to the right and
2529200	2533600	at some point it disappears. So you have a singularity. At this point, your model is no
2533600	2538080	longer valid. You cannot predict what happens beyond the singularity. But you can observe again
2538080	2542880	and you will see it hit another spiral and at this point it disappeared. So we now have a second
2542880	2547680	order law. And if you make 30 layers of these laws, then you have a description of the world
2547680	2551360	that is similar to the one that we come up with when we describe the reality around us.
2551360	2556080	It's reasonably predictive. It does not cut to the core of it. It does not explain how it's
2556080	2561200	being generated, how it actually works. But it's relatively good to explain the universe that we
2561200	2565200	are entangled with. But you don't think the tools of computer science, the tools of physics could
2566080	2571680	step outside, see the whole drawing and get at the basic mechanism of how the pattern,
2571680	2576960	the spiral is generated? Imagine you would find yourself embedded into a Mandelbrot fractal and
2576960	2580960	you try to figure out what works and you somehow have a Turing machine with enough
2580960	2587840	memory to think. And as a result, you come to this idea, it must be some kind of automaton.
2587840	2591840	And maybe you just enumerate all the possible automata until you get to the one that produces
2591840	2596560	your reality. So you can identify necessary and sufficient condition. For instance,
2596560	2601680	we discovered that mathematics itself is the domain of all languages. And then we see that
2601680	2606480	most of the domains of mathematics that we have discovered are in some sense describing the same
2606480	2610640	fractals. This is what category theory is obsessed about, that you can map these different domains
2610640	2615360	to each other. So there are not that many fractals. And some of these have interesting
2615360	2622320	structures and symmetry breaks. And so you can discover what region of this global fractal you
2622320	2626240	might be embedded in from first principles. But the only way you can get there is from
2626240	2630880	first principles. So basically, your understanding of the universe has to start with automata and
2630880	2635840	then number theory and then spaces and so on. Yeah, I think like Stephen Wolfram still dreams
2636800	2642400	that he'll be able to arrive at the fundamental rules of the cellular automata or the generalization
2642400	2650800	of which is behind our universe. You've said on this topic, you said in a recent conversation
2650800	2656640	that quote, some people think that a simulation can't be conscious and only a physical system can,
2657200	2661440	but they got it completely backward. A physical system cannot be conscious.
2661440	2665360	Only a simulation can be conscious. Consciousness is a simulated property,
2665360	2672800	the simulated self. Just like you said, the mind is kind of the, we'll call it story narrative.
2672800	2675760	There's a simulation. So our mind is essentially a simulation.
2676560	2682800	Usually I try to use the terminology so that the mind is basically the principles that produce the
2682800	2688400	simulation. It's the software that is implemented by your brain. And the mind is creating both the
2688480	2694640	universe that we are in and the self, the idea of a person that is on the other side of attention
2694640	2699680	and is embedded in this world. Why is that important? That idea of a self,
2699680	2705360	why is that an important feature in the simulation? It's basically a result of the
2705360	2709600	purpose that the mind has. It's a tool for modeling, right? We are not actually monkeys.
2709600	2716400	We are side effects of the regulation needs of monkeys. And what the monkey has to regulate is
2717200	2722880	the relationship of an organism to an outside world that is in large part also consisting of
2722880	2729040	other organisms. And as a result, it basically has regulation targets that it tries to get to.
2729040	2733280	These regulation targets start with priors. They're basically like unconditional reflexes
2733280	2737120	that we are more or less born with. And then we can reverse engineer them to make them more
2737120	2741360	consistent. And then we get more detailed models about how the world works and how to interact with
2741360	2747680	it. And so these priors that you commit to are largely target values that our needs should
2747680	2754560	approach, set points. And this deviation to the set point creates some urge, some tension. And we
2754560	2759200	find ourselves living inside of feedback loops, right? The consciousness emerges over dimensions
2759200	2764640	of disagreements with the universe, things where you care, things are not the way they should be,
2764640	2769280	but you need to regulate. And so in some sense, the sense self is the result of all the
2769280	2773200	identifications that you're having. And identification is a regulation target that
2773200	2777040	you're committing to. It's a dimension that you care about, that you think is important.
2777600	2783280	And this is also what locks you in. If you let go of these commitments of these identifications,
2783280	2788080	you get free. There's nothing that you have to do anymore. And if you let go of all of them,
2788080	2790640	you're completely free and you can enter nirvana because you're done.
2791840	2797120	And actually, this is a good time to pause and say thank you to sort of a friend of mine,
2797120	2800720	Gustav Sortostrom, who introduced me to your work. I wanted to give him a shout out.
2801920	2806640	He's a brilliant guy. And I think the AI community is actually quite amazing. And Gustav is a good
2806640	2811360	representative of that. You are as well. So I'm glad, first of all, I'm glad the internet exists
2811360	2817760	and YouTube exists where I can watch your talks and then get to your book and study your writing
2817760	2824240	and think about, you know, that's amazing. Okay. But you've kind of described sort of this
2824240	2829840	emergent phenomena of consciousness from the simulation. So what about the hard problem of
2829840	2839760	consciousness? Can you just linger on it? Like, why does it still feel like I understand you're
2839760	2845680	kind of the self is an important part of the simulation. But why does the simulation feel
2845680	2851440	like something? So if you look at the book by, say, George R. R. Martin, where the characters
2851440	2856480	have plausible psychology and they stand on a hill because they want to conquer the city below
2856480	2860160	the hill and they're done in it. And they look at the color of the sky and they are apprehensive
2860720	2864800	and feel empowered and all these things. Why do they have these emotions? It's because it's written
2864800	2869200	into the story, right? And it's written to the story because it's an adequate model of the person
2869200	2874320	that predicts what they're going to do next. And the same thing has happened to us. So it's
2874320	2879680	basically a story that our brain is writing. It's not written in words. It's written in perceptual
2879680	2886240	content, basically multimedia content. And it's a model of what the person would feel if it existed.
2887120	2892720	So it's a virtual person. And you and me happen to be this virtual person. So this virtual person
2892720	2897680	gets access to the language center and talks about the sky being blue. And this is us.
2898400	2902080	But hold on a second. Do I exist in your simulation?
2903040	2909760	You do exist in an almost similar way as me. So there are internal states that are less
2909760	2917280	accessible for me that you have and so on. And my model might not be completely adequate.
2917280	2921760	There are also things that I might perceive about you that you don't perceive. But in some sense,
2921760	2928000	both you and me are some puppets, two puppets that enact a play in my mind. And I identify
2928000	2932640	with one of them because I can control one of the puppets directly. And with the other one,
2932640	2938320	I can create things in between. So for instance, we can go on an interaction that even leads to
2938320	2942960	a coupling to a feedback loop. So we can sync things together in a certain way or feel things
2942960	2947520	together. But this coupling is itself not a physical phenomenon. It's entirely a software
2947520	2951600	phenomenon. It's the result of two different implementations interacting with each other.
2951680	2959520	So that's interesting. The way you think about it is the entirety of existence,
2959520	2970720	the simulation and where each mind is a little sub simulation. Why doesn't your mind have access
2972160	2979600	to my mind's full state? For the same reason that my mind doesn't have access to its own full state.
2982560	2986880	I mean, there is no trick involved. So basically, when I know something about myself,
2986880	2991280	it's because I made a model. So one part of your brain is tasked with modeling what other parts
2991280	2997520	of your brain are doing. Yes. But there seems to be an incredible consistency about this world
2997520	3003360	in the physical sense, that there's repeatable experiments and so on. How does that fit into our
3003360	3009680	silly Descendants of Apes simulation of the world? So why is everything so repeatable?
3010160	3014720	Not everything. There's a lot of fundamental physics experiments that are repeatable
3016160	3021760	for a long time, all over the place, and so on. The laws of physics, how does that fit in?
3021760	3027920	It seems that the parts of the world that are not deterministic are not long lived. So if you build
3027920	3034560	a system, any kind of automaton, so if you build simulations of something, you'll notice that
3035120	3040640	the phenomena that endure are those that give rise to stable dynamics. So basically,
3040640	3045120	if you see anything that is complex in the world, it's the result usually of some control,
3045120	3049360	of some feedback that keeps it stable around certain attractors. And the things that are
3049360	3053840	not stable, that don't give rise to certain harmonic patterns and so on, they tend to get
3053840	3060640	weeded out over time. So if we are in a region of the universe that sustains complexity,
3060720	3066480	which is required to implement minds like ours, this is going to be a region of the universe that
3066480	3071920	is very tightly controlled and controllable. So it's going to have lots of interesting symmetries
3071920	3079520	and also symmetry breaks that allow the creation of structure. But they exist where? So there's
3079520	3082960	such an interesting idea that our mind is simulation that's constructing the narrative.
3083920	3091920	My question is just to try to understand how that fits with the entirety of the universe.
3091920	3096160	You're saying that there's a region of this universe that allows enough complexity to create
3096160	3103840	creatures like us, but what's the connection between the brain, the mind, and the broader
3103840	3109280	universe? Which comes first? Which is more fundamental? Is the mind the starting point
3109280	3113520	the universe is emergent? Is the universe the starting point the minds are emergent?
3114320	3119120	I think quite clearly the latter. That's at least a much easier explanation because it allows us to
3119120	3124960	make causal models and I don't see any way to construct an inverse causality. So what happens
3124960	3131280	when you die to your mind simulation? My implementation ceases. So basically the thing
3131280	3136080	that implements myself will no longer be present, which means if I am not implemented on the minds
3136080	3141840	of other people, the thing that I identify with. The weird thing is I don't actually have an
3141840	3148400	identity beyond the identity that I construct. If I was the Dalai Lama, he identifies as a form
3148400	3155280	of government. So basically the Dalai Lama gets reborn not because he's confused, but because he
3155280	3162080	is not identifying as a human being. He runs on a human being. He's basically a governmental software
3162080	3167200	that is instantiated in every new generation in you. So his advisors will pick someone who does
3167200	3172720	this in the next generation. So if you identify with this, you are no longer a human and you don't
3172720	3178240	die in the sense that what dies is only the body of the human that you run on. To kill the Dalai
3178240	3184080	Lama, you have to kill his tradition. And if we look at ourselves, we realize that we are to a
3184080	3188080	small part like this, most of us. So for instance, if you have children, you realize something lives
3188080	3193360	on in them. Or if you spark an idea in the world, something lives on. Or if you identify
3193360	3198240	with the society around you, because you are in part that you're not just this human being.
3198240	3203680	Yeah. So in a sense, you are kind of like a Dalai Lama in the sense that you,
3203680	3209280	Joshua Bach, is just a collection of ideas. So you have this operating system on which a bunch of
3209280	3215600	ideas live and interact. And then once you die, some of them jump off the ship.
3216320	3220000	Put it the other way. Identity is a software state. It's a construction.
3220000	3224560	It's not physically real. Identity is not a physical concept.
3224560	3227920	It's basically a representation of different objects on the same world line.
3228800	3237760	But identity lives and dies. Are you attached? What's the fundamental thing? Is it the ideas
3237760	3243040	that come together to form identity? Or is each individual identity actually a fundamental thing?
3243040	3246560	It's a representation that you can get agency over if you care. So basically,
3246560	3249760	you can choose what you identify with if you want to.
3249760	3261200	No, but it just seems if the mind is not real, that the birth and death is not a crucial part of
3261200	3274720	it. Maybe I'm attached to this whole biological organism, but it seems that being a physical
3274720	3281680	object in this world is an important aspect of birth and death. It feels like it has to be
3281680	3285680	physical to die. It feels like simulations don't have to die.
3286640	3291040	The physics that we experience is not the real physics. There is no color and sound in the
3291040	3296400	real world. Color and sound are types of representations that you get if you want to
3296400	3301120	model reality with oscillators. So colors and sound, in some sense, have octaves,
3301120	3305760	and it's because they are represented properly with oscillators. So that's why colors form a
3305760	3311360	circle of views. And colors have harmonics, sounds have harmonics as a result of synchronizing
3311360	3317760	oscillators in the brain. So the world that we subjectively interact with is fundamentally
3317760	3322400	the result of the representation mechanisms in our brain. They are mathematically, to some degree,
3322400	3326960	universal. There are certain regularities that you can discover in the patterns and not others.
3326960	3331280	But the patterns that we get, this is not the real world. The world that we interact with is
3331280	3336080	always made of too many parts to count. So when you look at this table and so on,
3336080	3341280	it's consisting of so many molecules and atoms that you cannot count them. So you only look
3341280	3347440	at the aggregate dynamics, at limit dynamics. If you had almost infinitely many particles,
3347920	3351840	what would be the dynamics of the table? And this is roughly what you get. So geometry that
3351840	3356800	we are interacting with is the result of discovering those operators that work in the
3356800	3361280	limit that you get by building an infinite series that converges. For those parts where
3361280	3365040	it converges is geometry. For those parts where it doesn't converge, it's chaos.
3365600	3371840	Right. And then so all of that is filtered through the consciousness that's emergent in our
3371840	3376640	narrative. So the consciousness gives it color, gives it feeling, gives it flavor.
3377280	3383360	So I think the feeling, flavor and so on is given by the relationship that a feature has to all the
3383360	3389360	other features. It's basically a giant relational graph that is our subjective universe. The color
3389360	3395280	is given by those aspects of the representation or this experiential color where you care about,
3395280	3399520	where you have identifications, where something means something, where you are the inside of a
3399520	3404960	feedback loop. And the dimensions of caring are basically dimensions of this motivational system
3404960	3410640	that we emerge over. The meaning of the relations, the graph. Can you elaborate
3410640	3417600	that a little bit? Maybe we can even step back and ask the question of what is consciousness
3417600	3423680	to be sort of more systematic? How do you think about consciousness?
3423680	3428800	I think that consciousness is largely a model of the contents of your attention. It's a mechanism
3428800	3434560	that has evolved for a certain type of learning. At the moment, our machine learning systems
3434560	3441760	largely work by building chains of weighted sums of real numbers with some non-linearity.
3441760	3449280	And you will learn by piping an error signal through these different chain layers and adjusting
3449280	3455280	the weights in these weighted sums. And you can approximate most polynomials with this
3456240	3461200	if you have enough training data. But the price is you need to change a lot of these weights.
3461280	3466560	Basically, the error is piped backwards into the system until it accumulates at certain junctures
3466560	3471360	in the network. And everything else evens out statistically. And only at these junctures,
3471360	3474880	this is where you had the actual error in the network and you make the change there. This is
3474880	3479360	a very slow process. And our brains don't have enough time for that because we don't get old
3479360	3484320	enough to play Go the way that our machines learn to play Go. So instead, what we do is an
3484320	3488400	attention-based learning. We pinpoint the probable region in the network where we
3489360	3495440	can make an improvement. And then we store this binding state together with the expected outcome
3495440	3500240	in a protocol. And this ability to make indexed memories for the purpose of learning to revisit
3500240	3506720	these commitments later, this requires a memory of the contents of our attention.
3506720	3511600	Another aspect is when I construct my reality, I make mistakes. So I see things that turn out
3511600	3516800	to be reflections or shadows and so on, which means I have to be able to point out which features
3516800	3522800	of my perception gave rise to the present construction of reality. So the system needs
3522800	3528800	to pay attention to the features that are currently in its focus. And it also needs
3528800	3532560	to pay attention to whether it pays attention itself, in part because the attentional system
3532560	3537120	gets trained with the same mechanism, so it's reflexive, but also in part because your attention
3537120	3542080	lapses if you don't pay attention to the attention itself. So it's the thing that I'm currently
3542080	3547600	seeing, just a dream that my brain has spun off into some kind of daydream, or am I still
3547600	3552480	paying attention to my percept? So you have to periodically go back and see whether you're still
3552480	3556800	paying attention. And if you have this loop and you make it tight enough between the system
3556800	3561120	becoming aware of the contents of its attention and the fact that it's paying attention itself
3561120	3565600	and makes attention the object of its attention, I think this is the loop over which we wake up.
3565600	3571360	So there's this attentional mechanism that's somehow self-referential,
3571360	3577520	that's fundamental to what consciousness is. So just to ask you a question, I don't know how
3577520	3582400	much you're familiar with the recent breakthroughs in natural language processing, they use attentional
3582400	3591440	mechanism, they use something called transformers to learn patterns and sentences by allowing a
3591440	3597200	work to focus its attention to particular parts of the sentence at each individual. So like
3597200	3603680	parametrize and make it learnable, the dynamics of a sentence by having like a little window
3603680	3612160	into the sentence. Do you think that's like a little step towards that eventually will take us
3612160	3617840	to the attentional mechanisms from which consciousness can emerge? Not quite. I think
3617840	3624080	it models only one aspect of attention. In the early days of automated language translation,
3624080	3628960	there was an example that I found particularly funny where somebody tried to translate a text
3628960	3636800	from English into German and it was a bat broke the window. And the translation in German was
3637520	3643600	eine Fledermaus zerbracht das Fenster mit einem Baseballschläger. So to translate back into English
3643600	3651360	a bat, this flying mammal broke the window with a baseball bat. And it seemed to be the most similar
3651360	3657200	to this program because it somehow maximized the possibility of translating the concept bat
3657200	3662480	into German in the same sentence. And this is a mistake that the transformer model is not doing
3662480	3667200	because it's tracking identity. And the attentional mechanism in the transformer model is basically
3667200	3673680	putting its finger on individual concepts and make sure that these concepts pop up later in the text
3674240	3680160	and tracks basically the individuals through the text. And that's why the system can learn
3680160	3684320	things that other systems couldn't before it, which makes it, for instance, possible to write
3684320	3688880	a text where it talks about the scientist, then the scientist has a name and has a pronoun, and
3689440	3694160	it gets a consistent story about that thing. What it does not do, it doesn't fully integrate this.
3694160	3699120	So this meaning falls apart at some point. It loses track of this context. It does not yet
3699120	3703600	understand that everything that it says has to refer to the same universe. And this is where this
3703600	3708800	thing falls apart. But the attention in the transformer model does not go beyond tracking
3708800	3713840	identity. And tracking identity is an important part of attention, but it's a different, very
3713840	3718560	specific attentional mechanism. And it's not the one that gives rise to the type of consciousness
3718560	3723280	that we have. Just to linger on, what do you mean by identity in the context of language?
3723840	3729200	So when you talk about language, you have different words that can refer to the same concept.
3729200	3735840	Got it. So space of concepts. Yes. And it can also be in a nominal sense or
3736640	3742160	inexical sense that you say this word does not only refer to this class of objects,
3742160	3748160	but it refers to a definite object, to some kind of agent that waves their way through the story,
3748160	3753360	and is only referred by different ways in the language. So the language is basically a
3753360	3760080	projection from a conceptual representation, from a scene that is evolving into a discrete
3760080	3766320	string of symbols. And what the transformer is able to do, it learns aspects of this projection
3766320	3771360	mechanism that other models couldn't learn. So have you ever seen an artificial intelligence
3771360	3777360	or any kind of construction idea that allows for, unlike neural networks or perhaps within
3777360	3784080	neural networks, that's able to form something where the space of concepts continues to be
3784080	3790720	integrated? So what you're describing, building a knowledge base, building this consistent,
3790720	3794880	larger and larger sets of ideas that would then allow for deeper understanding?
3795840	3800640	Wittgenstein thought that we can build everything from language, from basically a logical
3800640	3805680	grammatical construct. And I think to some degree, this was also what Minsky believed.
3806240	3809760	So that's why he focused so much on common sense reasoning and so on. And
3810720	3812880	a project that was inspired by him was Psyche.
3814880	3816160	That's still going on.
3816160	3820240	Yes. Of course, ideas don't die. Only people die.
3822400	3822960	That's true.
3823760	3827600	And Psyche is a productive project. It's just probably not one that is going to
3828320	3832080	converge to general intelligence. The thing that Wittgenstein couldn't solve,
3832080	3836720	and he looked at this in his book at the end of his life, Philosophical Investigations,
3836720	3841600	was the notion of images. So images play an important role in tractatus. The tractatus
3841600	3846000	is an attempt to basically turn philosophy into logical programming language, to design a logical
3846000	3850880	language in which you can do actual philosophy that's rich enough for doing this. And the
3850880	3857040	difficulty was to deal with perceptual content. And eventually, I think he decided that he was
3857040	3863120	not able to solve it. And I think this preempted the failure of the Logitus program in AI. And
3863120	3867920	the solution, as we see it today, is we need more general function approximation. There are
3867920	3872960	functions, geometric functions, that we learn to approximate that cannot be efficiently expressed
3872960	3878080	and computed in a grammatical language. We can, of course, build automata that go via number theory
3878080	3885200	and so on to learn in algebra and then compute an approximation of this geometry. But to equate
3885200	3888320	language and geometry is not an efficient way to think about it.
3889200	3892960	So functional, well, you kind of just said that neural networks are sort of,
3893920	3900080	the approach that neural networks takes is actually more general than what can be expressed through
3900960	3906640	language. Yes. So what can be efficiently expressed through language at the data rates
3906640	3911920	at which we process grammatical language. Okay. So you don't think languages,
3911920	3915440	so you disagree with Wittgenstein that language is not fundamental to...
3915440	3922640	I agree with Wittgenstein. I just agree with the late Wittgenstein. And I also agree with the
3922640	3927280	beauty of the early Wittgenstein. I think that the tractatus itself is probably the most beautiful
3927280	3932960	philosophical text that was written in the 20th century. But language is not fundamental to
3932960	3938080	cognition and intelligence and consciousness. So I think that language is a particular way,
3938080	3942080	or the natural language that we're using is a particular level of abstraction that we
3942080	3948000	use to communicate with each other. But the languages in which we express geometry
3948000	3952240	are not grammatical languages in the same sense. So they work slightly different. They're more
3952240	3958400	general expressions of functions. And I think the general nature of a model is you have a bunch of
3958400	3964560	parameters. These have a range. These are the variances of the world. And you have relationships
3964560	3969040	between them, which are constraints, which say if certain parameters have these values,
3969040	3975440	then other parameters have to have the following values. And this is a very early insight in
3975440	3980160	computer science. And I think some of the earliest formulations is the Boltzmann machine.
3980160	3984400	And the problem with the Boltzmann machine is that it has a measure of whether it's good. This is
3984400	3988160	basically the energy on the system, the amount of tension that you have left in the constraints,
3988160	3993840	where the constraints don't quite match. It's very difficult to, despite having this global measure,
3993840	3999360	to train it. Because as soon as you add more than trivially few elements,
3999360	4003520	parameters into the system, it's very difficult to get it settled in the right architecture.
4004320	4011520	And so the solution that Hinton and Sanofsky found was to use a restricted Boltzmann machine,
4011520	4016160	which uses the hidden links, the internal links in the Boltzmann machine, and only
4016160	4021200	has based the input and output layer. But this limits the expressivity of the Boltzmann machine.
4021200	4025600	So now he builds a network of small of these primitive Boltzmann machines. And in some sense,
4025600	4030080	you can see almost continuous development from this to the deep learning models that we're using
4030080	4034640	today, even though we don't use Boltzmann machines at this point. But the idea of the
4034640	4038320	Boltzmann machine is you take this model, you clamp some of the values to perception,
4038320	4042400	and this forces the entire machine to go into a state that is compatible with the states that
4042400	4048160	you currently perceive. And this state is your model of the world. Right. So I think it's a
4048160	4054400	very general way of thinking about models, but we have to use a different approach to make it work.
4054400	4059440	This is we have to find different networks that train the Boltzmann machine. So the mechanism
4059440	4063120	that trains the Boltzmann machine and the mechanism that makes the Boltzmann machine
4063120	4067920	settle into its state are distinct from the constrained architecture of the Boltzmann machine
4067920	4074160	itself. The kind of mechanism that we want to develop, you're saying? Yes. So there's a
4074160	4079440	direction in which I think our research is going to go. It's going to, for instance,
4079440	4084160	what you notice in perception is our perceptual models of the world are not probabilistic,
4084160	4089520	but possibleistic, which means you should be able to perceive things that are improbable, but possible.
4090960	4095200	Right. Perceptual state is valid, not if it's probable, but if it's possible,
4095200	4099760	if it's coherent. So if you see a tiger coming after, you should be able to see this,
4099760	4106320	even if it's unlikely. And the probability is necessary for convergence of the model. So
4106320	4112240	given the state of possibilities that is very, very large and a set of perceptual features,
4112240	4117600	how should you change the states of the model to get it to converge with your perception?
4118560	4126320	But the space of ideas that are coherent with the context that you're sensing
4126320	4133520	is perhaps not as large. I mean, that's perhaps pretty small. The degree of coherence that you
4133520	4138640	need to achieve depends, of course, how deep your models go. For instance, politics is very simple
4138640	4142960	when you know very little about game theory and human nature. So the younger you are,
4142960	4148880	the more obvious this is how politics should work. Because you get coherent aesthetics from
4148880	4154480	relatively few inputs. And the more layers you model, the more layers you model reality,
4154480	4161280	the harder it gets to satisfy all the constraints. So, you know, the current neural networks are
4161280	4166240	fundamentally supervised learning system with a feed forward neural network is back propagation
4166240	4171920	to learn. What's your intuition about what kind of mechanisms might we move towards to improve
4172800	4178960	the learning procedure? I think one big aspect is going to be meta learning and architecture search
4179040	4184720	starts in this direction. In some sense, the first wave of classical AI work by identifying
4184720	4188880	a problem into a possible solution and implementing the solution, right program that plays chess.
4189440	4194880	And right now we are in the second wave of AI. So instead of writing the algorithm that implements
4194880	4199920	the solution, we write an algorithm that automatically searches for an algorithm that
4199920	4205360	implements the solution. So the learning system in some sense is an algorithm that itself discovers
4205360	4210400	the algorithm that solves the problem like Go. Go is too hard to implement the solution by hand,
4210400	4215440	but we can implement an algorithm that finds the solution. So now let's move to the third stage,
4215440	4220000	right? The third stage would be meta learning. Find an algorithm that discovers a learning
4220000	4223920	algorithm for the given domain. Our brain is probably not a learning system, but a meta
4223920	4229520	learning system. This is one way of looking at what we are doing. There is another way if you
4229520	4233760	look at the way our brain is, for instance, implemented. There is no central control that
4233760	4238560	tells all the neurons how to wire up. Instead, every neuron is an individual reinforcement
4238560	4243200	learning agent. Every neuron is a single-celled organism that is quite complicated and in some
4243200	4248240	sense quite motivated to get fed. And it gets fed if it fires on average at the right time.
4249280	4256800	And the right time depends on the context that the neuron exists in, which is the electrical
4256800	4262080	and chemical environment that it has. So it basically has to learn a function over its
4262080	4266320	environment that tells us when to fire to get fed. Or if you see it as a reinforcement
4266320	4271200	learning agent, every neuron is in some sense making a hypothesis when it sends a signal
4271200	4275840	and tries to pipe a signal through the universe and tries to get positive feedback for it.
4275840	4280720	And the entire thing is set up in such a way that it's robustly self-organizing into a brain,
4281360	4286000	which means you start out with different neuron types that have different priors on which
4286000	4290960	hypothesis to test on how to get its reward. And you put them into different concentrations
4290960	4296320	in a certain spatial alignment. And then you entrain it in a particular order.
4296320	4298720	And as a result, you get a well-organized brain.
4298720	4306160	Yeah. So the brain is a meta-learning system with a bunch of reinforcement learning agents.
4307120	4314480	And I think you said, but just to clarify, there's no centralized
4314720	4320640	government that tells you, here's a loss function, here's a loss function, here's a loss function.
4320640	4324640	Who says what's the objective of what's been happening?
4324640	4328880	There are also governments which impose loss functions on different parts of the brain.
4328880	4332800	So we have differential attention. Some areas in your brain get especially rewarded when
4332800	4336880	you look at faces. If you don't have that, you will get prosopagnosia, which basically
4337600	4343280	the inability to tell people apart by their faces. And the reason that happens is because
4344080	4348000	it had an evolutionary advantage. So evolution comes into play here.
4348000	4352080	It's basically an extraordinary attention that we have for faces. I don't think that
4352080	4357360	people with prosopagnosia have per se a defective brain. The brain just has an average attention
4357360	4361680	for faces. So people with prosopagnosia don't look at faces more than they look at cups.
4362240	4368400	So the level at which they resolve the geometry of faces is not higher than for cups. And people
4368400	4373680	that don't have prosopagnosia look obsessively at faces, right? For you and me, it's impossible
4373680	4378880	to move through a crowd without scanning the faces. And as a result, we make insanely detailed
4378880	4382080	models of faces that allow us to discern mental states of people.
4382080	4388560	So obviously we don't know 99% of the details of this meta-learning system that's in our mind.
4390560	4396960	But still we took a leap from something much dumber to that through the evolutionary process.
4397920	4405920	Can you first of all maybe say how big of a leap is that from our brain, from our ape
4405920	4418320	ancestors to multi-cell organisms? And as we start to think about how to engineer intelligence,
4418320	4425520	is there something we can learn from evolution? In some sense, life exists because of the market
4425520	4431280	opportunity of controlled chemical reactions. We compete with dumb chemical reactions and we
4431280	4436400	win in some areas against the dumb combustion because we can harness those entropy gradients
4436400	4440080	where you need to add a little bit of energy in a specific way to harvest more energy.
4440080	4442160	So we out-competed combustion.
4442160	4446720	Yes, in many regions we do. We try very hard because when we are in direct competition,
4446720	4452400	we lose, right? Because the combustion is going to close the entropy gradients much
4452400	4460560	faster than we can run. So basically we do this because every cell has a Turing machine built into
4460560	4467840	it. It's like literally a read-write head on a tape. So everything that's more complicated
4467840	4474480	than a molecule that just is a vortex around the tractors, that needs a Turing machine for
4474480	4480160	its regulation. And then you bind cells together and you get next level organization, an organism
4480160	4487040	where the cells together implement some kind of software. And for me, a very interesting discovery
4487040	4491520	in the last year was the word spirit because I realized that what spirit actually means
4491520	4495760	it's an operating system for an autonomous robot. And when the word was invented,
4495760	4500000	people needed this word, but they didn't have robots that they built themselves.
4500000	4504720	The only autonomous robots that were known were people, animals, plants, ecosystems, cities,
4504720	4509680	and so on. And they all had spirits. And it makes sense to say that the plant has an operating
4509680	4513920	system, right? If you pinch the plant in one area, then it's going to have repercussions
4513920	4518800	throughout the plant. Everything in the plant is in some sense connected into some global aesthetics
4518800	4523760	like in other organisms. An organism is not a collection of cells, it's a function that
4523760	4530160	tells cells how to behave. And this function is not implemented as some kind of supernatural thing,
4530800	4536560	like some morphogenetic field. It is an emergent result of the interactions of each cell with each
4536560	4544560	other cell. Oh my God. So what you're saying is the organism is a function that tells what
4547120	4553760	to do and the function emerges from the interaction of the cells.
4553760	4559680	Yes. So it's basically a description of what the plant is doing in terms of macrostates.
4560560	4566080	And the macrostates, the physical implementation are too many of them to describe them. So the
4566080	4570880	software that we use to describe what the plant is doing, the spirit of the plant is the software,
4570880	4576160	the operating system of the plant, right? This is a way in which we, the observers,
4576160	4580560	make sense of the plant. And the same is true for people. So people have spirits,
4580560	4584720	which is their operating system in a way, right? And there are aspects of that operating system
4584720	4589520	that relate to how your body functions and others, how you socially interact, how you interact with
4589520	4596240	yourself and so on. And we make models of that spirit. And we think it's a loaded term because
4596240	4602480	it's from a pre-scientific age. But it took the scientific age a long time to rediscover a term
4602480	4607440	that is pretty much the same thing. And I suspect that the differences that we still see between the
4607440	4611040	old word and the new word are translation errors that have been over the centuries.
4612080	4616400	Can you actually linger on that? Why do you say that spirit? Just to clarify,
4616400	4621680	because I'm a little bit confused. So the word spirit is a powerful thing. But why did you say
4621680	4626800	in the last year or so that you discovered this? Do you mean the same old traditional idea of a
4626800	4632880	spirit? I try to find out what people mean by spirit. When people say spirituality in the US,
4632880	4636800	it usually refers to the phantom limb that they develop in the absence of culture.
4637520	4642960	And a culture is in some sense, you could say the spirit of a society that is long game.
4643760	4649280	This thing that becomes self-aware at a level above the individuals where you say,
4649280	4654000	if you don't do the following things, then the grand grandchildren of our children will not have
4654000	4659760	nothing to eat. So if you take this long scope where you try to maximize the length of the game
4659760	4663760	that you are playing as a species, you realize that you are part of a larger thing that you
4663760	4669440	cannot fully control. You probably need to submit to the ecosphere instead of trying to completely
4669440	4675360	control it. There needs to be a certain level at which we can exist as a species if you want to
4675360	4681120	endure. And our culture is not sustaining this anymore. We basically made this bet with the
4681120	4684960	Industrial Revolution that we can control everything. And the modernist societies with
4684960	4691200	basically unfettered growth led to a situation in which we depend on the ability to control
4691200	4698080	the entire planet. And since we are not able to do that, as it seems, this culture will die.
4698880	4703120	We realize that it doesn't have a future. We called our children generations that
4703920	4709680	it's such a very optimistic thing to do. Yeah. So you can have this kind of intuition that
4710240	4717840	our civilization, you said culture, but you really mean the spirit of the civilization,
4717840	4725920	the entirety of the civilization may not exist for long. Can you untangle that? What's your
4725920	4731120	intuition behind that? So you kind of offline mentioned to me that the Industrial Revolution
4731120	4739440	was kind of the moment we agreed to accept the offer sign on the paper on the dotted line with
4739440	4744720	the Industrial Revolution, we doomed ourselves. Can you elaborate on that? There's a suspicion.
4744720	4751440	I of course don't know how it plays out, but it seems to me that in a society in which you
4752400	4757120	leverage yourself very far over an entropic abyss without land on the other side,
4757120	4762160	it's relatively clear that your cantilever is at some point going to break down into this
4762160	4768960	entropic abyss and you have to pay the bill. Okay. Russia is my first language and I'm also an idiot.
4771840	4779840	This is just two apes instead of playing with the banana trying to have fun by talking. Okay.
4780720	4787600	Anthropic what? And what's entropic? Entropic. So entropic in the sense of entropy. Oh,
4787600	4795920	entropic. What was the other word? Abyss. What's that? It's a big gorge. Oh, abyss. Abyss, yes.
4795920	4801280	Entropic abyss. So many of the things you say are poetic. It's hurting my brain. It's amazing,
4801280	4810640	right? It's mispronounced, which makes it even more poetic. Wittgenstein would be proud. So
4810640	4818960	entropic abyss. Okay. Let's rewind then the Industrial Revolution. So how does that get us
4818960	4825280	into the entropic abyss? So in some sense, we burned a hundred million years worth of trees
4825280	4831040	to get everybody plumbing. Yes. And the society that we had before that had a very limited number
4831040	4839520	of people. So basically since zero BC, we hovered between 300 and 400 million people. And this only
4839520	4845360	changed with the Enlightenment and the subsequent Industrial Revolution. And in some sense,
4845360	4851200	the Enlightenment freed our rationality and also freed our norms from the pre-existing order
4851200	4856240	gradually. And it was a process that basically happened in feedback loops. So it was not that
4856240	4862160	just one caused the other. It was a dynamic that started. And the dynamic worked by basically
4862160	4869200	increasing productivity to such a degree that we could feed all our children. And I think the
4869200	4876000	definition of poverty is that you have as many children as you can feed before they die,
4876000	4879040	which is in some sense the state that all animals on earth are in.
4880480	4885040	The definition of poverty is having enough- So you can have only so many children as you can
4885040	4890000	feed. And if you have more, they die. Yes. And in our societies, you can basically have as many
4890000	4896080	children as you want and they don't die. Right. So the reason why we don't have as many children
4896080	4900400	as we want is because we also have to pay a price in terms of we have to insert ourselves in the
4900400	4906080	lower social stratum if we have too many. So basically everybody in the under middle and
4906080	4911200	lower upper class has only a limited number of children because having more of them would mean
4911680	4916160	a big economic hit to the individual families. Yes. Because children, especially in the US,
4916160	4921840	super expensive to have. And you only are taken out of this if you are basically super rich
4921840	4925280	or if you are super poor. If you're super poor, it doesn't matter how many kids you have because
4925280	4929840	your status is not going to change. And these children are largely not going to die of hunger.
4931040	4935920	So how does this lead to self-destruction? So there's a lot of unpleasant properties
4935920	4940800	about this process. So basically what we try to do is we try to let our children survive,
4940800	4949680	even if they have diseases. Like I would have died before my mid-twenties without modern medicine
4949680	4955680	and most of my friends would have as well. So many of us wouldn't live without the advantages
4955680	4962000	of modern medicine and modern industrialized society. We get our protein largely by subduing
4962000	4968000	the entirety of nature. Imagine there would be some very clever microbe that would live in our
4968000	4976240	organisms and would completely harvest them and change them into a thing that is necessary
4976240	4980800	to sustain itself. And it would discover that, for instance, brain cells are kind of edible,
4980800	4985520	but they're not quite nice. So you need to have more fat in them and you turn them into more fat
4985520	4990880	cells. And basically this big organism would become a vegetable that is barely alive and it's going
4990880	4995680	to be very brittle and not resilient when the environment changes. Yeah. But some part of that
4995680	5001280	organism, the one that's actually doing all the using of the, there'll still be somebody thriving.
5002320	5008640	So it relates back to this original question. I suspect that we are not the smartest thing on
5008640	5013520	this planet. I suspect that basically every complex system has to have some complex regulation
5015280	5021520	if it depends on feedback loops. And so for instance, it's likely that we should describe
5021600	5026240	a certain degree of intelligence to plants. The problem is that plants don't have a nervous
5026240	5031440	system. So they don't have a way to telegraph messages over large distances almost instantly
5031440	5036800	in the plant. And instead they will rely on chemicals between adjacent cells, which means
5036800	5041600	the signal processing speed depends on the signal processing with a rate of a few
5042240	5047680	millimeters per second. And as a result, if the plant is intelligent, it's not going to be
5047680	5053360	intelligent at similar timescales. Yeah. The time scale is different. So you suspect
5054240	5059600	we might not be the most intelligent, but we're the most intelligent in this
5059600	5065600	spatial scale and our time scale. So basically if you would zoom out very far, we might discover
5065600	5071680	that there have been intelligent ecosystems on the planet that existed for thousands of years in an
5071680	5076080	almost undisturbed state. And it could be that these ecosystems actively related their
5076080	5081040	environment. So basically change the course of the evolution within this ecosystem to make it
5081040	5086320	more efficient and less brittle. So it's possible something like plants is actually a set of living
5086320	5091440	organisms, an ecosystem of living organisms that are just operating at different timescale and
5091440	5095840	are far superior in intelligence than human beings. And then human beings will die out
5095840	5101680	and plants will still be there and they'll be there. Yeah. There's an evolutionary adaptation
5101680	5105840	playing a role at all of these levels. For instance, if mice don't get enough food
5105840	5109600	and get stressed, the next generation of mice will be more sparse and more scrawny.
5110240	5114640	And the reason for this is because in the natural environment, the mice have probably
5114640	5120480	hidden a drought or something else. And if they overgraze, then all the things that sustain them
5120480	5125680	might go extinct and there will be no mice a few generations from now. So to make sure that there
5125680	5130880	will be mice in five generations from now, basically the mice scale back. And a similar
5130880	5134480	thing happens with the predators of mice. They should make sure that the mice don't completely
5134480	5140000	go extinct. So in some sense, if the predators are smart enough, they will be tasked with
5140000	5146320	shepherding their food supply. Maybe the reason why lions have much larger brains than antelopes
5146320	5151920	is not so much because it's so hard to catch an antelope as opposed to run away from the lion.
5151920	5157200	But the lions need to make complex models of their environment, more complex than the antelopes.
5157840	5162080	So first of all, just describing that there's a bunch of complex systems and human beings may
5162080	5166000	not even be the most special or intelligent of those complex systems, even on earth,
5166800	5170720	makes me feel a little better about the extinction of human species that we're talking about.
5170720	5174000	Yes, maybe we are just Gaia's ploy to put the carbon back into the atmosphere.
5174000	5176640	Yeah, this is just a nice, we tried it out.
5176640	5179120	The big stain on evolution is not us, it was trees.
5180080	5184480	Earth evolved trees before they could be digested again. There were no insects that could break all
5184480	5190560	of them apart. Cellulose is so robust that you cannot get all of it with microorganisms,
5190560	5195360	so many of these trees fell into swarms, and all this carbon became inert and could no longer be
5195360	5200080	recycled into organisms. And we are the species that is destined to take care of that.
5200080	5200960	So this is kind of...
5202320	5206400	To get out of the ground, put it back into the atmosphere, and the earth is already greening.
5206400	5211280	So within a million years or so when the ecosystems have recovered from the rapid changes
5211840	5215760	that they're not compatible with right now, the earth is going to be awesome again.
5215760	5218560	And there won't be even a memory of us little apes.
5218560	5222720	I think there will be memories of us. I suspect we are the first generally intelligent species
5222720	5226560	in this sense. We are the first species with an industrial society because we
5226560	5229120	will leave more phones than bones in this stratosphere.
5231120	5236880	Phones than bones, I like it. But then let me push back. You've kind of suggested that
5237680	5242960	we have a very narrow definition of... I mean, why aren't trees more general,
5244160	5246000	a higher level of general intelligence?
5246000	5249680	Well, if trees were intelligent, then they would be at different timescales,
5249680	5253440	which means within 100 years, the tree is probably not going to make models that are
5253440	5255440	as complex as the ones that we make in 10 years.
5256240	5259840	But maybe the trees are the ones that made the phones, right?
5262240	5266560	You could say the entirety of life did it. The first cell never died.
5267040	5272560	The first cell only split and every divided. And every cell in our body is still an instance
5272560	5276000	of the first cell that split off from that very first cell. There was only one cell on
5276000	5281120	this planet as far as we know. And so the cell is not just a building block of life.
5281120	5284640	It's a hypoorganism. And we are part of this hypoorganism.
5286240	5292800	So nevertheless, this hypoorganism, no, this little particular branch of it,
5292800	5297680	which is us humans because of the industrial revolution and maybe the exponential growth
5297680	5304160	of technology might somehow destroy ourselves. So what do you think is the most likely way we
5304160	5309840	might destroy ourselves? So some people worry about genetic manipulation. Some people, as we've
5309840	5315040	talked about, worry about either dumb artificial intelligence or super intelligent artificial
5315040	5322320	intelligence destroying us. Some people worry about nuclear weapons and weapons of war in general.
5322320	5327520	What do you think? If you were a betting man, what would you bet on in terms of self-destruction?
5328160	5335280	And would it be higher than 50%? So it's very likely that nothing that we bet on matters
5336080	5340640	after we win our bets. So I don't think that bets are literally the right way to go about this.
5340640	5344880	I mean, once you're dead, you won't be there to collect the winnings.
5344880	5350560	It's also not clear if we as a species go extinct. But I think that our present civilization is not
5350560	5355200	sustainable. So the thing that will change is there will be probably fewer people on the planet
5355200	5360320	than there are today. And even if not, then still most of people that are alive today will not have
5360320	5364880	offspring in 100 years from now because of the geographic changes and so on and the changes in
5364880	5371200	the food supply. It's quite likely that many areas of the planet will only be livable with a close
5371200	5378400	cooling chain in 100 years from now. So many of the areas around the equator and in subtropical
5378400	5384160	climates that are now quite pleasant to live in will stop to be inhabitable without air conditioning.
5384160	5388880	So you honestly, wow, cooling chain, close-knit cooling chain communities.
5389440	5394720	So you think you have a strong worry about the effects of global warming?
5394720	5399120	By itself, it's not a big issue. If you live in Arizona right now, you have basically three
5399120	5404080	months in the summer in which you cannot be outside. And so you have a close cooling chain,
5404080	5407760	you have air conditioning in your car and in your home and you're fine. And if the air conditioning
5407760	5413360	would stop for a few days, then in many areas you would not be able to survive, right?
5413360	5420640	Can we just pause for a second? You say so many brilliant poetic things. Do people use that term
5420640	5426720	closed cooling chain? I imagine that people use it when they describe how they get meat into a
5426720	5431200	supermarket, right? If you break the cooling chain and this thing starts to thaw, you're in trouble
5431200	5437200	and you have to throw it away. That's such a beautiful way to put it. It's like calling a city
5438000	5442480	a closed social chain or something like that. I mean, that's right. I mean, the locality of it
5442480	5445760	is really important. Yeah, but it basically means you wake up in a climatized room, you go to work
5445760	5450400	in a climatized car, you work in a climatized office, you shop in a climatized supermarket
5450400	5454960	and in between you have very short distance which you run from your car to the supermarket,
5454960	5459360	but you have to make sure that your temperature does not approach the temperature of the
5459360	5463520	environment. The crucial thing is the wet bulb temperature. The what? The wet bulb temperature.
5463520	5470960	It's what you get when you take a wet cloth and you put it around your thermometer and then you
5470960	5478160	move it very quickly through the air so you get the evaporation heat. And as soon as you can no
5478160	5484800	longer cool your body temperature via evaporation to a temperature below something like I think 35
5484800	5493040	degrees, you die. Which means if the outside world is dry, you can still cool yourself down by sweating
5493040	5497280	but if it has a certain degree of humidity or if it goes over a certain temperature,
5497280	5502880	then sweating will not save you. And this means even if you're a healthy, fit individual within
5502880	5508080	a few hours, even if you try to be in the shade and so on, you'll die. Unless you have some
5508080	5514320	climatizing equipment. And this itself, as long as you maintain civilization and you have energy
5514320	5518480	supply and you have food trucks coming to your home that are climatized, everything is fine.
5518480	5524080	But what if you lose large scale open agriculture at the same time? So basically you run into food
5524080	5529120	insecurity because climate becomes very irregular or weather becomes very irregular and you have a
5529120	5536400	lot of extreme weather events. So you need to roll most of your food maybe indoor or you need to
5536400	5541360	import your food from certain regions. And maybe you're not able to maintain the civilization
5541360	5545200	throughout the planet to get the infrastructure to get the food to your home.
5546000	5550480	But there could be significant impacts in the sense that people begin to suffer.
5550480	5555680	There could be wars over resources and so on. But ultimately, do you not have a
5556880	5566000	faith, but what do you make of the capacity of technological innovation to help us prevent
5566080	5573280	some of the worst damages that this condition can create? So as an example,
5574080	5578800	as an almost out there example, is the work that SpaceX and Elon Musk is doing of trying to
5580160	5587120	also consider our propagation throughout the universe in deep space to colonize other planets.
5587120	5592560	That's one technological step. But of course, what Elon Musk is trying on Mars is not to save us
5592560	5598080	from global warming because Mars looks much worse than Earth will look like after the worst
5598080	5603520	outcomes of global warming imaginable, right? Mars is essentially not habitable.
5603520	5608240	It's exceptionally harsh environment, yes. But what he is doing, what a lot of people throughout
5608240	5612960	history since the Industrial Revolution are doing, are just doing a lot of different technological
5612960	5617680	innovation with some kind of target. And when it ends up happening, it's totally unexpected.
5617680	5624800	New things come up. So trying to terraform or trying to colonize Mars, an extremely harsh
5624800	5632000	environment, might give us totally new ideas of how to expand or increase the power of this
5632720	5641520	closed cooling circuit that empowers the community. So it seems like there's a little bit of a race
5641520	5651200	between our open-ended technological innovation of this communal operating system that we have
5651200	5659120	and our general tendency to want to overuse resources and thereby destroy ourselves.
5660000	5664880	You don't think technology can win that race? I think the probability is relatively low,
5665440	5672640	given that our technology is stagnating since the 1970s, roughly, in terms of technology.
5672640	5676480	Most of the things that we do are the result of incremental processes.
5676480	5678800	What about Intel? What about Moore's Law?
5678800	5682320	It's basically, it's very incremental. The things that we're doing is,
5682320	5689440	so the invention of the microprocessor was a major thing, right? The miniaturization
5689440	5696560	of transistors was really major. But the things that we did afterwards largely were not that
5696560	5706560	innovative. We had gradual changes of scaling things from CPUs into GPUs and things like that.
5706560	5711440	But I don't think that there are, basically, there are not many things. If you take a person
5711440	5716560	that died in the 70s and was at the top of their game, they would not need to read that many books
5716640	5719920	to be current again. But it's all about books. Who cares about books?
5721280	5724880	There might be things that are beyond books. Or say papers.
5724880	5728960	No, papers. Forget papers. There might be things that are, so papers and books and knowledge,
5730160	5734720	that's a concept of a time when you were sitting there by candlelight and individual consumers
5734720	5740480	of knowledge. What about the impact that we're not in the middle of, might not be understanding
5740480	5746800	of Twitter, of YouTube? The reason you and I are sitting here today is because of Twitter and
5746800	5754240	YouTube. So the ripple effect, and there's two minds, sort of two dumb apes, coming up with the
5754240	5762080	new, perhaps a new clean insights. And there's 200,000 other apes listening right now. And
5763040	5767760	that effect, it's very difficult to understand what that effect will have. That might be bigger
5767760	5772000	than any of the advancements in the microprocessor or in the industrial revolution,
5772000	5782480	the ability of spread knowledge. And that knowledge, it allows good ideas to reach
5782480	5788560	millions much faster. And the effect of that, that might be the new, that might be the 21st century,
5788560	5795200	is the multiplying of ideas, of good ideas. Because if you say one good thing today,
5795840	5801360	that will multiply across huge amounts of people. And then they will say something,
5801360	5804720	and then they will have another podcast, and they'll say something, and then they'll write
5804720	5811520	a paper. That could be a huge, you don't think that- Yeah, we should have billions of von Neumanns
5811520	5816160	right now in two rings, and we don't for some reason. I suspect the reason is that we destroy
5816160	5820640	our attention span. Also, the incentives are of course different. Yeah, we have some Kardashians,
5820640	5824640	yeah. So the reason why we are sitting here and doing this as a YouTube video is because
5824640	5828480	you and me don't have the attention span to write a book together right now, and you guys
5828480	5832240	probably don't have the attention span to read it. So let me tell you- But I guarantee you they're
5832240	5838960	still listening. ... take care of your attention. It's very short. But we're an hour and 40 minutes
5838960	5843840	in, and I guarantee you that 80% of the people are still listening. So there is an attention span.
5843840	5849760	It's just the form. Who said that the book is the optimal way to transfer information?
5849760	5853360	That's still an open question. I mean, that's what we're- There's something that social media
5853360	5857680	could be doing that other forms could not be doing. I think the end game of social media is
5857680	5862560	a global brain, and Twitter is in some sense a global brain that is completely hooked on dopamine,
5862560	5866800	doesn't have any kind of inhibition, and as a result is caught in a permanent seizure.
5866800	5872560	Yes. It's also in some sense a multiplayer role-playing game, and people use it to play
5872560	5876800	an avatar that is not like them as they were in this sane world, and they look through the world
5876800	5880400	through the lens of their phones and think it's the real world, but it's the Twitter world that
5880400	5886400	is distorted by the popularity incentives of Twitter. Yeah. The incentives and just our natural
5886400	5897760	biological, the dopamine rush of alike, no matter how- I try to be very zen-like and minimalist and
5897760	5902160	not be influenced by likes and so on, but it's probably very difficult to avoid that to some
5902160	5911360	degree. Speaking at a small tangent of Twitter, how can Twitter be done better?
5912400	5917360	I think it's an incredible mechanism that has a huge impact on society by doing exactly what
5917360	5921680	you're doing. Sorry, doing exactly what you described, which is having this-
5924480	5930080	We're like, is this some kind of game, and we're kind of individual RL agents in this game,
5930080	5934080	and it's uncontrollable because there's not really a centralized control. Neither Jack Dorsey nor
5934080	5941520	the engineers at Twitter seem to be able to control this game, or can they? That's sort of
5941520	5946800	a question. Is there any advice you would give on how to control this game? I wouldn't give advice
5946800	5954000	because I am certainly not an expert, but I can give my thoughts on this. Our brain has solved
5954000	5959360	this problem to some degree. Our brain has lots of individual agents that manage to play together
5959360	5964720	in a way, and they have also many contexts in which other organisms have found ways to solve
5964720	5972240	the problems of cooperation that we don't solve on Twitter. Maybe the solution is to go for an
5972240	5979120	evolutionary approach. Imagine that you have something like Reddit or something like Facebook
5979120	5983040	and something like Twitter, and you think about what they have in common. What they have in common
5983040	5989280	are companies that in some sense own a protocol, and this protocol is imposed on a community,
5989280	5995280	and the protocol has different components for monetization, for user management, for user
5995280	6000480	display, for rating, for anonymity, for import of other content, and so on. Now imagine that you
6000480	6007120	take these components of the protocol apart, and you do it in some sense like communities within
6007120	6011920	this social network, and these communities are allowed to mix and match their protocols and
6011920	6018240	design new ones. For instance, the UI and the UX can be defined by the community. The rules for
6018240	6023600	sharing content across communities can be defined. The monetization can be redefined. The way you
6023600	6029440	reward individual users for what can be redefined. The way users can represent themselves and to
6029440	6035120	each other can be redefined. Who could be the redefiner? Can individual human beings build
6035120	6039440	enough intuition to redefine those things? Itself can become part of the protocol. For instance,
6039440	6044240	it could be in some communities it will be a single person that comes up with these things,
6044240	6049120	and others it's a group of friends. Some might implement a voting scheme that has some interesting
6049120	6053280	weighted voting. Who knows? Who knows what will be the best self-organizing principle for this?
6053280	6056480	But the process can't be automated. I mean, it seems like the brain...
6056480	6062000	It can be automated so people can write a software for this, and eventually the idea is
6062560	6066800	let's not make an assumption about this thing if you don't know what the right solution is.
6066800	6072160	In those areas we have no idea whether the right solution will be people designing this ad hoc
6072160	6077920	or machines doing this. Whether you want to enforce compliance by social norms like Wikipedia
6077920	6083120	or with software solutions or with AI that goes through the posts of people or with a
6083120	6089040	legal principle and so on. This is something maybe you need to find out. And so the idea would be
6089040	6095200	if you let the communities evolve and you just control it in such a way that you are incentivizing
6095200	6102000	the most sentient communities, the ones that produce the most interesting behaviors that
6102000	6107120	allow you to interact in the most helpful ways to the individuals. So you have a network that
6107120	6111760	gives you information that is relevant to you. It helps you to maintain relationships to others
6111760	6116720	in healthy ways. It allows you to build teams. It allows you to basically bring the best of you
6116720	6121280	into this thing and goes into a coupling, into a relationship with others in which you produce
6121280	6128160	things that you would be unable to produce alone. Yes, beautifully put. But the key process of that
6128160	6137120	with incentives and evolution is things that don't adopt themselves to effectively get the
6137120	6144080	incentives have to die. And the thing about social media is communities that are unhealthy or whatever
6144080	6149440	you want to define as the incentives really don't like dying. One of the things that people really
6150240	6156320	protest aggressively is when they're censored, especially in America. I don't know much about
6156320	6162000	the rest of the world, but the idea of freedom of speech, the idea of censorship is really painful
6162000	6173280	in America. And so what do you think about that having grown up in East Germany?
6173280	6180320	Do you think censorship is an important tool in our brain, in the intelligence, and in
6181200	6188880	the social networks? So basically, if you're not a good member of the entirety of the system,
6188880	6191920	then you should be blocked away. Well, locked away, blocked.
6192800	6195600	An important thing is who decides that you are a good member.
6195600	6197200	Who? Is it distributed or?
6197200	6202400	And what is the outcome of the process that decides it, both for the individual and for
6202400	6207360	society at large? For instance, if you have a high trust society, you don't need a lot of
6207360	6213040	surveillance. And the surveillance is even in some sense undermining trust, because it's
6214160	6220240	basically punishing people that look suspicious when surveyed, but do the right thing anyway.
6220240	6225360	And the opposite, if you have a low trust society, then surveillance can be a better trade-off.
6225360	6229920	And the US is currently making a transition from a relatively high trust or mixed trust society to
6229920	6234720	a low trust society, so surveillance will increase. Another thing is that beliefs are
6234720	6238560	not just inert representations. There are implementations that run code on your brain
6239200	6242880	and change your reality and change the way you interact with each other at some level.
6243440	6250000	And some of the beliefs are just public opinions that we use to display our alignment. So for
6250000	6256240	instance, people might say all cultures are the same and equally good, but still they prefer to
6256240	6261280	live in some cultures over others. Very, very strongly so. And it turns out that the cultures
6261280	6265520	are defined by certain rules of interaction. And these rules of interaction lead to different
6265520	6270960	results when you implement them. So if you adhere to certain rules, you get different outcomes in
6270960	6276960	different societies. And this all leads to very tricky situations when people do not have a
6276960	6282720	commitment to shared purpose. And our societies probably need to rediscover what it means to have
6282720	6290000	a shared purpose and how to make this compatible with a non-totalitarian view. So in some sense,
6290000	6297600	the US is caught in a conundrum between totalitarianism and diversity and doesn't
6297600	6302240	need to how to resolve this. And the solutions that the US has found so far are very crude
6302240	6306560	because it's a very young society that is also under a lot of tension. It seems to me that the
6306560	6315360	US will have to reinvent itself. What do you think, just philosophizing, what kind of mechanisms
6315360	6320800	of government do you think we as a species should be evolving with? US or broadly,
6320800	6327200	what do you think will work well as a system? Of course, we don't know. It all seems to work
6327200	6332640	pretty crapily. Some things worse than others. Some people argue that communism is the best.
6332640	6338880	Others say, yeah, look at the Soviet Union. Some people argue that anarchy is the best and then
6338880	6344560	completely discarding the positive effects of government. There's a lot of arguments.
6345200	6352000	US seems to be doing pretty damn well in the span of history. There's respect for human rights,
6352000	6357520	which seems to be a nice feature, not a bug. And economically, a lot of growth, a lot of technological
6357520	6362560	development. People seem to be relatively kind on the grand scheme of things.
6364320	6368960	What lessons do you draw from that? What kind of government system do you think is good?
6370960	6377120	Ideally, government should not be perceivable. It should be frictionless. The more you notice
6377120	6382800	the influence of the government, the more friction you experience, the less effective and efficient
6382800	6388320	the government probably is. A government, game theoretically, is an agent that imposes
6389040	6395920	an offset on your payout metrics to make your national equilibrium compatible with the common
6395920	6403840	good. You have these situations where people act on these local incentives. Everybody does
6403840	6408000	the thing that's locally the best for them, but the global outcome is not good. This is even the
6408000	6412880	case when people care about the global outcome because a regulation mechanism exists that creates
6412880	6417040	a causal relationship between what I want to have for the global good and what I do. For instance,
6417040	6422400	if I think that we should fly less and I stay at home, there's not a single plane that is going to
6422400	6426560	not start because of me. It's not going to have an influence, but I don't get from A to B.
6427680	6433360	The way to implement this would be to have a government that is sharing this idea that we
6433360	6437360	should fly less and is then imposing a regulation that, for instance, makes flying more expensive.
6438000	6443600	And it gives incentives for inventing other forms of transportation that are
6446080	6447840	putting less strain on the environment, for instance.
6449120	6453680	There's so much optimism in so many things you describe, and yet there's the pessimism of you
6453680	6459040	think our civilization is going to come to an end. So that's not 100% probability, nothing in this
6459040	6466240	world is. So what's the trajectory out of self-destruction, do you think?
6466240	6470240	I suspect that in some sense, we are both too smart and not smart enough,
6470240	6474240	which means we are very good at solving near-term problems. And at the same time,
6474240	6481280	we are unwilling to submit to the imperatives that we would have to follow in if we want to
6481280	6487680	stick around. So that makes it difficult. If you were unable to solve everything technologically,
6487680	6492400	you can probably understand how high the child mortality needs to be to absorb the mutation rate
6492400	6498160	and how high the mutation rate needs to be to adapt to a slowly changing ecosystem environment.
6498160	6503120	So you could, in principle, compute all these things game theoretically and adapt to it.
6503120	6509040	But if you cannot do this because you are like me and you have children, you don't want them to die,
6509040	6514960	you will use any kind of medical information to keep child mortality low. Even if it means
6514960	6520320	that within a few generations, we have enormous genetic drift, and most of us have allergies as
6520320	6524080	a result of not being adapted to the changes that we made to our food supply.
6524080	6528400	That's for now, I say, technologically speaking, we're just very young,
6528400	6532960	300 years industrial revolution. We're very new to this idea. So you're attached to your
6532960	6537840	kids being alive and not being murdered for the greater good of society. But that might be a very
6537840	6544320	temporary moment of time that we might evolve in our thinking. So like you said, we're both
6544960	6547040	smart and not smart enough.
6547120	6552640	We are probably not this first human civilization that has discovered technology that allows to
6552640	6558640	efficiently overgraze our resources. And this overgrazing, at some point, we think we can
6558640	6563040	compensate this because if we have eaten all the grass, we will find a way to grow mushrooms.
6564400	6569280	But it could also be that the ecosystems tip. And so what really concerns me is not so much
6569280	6574400	the end of the civilization because we will invent a new one. But what concerns me is
6574800	6580320	the fact that, for instance, the oceans might tip. So, for instance, maybe the plankton dies
6580320	6585440	because of ocean acidification and cyanobacteria take over. And as a result, we can no longer
6585440	6590080	breathe the atmosphere. This would be really concerning. So basically a major reboot of most
6590080	6596480	complex organisms on earth. And I think this is a possibility. I don't know what the percentage
6596480	6600720	for this possibility is, but it doesn't seem to be outlandish to me if you look at the scale of
6601280	6606000	the changes that we've already triggered on this planet. And so Danny Hillis suggests that, for
6606000	6611840	instance, we may be able to put chalk into the stratosphere to limit solar radiation. Maybe it
6611840	6616640	works. Maybe this is sufficient to counter the effects of what we've done. Maybe it won't be.
6616640	6622240	Maybe we won't be able to implement it by the time it's prevalent. I have no idea how the future is
6622240	6628000	going to play out in this regard. I think it's quite likely that we cannot continue like this.
6628000	6630320	All our cousin species, the other hominids are gone.
6632800	6639760	So the right step would be to what? To rewind towards the industrial revolution and slow
6641200	6647760	to try to contain the technological process that leads to the overconsumption of resources?
6648720	6654080	Imagine you get to choose. You have one lifetime. You get born into a sustainable agricultural
6654080	6661600	civilization, 300, maybe 400 million people on the planet tops. Or before this, some kind of
6661600	6667840	nomadic species was like a million or two million. And so you don't meet new people unless you give
6667840	6672560	birth to them. You cannot travel to other places in the world. There is no internet. There is no
6672560	6676800	interesting intellectual tradition that reaches considerably deep. So you would not discover
6676800	6683120	to run completeness probably and so on. We wouldn't exist. And the alternative is you get born into an
6683120	6688160	insane world. One that is doomed to die because it has just burned 100 million years worth of
6688160	6694880	trees in a single century. I think I like this one. It's a very weird thing when you find yourself
6694880	6699200	on a Titanic and you see this iceberg and it looks like we are not going to miss it.
6699200	6703280	And a lot of people are in denial. And most of the counter arguments sound like denial to me.
6703280	6708160	They don't seem to be rational arguments. And the other thing is we are born on this Titanic.
6708160	6711760	Without this Titanic, we wouldn't have been born. We wouldn't be here. We wouldn't be talking. We
6711760	6717200	wouldn't be on the internet. We wouldn't do all the things that we enjoy. And we are not responsible
6717200	6724320	for this happening. If we had the choice, we would probably try to prevent it. But when we were born,
6725120	6728720	we were never asked when we want to be born, in which society we want to be born,
6728720	6733680	what incentive structures we want to be exposed to. We have relatively little agency in the entire
6733680	6738080	thing. Humanity has relatively little agency in the whole thing. It's basically a giant machine
6738080	6741920	that's tumbling down a hill and everybody is frantically trying to push some buttons.
6741920	6747120	Nobody knows what these buttons are meaning, what they connect to. And most of them are not
6747120	6751680	stopping this tumbling down the hill. Is it possible that artificial intelligence
6751680	6760560	will give us an escape latch somehow? There's a lot of worry about existential threats of
6761360	6765840	artificial intelligence. But what AI also allows in general forms of automation
6766160	6774480	allows the potential of extreme productivity growth that will also perhaps in a positive way
6774480	6788080	transform society, that may allow us to inadvertently to return to the same kind of ideals of closer to
6789040	6796240	nature that's represented in hunter-gatherer societies, that's not destroying the planet,
6796240	6801440	that's not doing overconsumption and so on. Generally speaking, do you have hope that AI
6801440	6808640	can help somehow? I think it's not fun to be very close to nature until you completely subdue nature.
6809840	6816720	Our idea of being close to nature means being close to agriculture, basically forests that
6816720	6823920	don't have anything in them that eats us. I want to disagree with that. I think the niceness of
6824480	6832960	being close to nature is to being fully present. When survival becomes your primary,
6833680	6844320	not just your goal, but your whole existence, I'm not just romanticizing, I can just speak for
6844320	6852880	myself. I am self-aware enough that that is a fulfilling existence. I personally prefer to be
6852880	6859200	in nature and not fight for my survival. I think fighting for your survival while being in the cold
6859200	6864400	and in the rain and being hunted by animals and having open wounds is very unpleasant.
6864400	6870560	There's a contradiction in there. Yes, I and you, just as you said, would not choose it.
6871520	6875200	But if I was forced into it, it would be a fulfilling existence.
6875200	6881520	Yes, if you are adapted to it. Basically, if your brain is fed up in such a way that you get
6881520	6887600	rewards optimally in such an environment. There's some evidence for this that for a certain degree
6887600	6892400	of complexity, basically people are more happy in such an environment because it's what we largely
6892400	6897440	have evolved for. In between, we had a few thousand years in which I think we have evolved for a
6897520	6902800	slightly more comfortable environment. So there is probably something like an intermediate stage
6903360	6908480	in which people would be more happy than they would be if they would have to fend for themselves
6908480	6914800	in small groups in the forest and often die versus something like this where we now have basically
6914800	6923280	a big machine, a big mordor in which we run through concrete boxes and press buttons and machines
6923280	6928080	and largely don't feel well cared for as the monkeys that we are.
6929520	6937680	So returning briefly to, not briefly, but returning to AI, let me ask a romanticized question. What is
6937680	6944320	the most beautiful to you, silly ape, the most beautiful or surprising idea in the development
6944320	6948800	of artificial intelligence, whether in your own life or in the history of artificial intelligence
6948800	6955840	that you've come across? If you built an AI, it probably can make models at an arbitrary degree
6955840	6961680	of detail of the world. And then it would try to understand its own nature. It's tempting to think
6961680	6965680	that at some point when we have general intelligence, we have competitions where we will
6965680	6971040	let the AIs wake up in different kinds of physical universes and we measure how many movements of the
6971040	6975920	Rubik's cube it takes until it's figured out what's going on in its universe and what it is
6975920	6981280	and its own nature and its own physics and so on. So what if we exist in the memory of an AI
6981280	6985760	that is trying to understand its own nature and remembers its own genesis and remembers
6985760	6991680	Lex and Joscha sitting in a hotel room, sparking some of the ideas of that led to the development
6991680	6995520	of general intelligence? So we're a kind of simulation that's running in an AI system
6995520	7001600	that's trying to understand itself. It's not that I believe that, but I think it's a beautiful
7002160	7011600	idea. You kind of returned to this idea with the Turing test of intelligence being
7013440	7019040	the process of asking and answering what is intelligence.
7025520	7031440	Do you think there is an answer? Why is there such a search for an answer? Does there have to be
7031920	7039520	like an answer? You just said an AI system that's trying to understand itself.
7041520	7047680	Is that a fundamental process of greater and greater complexity, greater and greater intelligence?
7047680	7053440	Is the continuous trying of understanding itself? No, I think you will find that most people don't
7053440	7058800	care about that because they're well adjusted enough to not care. And the reason why people
7058800	7063120	like you and me care about it probably has to do with the need to understand ourselves.
7063120	7067120	It's because we are in fundamental disagreement with the universe that we wake up in.
7068000	7071360	I look down on me and I see, oh my God, I'm caught in a monkey. What's that?
7072400	7077040	Some people are unhappy with the government and I'm unhappy with the entire universe that I find
7077040	7083600	myself in. So you don't think that's a fundamental aspect of human nature that some people are just
7083600	7088720	suppressing? That they wake up shocked that they're in the body of a monkey?
7088720	7093520	No, there is clear adaptive value to not be confused by that and by...
7094560	7102480	Well, no, that's not what I asked. So you have this clear adaptive value, then there's clear
7102480	7107520	adaptive value to while fundamentally your brain is confused by that, by creating an illusion.
7108000	7116240	Another layer of the narrative that tries to suppress that and instead say that what's
7116240	7119280	going on with the government right now is the most important thing, or what's going on with
7119280	7126800	my football team is the most important thing. But it seems to me, for me, it was a really
7126800	7135920	interesting moment reading Ernest Becker's Denial of Death, this idea that we're all
7136880	7146240	the fundamental thing from which most of our human mind springs is this fear of mortality
7146240	7149840	and being cognizant of your mortality and the fear of that mortality.
7150400	7158880	And then you construct illusions on top of that. Just to push on it, you really don't think
7159520	7165680	it's possible that this worry of the big existential questions is actually fundamental
7166880	7169440	as the existentialist thought to our existence.
7170000	7174560	I think that the fear of death only plays a role as long as you don't see the big picture.
7175120	7179920	The thing is that minds are software states, right? Software doesn't have identity.
7180560	7182320	Software in some sense is a physical law.
7182640	7190480	But it feels like there's an identity. I thought that was for this particular piece of software
7190480	7193680	and the narrative it tells, that's the fundamental property of it.
7193680	7199040	The maintenance of the identity is not terminal, it's instrumental to something else.
7199040	7203360	You maintain your identity so you can serve your meaning, so you can do the things that
7203360	7208080	you're supposed to do before you die. And I suspect that for most people, the fear of death
7208080	7211600	is the fear of dying before they're done with the things that they feel they have to do even
7211600	7214400	though they cannot quite put their finger on it, what that is.
7215920	7223920	Right. But in the software world, to return to the question, then what happens after we die?
7226880	7231600	Why would you care? You will not be longer there. The point of dying is that you're gone.
7231600	7237680	Well, maybe I'm not. This is what... It seems like there's so much...
7238080	7244720	In the idea that the mind is just a simulation that's constructing a narrative around some
7244720	7252000	particular aspects of the quantum mechanical wave function world that we can't quite get
7252000	7260720	direct access to, then the idea of mortality seems to be fuzzy as well. Maybe there's not a clear...
7260720	7266160	No, the fuzzy idea is the one of continuous existence. We don't have continuous existence.
7266160	7270720	How do you know that? Because it's not computable.
7272080	7274320	Because you're saying it's... There is no continuous process.
7274320	7278000	The only thing that binds you together with the Lex Friedman from yesterday is the illusion that
7278000	7282320	you have memories about him. So if you want to upload, it's very easy. You make a machine
7282320	7286320	that thinks it's you. Because it's the same thing that you are. You are a machine that thinks it's
7286320	7292240	you. But that's immortality. Yeah, but it's just a belief. You can create this belief very easily.
7292240	7297760	Once you realize that the question whether you are immortal or not depends entirely on
7297760	7303440	your beliefs and your own continuity. But then you can be immortal by the
7303440	7309360	continuity of the belief. You cannot be immortal, but you can stop being afraid of your mortality
7309360	7312960	because you realize you were never continuously existing in the first place.
7314400	7318240	Well, I don't know if I'd be more terrified or less terrified by that. It seems like
7318240	7322800	the fact that I existed... Also, you don't know this state in which you don't have a self.
7323360	7326960	You can't turn off yourself, you know. I can't turn off myself.
7326960	7329280	You can't turn it off. You can't turn it off. I can.
7329280	7333760	Yes. And you can basically meditate yourself in a state where you are still conscious,
7333760	7337440	where still things are happening, where you know everything that you knew before,
7337440	7342000	but you're no longer identified with changing anything. And this means that
7342000	7347600	yourself in a way dissolves. There is no longer this person. You know that this person construct
7347600	7354240	exists in other states and it runs on this brain of Lex Friedman, but it's not a real thing. It's
7354240	7360080	a construct. It's an idea. And you can change that idea. And if you let go of this idea,
7360080	7364320	if you don't think that you are special, you realize it's just one of many people and it's
7364320	7369200	not your favorite person even, right? It's just one of many. And it's the one that you are doomed
7369200	7374640	to control for the most part. And that is basically informing the actions of this organism
7374720	7379920	as a control model. And this is all there is. And you are somehow afraid that this control model
7379920	7386720	gets interrupted or loses the identity of continuity. Yeah. So I'm attached. I mean,
7386720	7392560	yeah, it's a very popular, it's a somehow compelling notion that being attached,
7392560	7396240	like there's no need to be attached to this idea of an identity.
7397040	7401360	But that in itself could be an illusion that you construct. So the process of meditation,
7401360	7406720	while popular is thought of as getting under the concept of identity, it could be just putting a
7406720	7416400	cloak over it, just telling it to be quiet for the moment. I think that meditation is eventually
7416400	7421040	just a bunch of techniques that let you control attention. And when you can control attention,
7421440	7425840	you can get access to your own source code, hopefully not before you understand what you're
7425840	7429600	doing. And then you can change the way it works temporarily or permanently.
7430400	7435680	So yeah, meditation is to get a glimpse at the source code, get under the sort of basically
7435680	7438320	control or turn off the attention. Yeah, but the entire thing is that you learn to control
7438320	7441920	attention. So everything else is downstream from controlling attention.
7441920	7445360	And control the attention that's looking at the attention.
7445360	7449360	Normally, we only get attention in the parts of our body that we don't know about.
7449360	7453040	We only get attention in the parts of our mind that create heat, where you have a mismatch
7453040	7458480	between model and the results that are happening. And so most people are not self-aware because
7458480	7463440	their control is too good. If everything works out roughly the way you want, and the only things
7463440	7468240	that don't work out is whether your football team wins, then you will mostly have models
7468240	7473040	about these domains. And it's only when, for instance, your fundamental relationships to
7473040	7477920	the world around you don't work because the ideology of your country is insane, and the
7477920	7483120	other kids are not nerds and don't understand why you understand physics, and why you want
7483120	7487200	to understand physics, and you don't understand why somebody would not want to understand physics.
7488560	7494160	So we kind of brought up neurons in the brain as reinforcement learning agents.
7496400	7501600	And there's been some successes, as you brought up with Go, with AlphaGo, AlphaZero,
7501600	7505920	with ideas of self-play, which I think are incredibly interesting ideas of systems playing
7505920	7515120	each other in an automated way to improve by playing other systems in a particular construct
7515120	7520640	of a game that are a little bit better than itself and thereby improving continuously.
7520640	7526160	All the competitors in the game are improving gradually, so being just challenging enough
7526160	7532080	and learning from the process of the competition. Do you have hope for that reinforcement learning
7532080	7536400	process to achieve greater and greater level of intelligence? So we talked about different
7536400	7543680	ideas in AI that we need to be solved. Is RL a part of that process of trying to create an
7543680	7547040	AGI system? What do you think? Definitely forms of unsupervised learning,
7547040	7552320	but there are many algorithms that can achieve that. And I suspect that ultimately the algorithms
7552320	7558080	that work, there will be a class of them or many of them, and they might have small differences
7558080	7563760	of magnitude and efficiency, but eventually what matters is the type of model that you form.
7564400	7567360	And the types of models that we form right now are not sparse enough.
7569840	7576160	Sparse? What does it mean to be sparse? It means that ideally every potential model
7576160	7583200	state should correspond to a potential world state. So basically if you vary states in your model,
7583200	7588240	you always end up with valid world states. And our mind is not quite there. So an indication
7588240	7592880	is basically what we see in dreams. The older we get, the more boring our dreams become,
7592880	7597040	because we incorporate more and more constraints that we learned about how the world works.
7597680	7601920	So many of the things that we imagine to be possible as children turn out to be
7601920	7608240	constrained by physical and social dynamics. And as a result, fewer and fewer things remain
7608240	7612480	possible. And it's not because our imagination scales back, but the constraints under which
7612480	7618320	it operates become tighter and tighter. And so the constraints under which our neural networks
7618320	7623600	operate are almost limitless, which means it's very difficult to get a neural network to imagine
7623600	7631440	things that look real. So I suspect part of what we need to do is we probably need to build dreaming
7631440	7637360	systems. I suspect that part of the purpose of dreams is similar to a generative adversarial
7637360	7642720	network to learn certain constraints. And then it produces alternative perspectives
7642720	7647360	on the same set of constraints. So you can recognize it under different circumstances.
7647360	7652080	Maybe we have flying dreams as children, because we recreate the objects that we know and the maps
7652080	7656160	that we know from different perspectives, which also means from a bird's eye perspective.
7656880	7663040	So I mean, aren't we doing that anyway? I mean, not with our eyes closed when we're sleeping.
7663120	7667680	Aren't we just constantly running dreams and simulations in our mind as we try to
7667680	7672240	interpret the environment? I mean, sort of considering all the different possibilities.
7672880	7679760	The way we interact with the environment seems like essentially, like you said,
7680640	7686720	sort of creating a bunch of simulations that are consistent with our expectations, with our previous
7686720	7694800	experiences, with the things we just saw recently. And through that hallucination process,
7695680	7702000	we are able to then somehow stitch together what actually we see in the world with the
7702000	7707600	simulations that match it well and thereby interpret it. I suspect that you and my brain
7707600	7713280	are slightly unusual in this regard, which is probably what got you into MIT. So this obsession
7713280	7718080	of constantly pondering possibilities and solutions to problems.
7718080	7724960	Oh, stop it. I think I'm not talking about intellectual stuff. I'm talking about just
7726000	7730240	doing the kind of stuff it takes to walk and not fall.
7730960	7732560	Yes, this is largely automatic.
7736080	7738320	Yes, but the process is, I mean-
7738320	7742480	It's not complicated. It's relatively easy to build a neural network that in some sense
7742480	7747520	learns the dynamics. The fact that we haven't done it right so far, it doesn't mean it's hard
7747520	7752960	because you can see that a biological organism does it with relatively few neurons. So basically,
7752960	7756560	you build a bunch of neural oscillators that entrain themselves with the dynamics of your
7756560	7761760	body in such a way that the regulator becomes isomorphic in its model to the dynamics that
7761760	7766560	it regulates and then is automatic. And it's only interesting in the sense that it captures
7766560	7771200	attention when the system is off. See, but thinking of the kind of
7771200	7777440	mechanism that's required to do walking as a controller, as a neural network, I think
7779360	7787280	it's a compelling notion, but it discards quietly or at least makes implicit the fact that you need
7787280	7792960	to have something like common sense reasoning to walk. It's an open question whether you do or not,
7792960	7801040	but my intuition is to act in this world. There's a huge knowledge base that's underlying it somehow.
7801520	7808480	There's so much information of the kind we have never been able to construct in our
7809200	7813680	in neural networks or in artificial intelligence systems period, which is like it's humbling,
7814800	7819040	at least in my imagination, the amount of information required to act in this world
7819840	7825840	humbles me. And I think saying that neural networks can accomplish it is missing
7826720	7832400	is missing the fact that we don't, yeah, we don't have yet a mechanism for constructing
7832400	7838240	something like common sense reasoning. I mean, what's your sense about
7839920	7846160	to linger on how much, you know, to linger on the idea of what kind of mechanism would be effective
7846160	7851040	at walking. You said just a neural network, not maybe the kind we have, but something a little
7851920	7857280	bit better would be able to walk easily. Don't you think it also needs to know
7859440	7864800	like huge amount of knowledge that's represented under the flag of common sense reasoning?
7864800	7868080	How much common sense knowledge do we actually have? Imagine that you are
7868080	7873120	really hardworking for all your life and you form two new concepts every half hour. So
7873120	7876080	you end up with something like a million concepts because you don't get that old.
7876080	7879760	So a million concepts, that's not a lot.
7882240	7886160	So it's not just a million concepts. I think it would be a lot. I personally think it might
7886160	7891040	be much more than a million. But if you think just about the numbers, you don't live that long.
7891920	7896080	If you think about how many cycles do your neurons have in your life, it's quite limited.
7896080	7902400	You don't get that old. Yeah, but the powerful thing is the number of concepts, and they're
7902960	7908720	probably deeply hierarchical in nature. The relations, as you described between them,
7908720	7914640	is the key thing. So it's like even if it's a million concepts, the graph of relations that's
7914640	7921920	formed and some kind of perhaps some kind of probabilistic relationships, that's what's
7921920	7924320	common sense reasoning is the relationship between things.
7925920	7931120	Yeah, so in some sense, I think of the concepts as the address space for our behavior programs.
7931200	7934720	And the behavior programs allow us to recognize objects and interact with them,
7934720	7940080	also mental objects. And a large part of that is the physical world that we interact with,
7940080	7944560	which is this res extensor thing, which is basically navigation of information in space.
7945360	7952640	And basically, it's similar to a game engine. It's a physics engine that you can use to describe
7952640	7958000	and predict how things that look in a particular way, that feel when you touch them in a particular
7958000	7962640	way, the proprioception, the auditory perception, and so on, how they work out. So basically,
7962640	7968400	the geometry of all these things. And this is probably 80% of what our brain is doing
7968400	7973120	is dealing with that, with this real-time simulation. And by itself, a game engine is
7973120	7978000	fascinating, but it's not that hard to understand what it's doing, right? And our game engines
7978000	7984800	are already in some sense approximating the fidelity of what we can perceive.
7984800	7991360	So if we put on an Oculus Quest, we get something that is still relatively crude with respect to
7991360	7995680	what we can perceive, but it's also in the same ballpark already, right? It's just a couple order
7995680	8002400	of magnitudes away to home saturating our perception in terms of the complexity that it can produce.
8002400	8008080	So in some sense, it's reasonable to say that the computer that you can buy and put into your home
8008080	8013600	is able to give a perceptual reality that has a detail that is already in the same ballpark as
8013600	8019440	what your brain can process. And everything else are ideas about the world. And I suspect that
8019440	8024800	they are relatively sparse and also the intuitive models that we form about social interaction.
8024800	8030080	Social interaction is not so hard. It's just hard for us nerds because we have our wires crossed,
8030080	8035120	so we need to deduce them. But the priors are present in most social animals. So it's an
8035120	8041680	interesting thing to notice that many domestic social animals like cats and dogs have better
8041680	8048080	social cognition than children. Right. I hope so. I hope it's not that many concepts,
8048080	8054640	fundamentally, to exist in this world. For me, it's more like I'm afraid so, because this thing
8054640	8059360	that we only appear to be so complex to each other because we are so stupid is a little bit
8059360	8068240	depressing. To me, that's inspiring if we're indeed as stupid as it seems. The things our
8068240	8072560	brains don't scale and the information processing that we build tend to scale very well.
8073440	8080560	Yeah, but one of the things that worries me is that the fact that the brain doesn't scale
8080560	8086640	means that that's actually a fundamental feature of the brain. All the flaws of the brain,
8086640	8091680	everything we see as limitations, perhaps there's a fundamental... The constraints on the system
8091680	8100240	could be a requirement of its power, which is different than our current understanding
8100240	8105280	of intelligent systems where scale, especially with deep learning, especially with reinforcement
8105280	8112880	learning, the hope behind open AI and DeepMind, all the major results really have to do with
8112880	8119600	huge compute. It could also be that our brains are so small, not just because they take up so much
8119680	8125120	glucose in our body, like 20% of the glucose, so they don't arbitrarily scale. There are some
8125120	8128800	animals like elephants, which have larger brains than us, and they don't seem to be smarter.
8129520	8133360	Elephants seem to be autistic. They have very, very good motor control and they're really good
8133360	8139440	with details, but they really struggle to see the big picture. You can make them recreate drawings
8139440	8144480	stroke by stroke. They can do that, but they cannot reproduce a still life, so they cannot
8144480	8148720	make a drawing of a scene that they see. They will always be only able to reproduce the line
8148720	8154560	drawing, at least as far from what I could see in the experiments. Why is that? Maybe smarter
8154560	8158800	elephants would meditate themselves out of existence because their brains are too large.
8158800	8162400	Basically, the elephants that were not autistic, they didn't reproduce.
8163520	8166720	We have to remember that the brain is fundamentally interlinked with the body
8166720	8171680	in our human and biological systems. Do you think that AGI systems that we try to create
8171680	8176880	or greater intelligence systems would need to have a body? I think they should be able to make
8176880	8182080	use of a body if you give it to them, but I don't think that they fundamentally need a body. I
8182080	8188080	suspect if you can interact with the world by moving your eyes and your head, you can make
8188080	8195440	controlled experiments. This allows you to have many magnitudes fewer observations in order to
8196240	8200640	reduce the uncertainty in your models. You can pinpoint the areas in your models where you're
8200640	8204720	not quite sure, and you just move your head and see what's going on over there, and you get
8204720	8209280	additional information. If you just have to use YouTube as an input and you cannot do anything
8209280	8215840	beyond this, you probably need just much more data, but we have much more data. If you can build a
8215840	8220640	system that has enough time and attention to browse all of YouTube and extract all the information
8220640	8225440	that there is to be found, I don't think there's an obvious limit to what it can do.
8226240	8231040	It seems that the interactivity is a fundamental thing that the physical body allows you to do,
8231040	8236960	but let me ask on that topic. That's what a body is. It's allowing the brain to touch things and
8236960	8243440	move things and interact with whether the physical world exists or not, whatever, but interact with
8243440	8251200	some interface to the physical world. What about a virtual world? Do you think we can do the same
8251200	8260480	kind of reasoning, consciousness, intelligence if we put on a VR headset and move over to that world?
8260480	8264720	Do you think there's any fundamental difference between the interface to the physical world that
8264720	8269120	is here in this hotel and if we were sitting in the same hotel in a virtual world?
8269120	8275440	The question is, does this non-physical world or this other environment entice you to solve
8275440	8280480	problems that require general intelligence? If it doesn't, then you probably will not develop
8280480	8284640	general intelligence. Arguably, most people are not generally intelligent because they don't have to
8284640	8289520	solve problems that make them generally intelligent. Even for us, it's not yet clear if we are smart
8289520	8295360	enough to build AI and understand our own nature to this degree. It could be a matter of capacity
8295360	8298960	and for most people, it's in the first place a matter of interest. They don't see the point
8298960	8303600	because the benefit of attempting this project are marginal because you're probably not going
8303600	8308320	to succeed in it and the cost of trying to do it requires complete dedication of your entire life.
8309600	8314800	It seems like the possibilities of what you can do in the virtual world is much greater than
8315280	8320400	in the real world. Imagine a situation, maybe an interesting option for me. If somebody came to me
8320400	8329680	and offered what I'll do. From now on, you can only exist in the virtual world. You put on this
8329680	8335840	headset and when you eat, we'll make sure to connect your body up in a way that when you
8335840	8340240	eat in the virtual world, your body will be nourished in the same way in the virtual world.
8341200	8346000	Align the incentives between our common real world and the virtual world,
8346000	8353280	but then the possibilities become much bigger. I could be other kinds of creatures. I can break
8353280	8359440	the laws of physics as we know them. The possibilities are endless as far as we think.
8360160	8366720	It's an interesting thought. What existence would be like? What kind of intelligence would
8366720	8374160	emerge there? What kind of consciousness? Maybe greater intelligence, even in me, Lex,
8374160	8378560	even at this stage in my life, if I spend the next 20 years in that world to see how that
8378560	8384560	intelligence emerges. If that happened at the very beginning before I was even cognizant of
8384560	8389680	my existence in this physical world, it's interesting to think how that child would develop.
8390320	8395760	The way virtual reality and digitization of everything is moving is not completely out of
8395760	8403040	the realm of possibility that some part of our lives will, if not entirety of it,
8403040	8408880	will live in a virtual world to a greater degree than we currently have living on Twitter and
8408880	8416960	social media and so on. Does something draw you intellectually or naturally in terms of
8416960	8423600	thinking about AI to this virtual world? More possibilities? I think that currently it's a
8423600	8428080	waste of time to deal with the physical world before we have mechanisms that can automatically
8428080	8433920	learn how to deal with it. The body gives you a second order agency, but what constitutes the
8433920	8439920	body is the things that you can indirectly control. Third order are tools and the second
8439920	8444640	order is the things that are basically always present, but you operate on them with first
8444640	8450480	order things which are mental operators and the zero order is in some sense the direct sense of
8451200	8457680	what you're deciding. You observe yourself initiating an action, there are features that
8457680	8462880	you interpret as the initiation of an action, then you perform the operations that you perform to
8462880	8468320	make that happen, and then you see the movement of your limbs and you learn to associate those
8468320	8472720	and thereby model your own agency over this feedback. But the first feedback that you get
8472720	8477120	is from this first order thing already. Basically, you decide to think a thought and the thought is
8477120	8481440	being thought. You decide to change the thought and you observe how the thought is being changed.
8482160	8487120	In some sense, this is, you could say, an embodiment already. I suspect it's sufficient
8487120	8491680	as an embodiment or intelligence. It's not that important, at least at this time,
8491680	8498640	to consider variations in the second order. Yes, but the thing that you also mentioned just now is
8499600	8503040	physics that you could change in any way you want. You need an environment that
8503040	8507440	puts up resistance against you. If there's nothing to control, you cannot make models.
8508000	8513120	There needs to be a particular way that resists you. By the way, your motivation is usually
8513120	8517920	outside of your mind. It resists your motivation. It's what gets you up in the morning even though
8517920	8524320	it would be much less work to stay in bed. It's basically forcing you to resist the environment
8524320	8531680	and it forces your mind to serve it, to serve this resistance to the environment. In some sense,
8531680	8536080	it is also putting up resistance against the natural tendency of the mind to not do anything.
8536960	8541360	Some of that resistance, just like you described with motivation, is in the first order. It's in
8541360	8546800	the mind. Some resistance is in the second order, like the actual physical objects pushing against
8546800	8551120	you and so on. It seems that the second order stuff in virtual reality could be recreated.
8551120	8556000	Of course, but it might be sufficient that you just do mathematics and mathematics is already
8556000	8560800	putting up enough resistance against you. Basically, just with an aesthetic motive,
8560800	8566400	this could maybe be sufficient to form a type of intelligence. It would probably not be a very
8566400	8574320	human intelligence, but it might be one that is already general. To mess with this zeroth order,
8574320	8579760	maybe first order, what do you think about ideas of brain-computer interfaces? Again, returning to
8579760	8584800	our friend Elon Musk and Neuralink, a company that's trying to, of course, there's a lot of
8585360	8590240	trying to cure diseases and so on with the near-term, but the long-term vision is to add
8590240	8597200	an extra layer to basically expand the capacity of the brain connected to the computational world.
8599680	8603360	Do you think one, that's possible, or two, how does that change the fundamentals of the zeroth
8603360	8607120	order and the first order? It's technically possible, but I don't see that the FDA would
8607120	8612240	ever allow me to drill holes in my skull to interface my neocortex the way Elon Musk envisions.
8612400	8618400	At the moment, I can do horrible things to mice, but I'm not able to do useful things to people,
8618400	8624400	except maybe at some point down the line in medical applications. This thing that we are envisioning,
8624400	8630480	which means recreational and creational brain-computer interfaces, are probably
8630480	8636560	not going to happen in the present legal system. I love it how I'm asking you out there philosophical
8637360	8644640	and engineering questions, and for the first time ever you jumped to the legal FDA.
8644640	8648320	There would be enough people that would be crazy enough to have holes drilled in their skull to
8648320	8654240	try a new type of brain-computer interface. Also, if it works, the FDA will approve it.
8657440	8660800	I work a lot with autonomous vehicles. Yes, you can say that it's going to be a very difficult
8660800	8664880	regulatory process of approving autonomy, but that doesn't mean autonomous vehicles are never
8664880	8670160	going to happen. No, they will totally happen as soon as we create jobs for at least two
8670160	8681200	lawyers and one regulator per car. Lawyers are the fundamental substrate of reality.
8682800	8687440	It's a very weird system. It's not universal in the world. The law is a very interesting
8687440	8692880	software once you realize it. These circuits are in some sense streams of software, and it
8692880	8697280	largely works by exception handling. You make decisions on the ground and they get synchronized
8697280	8703520	with the next level structure as soon as an exception is being thrown. It escalates the
8703520	8708560	exception handling. The process is very expensive, especially since it incentivizes the lawyers for
8709200	8715440	producing work for lawyers. The exceptions are actually incentivized for firing often.
8716320	8727040	But to return, outside of lawyers, is there anything interesting and insightful about the
8727040	8732880	possibility of this extra layer of intelligence added to the brain? Yes, I do think so, but I
8732880	8738720	don't think that you need technically invasive procedures to do so. We can already interface
8738720	8742800	with other people by observing them very, very closely and getting some kind of empathetic
8742960	8750080	resonance. I'm not very good at this, but I notice that people are able to do this to some degree.
8750800	8755680	It basically means that we model an interface layer of the other person in real time.
8757040	8761280	It works despite our neurons being slow because most of the things that we do are
8761280	8765520	built on periodic processes. You just need to entrain yourself with the oscillation that
8765520	8770320	happens. If the oscillation itself changes slowly enough, you can basically follow along.
8773280	8779600	But the bandwidth of that interaction, it seems like you can do a lot more computation.
8780640	8784640	Of course, but the other thing is that the bandwidth that our brain, our own mind,
8784640	8789440	is running on is actually quite slow. The number of thoughts that I can productively think
8789440	8796000	in any given day is quite limited. If they had the discipline to write it down and the speed to
8796000	8800480	write it down, maybe it would be a book every day or so. But if you think about the computers
8800560	8803520	that we can build, the magnitudes at which they operate,
8804400	8807280	this would be nothing. It's something that they can put out in a second.
8807280	8812560	Well, I don't know. As possible, the number of thoughts you have in your brain,
8813680	8817920	it could be several orders of magnitude higher than what you're possibly able to
8817920	8820000	express through your fingers or through your voice.
8821680	8828640	Most of them are going to be repetitive because they have to control the same problems every day.
8828640	8833440	When I walk, there are going to be processes in my brain that model my walking pattern and
8833440	8836480	regulate them and so on, but it's going to be pretty much the same every day.
8838320	8840960	I'm talking about intellectual reasoning. I'm thinking, so the question,
8840960	8844880	what is the best system of government? So you sit down and start thinking about that.
8844880	8851440	One of the constraints is that you don't have access to a lot of facts, a lot of studies.
8852400	8859840	You always have to interface with something else to learn more, to aid in your reasoning process.
8859840	8864000	If you can directly access all of Wikipedia in trying to understand what is the best form
8864000	8871120	of government, then every thought that requires some extra piece of information will be able to
8871120	8877760	grab it really quickly. That's the possibility of if the bottleneck is literally the information
8878720	8886160	that the bottleneck of breakthrough ideas is just being able to quickly access huge amounts of
8886160	8891200	information, then the possibility of connecting your brain to the computer could lead to totally
8891200	8901760	new breakthroughs. You can think of mathematicians being able to just up the orders, the magnitude of
8902720	8905280	power in their reasoning about mathematical rules.
8905280	8909680	What if humanity has already discovered the optimal form of government through
8909680	8915920	a revolutionary process? There is an evolution going on. So what we discover is that maybe
8915920	8920240	the problem of government doesn't have stable solutions for us as a species because we are
8920240	8926000	not designed in such a way that we can make everybody conform to them. But there could be
8926000	8930640	solutions that work under given circumstances or that are the best for a certain environment
8930640	8936320	and depends on, for instance, the primary forms of ownership and the means of production. So if the
8936320	8943840	main means of production is land, then the forms of government will be regulated by the landowners
8943840	8950240	and you get a monarchy. If you also want to have a form of government in which you depend on some
8950240	8955200	form of slavery, for instance, where the peasants have to work very long hours for very little gain
8955280	8961280	so very few people can have plumbing, then maybe you need to promise them that you get paid in the
8961280	8968960	afterlife, the overtime. So you need a theocracy. And so for much of human history in the West,
8968960	8975120	we had a combination of monarchy and theocracy that was our form of governance. At the same time,
8975120	8981760	the Catholic Church implemented game theoretic principles. I recently re-read Thomas O'Kinus.
8981760	8986320	It's very interesting to see this because he was not dualist. He was translating Aristotle in a
8986320	8993920	particular way for designing an operating system for the Catholic society. And he says that basically
8993920	8998400	people are animals in very much the same way as Aristotle envisions, which basically
8998400	9003360	organisms with cybernetic control. And then he says that there are additional rational principles
9003360	9008160	that humans can discover and everybody can discover them so they are universal. If you are sane,
9008160	9012400	you should understand you should submit to them because you can rationally deduce them.
9012400	9020560	And these principles are roughly you should be willing to self-regulate correctly. You should
9020560	9028400	be willing to do correct social regulation. It's intra-organismic. You should be willing to act on
9028400	9035680	your models so you have skin in the game. And you should have goal rationality. You should be
9036240	9042560	choosing the right goals to work on. So basically these three rational principles, goal rationality
9042560	9049440	he calls prudence or wisdom, social regulation is justice, the correct social one, and the internal
9049440	9056880	regulation is temperance. And this willingness to act on your models is courage. And then he says
9056880	9061840	that there are additionally to these four cardinal virtues, three divine virtues. And these three
9061840	9065920	divine virtues cannot be rationally deduced, but they reveal themselves by the harmony,
9065920	9069680	which means if you assume them and you extrapolate what's going to happen,
9069680	9075440	you will see that they make sense. And it's often been misunderstood as God has to tell you that
9075440	9079920	these are the things. So basically there's something nefarious going on. The Christian
9079920	9086880	conspiracy forces you to believe some guy with a long beard that they discovered this. So these
9086880	9091840	principles are relatively simple. Again, it's for high-level organization, for the resulting
9091840	9098080	civilization that you form, commitment to unity. So basically you serve this higher, larger thing,
9098080	9104320	this structural principle on the next level, and he calls that faith. And there needs to be
9104800	9108720	a commitment to shared purpose. This is basically this global reward that you try to figure out
9108720	9112560	what that should be and how you can facilitate this. And this is love. The commitment to shared
9112560	9117520	purpose is the core of love. You see this sacred thing that is more important than your own
9117520	9122400	organismic interests in the other. And you serve this together and this is how you see the sacred
9122400	9127520	in the other. And the last one is hope, which means you need to be willing to act on that
9128080	9132160	principle without getting rewards in the here and now, because it doesn't exist yet.
9132160	9136560	Then you start out building the civilization. So you need to be able to do this in the absence
9137200	9141440	of its actual existence yet. So it can come into being.
9141440	9145520	So yeah. So the way it comes into being is by you accepting those notions. And then you see
9147360	9150160	these three divine concepts and you see them realized.
9150160	9154560	And now the problem is divine is a loaded concept in our world, right? Because we are outside of
9154560	9159200	this cult and we are still scarred from breaking free of it. But the idea is basically we need to
9159200	9164400	have a civilization that acts as an intentional agent, like an insect state. And we are not
9164400	9169760	actually a tribal species. We are a state building species. And what enabled state building is
9170400	9175520	basically the formation of religious states and other forms of rule-based administration
9175520	9179440	in which the individual doesn't matter as much as the rule or the higher goal.
9179440	9181440	Right. We got there by the question,
9181440	9186080	what's the optimal form of governance? So I don't think that Catholicism is the optimal form of
9186080	9190560	governance because it's obviously on the way out, right? So it is for the present type of society
9190560	9196720	that we are in. Religious institutions don't seem to be optimal to organize that. So what
9196720	9202320	we discovered right now that we live in in the West is democracy. And democracy is the rule of
9202320	9207280	oligarchs that are the people that currently own the means of production that is administered not
9207280	9212480	by the oligarchs themselves because there's too much disruption, right? We have so much innovation
9212480	9218080	that we have in every generation new means of production that we invent. And corporations die
9218720	9223600	usually after 30 years or so and something other takes a leading role in our societies.
9223600	9228000	So it's administered by institutions. And these institutions themselves are not elected,
9228000	9234800	but they provide continuity. And they are led by electable politicians. And this makes it
9234800	9238720	possible that you can adapt to change without having to kill people, right? So you can,
9238720	9242240	for instance, have a change in governments. If people think that the current government
9242240	9247440	is too corrupt or it's not up to date, you can just elect new people. Or if a journalist finds
9247440	9253440	out something inconvenient about the institution and the institution has no plan B, like in Russia,
9253440	9259680	the journalist has to die. This is when you run society by the deep state. So ideally, you have
9259680	9266160	an administration layer that you can change if something bad happens, right? So you will
9266160	9270240	have a continuity in the whole thing. And this is the system that we came up in in the West.
9270240	9274560	And the way it's set up in the US is largely a result of low-level models. So it's mostly just
9274560	9279360	second, third-order consequences that people are modeling in the design of these institutions.
9279360	9283680	So it's a relatively young society that doesn't really take care of the downstream effects of
9283680	9289440	many of the decisions that are being made. And I suspect that AI can help with this in a way,
9289440	9295680	if you can fix the incentives. The society of the US is a society of cheaters. It's basically
9295680	9299680	cheating is so indistinguishable from innovation, and we want to encourage innovation.
9299680	9301600	Can you elaborate on what you mean by cheating?
9301600	9305920	It's basically people do things that they know are wrong. It's acceptable to do things that you
9305920	9310880	know are wrong in the society to a certain degree. You can, for instance, suggest some
9310880	9315120	non-sustainable business models and implement them. Right. But you're always pushing the
9315120	9319280	boundaries. I mean, you're- Yes. And yes, this is seen as a good thing, largely.
9320480	9325760	Yes. And this is different from other societies. So for instance, social mobility is an aspect of
9325760	9330880	this. Social mobility is the result of individual innovation that would not be sustainable at scale
9330880	9335280	for everybody else. Right. Normally, you should not go up. You should go deep. We need bakers,
9335280	9339360	and we need to be very, very good bakers. But in a society that innovates, maybe you can replace
9339360	9344800	all the bakers with a really good machine. Right. That's not a bad thing. And it's a thing that made
9344800	9349360	the US so successful. But it also means that the US is not optimizing for sustainability,
9349360	9355200	but for innovation. And so it's not obvious as the evolutionary process is unrolling,
9355200	9360800	it's not obvious that that long-term would be better. It has side effects. So you basically
9360800	9365920	if you cheat, you will have a certain layer of toxic sludge that covers everything that
9365920	9370480	is a result of cheating. And we have to unroll this evolutionary process to figure out if these
9370480	9375600	side effects are so damaging that the system is horrible, or if the benefits actually outweigh
9377520	9384560	the negative effects. How do we get to which system of government is best? That was from,
9384560	9390720	I'm trying to trace back the last five minutes. I suspect that we can find a way back to AI
9390720	9396400	by thinking about the way in which our brain has to organize itself. In some sense, our brain is
9396400	9404240	a society of neurons, and our mind is a society of behaviors. And they need to be organizing
9404240	9410080	themselves into a structure that implements regulation. And government is social regulation.
9410720	9415040	We often see government as the manifestation of power or local interests, but it's actually a
9415040	9420800	platform for negotiating the conditions of human survival. And this platform emerges
9420800	9425360	over the current needs and possibilities and the trajectory that we have. So given the present
9425360	9430720	state, there are only so many options on how we can move into the next state without completely
9430720	9435040	disrupting everything. And we mostly agree that it's a bad idea to disrupt everything because
9435040	9440400	it will endanger our food supply for a while and the entire infrastructure and fabric of society.
9440400	9445200	So we do try to find natural transitions. And there are not that many natural
9445200	9448720	transitions available at any given point. What do you mean by natural transitions?
9448720	9451440	So we try not to have revolutions if we can have it.
9451440	9457440	Right. So speaking of revolutions and the connection between government systems and the
9457440	9464560	mind, you've also said that in some sense, becoming an adult means you take charge of
9464560	9471760	your emotions. Maybe you never said that. Maybe I just made that up. But in the context of the mind,
9472400	9478800	what's the role of emotion? And what is it? First of all, what is emotion? What's its role?
9478800	9483920	It's several things. So psychologists often distinguish between emotion and feeling and then
9484560	9489440	common day parlance we don't. I think that an emotion is a configuration of the cognitive
9489440	9494400	system. And that's especially true for the lowest level for the affective state. So when you have
9494400	9499200	an affect, it's the configuration of certain modulation parameters like arousal, valence,
9500560	9505280	your attentional focus, whether it's wide or narrow, inter reception or extra reception and so on.
9505280	9510080	And all these parameters together put you in a certain way to you relate to the environment
9510080	9514560	and to yourself. And this is in some sense an emotional configuration. And the more narrow
9514560	9520800	sense of emotion is an affective state that has an object. And the relevance of that object is given
9520800	9526080	by motivation. And motivation is a bunch of needs that are associated with rewards, things that give
9526080	9530960	you pleasure and pain. And you don't actually act on your needs. You act on models of your needs.
9530960	9535680	Because when the pleasure and pain manifest, it's too late. You've done everything. But so you act
9535680	9540400	on expectations, what will give you pleasure and pain. And these are your purposes. The needs don't
9540400	9544880	form a hierarchy. They just coexist and compete. And your organism has to, or your brain has to
9544880	9550400	find a dynamic homeostasis between them. But the purposes need to be consistent. So you basically
9550400	9556480	can create a story for your life and make plans. And so we organize them all into hierarchies.
9556480	9560400	And there is not a unique solution for this. Some people eat to make art and other people
9560400	9565920	make art to eat. They might end up doing the same things, but they cooperate in very different ways
9565920	9571280	because their ultimate goals are different. And we cooperate based on shared purpose. Everything
9571280	9574080	else that is not cooperation on shared purpose is transactional.
9575040	9582320	I don't think I understood that last piece of achieving the homeostasis.
9583200	9586800	Are you distinguishing between the experience of emotion and the expression of emotion?
9587440	9594080	Of course. So the experience of emotion is a feeling. And in this sense, what you feel is
9594080	9599360	an appraisal that your perceptual system has made of the situation at hand. And it makes this based
9599440	9606720	on your motivation and on your estimates, not your, but of the subconscious geometric parts
9606720	9611520	of your mind that assess the situation in the world with something like a neural network.
9612160	9616000	And this neural network is making itself known to the symbolic parts of your mind,
9616720	9622960	to your conscious attention via mapping them as features into a space. So what you will feel
9622960	9627840	about your emotion is a projection usually into your body map. So you might feel anxiety in your
9627920	9634320	solar plexus and you might feel it as a contraction, which is all geometry. Your body map is the space
9634320	9642720	that is always instantiated and always available. So it's a very obvious cheat if your non-symbolic
9642720	9646720	parts of your brain try to talk to your symbolic parts of your brain to map the feelings into the
9646720	9651680	body map. And then you perceive them as pleasant and unpleasant depending on whether the appraisal
9651680	9656640	has a negative or positive valence. And then you have different features of them that give you
9657440	9661280	more knowledge about the nature of what you're feeling. So for instance, when you feel connected
9661280	9665600	to other people, you typically feel this in your chest region around your heart. And you feel this
9665600	9671680	is an expansive feeling in which you're reaching out. And it's very intuitive to encode it like
9671680	9677040	this. That's why it's encoded like this for most people. It's a code in which the non-symbolic
9677040	9681920	parts of your mind talk to the symbolic ones. And then the expression of emotion is then the final
9681920	9686960	step that could be sort of gestural or visual and so on that's part of the communication.
9686960	9691920	Probably evolved as part of an adversarial communication. So as soon as you started to
9691920	9696960	observe the facial expression and posture of others to understand what emotional state they're in,
9696960	9701760	others started to use this as signaling and also to subvert your model of their emotional state.
9701760	9705920	So we now look at the inflections, at the difference between the standard face that
9705920	9710160	they're going to make in this situation. When you are at a funeral, everybody expects you to make
9710160	9714000	a solemn face, but the solemn face doesn't express whether you're sad or not. It just
9714000	9718880	expresses that you understand what face you have to make at a funeral. Nobody should know that you
9718880	9724640	are triumphant. So when you try to read the emotion of another person, you try to look at the delta
9724640	9732960	between a truly sad expression and the things that are animating this face behind the curtain.
9733600	9740080	The interesting thing is having done this podcast and the video component,
9740080	9745840	one of the things I've learned is that now I'm Russian and I just don't know how to express
9745840	9753200	emotion on my face. One, I see that as weakness, but whatever. The people look to me after you say
9753200	9759680	something, they look to my face to help them see how they should feel about what you said,
9759680	9764640	which is fascinating because then they'll often comment on why did you look bored or why did you
9764640	9769440	particularly enjoy that part or why did you whatever. It's a kind of interesting, it makes
9769440	9774800	me cognizant of I'm part, like you're basically saying a bunch of brilliant things, but I'm
9775440	9782560	part of the play that you're the key actor in by making my facial expressions and then
9782560	9788560	therefore telling the narrative of what the big point is, which is fascinating. It makes me
9788560	9792880	cognizant that I'm supposed to be making facial expressions. Even this conversation is hard
9792880	9798560	because my preference will be to wear a mask with sunglasses to where I could just listen.
9798560	9803600	Yes, I understand this because it's intrusive to interact with others this way and basically
9804320	9809200	Eastern European society have a taboo against that and especially Russia. The further you go
9809200	9815680	to the East and in the US, it's the opposite. You're expected to be hyper animated in your face
9815680	9822240	and you're also expected to show positive effect. If you show positive effect without
9822240	9828080	a good reason in Russia, people will think you are a stupid, unsophisticated person.
9829920	9836720	Exactly. Here positive effect without reason goes either appreciated or goes unnoticed.
9837440	9840560	No, it's the default. It's being expected. Everything is amazing.
9841200	9842560	Have you seen these...
9843200	9844240	Lego movie?
9844240	9849360	No, there was a diagram where somebody gave the appraisals that exist in the US and Russia,
9849360	9857440	so you have your bell curve. The lower 10% in the US, it's a good start.
9858320	9860960	Everything above the lowest 10% is amazing.
9860960	9861760	It's amazing.
9862560	9871600	And for Russians, everything below the top 10% is terrible and then everything except the top
9871600	9876400	percent is I don't like it and the top percent is even so.
9880320	9882160	It's funny, but it's kind of true.
9883760	9888000	There's a deeper aspect to this. It's also how we construct meaning in the US.
9888960	9894000	Usually, you focus on the positive aspects and you just suppress the negative aspects.
9895600	9902000	In our Eastern European traditions, we emphasize the fact that if you hold something above the
9902000	9906240	waterline, you also need to put something below the waterline because existence by itself is
9906240	9907120	as best neutral.
9907920	9913360	Right. That's the basic intuition. At best neutral or it could be just suffering, the default...
9913360	9918720	There are moments of beauty, but these moments of beauty are inextricably linked to the reality
9918720	9923760	of suffering. To not acknowledge the reality of suffering means that you are really stupid
9923760	9928320	and unaware of the fact that basically every conscious being spends most of the time suffering.
9929440	9936560	You just summarized the ethos of the Eastern Europe. Most of life is suffering with an
9936560	9942560	occasional moment of beauty. If your facial expressions don't acknowledge the abundance
9942560	9946400	of suffering in the world and in existence itself, then you must be an idiot.
9947280	9953200	It's an interesting thing when you raise children in the US and you in some sense preserve the
9953200	9958240	identity of the intellectual and cultural traditions that are embedded in your own families.
9958880	9965120	Your daughter asks you about Ariel the mermaid and asks you why is Ariel not allowed to play
9965120	9971280	with the humans and you tell her the truth. She's a siren. Sirens eat people. You don't
9971280	9975280	play with your food. It does not end well. And then you tell her the original story,
9975280	9979280	which is not the one by Anderson, which is the romantic one. And there's a much darker one,
9979840	9981680	the Undine story. What happened?
9981680	9989200	So Undine is a mermaid or a water woman. She lives on the ground of a river and she meets
9989200	9993920	this prince and they fall in love. And the prince really, really wants to be with her. And she says,
9993920	9998720	okay, but the deal is you cannot have any other woman if you marry somebody else, even though
9998720	10002480	you cannot be with me because obviously you cannot breathe under water and have other things to do
10002480	10009200	than managing your kingdom with you up here, you will die. And eventually after a few years,
10009200	10014560	he falls in love with some princess and marries her. And she shows up and quietly goes into his
10014560	10020240	chamber and nobody is able to stop her or willing to do so because she is fierce. And she comes
10020240	10025680	quietly and sat out of his chamber and they asked her, what has happened? What did you do? And she
10025680	10033760	said, I kissed him to death. And you know the Anderson story, right? In the Anderson story,
10033760	10039040	the mermaid is playing with this prince that she saves and she falls in love with him and
10039040	10045840	she cannot live out there. So she is giving up her voice and her tale for a human-like appearance.
10045840	10051440	So she can walk among the humans, but this guy does not recognize that she is the one that
10051440	10056400	he would marry. Instead, he marries somebody who has a kingdom and economical and political
10056400	10061760	relationships to his own kingdom and so on, as he should. It's quite tragic. And she dies.
10061760	10071520	Yeah. Yeah, instead Disney, the Little Mermaid story has a little bit of a happy ending. That's
10071520	10076240	the Western, that's the American way. My own problem is this, of course, that I read Oscar
10076240	10081280	Wilde before I read the other things. So I'm indoctrinated, inoculated with this romanticism.
10081280	10085280	And I think that the mermaid is right. You sacrifice your life for romantic love. That's
10085280	10091200	what you do because if you are confronted with either serving the machine and doing the obviously
10091200	10097120	right thing under the economic and social and all other human incentives, that's wrong. You
10097120	10106160	should follow your heart. So do you think suffering is fundamental to happiness along these lines?
10106160	10111280	Suffering is the result of caring about things that you cannot change. And if you are able to
10111280	10114560	change what you care about to those things that you can change, you will not suffer.
10115600	10121280	Would you then be able to experience happiness? Yes. But happiness itself is not important.
10121840	10126160	Happiness is like a cookie. When you are a child, you think cookies are very important and you want
10126160	10129680	to have all the cookies in the world. You look forward to being an adult because then you have
10129680	10134720	as many cookies as you want, right? But as an adult, you realize a cookie is a tool. It's a
10134720	10138800	tool to make you eat vegetables. And once you eat your vegetables anyway, you stop eating
10138800	10142720	cookies for the most part because otherwise you will get diabetes and will not be around for your
10142720	10148960	kids. Yes. But then the cookie, the scarcity of a cookie, if scarcity is enforced, nevertheless,
10148960	10153760	so like the pleasure comes from the scarcity. Yes. But the happiness is a cookie that your
10153760	10158240	brain bakes for itself. It's not made by the environment. The environment cannot make you
10158240	10162480	happy. It's your appraisal of the environment that makes you happy. And if you can change
10162480	10166320	the appraisal of the environment, which you can learn to, then you can create arbitrary states
10166320	10171120	of happiness. And some meditators fall into this trap. So they discover the womb, this basement
10171120	10175280	womb in their brain where the cookies are made, and they indulge in stuff themselves. And after
10175280	10179680	a few months, it gets really old and the big crisis of meaning comes. Because they thought
10179680	10184800	before that their unhappiness was the result of not being happy enough. So they fixed this,
10184800	10190640	right? They can release the newer transmitters at will if they train. And then the crisis of meaning
10190640	10195600	pops up at a deeper layer. And the question is, why do I live? How can I make a sustainable
10195600	10200000	civilization that is meaningful to me? How can I insert myself into this? And this was the problem
10200000	10207280	that you couldn't solve in the first place. But at the end of all this, let me then ask that same
10207280	10213120	question. What is the answer to that? What could the possible answer be of the meaning of life?
10214560	10219760	What could an answer be? What is it to you? I think that if you look at the meaning of life,
10219760	10226960	you look at what the cell is. Life is the cell. Or this principle, the cell. It's this
10227680	10231840	self-organizing thing that can participate in evolution. In order to make it work,
10231840	10235680	it's a molecular machine. It needs a self-replicator, a neck entropy extractor,
10235680	10239920	and a Turing machine. If any of these parts is missing, you don't have a cell and it is not
10239920	10244720	living. And life is basically the emergent complexity over that principle. Once you have
10244720	10249840	this intelligent super molecule, the cell, there is very little that you cannot make it do. It's
10249840	10255440	probably the optimal computronium, especially in terms of resilience. It's very hard to sterilize
10255440	10262320	the planet once it's infected with life. So its active function of these three components
10262320	10268240	over this super cell of cell is present in the cell, is present in us, and it's just...
10268240	10272240	We are just an expression of the cell. It's a certain layer of complexity in the organization
10272240	10277760	of cells. So in a way, it's tempting to think of the cell as a von Neumann probe.
10277760	10282400	If you want to build intelligence on other planets, the best way to do this is to infect
10282400	10287120	them with cells. And wait for long enough and there's a reasonable chance this stuff is going
10287120	10291520	to evolve into an information processing principle that is general enough to become sentient.
10293040	10298400	That idea is very akin to the same dream and beautiful ideas that express the cellular
10298400	10302640	automata in their most simple mathematical form. If you just inject the system with some
10303280	10309520	basic mechanisms of replication and so on, basic rules, amazing things would emerge.
10309520	10315120	The cell is able to do something that James Tardy calls existential design. He points out
10315120	10320000	that in technical design, we go from the outside in. We work in a highly controlled environment
10320000	10325280	in which everything is deterministic, like our computers, our labs, or our engineering workshops.
10325280	10330240	And then we use this determinism to implement a particular kind of function that we dream up and
10330240	10334800	that seamlessly interfaces with all the other deterministic functions that we already have in
10334800	10340960	our world. So it's basically from the outside in. And biological systems designed from the inside
10340960	10347920	out as seed will become a seedling by taking some of the relatively unorganized matter around it
10347920	10353600	and turn it into its own structure and thereby subdue the environment. And cells can cooperate
10353600	10358240	if they can rely on other cells having a similar organization that is already compatible. But
10358240	10364560	unless that's there, the cell needs to divide to create that structure by itself. So it's a
10364560	10369840	self-organizing principle that works on a somewhat chaotic environment. And the purpose of life in
10369840	10375760	the sense is to produce complexity. And the complexity allows you to harvest neg-entropy
10375760	10380400	gradients that you couldn't harvest without the complexity. And in this sense, intelligence and
10380400	10385440	life are very strongly connected because the purpose of intelligence is to allow control
10385440	10390400	under conditions of complexity. So basically, you shift the boundary between the ordered systems
10390400	10397440	into the realm of chaos. You build bridgeheads into chaos with complexity. And this is what
10397440	10401840	we are doing. This is not necessarily a deeper meaning. I think the meaning that we have priors
10401840	10405760	for, that we are evolved for, outside of the priors, there is no meaning. Meaning only exists
10405760	10411840	if a mind projects it, right? That is probably civilization. I think that what feels most
10411840	10416640	meaningful to me is to try to build and maintain a sustainable civilization.
10417280	10424320	And taking a slight step outside of that, we talked about a man with a beard and God, but
10428160	10433760	some mechanism perhaps must have planted the seed, the initial seed of the cell.
10434240	10439840	Do you think there is a God? What is a God? And what would that look like?
10439840	10446960	So if there was no spontaneous abiogenesis, in the sense that the first cell formed by some happy
10446960	10451120	random accidents where the molecules just happened to be in the right constellation to each other.
10451120	10457440	But there could also be the mechanism that allows for the random. I mean, there's like turtles all
10457440	10461280	the way down. There seems to be, there has to be a head turtle at the bottom.
10462240	10467520	Let's consider something really wild. Imagine, is it possible that a gas giant could become
10467520	10473440	intelligent? What would that involve? So imagine you have vortices that spontaneously emerge on
10473440	10479120	the gas giants, like big storm systems that endure for thousands of years. And some of these storm
10479120	10483600	systems produce electromagnetic fields because some of the clouds are ferromagnetic or something.
10483600	10488960	And as a result, they can change how certain clouds react rather than other clouds and thereby
10488960	10493200	produce some self stabilizing patterns that eventually lead to regulation feedback loops,
10493200	10498880	nested feedback loops and control. So imagine you have such a thing that basically has emergent,
10498880	10503200	self-sustaining, self-organizing complexity. And at some point this wakes up and realizes
10503200	10507920	I'm basically Lem Solaris. I am a thinking planet, but I will not replicate because I
10507920	10513680	can recreate the conditions of my own existence somewhere else. I'm just basically an intelligence
10513680	10519600	that has spontaneously formed because it could. And now it builds a von Neumann probe. And the
10519600	10523840	best von Neumann probe for such a thing might be the cell. So maybe it, because it's very, very
10523840	10529280	clever and very enduring, creates cells and sends them out. And one of them has infected our planet.
10529280	10533440	And I'm not suggesting that this is the case, but it would be compatible with the Prince Birmingham
10533440	10538960	hypothesis. And it was my intuition that abiogenesis is very unlikely. It's possible,
10538960	10543200	but you probably need to roll the cosmic dice very often, maybe more often than they are
10543200	10552560	planetary surfaces. I don't know. So God is just a system that's large enough
10552560	10556480	that allows randomness. Now, I don't think that God has anything to do with creation.
10556480	10562080	I think it's a mistranslation of the Talmud into the Catholic mythology. I think that
10562080	10569600	Genesis is actually the childhood memories of a God. So the childhood memories of a God,
10569600	10576880	it's basically a mind that is remembering how it came into being. And we typically interpret
10576880	10583840	Genesis as the creation of a physical universe by a supernatural being. And I think when you read it,
10584400	10590480	there is light and darkness that is being created. And then you discover sky and ground,
10590480	10597040	create them. You construct the plants and the animals, and you give everything their names
10597040	10602640	and so on. That's basically cognitive development. It's a sequence of steps that every mind has to
10602640	10606560	go through when it makes sense of the world. And when you have children, you can see how
10606560	10611200	initially they distinguish light and darkness, and then they make out directions in it, and they
10611200	10614800	discover sky and ground, and they discover the plants and the animals, and they give everything
10614800	10619200	their name. And it's a creative process that happens in every mind, because it's not given,
10619200	10623520	right? Your mind has to invent these structures to make sense of the patterns on your retina.
10624400	10629200	Also, if there was some big nerd who set up a server and runs this world on it,
10629200	10633760	this would not create a special relationship between us and the nerd. This nerd would not
10633760	10639520	have the magical power to give meaning to our existence, right? So this equation of a creator
10639520	10645680	God with the God of meaning is a sleight of hand. You shouldn't do it. The other one that is done
10645680	10651440	in Catholicism is the equation of the first mover, the prime mover of Aristotle, which is basically
10651440	10657040	the automaton that runs the universe. Aristotle says if things are moving and things seem to be
10657040	10661680	moving here, something must move them, right? If something moves them, something must move the
10661680	10666960	thing that is moving it, so there must be a prime mover. This idea to say that this prime mover is
10666960	10673440	a supernatural being is complete nonsense, right? It's an automaton in the simplest case. So we
10673440	10679280	have to explain the enormity that this automaton exists at all. But again, we don't have any
10679280	10685040	possibility to infer anything about its properties except that it's able to produce change in
10685040	10690240	information, right? So there needs to be some kind of computational principle. This is all there is.
10690240	10695040	But to say this automaton is identical again with the creator of first cause or with the thing that
10695040	10702480	gives meaning to our life is confusion. No, I think that what we perceive is the higher being
10702480	10707600	that we are part of. And the higher being that we are part of is the civilization. It's the thing
10707600	10713440	in which we have a similar relationship as the cell has to our body. And we have this prior
10713440	10719520	because we have evolved to organize in these structures. So basically, the Christian God in
10719520	10724080	its natural form without the mythology, if you undress it, is basically the Platonic form of
10724080	10732880	civilization. It's this ideal that you try to approximate when you interact with others,
10732960	10735600	not based on your incentives, but on what you think is right.
10737520	10744160	Wow, we covered a lot of ground. And we're left with one of my favorite lines, and there's many,
10744160	10754000	which is happiness is a cookie that the brain bakes itself. It's been a huge honor and a pleasure
10754000	10759680	to talk to you. I'm sure our paths will cross many times again. Joshua, thank you so much for
10759680	10764800	talking today. I really appreciate it. Thank you, Lex. It was so much fun. I enjoyed it. Awesome.
10765840	10770640	Thanks for listening to this conversation with Joshua Bach. And thank you to our sponsors,
10770640	10776400	ExpressVPN and Cash App. Please consider supporting this podcast by getting ExpressVPN
10776400	10785600	at expressvpn.com slash LexBod and downloading Cash App and using code LexPodcast. If you enjoy
10785600	10790560	this thing, subscribe on YouTube, review it with five stars on Apple Podcasts,
10790560	10796800	support it on Patreon, or simply connect with me on Twitter at Lex Friedman. And yes,
10796800	10802720	try to figure out how to spell it without the E. And now let me leave you with some words of wisdom
10802720	10809840	from Joshua Bach. If you take this as a computer game metaphor, this is the best level for humanity
10809840	10817520	to play. And this best level happens to be the last level, as it happens against the backdrop
10817520	10825120	of a dying world. But it's still the best level. Thank you for listening and hope to see you next
10825120	10830400	time.
