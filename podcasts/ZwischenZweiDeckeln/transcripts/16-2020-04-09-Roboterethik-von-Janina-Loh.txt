Hallo und herzlich willkommen zu Episode 15 von Zwischen zwei Deckeln, dem Sachbuchpodcast.
Ich bin wie immer der Nils und mit mir ist heute wieder der Christoph dabei.
Hallo zusammen.
Ja, schön, dass ihr da seid, dass ihr wieder eingeschaltet habt, wenn ihr jetzt zuhause sitzt oder irgendwie auf dem Abenteuer einkaufen seid oder zu denen gehört, die arbeiten müssen da draußen.
Danke an euch, dass ihr irgendwie den Laden am Laufen haltet.
Genau, ja. Wie sieht es aus bei dir? Irgendetwas Relevantes, Neues?
Nichts Spektakuläres auch. Ich bin überwiegend zuhause, kaufe einige Menschen mit ein und habe Zeit zum Lesen gefunden.
Was liest du denn gerade?
Also jetzt gerade tatsächlich wirklich für die Episode, die wir heute aufnehmen, habe ich das Buch von heute in den letzten anderthalb, zwei Wochen eben gelesen.
Also ihr habt es ja schon im Titel gesehen, es geht um Roboterethik von Janina Lo und ansonsten habe ich nochmal was bei der BBB bestellt und habe ein, zwei Krimis mir jetzt bei Twitter empfehlen lassen, die ich angehen möchte.
So ist mein Ziel. Aber sonst muss ich sagen, habe ich nicht viel mehr Zeit als vorher. Ich weiß nicht, wie es bei dir ist.
Ja, bei mir auch nicht so wirklich, weil meine Arbeit sich dann doch irgendwie eins zu eins ins Homeoffice übertragen lässt.
Und dadurch, dass die Pendelzeit wegfällt, sogar die klassische Lesezeit irgendwie ein bisschen kürzer wird.
Aber ich komme so ein bisschen zum Lesen. Ich habe jetzt gerade fertig gelesen das Zettelkastenprinzip von Sönke Ahrens.
Tatsächlich mal so ein bisschen der Versuch, diesen mysteriösen, ominösen, lumenschen Zettelkasten für so privates Notizenmanagement, Wissensmanagement aufzubereiten.
Und das fand ich echt total spannend. Da wird es zunächst wahrscheinlich auch den ein oder anderen Blogbeitrag zu geben bei mir.
Hast du Kommunikation mit Zettelkästen von Luman dazu gelesen?
Das ist ein Buch, ein Essay?
Nee, das ist ein Essay. Ich glaube, das sind nur so zehn Seiten oder so.
Ja, ich glaube, habe ich zumindest mal angefangen zu lesen.
Vielleicht, wenn wir dran denken, können wir es ja verlinken. Ich muss sagen, das ist relativ amüsant, weil Luman da eben klarmacht, wie egal es ihm ist, mit wem er eigentlich da irgendwie zu tun hat
und mit wem er zusammenarbeitet, also seinen Zettelkasten quasi als Assistenten begreift.
Und das ist ganz, ganz gut.
Aber das kann tatsächlich funktionieren. Das ist das Krasse dabei.
Genau, da können wir vielleicht auch eine Episode hier mal zumachen. Mal gucken.
Vielleicht auch das.
Ja, von euch da draußen ist irgendwie nichts an Fragen oder Kommentaren oder so zu der letzten Episode gekommen.
Fühlt euch da nochmal eingeladen, wenn ihr irgendwas diskutieren wollt.
Oder so uns einfach ein Audiophile zu schicken, einfach am Handy aufgenommen, vielleicht in der ruhigen Ecke oder irgendwie unter Blogbeitrag zu kommentieren.
Dann können wir das hier im Zweifel aufgreifen.
Also fühlt euch da nochmal herzlichst so eingeladen.
Apropos letzte Episode. Christoph, erinnerst du dich noch, worum ging es?
Ja, also du hast uns vorgestellt, Die Economist's Hour von Benjamin Applebaum.
Und da ging es, wenn ich mich jetzt recht im Sinne darum, wie im Prinzip so liberale, marktliberale Ökonomen Einfluss auf die Politik in den USA genommen haben.
Und ja, es zeigt sich, dass viel von dem, was so prognostiziert wurde, in dem Bereich gar nicht so eingetreten ist, wie es gesagt wurde.
So würde ich es in zwei, drei Sätzen zusammenfassen.
Genau, also wie es von den Ökonomen selbst vorher gesagt wurde.
Ja, genau, das meine ich.
Andere haben dann durchaus zumindest mal geahnt, was passieren könnte.
Ok, ja, genau, darum ging es letzte Woche, letzten Monat.
Diese Episode hast du ja schon gesagt, worum es geht.
Wir werden ein bisschen technikphilosophisch, wenn ich das richtig sehe, mit dem Buch Roboterethik eine Einführung von Janina Loh.
Die Autorin ist Philosophin und arbeitet an der Uni Wien im Bereich Technik und Medienphilosophie, sagst du.
Und sie habilitiert zu posthumanistischen Elementen in Hannah Arendts Werk und Denken.
Also ja, da merkt man schon ein bisschen, dass sowohl das klassischphilosophische als auch das technikphilosophische taucht ja schon in dem Titel auf.
Möchtest du anfangen, uns das Buch in der Kurzfassung kurz vorzustellen?
Ja, das mache ich gerne.
Also Janina Loh untersucht in ihrer Studie Roboterethik verschiedene Ansätze der Einordnung der Fähigkeiten und Zugeständnisse von und an Roboter durch EthikerInnen und PhilosophInnen.
Es zeigt sich, dass Begriffe wie Moral und Verantwortung traditionell dem Menschen vorbehalten sind und einzelnen Individuen zugeschrieben werden.
Loh stellt Gegenpositionen dar und macht Vorschläge, wie und warum wir unsere tradierten Denkmuster durchbrechen können und vielleicht auch sollten.
Okay, das klingt ja doch potentiell kontrovers.
Möglich, ja, mal gucken, wie kontrovers es wirklich wird.
Und wie danach so was wie haben Tiere auch Menschenrechte sozusagen.
So ein bisschen haben Roboter auch Menschenrechte.
Scheint wieder so ein bisschen das zu sein, was mir spontan einfällt.
Tatsächlich gibt es da auch einige Referenzen und auch Vergleiche zu dem, was eben Robotern zugetraut wird und Tieren oder nicht Tieren.
Genau, einige Fragestellen aus der Fragestellung aus der Roboterethik wurden eben offenbar in der Tierethik schon behandelt oder werden es immer noch oder man kann sie auch zusammen behandeln und so.
Genau, so ein bisschen die Frage nach dem Status von Nichtmenschen im Prinzip, mit denen wir aber irgendwie zu tun haben.
Im Prinzip ist das Buch doch, ja, ich glaube, ich weiß nicht, wie philosophische Studien sonst so aufgebaut sind, aber ziemlich stringent.
Und dadurch, ja, manchmal so ein bisschen habe ich das Gefühl, wird einiges einfach abgearbeitet.
Also es geht erstmal los mit einer Einleitung und dann stellt sich Loh die Frage, welche Bereiche der Robotik und der ethischen Fragen es überhaupt gibt.
Und dann geht es einmal die Arbeitsfelder der Robotik durch und dann geht es darum, inwiefern Roboter moralische Handlungssubjekte oder Objekte sein können, was das bedeutet.
Darüber sprechen wir noch.
Und dann geht es im dritten Kapitel um Verantwortungzuschreibung in der Interaktion zwischen Menschen und Robotern.
Und dann wird das wieder durchexerziert als Roboter als Verantwortungssubjekte und Roboter als Verantwortungsobjekte.
Und dann gibt es am Ende noch ein bisschen neuen Seiten, einmal ein bisschen Breitseite gegen alles und jeden quasi.
Also ein bisschen kritischer Ausblick kennt man ja auch aus vielen Büchern.
Aber ja, so ist das Buch erstmal aufgebaut und ich glaube, ich fange einfach mal an.
Loh steigt damit ein, dass sie erstmal sagt, dass wenn wir über Ethik und Roboter oder Technik sprechen, man immer davon ausgehen muss, dass Technik von Menschen erschaffen ist und dementsprechend von Normen geprägt ist.
Also egal, was wir tun, ist irgendwie zumindest implizit mit gesellschaftlichen Ideen, Normen und so weiter verknüpft.
Das heißt, so die Idee einer völlig neutralen Technik, die ja so wie objektiv vor uns liegt und quasi gar keinen, in sich keine ethischen Implikationen oder so hat, das verwirft sie direkt zum Anfang.
Dabei sagt sie, dass Roboterethik eben ein Teilbereich der Maschinenethik ist, was bedeutet, dass alle Roboter Maschinen sind, aber nicht alle Maschinen sind Roboter.
Ja, macht Sinn.
Und auch dann weist sie direkt darauf hin, dass die Fragestellungen durchaus der der Tierethik ähneln.
Also was kann ein Tier, was kann ein Roboter, was fühlt es, was fühlt er.
Ja, weil ich gerade gesagt habe, dass Roboter keine Maschinen sind, ist vielleicht einmal wichtig zu definieren, was Roboter überhaupt sind.
Und damit starten wir in die Definition von Maschinen.
Das sind nämlich künstliche Gebilde, die aus einem Antriebssystem durch Motor, Wind oder Wasser bewegten Teilen besteht und die Energie umsetzen.
Das ist eine Maschine, so ganz allgemein.
Roboter hingegen sind spezielle Maschinen.
Das Wort geht auf Roboter aus dem Tschechischen zurück.
Das kommt auch aus irgendeinem Science Fiction Roman.
Das ist ja so dein Spezialgebiet.
Ja, ich könnte Stanislav Lem sein, aber ich bin mir da nicht ganz sicher.
Ja, ich glaube, es war nicht Stanislav Lem.
Dann hätte ich nämlich, glaube ich, dazu geschrieben, weil den, den kenne ich quasi.
Naja, und das Wort geht, steht offen, also Roboter im Tschechischen steht offenbar für Arbeit, frohendienst oder Zwangsarbeit.
Das fand ich ganz interessant, gerade mit Blick auf eben ethische Implikation.
Und ja, also jetzt die Definition von Roboter.
Das ist eine elektromechanische Maschine, die A. über einen eigenständigen Körper und B. über mindestens einen Prozessor verfügt, C. Sensoren hat, D. über Effektoren oder Aktoren verfügt, die Signale in mechanische Abläufe übersetzen.
Roboter erscheinen zumindest autonom und können physisch Einfluss auf ihre Umwelt nehmen.
Genau, also relativ komplex irgendwie, aber man kann sich merken, gut, die haben einen eigenständigen Körper.
Sie haben irgendwas, womit sie irgendwie Dinge prozessieren können, also einen Prozessor, also das, was bei Menschen das Gehirn vielleicht ist.
Und sie haben irgendwie Sensoren, um die Umwelt wahrzunehmen.
Und sie haben irgendwas, wie sie um die Umwelt eingreifen können, also irgendwelche Roboterarme oder sie haben einen kleinen Staubsauger, wie diese Staubsaugerroboter und so weiter.
Aber danach macht Loh die These auf, dass es heute eigentlich keinen Bereich im menschlichen Alltag mehr gibt, in dem Roboter noch keinen Einzug gehalten hätten.
Kann man erst mal so auf sich wirken lassen und sich dann überlegen, ob das stimmt.
Ich habe erst gedacht, das klingt ja irgendwie relativ steil als These, aber ich glaube, sie hat schon recht.
Egal, wo wir hingucken, wir wissen, dass es zumindest Roboter gibt, auch wenn wir sie nicht tagtäglich sehen und mit ihnen zu tun haben sollten.
Ja, dann geht es im ersten Kapitel darum zu klären, worum es in diesem Buch eigentlich geht, also welche Bereiche aus der Robotik zur Sprache kommen oder in der Roboterethik eine Rolle spielen.
Das ist einmal die Industrierobotik, also die Frage nach Ersetzung von Arbeitsplätzen hat man sicherlich schon gehört und daran anschließend, wir hatten es ja gerade, sie habilitiert zu Hannah Arendt auch,
die Frage danach, was der Mensch ist, wenn er nicht mehr arbeitet, wenn er nichts mehr schafft oder nicht mehr Lohn arbeitet oder wie man es nennen möchte.
Also das sind Fragen, die daran anschließen an das Problem von, wenn Arbeitsplätze ersetzt werden, was resultiert eigentlich daraus.
Dann, was natürlich glaube ich auch ein verbreitetes Beispiel ist, das auch in dem Buch immer wieder vorkommt, ist die Frage nach dem autonomen Fahren.
Also wenn wir jetzt Automobile haben, die irgendwie intelligente Fahrsysteme haben, ist die Frage, wie diese Autos in Unfallsituationen reagieren sollen, wer bei Unfällen dann die Verantwortung trägt,
also kann das Auto selbst irgendwie verantwortlich gemacht werden, ist es der Konzern, der das Auto gebaut hat, ist es der Fahrer, der eigentlich noch hinterm Lenkrad sitzt, aber nicht mehr eingreifen muss und so weiter.
Dann kriegt das Auto drei Monate Fahrverbot, stelle ich mir auch lustig vor.
Ja, das ist irgendwie ganz interessant. Da geht später auch noch drum die Unterscheidung zwischen Verantwortung und Haftbarkeit, die so ein bisschen getrennt wird.
Macht Sinn.
Und sie weiß, glaube ich, drei oder viermal in dem Buch darauf hin, dass in der EU oder vom Europäischen Parlament gerade die Idee von elektronischen Personen erarbeitet wird,
also quasi analog zu juristischen Personen, die wir aus dem Recht schon kennen, wo dann irgendwelche Unternehmen vor Gericht wie Personen behandelt werden können, könnte man das eben auch für Maschinen oder Roboter vielleicht einsetzen.
Was aus diesem Projekt der Erarbeitung da genauer resultiert ist, das schreibt sie aber nicht, nur dass das eben im Prozess ist.
In der Medizin oder Therapie- und Pflege-Robotik stellt sich, wenn man da Roboter einsetzt, und das ist ja mittlerweile auch schon so,
ich persönlich habe das auch schon erlebt, also ich habe mal in einem alten Zentrum ein Praktikum gemacht, in dem diese Roboter-Robbe Paro zum Einsatz kam.
Ich weiß nicht, ob du die kennst.
Ja, sag mir was.
Also für die, die es nicht kennen, das ist quasi sowas wie ein großes Kuscheltier in Form einer Robbe, die es weiß und die hat verschiedenste Sensoren eingebracht und reagiert auf Berührung und Ansprache und so weiter ein Stück weit.
Und es ist auch ganz schön teuer, die kostet, glaube ich, um die 5000 Euro oder mehr.
Gibt es ganz viele Videos zu, wenn einem das interessiert.
Und im Zuge dieser Pflege-Robotik, und man kann sich da ja auch Sachen vorstellen, wie Roboter, die Menschen aus Betten heben oder auf die Toilette begleiten und so weiter,
stellt sich eben die Frage nach der Autonomie der Personen, die dort behandelt werden,
und inwiefern die Emotionen, die da vielleicht auch aufkommen gegenüber einer Roboter-Robbe oder einem Roboter als Gesprächspartner, kann man sich ja alles Mögliche vorstellen,
inwiefern diese Emotionen in gewisser Maße echt sind, inwiefern die Personen, die da beteiligt sind, getäuscht werden, sowas alles.
Ich nehme es gleich vorweg, Janina Loh sieht das alles nicht so problematisch.
Okay.
Sie sagt, dass eben Beziehungen zu Objekten oder Robotern durchaus echte Emotionen sein können und dass das keine Täuschung in dem Sinne darstellen muss.
Okay.
Vielleicht hat man das jetzt auch schon gehört, Sexroboter sind ja auch ein Ding, also die gibt es schon, kann man schon kaufen, kosten, glaube ich, immer noch relativ viel Geld.
Und was da in dem Buch augenfällig wird, da geht es dann häufig in dem Kontext um feministische Fragestellung.
Ja.
Also es gibt da irgendwie teilweise die Möglichkeit, eben Roboter zu vergewaltigen und dass das irgendwie im Kern problematisch sein kann.
Auf der anderen Seite vielleicht aber auch irgendwie befreiend, weil Leute, die so etwas haben, das dann ausleben können, ohne eben Menschen zu schädigen.
Ja.
Ja, da gibt es viele ethische Ansätze und viele Diskussionen darum.
Und darum geht dann natürlich auch um Objektifizierung von, gerade von Frauen, weil diese Sexroboter eben sich häufig an heterosexuelle Männer wenden.
Ja, also auch ein Bereich, in dem der Feminismus eine große Rolle spielt und auch spielen sollte.
Der letzte Bereich, den Lo aufzählt und das liegt, glaube ich, auf der Hand, dass das irgendwie ethisch relevant ist, ist eben die Frage nach Militärrobotern.
Ja.
Und wie wird auch unbefugten der Zugang zu Entscheidungen verwehrt und wie viel Autonomie dürfen artificielle Systeme eigentlich haben, wenn sie eben konkret über Leben und Tod entscheiden?
Da ist dann auch wieder die Schnittstelle zu dem autonomen Auto, das Entscheidende, auf wen es rettet.
Ja, absolut.
Ja, in der Roboterethik gibt es dann zwei klassische Arbeitsfelder, mit denen man sich beschäftigen kann.
Also einmal Roboter als moralische Akteurinnen, also inwiefern sie selbst irgendwie moralfähig sind und moralisch handeln.
Ein bisschen, das wir gerade eben schon besprochen haben.
Und die andere Frage ist danach, inwiefern Roboter als Objekte moralischen Handelns irgendwie relevant werden, wie zum Beispiel bei den Sexrobotern.
Ja.
Es ist auffällig, dass die Deutungshoheit über das ganze erste Arbeitsfeld, also inwiefern Roboter moralische Subjekte sein können, ziemlich anthropozentrisch sind.
Anthropozentrisch meint damit, der Mensch steht im Mittelpunkt und es wird von Menschen ausgedacht.
Also unser Bild davon, was irgendwie moral ist und sein kann, ist immer davon geleitet, was wir von Menschen können, wie wir glauben, dass Menschen sind.
Und das bedeutet am Ende auch, dass durchweg Menschen entscheiden, ob Roboter moralisch handeln können, ob sie sowas wie Willensfreiheit besitzen können und so weiter.
Ja.
Leuchtet, glaube ich, auch ein.
Ich glaube, was Lo damit aufzumachen, probiert es erstmal, einfach einen Reflektionspunkt, einfach mal darüber nachzudenken.
Okay, wir denken immer von uns aus, was vermutlich auch gar nicht anders möglich ist, aber wir nehmen uns damit eben auch die Freiheit und Hoheit darüber raus, zu entscheiden, wer eigentlich Teil von dem ist, was wir uns eigentlich nur zubelegen.
Genau, ja klar.
Sie sagt dabei, dass wir eigentlich gar nicht wissen, wie es überhaupt ist, frei zu sein.
Also Zitat jetzt.
Ja, zum zweiten so ein bisschen in die Richtung.
Roboter als Moralobjekte, da was ich gerade eben schon hatte, waren eben die Beispiele aus der Sexrobotik.
Sexroboter reproduzieren eben heteronormative, patriarchale und diskriminierende Strukturen.
Das ist durchaus ein Problem.
Würde man sie anders konzipieren, könnte man aber durchaus auch sich andere Sexroboter vorstellen, die vielleicht auch andere Möglichkeiten bieten.
Ich könnte mir vorstellen, dass gerade wenn es um Sexualtherapie geht und so die Begleitung von Menschen, die vielleicht zuerst Sexualpraktiken körperlich nicht eigenständig in der Lage sind, dass da eigentlich ein großes Feld der Emanzipation auch offen steht.
Also ein bisschen die Schnittstelle wieder zu der Therapie.
Ja, genau.
Dabei stellt sich immer wieder die Frage, wie Moral überhaupt in eine Maschine gelangen kann.
Also wenn Sie irgendwie Moral entwickeln können sollten, wie funktioniert das?
Und da gibt es eben verschiedene Ansätze, entweder Top-Down, Bottom-Up oder Hybrid-Ansätze.
Also entweder man programmiert halt sowas wie moralische Regeln ein oder man setzt darauf, dass artificielle Systeme selbst lernen können.
Also sowas, was unter KI verstanden wird oder man kombiniert ein bisschen.
Also einerseits gibt man irgendwelche Regeln vor, andererseits hat man irgendwie selbst der andere Ansätze.
Was auffällig ist, ist, dass jeder Ansatz irgendwie Top-Down, das zu bewältigen, total schwierig ist, weil Moral eben abstrakt ist und die Entscheidungen, die aber gefällt werden müssen, sehr konkret sind.
Die Rettung von Menschenleben ist irgendwie sehr abstrakt, aber die Entscheidung, in welche Richtung ein Auto ausweicht, ist eben sehr konkret.
Das heißt, man kann sich nicht einfach darauf verlassen zu sagen, rette möglichst viele Menschen zu einem Auto und das wird schon irgendwie funktionieren.
Wobei ich das ganz spannend finde, weil wir ja gerade auch jetzt in der aktuellen Situation mit der drohenden Überlastung oder in manchen anderen Ländern auch schon der Überlastung von Intensivstationen und Ähnlichem,
wo ja im Grunde genau diese moralischen Entscheidungen getroffen werden, wen können wir jetzt behandeln und wen nicht.
Und da eben danach gerufen wird, wir brauchen eine klare Triage-Regeln sozusagen, nach welchen Kriterien entscheiden wir, dass das eben nicht das Individuum machen muss, der einzelne Arzt oder die einzelne Ärztin oder der einzelne Pfleger,
die halt sagen, der kriegt jetzt die Beatmung und der nicht, sondern dass wir da im Grunde nach genau diesen abstrakten Regeln verlangen, die du jetzt sagst, die eigentlich schwer zu machen sind.
Das finde ich auch ziemlich interessant, weil mein Gefühl auch immer wäre, man kann probieren, das durchzureglementieren, wie so eine Triage stattfinden soll und wie die organisiert werden möchte.
Aber ich habe das Gefühl, das würde sich im Konkreten sowieso ergeben.
Ich habe das Gefühl, die eigentliche Funktion dahinter ist, eben Ärztinnen zu entlasten von der von der konkreten Entscheidung.
Aber ich glaube, ich vermute, dass die Entscheidung, die Ärztinnen treffen würden, ohne Triage-Regelung, ziemlich genau das widerspiegeln würden, was sie auch mit Regelungen jetzt entscheiden können, sollen oder dürfen.
Da ist ja gerade diese Diskussion, die auch gerade so aus der Rassismusdiskussion sozusagen ein bisschen kommt, ob man nicht mit objektiveren Triageregeln solche im Zweifel auch erst mal unbewussten Verzerrungen und wie eben rassistische Vorurteile und Ähnliches nicht auch ein bisschen ausgleichen kann.
Wenn man eben sagt, ich überlasse das nicht dem Einzelnen, der eventuell unbewusst irgendwas internalisiert hat, sondern ich sage halt, ne, stopp hier, das Alter muss so sein.
Und ja, wenn der jetzt zwei Jahre jünger ist, dann kriege ich das halt trotzdem.
Aber wenn du vielleicht gesagt hast, und so weiter und so fort.
Ich glaube, da kommt diese Objektivierung ins Spiel, die da vielleicht auch wirklich was nochmal verändern kann gegenüber Einzelnen.
Ja, stimmt, das ist nochmal ein ganz gelungener Gedankenanstoß da.
Und ich finde auch einfach die moralische Entlastung quasi von Ernst, den ist ja auch ein Eigenwert.
Das ist ein totaler Wert natürlich, gar keine Frage.
Inwiefern Roboter jetzt wirklich sowas wie, ja, also richtig Subjekte im moralischen Sinne sein können, hängt natürlich auch ganz stark davon ab, was moralische Akteurinnen überhaupt benötigen.
Und da jetzt einfach mal so, um darzustellen, wie dieses Buch geschrieben ist, nimmt Janina Losig eben immer verschiedene Autorinnen raus, sagt XY hat das und das geschrieben.
Und dann wird ein anderer Text quasi behandelt und da wird das und das gesagt.
Und das stellt sie eben da, was es manchmal ein bisschen schwierig macht, dieses Buch jetzt eben so ganz doll zusammenzufassen.
Im Prinzip müsste ich sagen, okay, es gibt sechs Studien, die untersucht wurden und die kommen jeweils zu dem Schluss.
Deswegen probiere ich das jetzt hier mal zusammenzufassen.
Manche Studienergebnisse lasse ich dann aber auch quasi hinten runterfallen und stelle sie nicht da.
Also manche Ethikerinnen kommen dann hier nicht vor.
Und am Ende eines jeden Kapitels bezieht Janina Loh das, was sie quasi gelesen hat, auf verschiedene Robotersysteme und probiert einzuordnen, inwiefern jetzt eben Subjekt, Objektverantwortungs möglich wären im Sinne der gelesenen Autorinnen.
Auch das lasse ich hinten noch rüber, weil ich glaube, wenn ich jetzt sechs oder sieben Roboter immer wieder aufzähle und sage, möglicherweise können die dies, das oder das können sie auch nicht.
Je nachdem, wem man fragt, das geht ein bisschen zu weit.
Wenn euch das im Konkreten interessiert, müsst ihr das Buch dann tatsächlich vielleicht einfach kaufen und lesen.
Naja, auffällig ist, dass bei Moral sich auch die Philosophinnen nicht so richtig einig sind, was denn eigentlich Moral ist und was Gut ist.
Also es gibt einerseits so Ideen von Gut ist, wer Gutes tut, so im Sinne von Forrest Gump quasi.
Also man guckt einfach nur darauf, was irgendwie der Output von einer Handlung ist.
Und wenn der gut ist, dann sind auch Roboter befähigt, quasi moralisch zu agieren oder auch wenn es schlecht ist.
Also man guckt einfach nur, was der Output einer Aktion ist und auf der anderen Seite gibt es natürlich irgendwie hochkomplexe Anforderungsprofile,
wo auch ein Eigenverständnis von Moral durch die handelnde Person, den handelnden Roboter oder das handelnde Tier irgendwie vorliegen muss.
Die Frage, die im Kern so ein bisschen da ist, ist, wann sind Roboter eigentlich wirklich moralfähig und wann nutzen sie nur Funktionsequivalente zu Moral?
Und tun dann quasi nur so, als wären sie moralfähig?
Und da hängt dann natürlich die große Frage nach dem Bewusstsein im Kern da dran.
Das erinnert mich an dieses Gedankenexperiment von dem Chinese Room, das chinesische Zimmer.
Ich weiß nicht, ob du das kennst.
Ah, erzähl mal.
Ja, da geht es da um Sprachverstehen im Grunde, da kommt das ursprünglich her.
Und das nimmt halt an, ich weiß gar nicht mehr von wem es kommt, dass du ein Zimmer hast, wo sämtliche Regeln der chinesischen Sprache drin stehen, in Büchern.
Und ganz ausgefeilte Regeln, wenn dieses Zeichen kommt, dann antwortest du mit diesem Zeichen und wenn diese Zeichen kommen, dann antwortest du mit dem Zeichen und so weiter und so fort.
Und jetzt ist halt eine Person in diesem Zimmer, die bekommt per Zettel oder was, eine Reihe chinesischer Schriftzeichen reingereicht, befolgt die Regeln und gibt eine Antwort.
So, und diese Antwort ist natürlich sinnvoll, ohne dass diese Person verstanden hätte, was sie da tut inhaltlich, also dass die Sprache sozusagen verstanden hätte.
Und jetzt gibt es die Frage, dieses Zimmer, ist das Verstehen, was da passiert?
Wenn es von außen so aussieht, als wäre es Verstehen?
Ja.
Das schließt dann wahrscheinlich an den Turing-Test im Grunde an.
Das klingt so, als wäre das das.
Ja.
Ja, in einem Interview weiß, das verlinke ich ja auch nachher, bei Sein und Streit war sie bei Deutschland von Kultur, da weiß Janina Loh natürlich auch darauf hin, dass diese ganze Frage nach dem Bewusstsein total schwierig ist.
Und halt sowas wie eine, sie nennt es, glaube ich, philosophisches Zusatzargument, also einfach die absolute Annahme, dass Menschen Bewusstsein haben, ist empirisch nicht beweisbar.
Man kann quasi nicht den Kopf aufschneiden, ins Gehirn gucken und sagen, da ist es, das Bewusstsein, sondern auch da sind ja nur irgendwie Verkettungen von Nerven und Datenströmen im Prinzip irgendwie.
Und das heißt, ja, es gibt einfach Dinge, die kann man nicht so richtig nachvollziehen und das wird man also vermutlich absehbar zumindest auch nicht beantworten können, inwiefern irgendwas Bewusstsein erlangen kann, außer dass wir es Menschen halt auf jeden Fall zubelegen.
Ja.
Was auffällig ist, wenn es darum geht, ob Dinge, Tiere, Roboter, wie auch immer moralische Akteure sein können, ist, dass Autonomie irgendwie im Zentrum von moralischer Akteurschaft steht.
Das heißt, diese Top-Down-Ansätze, wenn einfach nur einprogrammiert ist, tu dies, wenn das, realisieren diese Moralfähigkeit nicht, weil eben komplett determiniert ist, wie sich der Roboter verhält.
Okay, aber jetzt trifft sie ja im Grunde eine Entscheidung. Sie hat ja gesagt, ist gut, wer Gutes tut oder ist gut, wer weiß, dass er Gutes tut sozusagen.
Ja.
Und hier sagt sie jetzt ja, nee, es ist gut, wer weiß, dass er Gutes tut.
Ja, genau, also diesem, quasi diesem, sie nennt das moralischem Schwellenwert, diesen Ansatz, den ich da vorgestellt habe, dem steht sie durchaus kritisch gegenüber.
Okay.
Ja, das merkt man auch. Genau.
Ja, und Autonomie und so weiter referenzieren dann immer weiter eigentlich auf sowas wie Intentionalität.
Also, wenn Roboter wirklich moralfähig sein sollten, müssten sie sowieso etwas wie Gründe für ihr Handeln anführen können.
Ja.
Man muss irgendwie sagen können, warum man getan hat, was man getan hat. Man kann nicht einfach nur sagen, ich hab das getan, weil das wurde mir halt so gesagt oder beigebracht.
Mhm.
Ja, dann geht es darum, inwiefern Roboter moralische Handlungsobjekte sein können und da gibt es so etwas wie eine Standardposition.
Also Handlungsobjekt heißt, in dem Sinne moralisches Handlungsobjekt, müssen wir uns eigentlich, also unabhängig davon, ob Roboter selbst moralisch als Subjekt handeln können, müssen wir uns denen eigentlich gegenüber vernünftig verhalten auf eine Art.
Ja.
Und die philosophische Standardposition, so wie sie das nennt, ist, dass eigentlich nur Dinge einen entsprechenden Status zugebilligt bekommen, die selbst auch Subjekte sind.
Okay.
Ja.
Das finde ich krass eigentlich.
Ja, wer kein Moralsubjekt ist, wird auch kein Moralobjekt.
Also, wenn man das weiterdenkt, bedeutet das, man kann mit Robotern verfahren, wie man will, das ist moralisch nicht problematisch.
Ja, zumindest nicht, zumindest wenn man bei der Frage nach dem Moralsubjekt zu der Antwort kommt, nein, ist es nicht.
Genau, dann kann man eigentlich mit denen machen, was man möchte.
Das ist eine krasse, gerade weil du sagst, dass das die Standardposition ist.
Ich würde jetzt dazu kommen, meine erste Intuition wäre, dass die total umstritten sein muss.
Ja.
Weil man das ja gerade genau auf die Tiere wieder wunderbar übertragen kann.
Das hat mich da auch ein bisschen gewundert, weil ich das Gefühl hatte, das kam nicht so richtig vor quasi als Vergleich.
Weil ich auch dachte, naja, dann könnten wir ja auch, also wenn das die Idee ist, dann könnte man ja auch mit Tieren tun und lassen, was man möchte.
Und das scheint mir keine Standardposition zu sein.
Genau.
Ich meine, das steht ja sogar in unserem Grundgesetz drin quasi, dass das eben nicht in Ordnung ist.
Ja.
Und das ist ja auch irgendwie philosophisch abgeleitet.
Das hat mich auch ein bisschen irritiert.
Aber gut, sie sagt das erstmal so, dass das eben so die allgemeine Position ist.
Ich weiß nicht, ob das jetzt vielleicht auch langsam ins Rutschen kommt.
Ich meine, wenn man an irgendwelche Roboter in der industriellen Fertigung oder so denkt, dann weiß ich nicht.
Da entspinnt sich bei mir auch nicht sofort das Bild der Notwendigkeit des quasi verantwortungsvollen Umgangs.
Da würde ich auch sagen, naja, wenn du kaputt bist, kommst du auf den Müll.
Und wenn du mich nervst, dann schreite ich auch an, ist mir egal.
So.
Interessant.
Das wird dann wieder interessant, wenn man jetzt die Sexroboter oder so ins Spiel bringt zum Beispiel.
Ja, total.
Wo dann auf einmal eine andere, die Roboter sind ja immer noch die gleichen.
Aber es kommt irgendwie eine ganz andere moralische Wertung ins Spiel.
Sie macht da noch ein ganz spannendes Beispiel auf, was mich irgendwie so ein bisschen ins Denken gebracht hat.
Da komme ich gleich noch zu.
Na ja, was ihr erstmal feststellt, ist, dass Roboter häufig in Aussehen als auch im Verhalten vermenschlicht werden.
Also man nennt das dann Anthropomorphisierung.
Sie entwickeln sich dann irgendwie zu sowas wie Gefährten von Menschen.
Und diese Anthropomorphisierung wird von der Psychologie meistens traditionell negativ gesehen.
Also als eine Voreingenommenheit oder einen sogenannten Kategorienfehler.
Ja.
Das, was ich am Anfang als Betrug quasi betitelt habe, bei dieser Parorobbe zum Beispiel.
Und da macht die Psychologie den Vorwurf, dass es eben die Illusion von Beziehungen irgendwie entsteht,
wenn man Roboter quasi so moralzentriert oder auch menschlich behandelt.
Ja.
Lo macht dann die Frage danach auf, wann ist eine Beziehung eigentlich eine Beziehung?
Also wie unterscheiden sich Roboter von Tieren oder auch Beziehungen oder Bezugsnamen zu Dingen im Allgemeinen?
Es gibt ja durchaus Menschen, die objektsexuell sind.
Das sind wenige Menschen, aber die gibt es halt, die führen Beziehungen zu Dingen.
Und wenn ich so einfach daran denke, jede Person, die ich kenne, hat definitiv auch Dinge, zu denen sie irgendwie eine emotionale Bindung zum Beispiel hat.
Und das wird im Allgemeinen auch nicht besonders als völlig abstruse oder verwerflich angesehen.
Was man da dann bei Robotern sehen kann, ist, dass sie so klassische Kategorisierungen von Subjekt und Objekt oder Belebt und Unbelebt so ein bisschen aufweichen.
Also die kommen so zunehmend, gerade wenn sie eben so vermenschlicht auftreten und Menschen auch sehr emotional oder menschlich eben auf sie reagieren, kommt es in so eine Grenzregion.
Was sie am Ende sagt, ist, dass eine Definition von Beziehung eigentlich ist, dass eine Beziehung bis hin zu einer Freundschaft kann man desto eher zu einem Gegenüber eingehen,
je mehr es eine befriedigende Antwort auf die eigenen Bedürfnisse zu geben, imstande ist.
Da spielt glaube ich ein bisschen rein, was wir gerade eben schon hatten, dass uns eigentlich auch menschliche Gegenüber schon ziemlich intransparent sind und wir gar nicht so genau hinter deren Kopf gucken können.
Und uns häufig auch egal ist, ob, keine Ahnung, stellen wir uns vor, wir sind an der Supermarktkasse und wünschen uns einen freundlichen Umgang mit der Person, die da kassiert.
Dann geht es uns um den freundlichen Umgang und nicht darum, ob uns die Person wirklich wohlgesonnen ist.
Dann gibt es in der Philosophie offenbar die Position und die finde ich eigentlich ganz überzeugend ist, dass für Roboter als Objekte der Moral die These spricht,
dass es Menschen besser gelingt menschlich zu bleiben, wenn sie Roboter und andere human behandeln.
Also das heißt, wenn wir mit Dingen und Robotern eben vernünftig menschlich und nicht völlig gewaltvoll oder so umgehen,
dass wir dann eben auch menschselbst bleiben und eben ja nicht sonst wo unsere Gewalt ausleben und die dann vielleicht wieder zu Menschen hin zurück transformieren und sie auch dafür legitim auf einmal erachten.
Da gibt es doch diese Diskussion, die ich letztes Mal so am Rande mitgekriegt habe, zum Thema wie Kinder mit so Alexa und Siri und so weiter umgehen,
dass sie halt gewöhnt sind so nach dem Motto Alexa spiel XY und dann eventuell auch anfangen im Alltag mit Leuten so umzugehen und einfach nur aufzufordern, ohne da wirklich eine soziale Interaktion einzutreten.
Da geht es also ein bisschen um dieses Vorbildhafte sozusagen. Wenn ich mich da mal dran gewöhnt habe, dann bin ich auch Menschenpampiger.
Ja, ich hatte neulich Besuch von einem guten Freund und habe mir einen Tee aufgegossen und wenn ich mir einen Tee aufgegieße, dann schrei ich im Normalfall Siri an.
Also nein, ich sage zu Siri, Siri stelle einen Timer auf drei Minuten oder so.
Und naja, ich hatte mich gerade mit dem Freund unterhalten, habe mir parallel meinen Tee aufgegossen und dann eben zu meinem Handy gesagt, stelle einen Timer auf drei Minuten.
Aber halt dann zu einem sehr neutralen Ton und ich wurde dann von meinem Freund so ein bisschen irritiert angeguckt.
Aber er hat dann sein Handy rausgeholt und wollte eben Timer stellen.
Aber ja, das war definitiv irritierend und von daher kann ich ein Beispiel gut nachvollziehen, weil eben meine Ansprache gegenüber dieser Technik Siri offenbar eine ganz andere war, die ich gegenüber einem guten Freund so nie an den Tag legen würde.
So würde ich nie einen Freund darum bitten, doch mal einen Kurzzeitbäcker zu stellen.
Naja, und die Frage danach, ob das jetzt aber am Ende irgendwie richtig oder falsch ist, da macht Lo die Frage auf, naja, aber wie verhalten wir uns zum Beispiel gegenüber Kunstwerken oder auch Romanfiguren?
Ist es wirklich egal, wie wir ihnen begegnen?
Da würden wir vermutlich nicht zu dem Schluss kommen zu sagen, naja, wie ich auf ein Kunstwerk reagiere, ob ich da in einem Museum völlig, keine Ahnung, gefühlskalt und so ohne jeden Respekt vor der Kunst dem gegenüber trete.
Naja, da würde niemand sagen, dass das so ganz egal ist, wie wir uns da eigentlich verhalten.
Ja, stimmt.
Und die Frage ist, warum sollte es dann bei Robotern eben egal sein, ne?
Spannend, spannend.
Das fand ich auch ganz spannend, naja.
Dann gibt es zu dem Ganzen sowas, was sie inklusiver Ansätze nimmt, also die probieren so ein bisschen weiterzudenken, das macht sie dann für den zweiten Teil des Buches auch nochmal.
Und diese inklusiven Ansätze bemühen sich eben um so eine Loslösung von dem, was sie Anthropozentrismus nennt, also von diesem Ausgang von, wir gucken nur vom Menschen aus, wie das eigentlich alles beurteilt werden kann.
Den Menschen soll dabei an sich nichts abgesprochen werden, also es geht dann nicht darum, den Menschen quasi kleiner zu machen, als er es traditioneller Philosophie gemacht wird.
Aber ja, Janina Loh macht auch ganz viel zum Thema Posthumanismus und Transhumanismus.
Und da fasst sich so zusammen, dass der kritische Posthumanismus die tradierten und zumeist humanistischen Dichotomien, wie etwa Frau-Mann, Natur-Kultur oder Subjekt-Objekt hinterfragt.
Also darum geht es so ein bisschen.
Eine Autorin, auf die sie viel referenziert ist, Donna Haraway, die, glaube ich, Biologin und Verhaltensforscherin und Philosophin ist und die ganz lange schon zu Cyborgs und Tieren schreibt.
Und genau, sie hat seit neuerem wohl mehr über Tiere und Tierethik und die auch einen eigenen Hund hat, den aber nicht als Haustier bezeichnet, sondern als Forschungsbegleiterin.
Aber früher hat sie ja halt über Cyborgs offenbar geschrieben.
Und die weiß daraufhin, dass Cyborgs natürlich irgendwie sind so kybernetische Organismen und ja, hybride aus Maschine und Organismus.
Und die gibt es sowohl in der gesellschaftlichen Wirklichkeit als auch in der Fiktion.
Also ja, und die These, die Haraway offenbar aufmacht und die Loh, glaube ich, nicht unsympathisch findet, ist, dass wir alle schon Cyborgs sind.
Also wir benutzen irgendwie schon unsere Handys. Im Prinzip benutzen wir auch alle schon künstliche Dinge wie Pullover und Hosen und fahren eigentlich auch mit Straßenbahnen durch die Gegend und so.
Also wenn man das ein bisschen weiter denkt, dann ist die Maschine schon Teil von uns.
Nur um mal aufzuzeigen, man kann über diese Differenz von Maschine und Mensch anders denken, als es klassischerweise unterm Alltag getan wird.
Ja, dann geht Loh noch so ein paar Beispiele durch. Und das will ich auch ein bisschen zumindest machen.
Sie zitiert eine Person, die heißt Suchmann. Ich weiß nicht genau, wie man sie ausspricht.
Die sagt zum Beispiel, na ja, man kann auch über Materie zum Beispiel anders denken und als man es gemeinhin tut, also die Differenz zwischen belebt und unbelebt ein bisschen auflösen.
Das kam mir erst ein bisschen merkwürdig vor, weil Suchmann offenbar sagt, dass Materie diskursiv ist.
Aber sie bringt als Beispiel eine kalifornische Rosine, die, wenn ich sie esse, mehr ist als eine kalifornische Rosine.
Sondern wenn ich so eine Rosine esse, dann beschäftige ich mich oder habe Einfluss oder nehme in mich auch so Konzepte wie Kapitalismus einerseits,
weil die Rosine wurde ja irgendwo erwirtschaftet und ich habe sie gekauft und so weiter, als auch Kolonialismus, also gerade in den USA.
Und vielleicht auch Rassismus, denn wer hat diese Rosine zum Beispiel gepflückt?
Also die Idee ist einfach das, was da aufgezeigt werden soll, ist, man kann anders über das Verhältnis von Materie und Mensch und unbelebt und belebt denken.
Wobei da natürlich mal schnell bei dieser Ebene sind einfach kulturelle Überformungen sozusagen von Materie zu fassen.
Das ist jetzt ja gar nicht so neu, darüber nachzudenken.
Absolut, naja.
Naja, und daher gibt es dann jetzt eben noch ein paar Mehrbeispiele, die gehe ich jetzt nicht noch alle durch.
Genau, wer das Buch liest, sie zitiert häufig eine Person namens Körkelberth, denke ich, also C-O-E-C-K-E-L-B-E-R-G-H geschrieben.
Der Person steht sie immer ziemlich positiv gegenüber und diesen Ansätzen.
Wenn ich es richtig sehe, ist das ihr Vorgesetzter an der Uni Wien, also quasi ihr Chef.
Das sollte man im Hinterkopf haben, wenn man das alles probiert, durchzubewerten.
Also ich vermute, dass sie es auch wirklich gut findet, aber nur um zu wissen, dass sie einfach aus der gleichen Kerke herbekommt.
Ihre Zwischenbilanz zu dem Ganzen ist dann jetzt erstmal, dass Roboter als moralische Akteure, hatte ich ja schon gesagt,
sowas wie eigentlich Autonomie und sowas wie Lernfähigkeit brauchen, eigentlich auch Intentionalität,
irgendeine Form von Urteilskraft und Verantwortung und das eben auffällig ist, dass immer von Menschen ausgedacht wird.
Als Handlungsobjekte gelten sie im Normalfall eben nur dann, wenn sie auch Handlungsubjekte mal irgendwann sein sollten.
Aber man könnte auch sagen, da wo eine emotionale Bindung entsteht zu etwas, zu jemandem, entsteht auch Moralität.
Also die These macht sie auch auf.
Also es gibt so relationale Moraltheorien, die eben sagen, da wo etwas zwischen zwei Wesen entsteht, sowas wie eine Bindung oder so,
da ist eben auch Moral am Werk und das finde ich, kann man zumindest sehr gut an Tieren erkennen, da ist es ja auch nicht egal.
Also wenn wir uns an die Binden, empfinden wir sowas wie eine moralische Verantwortung ziemlich schnell
und warum sollte das bei Robotern eigentlich anders sein, weil ob die Kapazitäten, die gedanklich in sich so groß unterscheiden, sei mal dahingestellt.
Das ist was, was du gerade auch bei diesem Beispiel mit dem Kunstwerk meinst, was mir gerade so ein bisschen reinkommt,
dass dieser moralische Bezug gegenüber Objekten meist in erster Linie daran hängt, wie andere Menschen emotional an diese Objekte gebunden sind.
Ja, das stimmt.
Dass der Roboter, der in der Fabrik irgendwelche Autos zusammenbaut, da ist kein Mensch emotional daran gebunden.
Deswegen fällt es uns da leicht zu sagen, ja komm schmeiß den halt auf den Müll.
Wenn das jetzt aber der Objekt Liebe oder Objekt Sexuell sozusagen, wenn das irgendwie das, was auch immer das für ein Gegenstand ist,
zu dem jemand irgendwie eine sehr enge emotionale Bindung aufgebaut hat, dann ist mit diesem Gegenstand umzugehen,
ja einen mit diesen Menschen umgehen und dadurch dann sehr stark moralisch geprägt.
Das finde ich gerade spannend, das ist nicht nur die Beziehung, wie gehe ich mit einem Gegenstand um,
sondern auch wie sind andere Menschen moralisch an diesen Gegenstand oder emotional in diesen Gegenstand gebunden, die dann da mit ins Spiel laufen.
Ich finde dabei durchaus auch zu beachten, gerade wenn es irgendwie um Kunstwerke und das Museumsbeispiel, was sich da gebracht hatte,
oder auch um Romanenfiguren geht, ja also die Kunstwerke und Romanenfiguren referenzieren immer auch sehr direkt auf die Person,
die diese Dinge erschaffen hat, also das steht ja irgendwie im Hintergrund, wenn ich ein Kunstwerk gegenüber trete,
ist das ja auch immer der Person, die es erschaffen hat irgendwie, ja.
Von da aus geht es dann eben über zu Robotern als Verantwortungs-Subjekten und ich muss sagen,
mit meiner nicht großen philosophischen Vorbildung ist es mir da manchmal ein bisschen schwer gefallen irgendwie nachzuvollziehen,
warum diese Trennung so stark gemacht wurde zwischen Moral-Subjekt und Objekt und Verantwortungs-Subjekt und Objekt,
aber gut, nehmen wir es erstmal so hin, aber genau, rein von der Zugänglichkeit her für Leute, die vielleicht philosophisch nicht so bewandert sind,
hätte ich mich da darüber gefreut, wenn das vielleicht einfach in einem Guss gewesen wäre und nicht so stark getrennt,
aber ich probiere mal das irgendwie nachzuziehen.
Na ja, da startet sie damit zu sagen, dass Verantwortung und die Möglichkeit zur Verantwortung an Akteurschaft gebunden ist
und dass man Verantwortung da von Haftbarkeit unterscheiden sollte, also das ist nicht das Gleiche,
also ein bisschen die Unterscheidung zwischen verantwortlichem Handeln einerseits, die irgendwie moralisch total aufgeladen ist
und der rein begrenzten Verursachung, also auch, haben wir jetzt Beispiel, wenn ein Kind irgendwas kaputtmacht,
dann ist es vielleicht haftbar zu machen dafür oder halt die Eltern, aber man würde nicht unbedingt sagen,
dass es in dem Sinne moralisch verantwortlich ist, auch wenn es im Deutschen zumindest von der Sprache her manchmal so ein bisschen verwischt,
also Verantwortung und Verursachung quasi.
Interessant, da hätte ich jetzt, mein intuitives Begriffsverständnis wäre quasi genau andersrum gewesen,
dass gerade derjenige, der verantwortlich ist, dafür haftbar ist, weil die Eltern zum Beispiel nicht genug auf das Kind aufgepasst haben.
Genau, das wäre vermutlich das, aber sie haben es nicht verursacht, sondern das Kind hat verursacht.
Genau, und deswegen Kind als Verursacher, Eltern als Verantwortliche, vielleicht.
Ja, Lo macht ihre eigene Position auch deutlich, also sie sagt, dass manche Roboter die notwendigen Kompetenzen dafür haben,
Verantwortung zu simulieren zumindest, was das dann von, das ist das, worüber wir gerade eben gesprochen haben,
inwiefern man nachgucken kann, ob sie es nun wirklich verantwortlich sich irgendwann mal fühlen können, wissen wir nicht so genau.
Ja, und dass Menschen absehbar erst mal besser für Verantwortungsübernahme qualifiziert sind, überrascht jetzt auch.
Verantwortung ist dabei noch ein relativ modernes Konzept im Prinzip, was irgendwie so mit komplexen Gesellschaften entstanden ist.
Also, weil man irgendwie nicht mehr direkt zuordnen konnte, wer welches Vergehen eigentlich ausgemacht hat,
muss man darauf umstellen, quasi gesellschaftlich zu sagen, verhaltet euch mal verantwortlich,
weil ansonsten haben wir hier irgendwie Dynamiken, die wir nicht mehr so richtig überblicken können.
Aber auch da ist es doch wieder eigentlich extrem ähnlich wie die Haftbarkeit, weil bei der Haftbarkeit geht es ja auch wieder genau darum, wer ist denn jetzt haftbar dafür.
Ja, total.
Also für mich fällt dieses Verantwortung und Haftbarkeit extrem zusammen.
Das hat sich mir noch nicht erschlossen, wie das unterschiedlich ist.
Ja, ich glaube, das ist tatsächlich eine philosophische Unterscheidung, weil mit Verantwortung definitiv mehr einhergeht.
Das wäre glaube ich ein bisschen die These, da hängt halt irgendwie moralisch gut und böse mit dran und an Haftbarkeit eben nicht so sehr.
Aber Lo weist auch darauf hin, na ja, bei Verantwortung geht es eigentlich schon genuin darum, dass man ein konkretes Subjekt der Verantwortungsübernahme immer wieder ausmacht.
Okay.
Ich hatte als Gedanken dazu im Kopf, die Klagen in den USA gegen Volkswagen im Zuge dieses ganzen Abgasskandals und so weiter,
weil da ja dann tatsächlich am Ende auch nicht, also die Firma Volkswagen kann nicht ins Gefängnis gehen.
Die kann irgendwie Geld strafen oder so verantworten, aber das ist vielleicht auch gar nicht das, was manche Leute sehen wollen.
Und das heißt, dass am Ende tatsächlich konkrete Manager und Managerinnen ins Gefängnis mussten.
Also man probiert dann irgendwie auszumachen, die und die Person ist es jetzt gewesen.
Auch wenn glaube ich relativ klar ist, dass das, was Lo für ein Verantwortungsnetzwerk ist, eigentlich das wäre, was viel Treffener beschreibt, wo da die Verantwortlichkeiten liegen.
Das ist ja nicht der eine Manager, der jetzt irgendwie eigentlich verantwortlich ist, aber er wird zumindest haftbar gemacht.
Ja klar.
Ja, genau. Die Idee von Verantwortungsnetzwerken ist dann auch, dass alle beteiligten Parteien in einer Situation Verantwortung tragen.
Das Problem ist, dass sowas entwickelt wurde, um irgendwie solche Situationen, wie ich es hier gerade eben beschrieben habe, zu bewältigen.
Also rein philosophisch jetzt erstmal nicht justiziabel quasi, aber Lo weist darauf hin, dass wenn man probiert, eben Verantwortungsnetzwerke zu begreifen,
gerade im Kontext von autonomen Systemen, also wie der Beispiel Autounfall, wer ist jetzt eigentlich verantwortlich?
Das Auto kann es nicht so richtig sein, der, die Fahrerin vielleicht auch nicht.
Und welchen Anteil hat die Person, der die Unfall zugestoßen ist, eigentlich selbst?
Naja, und was haben die Ingenieure damit zu tun und die Programmierer, die das Programm geschrieben haben und so weiter?
Also so Verantwortungsnetzwerke, wenn man wirklich erfassen will, wer eigentlich schuld an irgendetwas ist, werden riesig groß.
Damit torpediert man quasi die Idee hinter Verantwortung wieder, nämlich dass man eine Person hat, die man haftbar machen kann.
Ja, da fällt es nämlich doch wieder zusammen.
Ja, total, ja. Und da unterscheidet man dann eben vor Gericht quasi zwischen Verantwortung und Haftbarkeit, würde ich sagen.
Gehen wir über zu Verantwortungsobjekten so ein bisschen.
Ja, da ist die Idee dahinter, dass selbst die komplexesten Maschinen nicht mehr als bloße Instrumente menschlichen handeln sind.
Man kennt das von der National Rifle Association aus den USA, also nicht Waffen töten Menschen, sondern Menschen töten Menschen.
Und da sagt Lou, naja, so einfach ist das nicht.
Also eine Person mit einer Pistole ist ein Schütze, eine Schützin und eine Person ohne eine Pistole ist eine Person.
Also sie denkt das schon eher zusammen. Und das Gleiche geht dann eben auch für Roboter.
Sie nennt da einen Philosophen, der sich eigentlich mit Computersystemen beschäftigt und das schon relativ lange.
Und der sieht Probleme im Verantwortungsbereich, in der Philosophie dann, wenn Computersysteme schneller und akkurater als Menschen in der Lage dazu sind, Entscheidungen zu treffen.
Und man kann sich das durchaus vorstellen, gerade in, so nennt sie das, Delikaten Kontexten wie militärischen Frühwarnsystem, wo es dann darum geht,
okay, ein Frühwarnsystem sieht, dass eine Rakete oder so auf einen zufliegt und muss dann darauf reagieren.
Und das kann dann kein Mensch mehr machen, sondern das muss dann eben, ja, muss dann ein Computersystem machen.
Da sieht dieser Hans Lenk eben ein Beleg dafür, dass die Abhängigkeit von Technologien dafür sorgt, dass Computersysteme auch heute schon Verantwortung und Verantwortung übernehmen.
Die Frage ist aber, wenn man so ein Computersystem nicht haftbar machen kann, wer steht dann am Ende Rede und Antwort?
Ja, genau.
Also die Frage nach Haftbarkeit, Verantwortung kommt da dann eben wieder auf.
Zusammenfassend kann man sagen, dass den verantwortlichen Subjekten generell eine moralische Verantwortung für die Produkte ihres Schaffens zugeschrieben wird.
Also auch heute ist es noch so, dass Computersysteme, Roboter und so weiter quasi als verlängerte Arm ihrer ErschafferInnen gelten.
Ja.
Was ich gerade eben schon gesagt hatte, ist, dass es so Ideen gibt von relationalen Konzepten von Moral und die gibt es eben auch im Bereich der Verantwortung.
Das heißt, inklusiven Ansätzen aufgemacht, dass Verantwortung sich ausschließlich in der Interaktion zwischen Wesen abspielt.
Die Idee dahinter ist, dass das Narrativ vom autarken Handlungssubjekt eine gesellschaftliche, rechtliche und politische Illusion ist.
Das ist wieder diese Haraway, die sie da zitiert.
Also dass wir alle so wunderbar autark und autonom sind, ist vielleicht notwendig, dass wir uns das so denken.
Aber faktisch ist das nicht so, weil wir natürlich irgendwie gesellschaftlich eingebunden sind und leben und manchmal gar nicht anders können, als die Situation es irgendwie zulässt.
Ich glaube, wir als Soziologen können das ganz gut nachvollziehen.
Ja.
Ja, dabei muss man irgendwie immer wieder daran erinnert werden, finde ich zumindest, dass die Idee der Individualität dahinter und der entsprechenden Verantwortung und Verantwortungsübernahme noch relativ neu ist.
Also Losak, die ist erst im Mittelalter entstanden.
Das Gleiche gilt übrigens für Begriffe wie Identität.
Also dass jeder Mensch eine eigene Identität hat, ist auch noch eine relativ moderne Erfindung.
Und dass auch solche Individualitätsideen ziemlich kulturraum explizit sind.
Sie meinen zum Beispiel, dass es im asiatischen Kulturraum gar nicht so verbreitet ist, dass man irgendwie die Idee hat, dass man irgendwie ein Individuum ist.
Ein bisschen die Unterscheidung zwischen individualistischen und kollektivistischen Kulturen.
Ich bin da nie in die Materie tief eingestiegen.
Ich weiß nicht, wie gut sich das tatsächlich aufrechterhalten lässt.
Also ich gebe das einfach nur erst mal so wieder.
Aus der Idee von dieser interaktionsbasierten Verantwortung gibt es dann das Konzept der Extended Agency aus.
Dass Interaktionen eben aus einer Menge von menschlichen und nichtmenschlichen Beteiligten bestehen.
Und eine Handlung sich eben nicht mehr nur auf die menschlichen Komponenten bezieht.
Fand ich auch ganz interessant für alle NichtsoziologInnen da draußen.
Die klassische Definition von Handlung ist nämlich nach Max Weber in der Soziologie eigentlich eben, dass sie mit Intention passiert und genuin menschlich ist.
Also Tiere können sich zum Beispiel verhalten, aber nur Menschen können handeln.
Da gibt es eben offenbar Ansätze, die probieren das ein bisschen anders zu sehen.
Wenn da diese Begriffe so das Gegenstände handeln können, das erinnert mich ein bisschen an Latours aktuellen Netzwerktheorie.
Ja ja witzig.
Das Latour taucht auf, da ich Latour nicht kenne, gebe ich dir hier nicht so richtig wieder.
Aber genau darauf wird offenbar viel auch referenziert in der Literatur.
Ja, so wie ich das jetzt gerade eben mal probiert habe, irgendwie knapp zusammenzufassen, gibt es einige Schwierigkeiten, die da auftauchen.
Und ich finde, die liegen auch relativ deutlich klar.
Autonomer werdende Systeme machen es zunehmend schwieriger, Verantwortlichkeiten zuzurechnen.
Also wenn wir mehr Systeme haben, die irgendwie in einem Zwischenraum zwischen ich mache so ein bisschen, was mir einprogrammiert wurde, ein bisschen, was ich irgendwie autonom gelernt habe.
Und aber es sind auch noch Menschen involviert, macht es irgendwie ganz schwierig zu sagen, wer ist denn jetzt eigentlich schuldig, haftbar, verantwortlich für irgendwelche Handlungen.
Und auch in diesen Relationskonstrukten von Verantwortung, wo man sagt, ok, wenn Dinge, Tiere, Roboter mit Menschen agieren oder auch Dinge mit Dingen oder Menschen mit Menschen, nur da entsteht Verantwortung,
ist, dass man da durch diese neuen technischen Systeme auch so Verantwortungslücken in den normativen Kriterien hat, weil man irgendwie für diese technischen Systeme gar nicht so richtig weiß, wie man das alles bewerten soll.
Ja, stimmt.
Ja, ja. Und worauf Lo noch hinweist, das fand ich noch ganz spannend, ist, dass sie sagt, wir erleben auch sowas wie neue Raum-Zeit-Dimensionen im Kontext.
Also sie sagt, die Rückbindung, globale Ereignisse, sie nennt Fluchtbewegungen oder auch den Klimawandel als Beispiele an einzelne Gruppen von Akteurinnen wird schwieriger.
Also man kann nicht mehr so richtig sagen, wer ist denn jetzt beim Klimawandel eigentlich, wer ist daran jetzt schuld, wem ziehen wir dafür zu Rechenschaften.
Oder wenn wir uns komplexe Migrations- und Fluchtbewegungen angucken, wie kann man sie einerseits vielleicht besser koordinieren, wie kann man das steuern, wie kann man dafür sorgen,
dass die Bedingungen deren Wegen geflohen wird, dass die geändert werden.
Auch das ist total schwierig da irgendwie zuzurechnen, wer da zur Verantwortung gezogen werden kann.
Wobei ich da sagen würde, das Genuin hängt jetzt nicht mit Robotern oder auch mit artificiellen Systemen zusammen.
Nur einfach mit unglaublich komplexen Wirkketten.
Ja, genau. Die Frage, die ich mir dabei jetzt am Ende gestellt habe, ist, wenn man jetzt so Verantwortungskonzepte irgendwie erweitert,
probiert in Netzwerke zu integrieren oder auch Roboter zur Verantwortung ziehen möchte.
Ich frage mich ein bisschen, was dadurch gewonnen werden kann, weil ich das Gefühl habe, dass Verantwortungsaufforderungen oder so,
oder die sich eben gerade dadurch da ausdrücken, dass sie nicht immer mit einer rechtlichen Keule um die Ecke kommen,
sondern sagen, verhaltet euch mal verantwortlich, weil es kommen noch Leute nach euch, wir müssen an unsere Kinder denken,
beim Klimawandel oder was auch immer. Und ich frage mich so ein bisschen, gewinnt man eigentlich praktisch irgendwas dadurch,
wenn man das alles so weit zieht quasi?
Ja, das habe ich auch gerade mal nachgedacht, weil ich mir diesen Begriff der Verantwortung überlegt habe,
gerade wenn sie ihn von der Haftbarkeit trennen so stark. Was heißt das überhaupt?
Ich meine, Verantwortung wirkt ja im Grunde als Handlungsprägen, kann das ja nur über mögliche Konsequenzen wirken.
Es können auf der einen Seite sein die Haftbarkeit, die klassische rechtliche, das kann aber auch so etwas sein wie sozialer Status,
wie irgendwie so ein, das bin nicht ich, so ein Selbstachtungsverlust oder Scham oder sowas in der Art.
Aber das kann ja immer nur über diese Wirkmechanismen sozusagen nach außen treten.
Dann ist natürlich die Frage, wie weit die bei irgendwelchen robotischen Systemen sozusagen wirken können.
Kann ein Roboter Scham empfinden?
Ja, das finde ich sind auch so die spannenden Fragestellungen. Das ist mir nicht so richtig klar.
Ich finde, das, was ich gelesen habe, weist eigentlich eher in die Richtung von, es ist zumindest erstmal nicht abzusehen,
dass da sowas wie genuines Bewusstsein wirklich ist. Und dann ist für mich ein bisschen die Frage, warum das alles?
Weil Roboter haben auch absehbar kein eigenes Konto, von dem du irgendwelche Dinge bezahlen kannst.
Das gleiche gilt auch für autonome Fahrsysteme.
Wem ist damit geholfen, wenn du ein Auto für einen Unfall zur Rechenschaft ziehen kannst?
Genau, was heißt das überhaupt ein Auto zur Rechenschaft zu ziehen?
Dann darf das dann drei Monate nicht fahren, weil es ihm ja Spaß macht zu fahren.
Ja, das finde ich auch, das ist irgendwie total schwierig.
Kriegt das irgendwie ein fiktives Gehalt für jeden Kilometer, den es fährt, und muss dann davon wieder was...
Also irgendwie so richtig erschließt sich mir das nicht.
Diese Wirkmechanismen, die fehlen da irgendwie komplett, so Sanktionsmechanismen im Grunde.
Ja, das stimmt.
Auf welcher Ebene auch immer.
Vielleicht muss man das Ganze auch eher als wirklich, ja Gedankenspiel finde ich, ist immer so ein bisschen respektierlich,
aber vielleicht muss man es einfach als wirklich philosophische Überlegung erstmal auch für sich stehen lassen und das als das begreifen.
Also zu sagen, okay, die philosophische Überlegung darüber, was bedeutet Verantwortung, was bedeutet Moral,
in quasi außerhumanen Kontexten ist ein Erkenntnisgewinn für sich.
Ich glaube, wenn man so darauf guckt, wird man dem vielleicht eher gerecht.
Wobei es mir gerade so spontan einfällt, wenn man so Sanktionen irgendwie auch als Lernreiz sozusagen versteht.
Also man sagt, man verheckt eine Sanktion, damit die Person das nicht wieder tut.
Oder im Vorhinein als abschreckend. Das kann man ja wiederum in Computersystemen sehr gut abbilden.
Gerade wenn du so ein selbstlernendes System hast und das bringt irgendwie jemand um, ihm dann ganz böse auf den Finger zu hauen und sagen,
ne, ne, ne, das machst du nie wieder. Das könnte man ja sogar technisch relativ gut abbilden.
Ja, vielleicht ist, ja, das stimmt. Das könnte ganz gut funktionieren.
Gerade wenn die These ist, dass eben Top-Darren-Programmierung offenbar, weil das alles so zwischen den abstrakten Regeln und dem konkreten Verhalten,
dass da so eine große Diskrepanz ist, dass eben Top-Darren nicht dauerhaft gut funktioniert.
Ja, also dann müsste man eben als Regel einprogrammieren, dass sich XY nicht gut anfühlt für das Auto.
Genau, was auch immer nicht gut anfühlen dann wieder heißt.
Genau, aber das kann man ja, das wäre die These dahinter. Und das wissen wir aber eigentlich schon für andere Leute nicht.
Also wir glauben halt nur zu wissen, dass die uns ähnlich, dass die so ähnlich fühlen wie wir. Spannend, spannend.
Naja, kritische Zwischenbilanz zu dem ganzen Verantwortungsthema ist, naja, Akteurschaft ist irgendwie notwendig, um überhaupt Verantwortung zurechnen zu können.
Hinter dem Ganzen steht irgendwie die Befürchtung, dass wir Roboter irgendwann nicht mehr kontrollieren können.
Also wir vertrauen unseren eigenen Geschöpfen quasi nicht mehr so richtig. Und man kann feststellen, dass Menschen zweierlei Verantwortung haben.
Also einmal sehr individuell als Designer innen von autonomen Systemen oder Robotern oder Programmiererinnen und so weiter.
Aber auch kollektiv als Unternehmen, die bestimmte Roboter oder nicht Roboter einsetzen.
Oder auch in Form von Ethikgremien, die irgendwas in Bezug auf diese ganze Roboter-Thematik entscheiden oder nicht entscheiden.
Und ja, was sie sagt, was bei dem ganzen klar wird, ist, Bedürfnis der modernen Gesellschaft, alles immer kontrollieren zu können, wird auch bei Robotern offenbar.
Warum sie das so hervorhebt, weiß ich nicht so ganz genau, weil ich das Gefühl habe, es gibt schon sehr, ich weiß nicht, vielleicht ist es einfach nur eine philosophische Feststellung.
Ich hatte erst mal das Gefühl, na ja, es gibt schon gute Gründe, die die die Resultate des eigenen Schaffens, die Ergebnisse kontrollieren können zu wollen.
Genau, dann schließt sie das Buch mit klassischen abschließenden Bemerkungen.
Das gibt es ja häufiger mal, dass dann eben so ein bisschen nochmal quasi in die Zukunft gedacht werden soll oder ein bisschen praktischer.
Hier in dem Fall ist jetzt so ein bisschen politische Implikation des Ganzen.
Sie wiederholt ihre Aussage, dass Produkte des menschlichen Handelns moralisch nie neutral sind.
Das gibt es eben im Bereich der Technik-Philosophie offenbar diese Position, na ja, es ist halt Technik, die kann man gut oder schlecht verwenden.
Und wenn sie eben schlecht verwendet wird, ist das zwar problematisch, aber die problematische Verwendung liegt eben bei Menschen und nicht in der Technik.
Und sie sagt, na ja, Vorsicht, Dinge sind eben moralisch immer aufgeladen, weil wir eben nach gewissen Normen agieren.
Das gibt es offenbar auch schon länger, das Konzept, also so Gedanken wie zum Beispiel, wenn wir uns einen Tisch vorstellen, der ein schmales Kopfende hat und dann eine lange Tafel hat,
dann drückt das im Prinzip schon eine hierarchische Struktur aus, die Vorsicht, dass jemand am Kopfende sitzt und Chef ist.
Ja, es ist ja auch bei den diplomatischen Verhandlungen, dass dann die kreisrunden Tische eingesetzt werden und solche Sachen, die genau das eben abbilden.
Na ja, Forderungen, die sie am Ende auf jeden Fall stellt, sind mehr Ethik- und Informationsunterricht in Schulen.
Sie sagt, Ingenieurswissenschaftliche Ausbildung sollte mit Ethikpflichtkursen behaftet sein, weil sie, also sie nimmt da das Beispiel der Medizin.
Man stelle sich nur mal vor, bei Dingen wie der Präimplantationsdiagnostik würden Ärzte und Ärztinnen irgendwie beraten, ohne je einen Ethikpflichtkurs im Bereich gemacht zu haben.
Und sie meint, na ja, wenn jetzt irgendwie Roboter entwickelt werden, muss man auch die ethischen Hintergründe kennen.
Und sie sagt, auch Unternehmen sollten Weiterbildungskurse im Bereich der Technik- und Roboterethik anbieten oder verpflichtend machen.
Und sie meint, wir brauchen auch zunehmend öffentliche Ethikgremien, die zum Thema Roboter irgendwie beschlagen und bewandert sind.
Und sie spricht sich als, ich glaube, kann man durchaus so sagen, linke Philosophin für eine Diskursöffnung aus, für die Vermittlung von Sachverstand, Reflektions- und Urteilskraften auf allen Ebenen, für alle Mitglieder einer Gesellschaft.
Was natürlich demokratischen Idealen absolut entspricht, dass man, wenn man sagt, okay, wir müssen darüber reden, wie wir mit Robotern umgehen, was sie dürfen, was sie nicht dürfen, was sie ersetzen sollen oder auch nicht, dann sollten da auch möglichst viele an diesem Diskurs beteiligt sein.
Sie hat auch bei Rahel Yagi promoviert, glaube ich, und das ist auf jeden Fall auch eine ziemlich interessante Person und auch auf jeden Fall aber linke Philosophin, die ursprünglich, glaube ich, so aus der Hausbesetzer-Szene kommt.
Genau. Was sie sagt, was manchmal ein bisschen schwierig ist in dieser ganzen Diskussion, gerade so im Bereich, wenn es um Superintelligenzen und so geht, ist, dass sie sagt, okay, es wird häufig so getan, als sei gesetzt, dass sowas wie eine Superintelligenz irgendwann entsteht, die deutlich cleverer, schneller, ethischer, besser wie auch immer ist als der Mensch oder halt völlig dystopisch.
Die alles kaputt macht und den Menschen überholt und uns braucht es dann gar nicht mehr. Und sie meint, das sind alles keine Naturgesetze, also es muss nicht alles passieren, was möglich ist.
Wir kennen das gerade in Deutschland, dass einige Technologien, die theoretisch möglich sind, wie zum Beispiel das Klonen oder so oder das, was immer wieder unter so Designer-Baby oder so filmiert, sowas wäre ja im Prinzip technisch möglich, zumindest grundlegend.
Das ist mit guten Gründen verboten und das kann man auch im Bereich der Roboter- und Computersystem-Ethik und so weiter auch fortführen.
Also es ist nicht ausgemacht, dass bestimmte Technologien irgendwann real werden und sie bittet darum, dass man sich das mal vor Augen führt.
Und das fand ich einen ganz schönen Hinweis, weil ich das tatsächlich bislang auch, glaube ich, wenig reflektiert habe.
Also mir schien das auch immer so wie ausgemacht, naja, irgendwann gibt es halt so eine, ja, wird es den Punkt geben, wo Computer allumfassend besser und klüger handeln können als wir.
Mal gucken, nur wann der kommt. Und sie meint, naja, das muss nicht so kommen. Wir können uns auch schon dagegen entscheiden, mit guten Gründen.
Stimmt.
Ja, das wäre es von mir zu diesem Buch. Hast du Fragen, Anmerkungen, Kritik?
Ja, ich glaube, wir haben ja schon währenddessen ziemlich viel diskutiert.
Deswegen, da ist, glaube ich, genug Anknüpfungspunkte auch für euch da draußen zum auch mal drüber nachdenken.
Mich hat es auf jeden Fall viel zu denken gebracht. Spannend.
Schön.
Bist du zwischendurch schon auf Bücher, Vorschläge des Weitermachens gestoßen oder soll ich das eben noch machen und du übernehmst da nach?
Ja, ich habe tatsächlich schon Ideen gehabt, auch kurz davor, als ich mir vor der Episode mal kurz zwei Minuten angeguckt hatte, worum es überhaupt geht.
Das ist einmal tatsächlich das Sachbuch, was ich gerade vor dem Zettelkastenprinzip zu Ende gelesen habe, nämlich Life in Code von Ellen Ullman.
Und das ist im Grunde so eine Essay-hafte Autobiografie von einer Frau, die halt irgendwie seit den 80er Jahren in der Softwareentwicklung gearbeitet hat.
Und im Grunde so dann auch als erfahrene Programmiererin so ein bisschen in die Start-up-Kultur reingerutscht ist und die beobachtet hat.
Und sie hat an einer Stelle, gibt es eine Stelle, wo sie eben drüber nachdenkt, wie es die Welt mit Robotern sozusagen aussieht, wenn man die irgendwie noch menschlicher behandelt.
Und da überlegt sie dann, was denn eigentlich für Roboter heißt, dass sie genießen.
Dass sie ein Essen oder irgendetwas genießen, dass es dann irgendwie darum geht, wie das Kühlmittel ihre Rohre durchzieht oder so.
Das fand ich ein sehr, sehr schönes Bild.
Und sie widmet sich eben auch so dieser Perspektive, Technik macht die Welt besser.
Das ist das, was du auch dahin, was du auch so angesprochen hast.
Das ist jetzt kein hardcoreiges Sachbuch mit irgendwelchen durchargumentierten Punkten oder so.
Aber vielen interessanten Gedanken über das, was wir so seit 20 Jahren über Technik denken.
Und wie Technik unsere Gesellschaft geprägt und verändert hat.
Mit vielen persönlichen Anekdoten und so weiter drin.
Aber es ist wirklich ein echt spannendes Buch.
Life in Code von Alan Ullman.
Und was natürlich...
Entschuldigung.
Nee, wirklich.
Das klingt sehr inspirierend, finde ich.
Ja, schön.
Und was natürlich, wenn es um Roboter geht, rührt das an meiner Science-Fiction-Seele.
Also Science-Fiction-Tipps, Weltenflüstern und so.
Mein zweiter Podcast.
Zwei Bücher, die mir da spontan eingefallen sind.
Das ist einmal ein Klassiker.
Der 200-Jährige.
Ursprünglich mal von Isaac Asimov als Kurzgeschichte geschrieben.
Sowieso Isaac Asimov, der große Roboter, Science-Fiction-Autor.
Und dann von Robert Silverberg zum Roman ausgearbeitet.
Und auch mit, äh, nicht doch mit Dustin Hoffman verfilmt.
Da geht es im Grunde um einen Roboter, der als Mensch anerkannt werden will.
Mit relativ klassischem Motiv so in den 70er Jahren.
In der Science-Fiction.
Und dann gibt es einen Roman zwischen zwei Sternen von Becky Chambers.
Sowieso eine ganz tolle Autorin, von der lohnen sich im Grunde alle Romane.
Aber inzwischen zwei Sternen geht es im Grunde um genau das gegenteilige Problem.
Um einen Roboter, der sich als Mensch ausgeben muss, das aber eigentlich nicht will.
Oh, das ist auch gut.
Also es ist ein schöner Twist zu diesem sehr klassischen Motiv.
Also diese beiden Romane, der 200-Jährige und zwischen zwei Sternen,
die kann man glaube ich auch mal direkt hintereinander lesen,
um diesen Unterschied sich ein bisschen deutlich zu machen.
Auch wie sich die Science-Fiction als Genre verändert hat, lustigerweise.
Das ist gut.
Das kommt tatsächlich in dem Buch, also Roboter-Ethik auch, zwischendurch eben mal vor.
Ich glaube bei dieser Haraway eben, wo so ein bisschen aufgemacht wird,
dass so die Grenzen zwischen Science-Fiction und Science irgendwie
und der Wirklichkeit absolut verwischen und gar nicht so klar sind,
wie man das vielleicht sich manchmal vorstellt.
Ich finde, man kann das jetzt auch gerade, merkt man das wieder,
dass so die Vorstellungen von Dystopien auch massiv von Filmen und Büchern geprägt sind.
Schönen Artikel zu, den ich auch gerne den Jones verlinken kann.
Ja, sehr gut.
Was mir noch gerade spontan einfällt, weil ich gerade Isaac Asimov erwähnt habe,
tauchen in dem Buch irgendwo seine drei Robotik-Gesetze auf,
zumindest mal in der Fußnote oder in einem Nebensatz?
Warte, ich gehe mal ganz kurz hinten ins Literaturverzeichnis, aber zitiert ist
The Complete Robot, The Definitive Collection of Robot Stories.
Okay, da wird es definitiv drin sein.
Ja, aber mir ist das jetzt zumindest nicht aufgefallen.
Weil es gibt irgendwie drei Roboter-Gesetze, ich weiß nicht, ob ich sie jetzt gerade auswendig hinkriege.
Das erste Gesetz ist auf jeden Fall, Roboter dürfen Menschen niemals schaden.
Ja.
Das zweite Gesetz ist, glaube ich, Roboter müssen Menschen immer helfen.
Und das dritte Gesetz ist, Roboter müssen immer die Befehle ihres Inhabers oder wie auch immer gehorchen.
Jeweils darf dadurch das vorherige Gesetz aber nicht verletzt werden.
Also du musst zwar Befehle befolgen, darfst dadurch aber niemandem schaden, sozusagen.
Das ist so ein Versuch, damit umzugehen, aber da verlinken wir euch auch die korrekten Sachen in den Show Notes.
Sehr gut.
Ich finde auch da merkt man, wie das so, also Asimov eigentlich als Science Fiction Autor,
wie das dann aber vermutlich jetzt auch in die Wirklichkeit von Roboter-Programmierung zurück spielt.
Natürlich.
Und das halt eben nicht irgendwie der Science Fiction Autor bleibt, sondern halt in die Realität einfach zurückgeht.
Naja, im Vorlauf zu diesem Buch oder während ich es gelesen habe, habe ich mit Jennifer,
mit der ich auch das Soziologische Kaffeekränzchen zusammen mache und die auch einen eigenen Podcast hat,
den Föllefanzcast, auch darüber gesprochen ein bisschen.
Und sie meinte, naja, wenn du hier diese Bücher da empfiehlst und vorschlägst,
schlag doch mal mit vor Maschinen wie ich von Ian McEwan.
Habe ich jetzt selbst nicht gelesen, scheint momentan aber irgendwie ganz schön viel Bass quasi drum zu sein.
Und ich weiß nicht, du weißt da glaube ich ein bisschen mehr.
Ich habe den Roman selber auch nicht gelesen.
Er wird so in Science Fiction Kreisen so ein bisschen kritisch gesehen, weil das jetzt wieder so ein Fall ist,
in Anführungszeichen literarischer Autor macht ein Thema auf, was die Science Fiction seit 30, 40 Jahren diskutiert.
Und bei der Science Fiction hört irgendwie nie jemand zu und jetzt springen auf einmal alle auf diesen Autoren.
Und er ist so der große Experte für das Thema.
Dadurch wird das da so ein bisschen kritisch gesehen. Der Roman an sich soll aber tatsächlich relativ gut sein.
Okay, das ist ja schon mal ganz schön.
Was ich vor einigen Jahren gelesen habe, ist Germany 2064 von Martin Walker,
der seinerseits schotte ist, wenn ich es jetzt vorher richtig nachgeschlagen habe,
finde ich einen ganz schönen Zukunfts Roman oder Thriller oder wie man es nennen möchte.
Das ist im Prinzip ein Kriminalfall. Ich habe aber vergessen, worum es um eigentlichen Krimi geht, also was der eigentliche Kriminalfall ist.
Aber es gibt eben einen Inspector, der 2064 an seiner Seite als engsten Mitarbeiter einen Roboter hat,
der, glaube ich, direkt zu Beginn quasi einen Upgrade erfährt, weil er in einem vorherigen Kriminalfall zu Bruch gegangen ist
und danach deutlich intelligenter ist, als er es vorher ist.
Und die beiden sind auch gut befreundet miteinander, also der Roboter und der Inspector.
Und die Welt, in der das Ganze stattfindet, ist eben Deutschland.
Und Deutschland ist so ein bisschen zweigeteilt, einerseits ein Hochtechnologieland mit allem, was dazugehört,
irgendwie fliegende Autos, selbstfahrende Autos und kluge Maschinen wie eben auch diese Roboter.
Es gibt aber parallel dazu auch Kommunen, die dem Ganzen entsagt haben.
Das muss man sich, glaube ich, vorstellen wie so ein paar Enklaven quasi in Deutschland.
Nicht wie richtig zweigeteilt, es gibt ein Land, das ist so, ein Land, das ist so, sondern es gibt eben so ein paar Kommunen,
die machen mal dem ganzen Kram nicht mit, die sind irgendwie wieder ziemlich zurück auf ganz normale Landwirtschaft,
haben, glaube ich, auch so rudimentäre Technik. Ich glaube, die haben sich irgendeinen Stichtag gesetzt,
irgendwas in den 90er Jahren oder so. Bis dahin benutzen sie alles und danach nicht mehr.
Also es ist aber gar kein großes Gegeneinander, aber es spielt eben in beiden Welten so ein bisschen.
Und ich glaube, der Kommissar muss eben in beiden Bereichen ein bisschen ermitteln.
Das habe ich damals auf jeden Fall gerne gelesen.
Und was ich noch mit empfehlen würde, ist ein Interview.
Ich weiß nicht, ich glaube, es ist ungefähr eine halbe Stunde oder ein bisschen mehr.
Janina Loh war im Zuge des Buches bei Sein und Streit einer empfehlenswerten Sendung ganz generell vom Deutschlandfunk Kultur.
Und das verlinke ich euch auch noch.
Grundsätzlich Deutschlandfunk hat eigentlich so mittlerweile gefühlt die besten Radiosendungen.
Ich höre fast nur noch Deutschlandfunk, das hätte ich mir vor einem halben Jahr noch nicht vorstellen können.
Genau, ja, das war doch mal wieder ein echt spannendes Buch mit viel Diskussionsansatz.
Habt ihr, glaube ich, auch gemerkt, dass wir ein bisschen mehr ins Diskutieren gekommen sind als sonst oft bei den Büchern.
Sehr schön und super ausgesucht.
Genau, jetzt bleibt uns im Grunde nur noch, euch wieder aufzurufen,
dass vielleicht jetzt die kontroversen Diskussionen als Anlass zu nutzen, uns irgendwie einen Kommentarbeitrag,
eine Frage oder irgendwas zuzuschicken, die ihr mal hier im Podcast aufgegriffen haben möchtet zu diesem Buch.
Das würden wir am Anfang der nächsten Episode machen.
Also habt ihr jetzt so zwei Wochen Zeit oder so vielleicht was einzuschicken.
Macht das im Idealfall natürlich einfach als Audiophile.
Es reicht, wenn ihr die kurz mit dem Handy aufnehmt, irgendwo in der ruhigen Ecke.
Dann können wir das direkt in den Podcast einbinden, was natürlich ganz toll ist.
Aber ihr könnt es auch einfach als Kommentar unter dem Beitrag auf der Webseite machen.
Den erreicht ihr unter zwischenzweideckeln.de
Da findet ihr die aktuelle Episode.
Und da findet ihr auch Links zu allen Möglichkeiten, wie ihr zwischen zwei Deckeln abonnieren könnt.
Im Grunde auf der Plattform eurer Wahl, mittlerweile auch bei Spotify, zu hören.
Da findet ihr auch Links zu unseren Social Media Aktivitäten.
Wir sind auf Instagram und Twitter jeweils als atdeckeln zu finden.
Bei Facebook gibt es zwischen zwei Deckeln eine Fanseite.
Da freuen wir uns auch über Likes, Kommentare, Shares und was da alles so geht.
Und wenn ihr jetzt diese Episode und auch vielleicht unsere anderen Episoden toll fandet,
dann freuen wir uns natürlich auch riesig, wenn ihr auf den diversen Plattformen vielleicht eine Rezension hinterlasst oder zumindest ein paar Sterne.
Das ist auch ganz toll, macht uns Motivation, hier noch weiter zu machen.
Aber auch keine Angst, wir haben jetzt nicht vor aufzuhören.
Das motiviert uns nun noch mehr und gibt uns natürlich auch nochmal ein gutes Gefühl, dass wir hier irgendwie was tun,
was andere Leute gerne hören und auch vielleicht irgendwie unterhält oder informiert.
Genau, hast du noch irgendwelche Sachen, die du ansprechen wolltest, Christoph?
Ich glaube, damit hast du vollumfänglich abgedeckt, was wir so am Ende zu sagen haben.
Von daher bleibt mir nur danke zu sagen, dass sie zugehört habt.
Und ja, bis zur nächsten Folge quasi. Und tschüss!
Untertitel im Auftrag des ZDF, 2021
