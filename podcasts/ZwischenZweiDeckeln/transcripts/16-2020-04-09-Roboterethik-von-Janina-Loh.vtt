WEBVTT

00:00.000 --> 00:21.840
Hallo und herzlich willkommen zu Episode 15 von Zwischen zwei Deckeln, dem Sachbuchpodcast.

00:21.840 --> 00:27.000
Ich bin wie immer der Nils und mit mir ist heute wieder der Christoph dabei.

00:27.000 --> 00:28.000
Hallo zusammen.

00:28.000 --> 00:37.000
Ja, schön, dass ihr da seid, dass ihr wieder eingeschaltet habt, wenn ihr jetzt zuhause sitzt oder irgendwie auf dem Abenteuer einkaufen seid oder zu denen gehört, die arbeiten müssen da draußen.

00:37.000 --> 00:40.000
Danke an euch, dass ihr irgendwie den Laden am Laufen haltet.

00:40.000 --> 00:45.000
Genau, ja. Wie sieht es aus bei dir? Irgendetwas Relevantes, Neues?

00:45.000 --> 00:53.000
Nichts Spektakuläres auch. Ich bin überwiegend zuhause, kaufe einige Menschen mit ein und habe Zeit zum Lesen gefunden.

00:53.000 --> 00:55.000
Was liest du denn gerade?

00:55.000 --> 01:03.000
Also jetzt gerade tatsächlich wirklich für die Episode, die wir heute aufnehmen, habe ich das Buch von heute in den letzten anderthalb, zwei Wochen eben gelesen.

01:03.000 --> 01:15.000
Also ihr habt es ja schon im Titel gesehen, es geht um Roboterethik von Janina Lo und ansonsten habe ich nochmal was bei der BBB bestellt und habe ein, zwei Krimis mir jetzt bei Twitter empfehlen lassen, die ich angehen möchte.

01:15.000 --> 01:22.000
So ist mein Ziel. Aber sonst muss ich sagen, habe ich nicht viel mehr Zeit als vorher. Ich weiß nicht, wie es bei dir ist.

01:22.000 --> 01:27.000
Ja, bei mir auch nicht so wirklich, weil meine Arbeit sich dann doch irgendwie eins zu eins ins Homeoffice übertragen lässt.

01:27.000 --> 01:32.000
Und dadurch, dass die Pendelzeit wegfällt, sogar die klassische Lesezeit irgendwie ein bisschen kürzer wird.

01:32.000 --> 01:39.000
Aber ich komme so ein bisschen zum Lesen. Ich habe jetzt gerade fertig gelesen das Zettelkastenprinzip von Sönke Ahrens.

01:39.000 --> 01:51.000
Tatsächlich mal so ein bisschen der Versuch, diesen mysteriösen, ominösen, lumenschen Zettelkasten für so privates Notizenmanagement, Wissensmanagement aufzubereiten.

01:51.000 --> 01:58.000
Und das fand ich echt total spannend. Da wird es zunächst wahrscheinlich auch den ein oder anderen Blogbeitrag zu geben bei mir.

01:58.000 --> 02:03.000
Hast du Kommunikation mit Zettelkästen von Luman dazu gelesen?

02:03.000 --> 02:05.000
Das ist ein Buch, ein Essay?

02:05.000 --> 02:08.000
Nee, das ist ein Essay. Ich glaube, das sind nur so zehn Seiten oder so.

02:08.000 --> 02:10.000
Ja, ich glaube, habe ich zumindest mal angefangen zu lesen.

02:10.000 --> 02:20.000
Vielleicht, wenn wir dran denken, können wir es ja verlinken. Ich muss sagen, das ist relativ amüsant, weil Luman da eben klarmacht, wie egal es ihm ist, mit wem er eigentlich da irgendwie zu tun hat

02:20.000 --> 02:26.000
und mit wem er zusammenarbeitet, also seinen Zettelkasten quasi als Assistenten begreift.

02:26.000 --> 02:28.000
Und das ist ganz, ganz gut.

02:28.000 --> 02:32.000
Aber das kann tatsächlich funktionieren. Das ist das Krasse dabei.

02:34.000 --> 02:37.000
Genau, da können wir vielleicht auch eine Episode hier mal zumachen. Mal gucken.

02:37.000 --> 02:38.000
Vielleicht auch das.

02:38.000 --> 02:44.000
Ja, von euch da draußen ist irgendwie nichts an Fragen oder Kommentaren oder so zu der letzten Episode gekommen.

02:44.000 --> 02:47.000
Fühlt euch da nochmal eingeladen, wenn ihr irgendwas diskutieren wollt.

02:47.000 --> 02:55.000
Oder so uns einfach ein Audiophile zu schicken, einfach am Handy aufgenommen, vielleicht in der ruhigen Ecke oder irgendwie unter Blogbeitrag zu kommentieren.

02:55.000 --> 02:58.000
Dann können wir das hier im Zweifel aufgreifen.

02:58.000 --> 03:01.000
Also fühlt euch da nochmal herzlichst so eingeladen.

03:01.000 --> 03:05.000
Apropos letzte Episode. Christoph, erinnerst du dich noch, worum ging es?

03:05.000 --> 03:10.000
Ja, also du hast uns vorgestellt, Die Economist's Hour von Benjamin Applebaum.

03:11.000 --> 03:24.000
Und da ging es, wenn ich mich jetzt recht im Sinne darum, wie im Prinzip so liberale, marktliberale Ökonomen Einfluss auf die Politik in den USA genommen haben.

03:24.000 --> 03:35.000
Und ja, es zeigt sich, dass viel von dem, was so prognostiziert wurde, in dem Bereich gar nicht so eingetreten ist, wie es gesagt wurde.

03:35.000 --> 03:38.000
So würde ich es in zwei, drei Sätzen zusammenfassen.

03:38.000 --> 03:40.000
Genau, also wie es von den Ökonomen selbst vorher gesagt wurde.

03:40.000 --> 03:41.000
Ja, genau, das meine ich.

03:41.000 --> 03:46.000
Andere haben dann durchaus zumindest mal geahnt, was passieren könnte.

03:46.000 --> 03:50.000
Ok, ja, genau, darum ging es letzte Woche, letzten Monat.

03:50.000 --> 03:52.000
Diese Episode hast du ja schon gesagt, worum es geht.

03:52.000 --> 04:01.000
Wir werden ein bisschen technikphilosophisch, wenn ich das richtig sehe, mit dem Buch Roboterethik eine Einführung von Janina Loh.

04:01.000 --> 04:09.000
Die Autorin ist Philosophin und arbeitet an der Uni Wien im Bereich Technik und Medienphilosophie, sagst du.

04:09.000 --> 04:15.000
Und sie habilitiert zu posthumanistischen Elementen in Hannah Arendts Werk und Denken.

04:15.000 --> 04:24.000
Also ja, da merkt man schon ein bisschen, dass sowohl das klassischphilosophische als auch das technikphilosophische taucht ja schon in dem Titel auf.

04:24.000 --> 04:29.000
Möchtest du anfangen, uns das Buch in der Kurzfassung kurz vorzustellen?

04:32.000 --> 04:34.000
Ja, das mache ich gerne.

04:34.000 --> 04:45.000
Also Janina Loh untersucht in ihrer Studie Roboterethik verschiedene Ansätze der Einordnung der Fähigkeiten und Zugeständnisse von und an Roboter durch EthikerInnen und PhilosophInnen.

04:45.000 --> 04:52.000
Es zeigt sich, dass Begriffe wie Moral und Verantwortung traditionell dem Menschen vorbehalten sind und einzelnen Individuen zugeschrieben werden.

04:52.000 --> 04:59.000
Loh stellt Gegenpositionen dar und macht Vorschläge, wie und warum wir unsere tradierten Denkmuster durchbrechen können und vielleicht auch sollten.

05:02.000 --> 05:06.000
Okay, das klingt ja doch potentiell kontrovers.

05:06.000 --> 05:09.000
Möglich, ja, mal gucken, wie kontrovers es wirklich wird.

05:09.000 --> 05:14.000
Und wie danach so was wie haben Tiere auch Menschenrechte sozusagen.

05:14.000 --> 05:17.000
So ein bisschen haben Roboter auch Menschenrechte.

05:17.000 --> 05:21.000
Scheint wieder so ein bisschen das zu sein, was mir spontan einfällt.

05:21.000 --> 05:29.000
Tatsächlich gibt es da auch einige Referenzen und auch Vergleiche zu dem, was eben Robotern zugetraut wird und Tieren oder nicht Tieren.

05:29.000 --> 05:39.000
Genau, einige Fragestellen aus der Fragestellung aus der Roboterethik wurden eben offenbar in der Tierethik schon behandelt oder werden es immer noch oder man kann sie auch zusammen behandeln und so.

05:39.000 --> 05:47.000
Genau, so ein bisschen die Frage nach dem Status von Nichtmenschen im Prinzip, mit denen wir aber irgendwie zu tun haben.

05:47.000 --> 05:56.000
Im Prinzip ist das Buch doch, ja, ich glaube, ich weiß nicht, wie philosophische Studien sonst so aufgebaut sind, aber ziemlich stringent.

05:56.000 --> 06:02.000
Und dadurch, ja, manchmal so ein bisschen habe ich das Gefühl, wird einiges einfach abgearbeitet.

06:02.000 --> 06:11.000
Also es geht erstmal los mit einer Einleitung und dann stellt sich Loh die Frage, welche Bereiche der Robotik und der ethischen Fragen es überhaupt gibt.

06:11.000 --> 06:20.000
Und dann geht es einmal die Arbeitsfelder der Robotik durch und dann geht es darum, inwiefern Roboter moralische Handlungssubjekte oder Objekte sein können, was das bedeutet.

06:20.000 --> 06:22.000
Darüber sprechen wir noch.

06:22.000 --> 06:28.000
Und dann geht es im dritten Kapitel um Verantwortungzuschreibung in der Interaktion zwischen Menschen und Robotern.

06:28.000 --> 06:34.000
Und dann wird das wieder durchexerziert als Roboter als Verantwortungssubjekte und Roboter als Verantwortungsobjekte.

06:34.000 --> 06:43.000
Und dann gibt es am Ende noch ein bisschen neuen Seiten, einmal ein bisschen Breitseite gegen alles und jeden quasi.

06:43.000 --> 06:46.000
Also ein bisschen kritischer Ausblick kennt man ja auch aus vielen Büchern.

06:46.000 --> 06:52.000
Aber ja, so ist das Buch erstmal aufgebaut und ich glaube, ich fange einfach mal an.

06:52.000 --> 07:06.000
Loh steigt damit ein, dass sie erstmal sagt, dass wenn wir über Ethik und Roboter oder Technik sprechen, man immer davon ausgehen muss, dass Technik von Menschen erschaffen ist und dementsprechend von Normen geprägt ist.

07:06.000 --> 07:13.000
Also egal, was wir tun, ist irgendwie zumindest implizit mit gesellschaftlichen Ideen, Normen und so weiter verknüpft.

07:13.000 --> 07:28.000
Das heißt, so die Idee einer völlig neutralen Technik, die ja so wie objektiv vor uns liegt und quasi gar keinen, in sich keine ethischen Implikationen oder so hat, das verwirft sie direkt zum Anfang.

07:28.000 --> 07:37.000
Dabei sagt sie, dass Roboterethik eben ein Teilbereich der Maschinenethik ist, was bedeutet, dass alle Roboter Maschinen sind, aber nicht alle Maschinen sind Roboter.

07:37.000 --> 07:38.000
Ja, macht Sinn.

07:38.000 --> 07:44.000
Und auch dann weist sie direkt darauf hin, dass die Fragestellungen durchaus der der Tierethik ähneln.

07:44.000 --> 07:48.000
Also was kann ein Tier, was kann ein Roboter, was fühlt es, was fühlt er.

07:48.000 --> 07:55.000
Ja, weil ich gerade gesagt habe, dass Roboter keine Maschinen sind, ist vielleicht einmal wichtig zu definieren, was Roboter überhaupt sind.

07:55.000 --> 07:59.000
Und damit starten wir in die Definition von Maschinen.

07:59.000 --> 08:08.000
Das sind nämlich künstliche Gebilde, die aus einem Antriebssystem durch Motor, Wind oder Wasser bewegten Teilen besteht und die Energie umsetzen.

08:08.000 --> 08:10.000
Das ist eine Maschine, so ganz allgemein.

08:10.000 --> 08:13.000
Roboter hingegen sind spezielle Maschinen.

08:13.000 --> 08:17.000
Das Wort geht auf Roboter aus dem Tschechischen zurück.

08:17.000 --> 08:19.000
Das kommt auch aus irgendeinem Science Fiction Roman.

08:19.000 --> 08:21.000
Das ist ja so dein Spezialgebiet.

08:21.000 --> 08:26.000
Ja, ich könnte Stanislav Lem sein, aber ich bin mir da nicht ganz sicher.

08:26.000 --> 08:28.000
Ja, ich glaube, es war nicht Stanislav Lem.

08:28.000 --> 08:32.000
Dann hätte ich nämlich, glaube ich, dazu geschrieben, weil den, den kenne ich quasi.

08:32.000 --> 08:39.000
Naja, und das Wort geht, steht offen, also Roboter im Tschechischen steht offenbar für Arbeit, frohendienst oder Zwangsarbeit.

08:39.000 --> 08:44.000
Das fand ich ganz interessant, gerade mit Blick auf eben ethische Implikation.

08:44.000 --> 08:47.000
Und ja, also jetzt die Definition von Roboter.

08:47.000 --> 09:02.000
Das ist eine elektromechanische Maschine, die A. über einen eigenständigen Körper und B. über mindestens einen Prozessor verfügt, C. Sensoren hat, D. über Effektoren oder Aktoren verfügt, die Signale in mechanische Abläufe übersetzen.

09:02.000 --> 09:08.000
Roboter erscheinen zumindest autonom und können physisch Einfluss auf ihre Umwelt nehmen.

09:08.000 --> 09:15.000
Genau, also relativ komplex irgendwie, aber man kann sich merken, gut, die haben einen eigenständigen Körper.

09:15.000 --> 09:23.000
Sie haben irgendwas, womit sie irgendwie Dinge prozessieren können, also einen Prozessor, also das, was bei Menschen das Gehirn vielleicht ist.

09:23.000 --> 09:26.000
Und sie haben irgendwie Sensoren, um die Umwelt wahrzunehmen.

09:26.000 --> 09:37.000
Und sie haben irgendwas, wie sie um die Umwelt eingreifen können, also irgendwelche Roboterarme oder sie haben einen kleinen Staubsauger, wie diese Staubsaugerroboter und so weiter.

09:37.000 --> 09:45.000
Aber danach macht Loh die These auf, dass es heute eigentlich keinen Bereich im menschlichen Alltag mehr gibt, in dem Roboter noch keinen Einzug gehalten hätten.

09:45.000 --> 09:48.000
Kann man erst mal so auf sich wirken lassen und sich dann überlegen, ob das stimmt.

09:48.000 --> 09:53.000
Ich habe erst gedacht, das klingt ja irgendwie relativ steil als These, aber ich glaube, sie hat schon recht.

09:53.000 --> 10:02.000
Egal, wo wir hingucken, wir wissen, dass es zumindest Roboter gibt, auch wenn wir sie nicht tagtäglich sehen und mit ihnen zu tun haben sollten.

10:03.000 --> 10:14.000
Ja, dann geht es im ersten Kapitel darum zu klären, worum es in diesem Buch eigentlich geht, also welche Bereiche aus der Robotik zur Sprache kommen oder in der Roboterethik eine Rolle spielen.

10:14.000 --> 10:25.000
Das ist einmal die Industrierobotik, also die Frage nach Ersetzung von Arbeitsplätzen hat man sicherlich schon gehört und daran anschließend, wir hatten es ja gerade, sie habilitiert zu Hannah Arendt auch,

10:25.000 --> 10:33.000
die Frage danach, was der Mensch ist, wenn er nicht mehr arbeitet, wenn er nichts mehr schafft oder nicht mehr Lohn arbeitet oder wie man es nennen möchte.

10:33.000 --> 10:40.000
Also das sind Fragen, die daran anschließen an das Problem von, wenn Arbeitsplätze ersetzt werden, was resultiert eigentlich daraus.

10:40.000 --> 10:47.000
Dann, was natürlich glaube ich auch ein verbreitetes Beispiel ist, das auch in dem Buch immer wieder vorkommt, ist die Frage nach dem autonomen Fahren.

10:48.000 --> 11:02.000
Also wenn wir jetzt Automobile haben, die irgendwie intelligente Fahrsysteme haben, ist die Frage, wie diese Autos in Unfallsituationen reagieren sollen, wer bei Unfällen dann die Verantwortung trägt,

11:02.000 --> 11:13.000
also kann das Auto selbst irgendwie verantwortlich gemacht werden, ist es der Konzern, der das Auto gebaut hat, ist es der Fahrer, der eigentlich noch hinterm Lenkrad sitzt, aber nicht mehr eingreifen muss und so weiter.

11:13.000 --> 11:17.000
Dann kriegt das Auto drei Monate Fahrverbot, stelle ich mir auch lustig vor.

11:17.000 --> 11:26.000
Ja, das ist irgendwie ganz interessant. Da geht später auch noch drum die Unterscheidung zwischen Verantwortung und Haftbarkeit, die so ein bisschen getrennt wird.

11:26.000 --> 11:27.000
Macht Sinn.

11:27.000 --> 11:38.000
Und sie weiß, glaube ich, drei oder viermal in dem Buch darauf hin, dass in der EU oder vom Europäischen Parlament gerade die Idee von elektronischen Personen erarbeitet wird,

11:38.000 --> 11:51.000
also quasi analog zu juristischen Personen, die wir aus dem Recht schon kennen, wo dann irgendwelche Unternehmen vor Gericht wie Personen behandelt werden können, könnte man das eben auch für Maschinen oder Roboter vielleicht einsetzen.

11:51.000 --> 11:58.000
Was aus diesem Projekt der Erarbeitung da genauer resultiert ist, das schreibt sie aber nicht, nur dass das eben im Prozess ist.

11:58.000 --> 12:06.000
In der Medizin oder Therapie- und Pflege-Robotik stellt sich, wenn man da Roboter einsetzt, und das ist ja mittlerweile auch schon so,

12:06.000 --> 12:15.000
ich persönlich habe das auch schon erlebt, also ich habe mal in einem alten Zentrum ein Praktikum gemacht, in dem diese Roboter-Robbe Paro zum Einsatz kam.

12:15.000 --> 12:16.000
Ich weiß nicht, ob du die kennst.

12:16.000 --> 12:17.000
Ja, sag mir was.

12:17.000 --> 12:33.000
Also für die, die es nicht kennen, das ist quasi sowas wie ein großes Kuscheltier in Form einer Robbe, die es weiß und die hat verschiedenste Sensoren eingebracht und reagiert auf Berührung und Ansprache und so weiter ein Stück weit.

12:33.000 --> 12:36.000
Und es ist auch ganz schön teuer, die kostet, glaube ich, um die 5000 Euro oder mehr.

12:36.000 --> 12:41.000
Gibt es ganz viele Videos zu, wenn einem das interessiert.

12:41.000 --> 12:51.000
Und im Zuge dieser Pflege-Robotik, und man kann sich da ja auch Sachen vorstellen, wie Roboter, die Menschen aus Betten heben oder auf die Toilette begleiten und so weiter,

12:51.000 --> 12:56.000
stellt sich eben die Frage nach der Autonomie der Personen, die dort behandelt werden,

12:56.000 --> 13:05.000
und inwiefern die Emotionen, die da vielleicht auch aufkommen gegenüber einer Roboter-Robbe oder einem Roboter als Gesprächspartner, kann man sich ja alles Mögliche vorstellen,

13:05.000 --> 13:12.000
inwiefern diese Emotionen in gewisser Maße echt sind, inwiefern die Personen, die da beteiligt sind, getäuscht werden, sowas alles.

13:12.000 --> 13:17.000
Ich nehme es gleich vorweg, Janina Loh sieht das alles nicht so problematisch.

13:17.000 --> 13:18.000
Okay.

13:18.000 --> 13:29.000
Sie sagt, dass eben Beziehungen zu Objekten oder Robotern durchaus echte Emotionen sein können und dass das keine Täuschung in dem Sinne darstellen muss.

13:29.000 --> 13:30.000
Okay.

13:30.000 --> 13:37.000
Vielleicht hat man das jetzt auch schon gehört, Sexroboter sind ja auch ein Ding, also die gibt es schon, kann man schon kaufen, kosten, glaube ich, immer noch relativ viel Geld.

13:37.000 --> 13:43.000
Und was da in dem Buch augenfällig wird, da geht es dann häufig in dem Kontext um feministische Fragestellung.

13:43.000 --> 13:44.000
Ja.

13:44.000 --> 13:53.000
Also es gibt da irgendwie teilweise die Möglichkeit, eben Roboter zu vergewaltigen und dass das irgendwie im Kern problematisch sein kann.

13:53.000 --> 14:00.000
Auf der anderen Seite vielleicht aber auch irgendwie befreiend, weil Leute, die so etwas haben, das dann ausleben können, ohne eben Menschen zu schädigen.

14:00.000 --> 14:01.000
Ja.

14:01.000 --> 14:05.000
Ja, da gibt es viele ethische Ansätze und viele Diskussionen darum.

14:05.000 --> 14:14.000
Und darum geht dann natürlich auch um Objektifizierung von, gerade von Frauen, weil diese Sexroboter eben sich häufig an heterosexuelle Männer wenden.

14:14.000 --> 14:20.000
Ja, also auch ein Bereich, in dem der Feminismus eine große Rolle spielt und auch spielen sollte.

14:20.000 --> 14:29.000
Der letzte Bereich, den Lo aufzählt und das liegt, glaube ich, auf der Hand, dass das irgendwie ethisch relevant ist, ist eben die Frage nach Militärrobotern.

14:29.000 --> 14:30.000
Ja.

14:30.000 --> 14:46.000
Und wie wird auch unbefugten der Zugang zu Entscheidungen verwehrt und wie viel Autonomie dürfen artificielle Systeme eigentlich haben, wenn sie eben konkret über Leben und Tod entscheiden?

14:46.000 --> 14:50.000
Da ist dann auch wieder die Schnittstelle zu dem autonomen Auto, das Entscheidende, auf wen es rettet.

14:50.000 --> 14:52.000
Ja, absolut.

14:53.000 --> 14:59.000
Ja, in der Roboterethik gibt es dann zwei klassische Arbeitsfelder, mit denen man sich beschäftigen kann.

14:59.000 --> 15:07.000
Also einmal Roboter als moralische Akteurinnen, also inwiefern sie selbst irgendwie moralfähig sind und moralisch handeln.

15:07.000 --> 15:10.000
Ein bisschen, das wir gerade eben schon besprochen haben.

15:10.000 --> 15:18.000
Und die andere Frage ist danach, inwiefern Roboter als Objekte moralischen Handelns irgendwie relevant werden, wie zum Beispiel bei den Sexrobotern.

15:18.000 --> 15:19.000
Ja.

15:19.000 --> 15:30.000
Es ist auffällig, dass die Deutungshoheit über das ganze erste Arbeitsfeld, also inwiefern Roboter moralische Subjekte sein können, ziemlich anthropozentrisch sind.

15:30.000 --> 15:34.000
Anthropozentrisch meint damit, der Mensch steht im Mittelpunkt und es wird von Menschen ausgedacht.

15:34.000 --> 15:44.000
Also unser Bild davon, was irgendwie moral ist und sein kann, ist immer davon geleitet, was wir von Menschen können, wie wir glauben, dass Menschen sind.

15:44.000 --> 15:54.000
Und das bedeutet am Ende auch, dass durchweg Menschen entscheiden, ob Roboter moralisch handeln können, ob sie sowas wie Willensfreiheit besitzen können und so weiter.

15:54.000 --> 15:55.000
Ja.

15:55.000 --> 15:57.000
Leuchtet, glaube ich, auch ein.

15:57.000 --> 16:04.000
Ich glaube, was Lo damit aufzumachen, probiert es erstmal, einfach einen Reflektionspunkt, einfach mal darüber nachzudenken.

16:04.000 --> 16:18.000
Okay, wir denken immer von uns aus, was vermutlich auch gar nicht anders möglich ist, aber wir nehmen uns damit eben auch die Freiheit und Hoheit darüber raus, zu entscheiden, wer eigentlich Teil von dem ist, was wir uns eigentlich nur zubelegen.

16:18.000 --> 16:19.000
Genau, ja klar.

16:19.000 --> 16:23.000
Sie sagt dabei, dass wir eigentlich gar nicht wissen, wie es überhaupt ist, frei zu sein.

16:23.000 --> 16:24.000
Also Zitat jetzt.

16:24.000 --> 16:38.000
Ja, zum zweiten so ein bisschen in die Richtung.

16:38.000 --> 16:45.000
Roboter als Moralobjekte, da was ich gerade eben schon hatte, waren eben die Beispiele aus der Sexrobotik.

16:45.000 --> 16:52.000
Sexroboter reproduzieren eben heteronormative, patriarchale und diskriminierende Strukturen.

16:52.000 --> 16:54.000
Das ist durchaus ein Problem.

16:54.000 --> 17:04.000
Würde man sie anders konzipieren, könnte man aber durchaus auch sich andere Sexroboter vorstellen, die vielleicht auch andere Möglichkeiten bieten.

17:04.000 --> 17:21.000
Ich könnte mir vorstellen, dass gerade wenn es um Sexualtherapie geht und so die Begleitung von Menschen, die vielleicht zuerst Sexualpraktiken körperlich nicht eigenständig in der Lage sind, dass da eigentlich ein großes Feld der Emanzipation auch offen steht.

17:21.000 --> 17:24.000
Also ein bisschen die Schnittstelle wieder zu der Therapie.

17:24.000 --> 17:25.000
Ja, genau.

17:25.000 --> 17:30.000
Dabei stellt sich immer wieder die Frage, wie Moral überhaupt in eine Maschine gelangen kann.

17:30.000 --> 17:36.000
Also wenn Sie irgendwie Moral entwickeln können sollten, wie funktioniert das?

17:36.000 --> 17:41.000
Und da gibt es eben verschiedene Ansätze, entweder Top-Down, Bottom-Up oder Hybrid-Ansätze.

17:41.000 --> 17:51.000
Also entweder man programmiert halt sowas wie moralische Regeln ein oder man setzt darauf, dass artificielle Systeme selbst lernen können.

17:51.000 --> 17:55.000
Also sowas, was unter KI verstanden wird oder man kombiniert ein bisschen.

17:55.000 --> 18:01.000
Also einerseits gibt man irgendwelche Regeln vor, andererseits hat man irgendwie selbst der andere Ansätze.

18:01.000 --> 18:13.000
Was auffällig ist, ist, dass jeder Ansatz irgendwie Top-Down, das zu bewältigen, total schwierig ist, weil Moral eben abstrakt ist und die Entscheidungen, die aber gefällt werden müssen, sehr konkret sind.

18:14.000 --> 18:22.000
Die Rettung von Menschenleben ist irgendwie sehr abstrakt, aber die Entscheidung, in welche Richtung ein Auto ausweicht, ist eben sehr konkret.

18:22.000 --> 18:29.000
Das heißt, man kann sich nicht einfach darauf verlassen zu sagen, rette möglichst viele Menschen zu einem Auto und das wird schon irgendwie funktionieren.

18:29.000 --> 18:41.000
Wobei ich das ganz spannend finde, weil wir ja gerade auch jetzt in der aktuellen Situation mit der drohenden Überlastung oder in manchen anderen Ländern auch schon der Überlastung von Intensivstationen und Ähnlichem,

18:41.000 --> 18:46.000
wo ja im Grunde genau diese moralischen Entscheidungen getroffen werden, wen können wir jetzt behandeln und wen nicht.

18:46.000 --> 19:00.000
Und da eben danach gerufen wird, wir brauchen eine klare Triage-Regeln sozusagen, nach welchen Kriterien entscheiden wir, dass das eben nicht das Individuum machen muss, der einzelne Arzt oder die einzelne Ärztin oder der einzelne Pfleger,

19:00.000 --> 19:09.000
die halt sagen, der kriegt jetzt die Beatmung und der nicht, sondern dass wir da im Grunde nach genau diesen abstrakten Regeln verlangen, die du jetzt sagst, die eigentlich schwer zu machen sind.

19:09.000 --> 19:18.000
Das finde ich auch ziemlich interessant, weil mein Gefühl auch immer wäre, man kann probieren, das durchzureglementieren, wie so eine Triage stattfinden soll und wie die organisiert werden möchte.

19:18.000 --> 19:23.000
Aber ich habe das Gefühl, das würde sich im Konkreten sowieso ergeben.

19:23.000 --> 19:31.000
Ich habe das Gefühl, die eigentliche Funktion dahinter ist, eben Ärztinnen zu entlasten von der von der konkreten Entscheidung.

19:31.000 --> 19:44.000
Aber ich glaube, ich vermute, dass die Entscheidung, die Ärztinnen treffen würden, ohne Triage-Regelung, ziemlich genau das widerspiegeln würden, was sie auch mit Regelungen jetzt entscheiden können, sollen oder dürfen.

19:44.000 --> 20:05.000
Da ist ja gerade diese Diskussion, die auch gerade so aus der Rassismusdiskussion sozusagen ein bisschen kommt, ob man nicht mit objektiveren Triageregeln solche im Zweifel auch erst mal unbewussten Verzerrungen und wie eben rassistische Vorurteile und Ähnliches nicht auch ein bisschen ausgleichen kann.

20:05.000 --> 20:14.000
Wenn man eben sagt, ich überlasse das nicht dem Einzelnen, der eventuell unbewusst irgendwas internalisiert hat, sondern ich sage halt, ne, stopp hier, das Alter muss so sein.

20:14.000 --> 20:17.000
Und ja, wenn der jetzt zwei Jahre jünger ist, dann kriege ich das halt trotzdem.

20:17.000 --> 20:20.000
Aber wenn du vielleicht gesagt hast, und so weiter und so fort.

20:20.000 --> 20:30.000
Ich glaube, da kommt diese Objektivierung ins Spiel, die da vielleicht auch wirklich was nochmal verändern kann gegenüber Einzelnen.

20:30.000 --> 20:35.000
Ja, stimmt, das ist nochmal ein ganz gelungener Gedankenanstoß da.

20:35.000 --> 20:40.000
Und ich finde auch einfach die moralische Entlastung quasi von Ernst, den ist ja auch ein Eigenwert.

20:40.000 --> 20:42.000
Das ist ein totaler Wert natürlich, gar keine Frage.

20:42.000 --> 20:54.000
Inwiefern Roboter jetzt wirklich sowas wie, ja, also richtig Subjekte im moralischen Sinne sein können, hängt natürlich auch ganz stark davon ab, was moralische Akteurinnen überhaupt benötigen.

20:54.000 --> 21:04.000
Und da jetzt einfach mal so, um darzustellen, wie dieses Buch geschrieben ist, nimmt Janina Losig eben immer verschiedene Autorinnen raus, sagt XY hat das und das geschrieben.

21:04.000 --> 21:09.000
Und dann wird ein anderer Text quasi behandelt und da wird das und das gesagt.

21:09.000 --> 21:15.000
Und das stellt sie eben da, was es manchmal ein bisschen schwierig macht, dieses Buch jetzt eben so ganz doll zusammenzufassen.

21:15.000 --> 21:21.000
Im Prinzip müsste ich sagen, okay, es gibt sechs Studien, die untersucht wurden und die kommen jeweils zu dem Schluss.

21:21.000 --> 21:23.000
Deswegen probiere ich das jetzt hier mal zusammenzufassen.

21:23.000 --> 21:28.000
Manche Studienergebnisse lasse ich dann aber auch quasi hinten runterfallen und stelle sie nicht da.

21:28.000 --> 21:30.000
Also manche Ethikerinnen kommen dann hier nicht vor.

21:30.000 --> 21:46.000
Und am Ende eines jeden Kapitels bezieht Janina Loh das, was sie quasi gelesen hat, auf verschiedene Robotersysteme und probiert einzuordnen, inwiefern jetzt eben Subjekt, Objektverantwortungs möglich wären im Sinne der gelesenen Autorinnen.

21:46.000 --> 21:54.000
Auch das lasse ich hinten noch rüber, weil ich glaube, wenn ich jetzt sechs oder sieben Roboter immer wieder aufzähle und sage, möglicherweise können die dies, das oder das können sie auch nicht.

21:54.000 --> 21:57.000
Je nachdem, wem man fragt, das geht ein bisschen zu weit.

21:57.000 --> 22:02.000
Wenn euch das im Konkreten interessiert, müsst ihr das Buch dann tatsächlich vielleicht einfach kaufen und lesen.

22:02.000 --> 22:10.000
Naja, auffällig ist, dass bei Moral sich auch die Philosophinnen nicht so richtig einig sind, was denn eigentlich Moral ist und was Gut ist.

22:10.000 --> 22:16.000
Also es gibt einerseits so Ideen von Gut ist, wer Gutes tut, so im Sinne von Forrest Gump quasi.

22:16.000 --> 22:23.000
Also man guckt einfach nur darauf, was irgendwie der Output von einer Handlung ist.

22:23.000 --> 22:33.000
Und wenn der gut ist, dann sind auch Roboter befähigt, quasi moralisch zu agieren oder auch wenn es schlecht ist.

22:33.000 --> 22:43.000
Also man guckt einfach nur, was der Output einer Aktion ist und auf der anderen Seite gibt es natürlich irgendwie hochkomplexe Anforderungsprofile,

22:43.000 --> 22:52.000
wo auch ein Eigenverständnis von Moral durch die handelnde Person, den handelnden Roboter oder das handelnde Tier irgendwie vorliegen muss.

22:52.000 --> 23:01.000
Die Frage, die im Kern so ein bisschen da ist, ist, wann sind Roboter eigentlich wirklich moralfähig und wann nutzen sie nur Funktionsequivalente zu Moral?

23:01.000 --> 23:06.000
Und tun dann quasi nur so, als wären sie moralfähig?

23:06.000 --> 23:10.000
Und da hängt dann natürlich die große Frage nach dem Bewusstsein im Kern da dran.

23:10.000 --> 23:15.000
Das erinnert mich an dieses Gedankenexperiment von dem Chinese Room, das chinesische Zimmer.

23:15.000 --> 23:16.000
Ich weiß nicht, ob du das kennst.

23:16.000 --> 23:17.000
Ah, erzähl mal.

23:17.000 --> 23:21.000
Ja, da geht es da um Sprachverstehen im Grunde, da kommt das ursprünglich her.

23:21.000 --> 23:30.000
Und das nimmt halt an, ich weiß gar nicht mehr von wem es kommt, dass du ein Zimmer hast, wo sämtliche Regeln der chinesischen Sprache drin stehen, in Büchern.

23:30.000 --> 23:40.000
Und ganz ausgefeilte Regeln, wenn dieses Zeichen kommt, dann antwortest du mit diesem Zeichen und wenn diese Zeichen kommen, dann antwortest du mit dem Zeichen und so weiter und so fort.

23:40.000 --> 23:50.000
Und jetzt ist halt eine Person in diesem Zimmer, die bekommt per Zettel oder was, eine Reihe chinesischer Schriftzeichen reingereicht, befolgt die Regeln und gibt eine Antwort.

23:50.000 --> 23:59.000
So, und diese Antwort ist natürlich sinnvoll, ohne dass diese Person verstanden hätte, was sie da tut inhaltlich, also dass die Sprache sozusagen verstanden hätte.

23:59.000 --> 24:05.000
Und jetzt gibt es die Frage, dieses Zimmer, ist das Verstehen, was da passiert?

24:05.000 --> 24:08.000
Wenn es von außen so aussieht, als wäre es Verstehen?

24:08.000 --> 24:09.000
Ja.

24:09.000 --> 24:12.000
Das schließt dann wahrscheinlich an den Turing-Test im Grunde an.

24:12.000 --> 24:14.000
Das klingt so, als wäre das das.

24:14.000 --> 24:15.000
Ja.

24:15.000 --> 24:26.000
Ja, in einem Interview weiß, das verlinke ich ja auch nachher, bei Sein und Streit war sie bei Deutschland von Kultur, da weiß Janina Loh natürlich auch darauf hin, dass diese ganze Frage nach dem Bewusstsein total schwierig ist.

24:26.000 --> 24:38.000
Und halt sowas wie eine, sie nennt es, glaube ich, philosophisches Zusatzargument, also einfach die absolute Annahme, dass Menschen Bewusstsein haben, ist empirisch nicht beweisbar.

24:38.000 --> 24:50.000
Man kann quasi nicht den Kopf aufschneiden, ins Gehirn gucken und sagen, da ist es, das Bewusstsein, sondern auch da sind ja nur irgendwie Verkettungen von Nerven und Datenströmen im Prinzip irgendwie.

24:50.000 --> 25:05.000
Und das heißt, ja, es gibt einfach Dinge, die kann man nicht so richtig nachvollziehen und das wird man also vermutlich absehbar zumindest auch nicht beantworten können, inwiefern irgendwas Bewusstsein erlangen kann, außer dass wir es Menschen halt auf jeden Fall zubelegen.

25:05.000 --> 25:06.000
Ja.

25:06.000 --> 25:19.000
Was auffällig ist, wenn es darum geht, ob Dinge, Tiere, Roboter, wie auch immer moralische Akteure sein können, ist, dass Autonomie irgendwie im Zentrum von moralischer Akteurschaft steht.

25:19.000 --> 25:33.000
Das heißt, diese Top-Down-Ansätze, wenn einfach nur einprogrammiert ist, tu dies, wenn das, realisieren diese Moralfähigkeit nicht, weil eben komplett determiniert ist, wie sich der Roboter verhält.

25:34.000 --> 25:44.000
Okay, aber jetzt trifft sie ja im Grunde eine Entscheidung. Sie hat ja gesagt, ist gut, wer Gutes tut oder ist gut, wer weiß, dass er Gutes tut sozusagen.

25:44.000 --> 25:45.000
Ja.

25:45.000 --> 25:47.000
Und hier sagt sie jetzt ja, nee, es ist gut, wer weiß, dass er Gutes tut.

25:47.000 --> 25:56.000
Ja, genau, also diesem, quasi diesem, sie nennt das moralischem Schwellenwert, diesen Ansatz, den ich da vorgestellt habe, dem steht sie durchaus kritisch gegenüber.

25:56.000 --> 25:57.000
Okay.

25:57.000 --> 25:58.000
Ja, das merkt man auch. Genau.

25:59.000 --> 26:06.000
Ja, und Autonomie und so weiter referenzieren dann immer weiter eigentlich auf sowas wie Intentionalität.

26:06.000 --> 26:12.000
Also, wenn Roboter wirklich moralfähig sein sollten, müssten sie sowieso etwas wie Gründe für ihr Handeln anführen können.

26:12.000 --> 26:13.000
Ja.

26:13.000 --> 26:21.000
Man muss irgendwie sagen können, warum man getan hat, was man getan hat. Man kann nicht einfach nur sagen, ich hab das getan, weil das wurde mir halt so gesagt oder beigebracht.

26:21.000 --> 26:22.000
Mhm.

26:22.000 --> 26:31.000
Ja, dann geht es darum, inwiefern Roboter moralische Handlungsobjekte sein können und da gibt es so etwas wie eine Standardposition.

26:31.000 --> 26:45.000
Also Handlungsobjekt heißt, in dem Sinne moralisches Handlungsobjekt, müssen wir uns eigentlich, also unabhängig davon, ob Roboter selbst moralisch als Subjekt handeln können, müssen wir uns denen eigentlich gegenüber vernünftig verhalten auf eine Art.

26:45.000 --> 26:46.000
Ja.

26:46.000 --> 26:58.000
Und die philosophische Standardposition, so wie sie das nennt, ist, dass eigentlich nur Dinge einen entsprechenden Status zugebilligt bekommen, die selbst auch Subjekte sind.

26:58.000 --> 26:59.000
Okay.

26:59.000 --> 27:00.000
Ja.

27:00.000 --> 27:02.000
Das finde ich krass eigentlich.

27:02.000 --> 27:06.000
Ja, wer kein Moralsubjekt ist, wird auch kein Moralobjekt.

27:06.000 --> 27:13.000
Also, wenn man das weiterdenkt, bedeutet das, man kann mit Robotern verfahren, wie man will, das ist moralisch nicht problematisch.

27:13.000 --> 27:19.000
Ja, zumindest nicht, zumindest wenn man bei der Frage nach dem Moralsubjekt zu der Antwort kommt, nein, ist es nicht.

27:19.000 --> 27:22.000
Genau, dann kann man eigentlich mit denen machen, was man möchte.

27:22.000 --> 27:26.000
Das ist eine krasse, gerade weil du sagst, dass das die Standardposition ist.

27:26.000 --> 27:30.000
Ich würde jetzt dazu kommen, meine erste Intuition wäre, dass die total umstritten sein muss.

27:30.000 --> 27:31.000
Ja.

27:31.000 --> 27:34.000
Weil man das ja gerade genau auf die Tiere wieder wunderbar übertragen kann.

27:34.000 --> 27:42.000
Das hat mich da auch ein bisschen gewundert, weil ich das Gefühl hatte, das kam nicht so richtig vor quasi als Vergleich.

27:42.000 --> 27:48.000
Weil ich auch dachte, naja, dann könnten wir ja auch, also wenn das die Idee ist, dann könnte man ja auch mit Tieren tun und lassen, was man möchte.

27:48.000 --> 27:51.000
Und das scheint mir keine Standardposition zu sein.

27:51.000 --> 27:52.000
Genau.

27:52.000 --> 27:58.000
Ich meine, das steht ja sogar in unserem Grundgesetz drin quasi, dass das eben nicht in Ordnung ist.

27:58.000 --> 27:59.000
Ja.

27:59.000 --> 28:03.000
Und das ist ja auch irgendwie philosophisch abgeleitet.

28:03.000 --> 28:05.000
Das hat mich auch ein bisschen irritiert.

28:05.000 --> 28:09.000
Aber gut, sie sagt das erstmal so, dass das eben so die allgemeine Position ist.

28:09.000 --> 28:14.000
Ich weiß nicht, ob das jetzt vielleicht auch langsam ins Rutschen kommt.

28:14.000 --> 28:22.000
Ich meine, wenn man an irgendwelche Roboter in der industriellen Fertigung oder so denkt, dann weiß ich nicht.

28:22.000 --> 28:30.000
Da entspinnt sich bei mir auch nicht sofort das Bild der Notwendigkeit des quasi verantwortungsvollen Umgangs.

28:30.000 --> 28:33.000
Da würde ich auch sagen, naja, wenn du kaputt bist, kommst du auf den Müll.

28:33.000 --> 28:37.000
Und wenn du mich nervst, dann schreite ich auch an, ist mir egal.

28:37.000 --> 28:38.000
So.

28:38.000 --> 28:39.000
Interessant.

28:39.000 --> 28:43.000
Das wird dann wieder interessant, wenn man jetzt die Sexroboter oder so ins Spiel bringt zum Beispiel.

28:43.000 --> 28:44.000
Ja, total.

28:44.000 --> 28:49.000
Wo dann auf einmal eine andere, die Roboter sind ja immer noch die gleichen.

28:49.000 --> 28:52.000
Aber es kommt irgendwie eine ganz andere moralische Wertung ins Spiel.

28:52.000 --> 28:56.000
Sie macht da noch ein ganz spannendes Beispiel auf, was mich irgendwie so ein bisschen ins Denken gebracht hat.

28:56.000 --> 28:58.000
Da komme ich gleich noch zu.

28:58.000 --> 29:04.000
Na ja, was ihr erstmal feststellt, ist, dass Roboter häufig in Aussehen als auch im Verhalten vermenschlicht werden.

29:04.000 --> 29:09.000
Also man nennt das dann Anthropomorphisierung.

29:09.000 --> 29:12.000
Sie entwickeln sich dann irgendwie zu sowas wie Gefährten von Menschen.

29:12.000 --> 29:18.000
Und diese Anthropomorphisierung wird von der Psychologie meistens traditionell negativ gesehen.

29:18.000 --> 29:22.000
Also als eine Voreingenommenheit oder einen sogenannten Kategorienfehler.

29:22.000 --> 29:23.000
Ja.

29:23.000 --> 29:29.000
Das, was ich am Anfang als Betrug quasi betitelt habe, bei dieser Parorobbe zum Beispiel.

29:29.000 --> 29:36.000
Und da macht die Psychologie den Vorwurf, dass es eben die Illusion von Beziehungen irgendwie entsteht,

29:36.000 --> 29:43.000
wenn man Roboter quasi so moralzentriert oder auch menschlich behandelt.

29:43.000 --> 29:44.000
Ja.

29:44.000 --> 29:48.000
Lo macht dann die Frage danach auf, wann ist eine Beziehung eigentlich eine Beziehung?

29:48.000 --> 29:56.000
Also wie unterscheiden sich Roboter von Tieren oder auch Beziehungen oder Bezugsnamen zu Dingen im Allgemeinen?

29:56.000 --> 30:00.000
Es gibt ja durchaus Menschen, die objektsexuell sind.

30:00.000 --> 30:05.000
Das sind wenige Menschen, aber die gibt es halt, die führen Beziehungen zu Dingen.

30:05.000 --> 30:15.000
Und wenn ich so einfach daran denke, jede Person, die ich kenne, hat definitiv auch Dinge, zu denen sie irgendwie eine emotionale Bindung zum Beispiel hat.

30:15.000 --> 30:21.000
Und das wird im Allgemeinen auch nicht besonders als völlig abstruse oder verwerflich angesehen.

30:21.000 --> 30:31.000
Was man da dann bei Robotern sehen kann, ist, dass sie so klassische Kategorisierungen von Subjekt und Objekt oder Belebt und Unbelebt so ein bisschen aufweichen.

30:31.000 --> 30:43.000
Also die kommen so zunehmend, gerade wenn sie eben so vermenschlicht auftreten und Menschen auch sehr emotional oder menschlich eben auf sie reagieren, kommt es in so eine Grenzregion.

30:43.000 --> 30:54.000
Was sie am Ende sagt, ist, dass eine Definition von Beziehung eigentlich ist, dass eine Beziehung bis hin zu einer Freundschaft kann man desto eher zu einem Gegenüber eingehen,

30:54.000 --> 30:59.000
je mehr es eine befriedigende Antwort auf die eigenen Bedürfnisse zu geben, imstande ist.

30:59.000 --> 31:09.000
Da spielt glaube ich ein bisschen rein, was wir gerade eben schon hatten, dass uns eigentlich auch menschliche Gegenüber schon ziemlich intransparent sind und wir gar nicht so genau hinter deren Kopf gucken können.

31:09.000 --> 31:19.000
Und uns häufig auch egal ist, ob, keine Ahnung, stellen wir uns vor, wir sind an der Supermarktkasse und wünschen uns einen freundlichen Umgang mit der Person, die da kassiert.

31:19.000 --> 31:26.000
Dann geht es uns um den freundlichen Umgang und nicht darum, ob uns die Person wirklich wohlgesonnen ist.

31:27.000 --> 31:37.000
Dann gibt es in der Philosophie offenbar die Position und die finde ich eigentlich ganz überzeugend ist, dass für Roboter als Objekte der Moral die These spricht,

31:37.000 --> 31:43.000
dass es Menschen besser gelingt menschlich zu bleiben, wenn sie Roboter und andere human behandeln.

31:43.000 --> 31:53.000
Also das heißt, wenn wir mit Dingen und Robotern eben vernünftig menschlich und nicht völlig gewaltvoll oder so umgehen,

31:53.000 --> 32:08.000
dass wir dann eben auch menschselbst bleiben und eben ja nicht sonst wo unsere Gewalt ausleben und die dann vielleicht wieder zu Menschen hin zurück transformieren und sie auch dafür legitim auf einmal erachten.

32:08.000 --> 32:15.000
Da gibt es doch diese Diskussion, die ich letztes Mal so am Rande mitgekriegt habe, zum Thema wie Kinder mit so Alexa und Siri und so weiter umgehen,

32:15.000 --> 32:26.000
dass sie halt gewöhnt sind so nach dem Motto Alexa spiel XY und dann eventuell auch anfangen im Alltag mit Leuten so umzugehen und einfach nur aufzufordern, ohne da wirklich eine soziale Interaktion einzutreten.

32:26.000 --> 32:34.000
Da geht es also ein bisschen um dieses Vorbildhafte sozusagen. Wenn ich mich da mal dran gewöhnt habe, dann bin ich auch Menschenpampiger.

32:34.000 --> 32:44.000
Ja, ich hatte neulich Besuch von einem guten Freund und habe mir einen Tee aufgegossen und wenn ich mir einen Tee aufgegieße, dann schrei ich im Normalfall Siri an.

32:44.000 --> 32:49.000
Also nein, ich sage zu Siri, Siri stelle einen Timer auf drei Minuten oder so.

32:49.000 --> 32:56.000
Und naja, ich hatte mich gerade mit dem Freund unterhalten, habe mir parallel meinen Tee aufgegossen und dann eben zu meinem Handy gesagt, stelle einen Timer auf drei Minuten.

32:56.000 --> 33:04.000
Aber halt dann zu einem sehr neutralen Ton und ich wurde dann von meinem Freund so ein bisschen irritiert angeguckt.

33:04.000 --> 33:07.000
Aber er hat dann sein Handy rausgeholt und wollte eben Timer stellen.

33:07.000 --> 33:23.000
Aber ja, das war definitiv irritierend und von daher kann ich ein Beispiel gut nachvollziehen, weil eben meine Ansprache gegenüber dieser Technik Siri offenbar eine ganz andere war, die ich gegenüber einem guten Freund so nie an den Tag legen würde.

33:23.000 --> 33:29.000
So würde ich nie einen Freund darum bitten, doch mal einen Kurzzeitbäcker zu stellen.

33:29.000 --> 33:42.000
Naja, und die Frage danach, ob das jetzt aber am Ende irgendwie richtig oder falsch ist, da macht Lo die Frage auf, naja, aber wie verhalten wir uns zum Beispiel gegenüber Kunstwerken oder auch Romanfiguren?

33:42.000 --> 33:45.000
Ist es wirklich egal, wie wir ihnen begegnen?

33:45.000 --> 34:02.000
Da würden wir vermutlich nicht zu dem Schluss kommen zu sagen, naja, wie ich auf ein Kunstwerk reagiere, ob ich da in einem Museum völlig, keine Ahnung, gefühlskalt und so ohne jeden Respekt vor der Kunst dem gegenüber trete.

34:02.000 --> 34:07.000
Naja, da würde niemand sagen, dass das so ganz egal ist, wie wir uns da eigentlich verhalten.

34:07.000 --> 34:08.000
Ja, stimmt.

34:09.000 --> 34:13.000
Und die Frage ist, warum sollte es dann bei Robotern eben egal sein, ne?

34:13.000 --> 34:15.000
Spannend, spannend.

34:15.000 --> 34:17.000
Das fand ich auch ganz spannend, naja.

34:17.000 --> 34:27.000
Dann gibt es zu dem Ganzen sowas, was sie inklusiver Ansätze nimmt, also die probieren so ein bisschen weiterzudenken, das macht sie dann für den zweiten Teil des Buches auch nochmal.

34:27.000 --> 34:38.000
Und diese inklusiven Ansätze bemühen sich eben um so eine Loslösung von dem, was sie Anthropozentrismus nennt, also von diesem Ausgang von, wir gucken nur vom Menschen aus, wie das eigentlich alles beurteilt werden kann.

34:38.000 --> 34:48.000
Den Menschen soll dabei an sich nichts abgesprochen werden, also es geht dann nicht darum, den Menschen quasi kleiner zu machen, als er es traditioneller Philosophie gemacht wird.

34:48.000 --> 34:54.000
Aber ja, Janina Loh macht auch ganz viel zum Thema Posthumanismus und Transhumanismus.

34:54.000 --> 35:07.000
Und da fasst sich so zusammen, dass der kritische Posthumanismus die tradierten und zumeist humanistischen Dichotomien, wie etwa Frau-Mann, Natur-Kultur oder Subjekt-Objekt hinterfragt.

35:07.000 --> 35:09.000
Also darum geht es so ein bisschen.

35:09.000 --> 35:21.000
Eine Autorin, auf die sie viel referenziert ist, Donna Haraway, die, glaube ich, Biologin und Verhaltensforscherin und Philosophin ist und die ganz lange schon zu Cyborgs und Tieren schreibt.

35:21.000 --> 35:31.000
Und genau, sie hat seit neuerem wohl mehr über Tiere und Tierethik und die auch einen eigenen Hund hat, den aber nicht als Haustier bezeichnet, sondern als Forschungsbegleiterin.

35:31.000 --> 35:34.000
Aber früher hat sie ja halt über Cyborgs offenbar geschrieben.

35:34.000 --> 35:44.000
Und die weiß daraufhin, dass Cyborgs natürlich irgendwie sind so kybernetische Organismen und ja, hybride aus Maschine und Organismus.

35:44.000 --> 35:49.000
Und die gibt es sowohl in der gesellschaftlichen Wirklichkeit als auch in der Fiktion.

35:49.000 --> 35:58.000
Also ja, und die These, die Haraway offenbar aufmacht und die Loh, glaube ich, nicht unsympathisch findet, ist, dass wir alle schon Cyborgs sind.

35:58.000 --> 36:12.000
Also wir benutzen irgendwie schon unsere Handys. Im Prinzip benutzen wir auch alle schon künstliche Dinge wie Pullover und Hosen und fahren eigentlich auch mit Straßenbahnen durch die Gegend und so.

36:12.000 --> 36:17.000
Also wenn man das ein bisschen weiter denkt, dann ist die Maschine schon Teil von uns.

36:17.000 --> 36:25.000
Nur um mal aufzuzeigen, man kann über diese Differenz von Maschine und Mensch anders denken, als es klassischerweise unterm Alltag getan wird.

36:25.000 --> 36:32.000
Ja, dann geht Loh noch so ein paar Beispiele durch. Und das will ich auch ein bisschen zumindest machen.

36:32.000 --> 36:38.000
Sie zitiert eine Person, die heißt Suchmann. Ich weiß nicht genau, wie man sie ausspricht.

36:38.000 --> 36:50.000
Die sagt zum Beispiel, na ja, man kann auch über Materie zum Beispiel anders denken und als man es gemeinhin tut, also die Differenz zwischen belebt und unbelebt ein bisschen auflösen.

36:50.000 --> 36:56.000
Das kam mir erst ein bisschen merkwürdig vor, weil Suchmann offenbar sagt, dass Materie diskursiv ist.

36:56.000 --> 37:04.000
Aber sie bringt als Beispiel eine kalifornische Rosine, die, wenn ich sie esse, mehr ist als eine kalifornische Rosine.

37:04.000 --> 37:16.000
Sondern wenn ich so eine Rosine esse, dann beschäftige ich mich oder habe Einfluss oder nehme in mich auch so Konzepte wie Kapitalismus einerseits,

37:16.000 --> 37:23.000
weil die Rosine wurde ja irgendwo erwirtschaftet und ich habe sie gekauft und so weiter, als auch Kolonialismus, also gerade in den USA.

37:23.000 --> 37:28.000
Und vielleicht auch Rassismus, denn wer hat diese Rosine zum Beispiel gepflückt?

37:28.000 --> 37:40.000
Also die Idee ist einfach das, was da aufgezeigt werden soll, ist, man kann anders über das Verhältnis von Materie und Mensch und unbelebt und belebt denken.

37:40.000 --> 37:45.000
Wobei da natürlich mal schnell bei dieser Ebene sind einfach kulturelle Überformungen sozusagen von Materie zu fassen.

37:45.000 --> 37:48.000
Das ist jetzt ja gar nicht so neu, darüber nachzudenken.

37:48.000 --> 37:50.000
Absolut, naja.

37:50.000 --> 37:55.000
Naja, und daher gibt es dann jetzt eben noch ein paar Mehrbeispiele, die gehe ich jetzt nicht noch alle durch.

37:55.000 --> 38:07.000
Genau, wer das Buch liest, sie zitiert häufig eine Person namens Körkelberth, denke ich, also C-O-E-C-K-E-L-B-E-R-G-H geschrieben.

38:07.000 --> 38:11.000
Der Person steht sie immer ziemlich positiv gegenüber und diesen Ansätzen.

38:11.000 --> 38:18.000
Wenn ich es richtig sehe, ist das ihr Vorgesetzter an der Uni Wien, also quasi ihr Chef.

38:18.000 --> 38:22.000
Das sollte man im Hinterkopf haben, wenn man das alles probiert, durchzubewerten.

38:22.000 --> 38:29.000
Also ich vermute, dass sie es auch wirklich gut findet, aber nur um zu wissen, dass sie einfach aus der gleichen Kerke herbekommt.

38:30.000 --> 38:37.000
Ihre Zwischenbilanz zu dem Ganzen ist dann jetzt erstmal, dass Roboter als moralische Akteure, hatte ich ja schon gesagt,

38:37.000 --> 38:43.000
sowas wie eigentlich Autonomie und sowas wie Lernfähigkeit brauchen, eigentlich auch Intentionalität,

38:43.000 --> 38:51.000
irgendeine Form von Urteilskraft und Verantwortung und das eben auffällig ist, dass immer von Menschen ausgedacht wird.

38:51.000 --> 38:59.000
Als Handlungsobjekte gelten sie im Normalfall eben nur dann, wenn sie auch Handlungsubjekte mal irgendwann sein sollten.

38:59.000 --> 39:07.000
Aber man könnte auch sagen, da wo eine emotionale Bindung entsteht zu etwas, zu jemandem, entsteht auch Moralität.

39:07.000 --> 39:09.000
Also die These macht sie auch auf.

39:09.000 --> 39:18.000
Also es gibt so relationale Moraltheorien, die eben sagen, da wo etwas zwischen zwei Wesen entsteht, sowas wie eine Bindung oder so,

39:18.000 --> 39:26.000
da ist eben auch Moral am Werk und das finde ich, kann man zumindest sehr gut an Tieren erkennen, da ist es ja auch nicht egal.

39:26.000 --> 39:31.000
Also wenn wir uns an die Binden, empfinden wir sowas wie eine moralische Verantwortung ziemlich schnell

39:31.000 --> 39:42.000
und warum sollte das bei Robotern eigentlich anders sein, weil ob die Kapazitäten, die gedanklich in sich so groß unterscheiden, sei mal dahingestellt.

39:42.000 --> 39:47.000
Das ist was, was du gerade auch bei diesem Beispiel mit dem Kunstwerk meinst, was mir gerade so ein bisschen reinkommt,

39:47.000 --> 39:56.000
dass dieser moralische Bezug gegenüber Objekten meist in erster Linie daran hängt, wie andere Menschen emotional an diese Objekte gebunden sind.

39:56.000 --> 39:59.000
Ja, das stimmt.

39:59.000 --> 40:06.000
Dass der Roboter, der in der Fabrik irgendwelche Autos zusammenbaut, da ist kein Mensch emotional daran gebunden.

40:06.000 --> 40:09.000
Deswegen fällt es uns da leicht zu sagen, ja komm schmeiß den halt auf den Müll.

40:09.000 --> 40:21.000
Wenn das jetzt aber der Objekt Liebe oder Objekt Sexuell sozusagen, wenn das irgendwie das, was auch immer das für ein Gegenstand ist,

40:21.000 --> 40:27.000
zu dem jemand irgendwie eine sehr enge emotionale Bindung aufgebaut hat, dann ist mit diesem Gegenstand umzugehen,

40:27.000 --> 40:31.000
ja einen mit diesen Menschen umgehen und dadurch dann sehr stark moralisch geprägt.

40:31.000 --> 40:36.000
Das finde ich gerade spannend, das ist nicht nur die Beziehung, wie gehe ich mit einem Gegenstand um,

40:36.000 --> 40:44.000
sondern auch wie sind andere Menschen moralisch an diesen Gegenstand oder emotional in diesen Gegenstand gebunden, die dann da mit ins Spiel laufen.

40:44.000 --> 40:50.000
Ich finde dabei durchaus auch zu beachten, gerade wenn es irgendwie um Kunstwerke und das Museumsbeispiel, was sich da gebracht hatte,

40:50.000 --> 40:59.000
oder auch um Romanenfiguren geht, ja also die Kunstwerke und Romanenfiguren referenzieren immer auch sehr direkt auf die Person,

40:59.000 --> 41:05.000
die diese Dinge erschaffen hat, also das steht ja irgendwie im Hintergrund, wenn ich ein Kunstwerk gegenüber trete,

41:05.000 --> 41:11.000
ist das ja auch immer der Person, die es erschaffen hat irgendwie, ja.

41:11.000 --> 41:17.000
Von da aus geht es dann eben über zu Robotern als Verantwortungs-Subjekten und ich muss sagen,

41:17.000 --> 41:24.000
mit meiner nicht großen philosophischen Vorbildung ist es mir da manchmal ein bisschen schwer gefallen irgendwie nachzuvollziehen,

41:24.000 --> 41:31.000
warum diese Trennung so stark gemacht wurde zwischen Moral-Subjekt und Objekt und Verantwortungs-Subjekt und Objekt,

41:31.000 --> 41:39.000
aber gut, nehmen wir es erstmal so hin, aber genau, rein von der Zugänglichkeit her für Leute, die vielleicht philosophisch nicht so bewandert sind,

41:39.000 --> 41:45.000
hätte ich mich da darüber gefreut, wenn das vielleicht einfach in einem Guss gewesen wäre und nicht so stark getrennt,

41:45.000 --> 41:48.000
aber ich probiere mal das irgendwie nachzuziehen.

41:48.000 --> 41:56.000
Na ja, da startet sie damit zu sagen, dass Verantwortung und die Möglichkeit zur Verantwortung an Akteurschaft gebunden ist

41:56.000 --> 42:03.000
und dass man Verantwortung da von Haftbarkeit unterscheiden sollte, also das ist nicht das Gleiche,

42:03.000 --> 42:11.000
also ein bisschen die Unterscheidung zwischen verantwortlichem Handeln einerseits, die irgendwie moralisch total aufgeladen ist

42:11.000 --> 42:20.000
und der rein begrenzten Verursachung, also auch, haben wir jetzt Beispiel, wenn ein Kind irgendwas kaputtmacht,

42:20.000 --> 42:27.000
dann ist es vielleicht haftbar zu machen dafür oder halt die Eltern, aber man würde nicht unbedingt sagen,

42:27.000 --> 42:34.000
dass es in dem Sinne moralisch verantwortlich ist, auch wenn es im Deutschen zumindest von der Sprache her manchmal so ein bisschen verwischt,

42:34.000 --> 42:36.000
also Verantwortung und Verursachung quasi.

42:36.000 --> 42:42.000
Interessant, da hätte ich jetzt, mein intuitives Begriffsverständnis wäre quasi genau andersrum gewesen,

42:42.000 --> 42:47.000
dass gerade derjenige, der verantwortlich ist, dafür haftbar ist, weil die Eltern zum Beispiel nicht genug auf das Kind aufgepasst haben.

42:47.000 --> 42:52.000
Genau, das wäre vermutlich das, aber sie haben es nicht verursacht, sondern das Kind hat verursacht.

42:52.000 --> 42:57.000
Genau, und deswegen Kind als Verursacher, Eltern als Verantwortliche, vielleicht.

42:59.000 --> 43:07.000
Ja, Lo macht ihre eigene Position auch deutlich, also sie sagt, dass manche Roboter die notwendigen Kompetenzen dafür haben,

43:07.000 --> 43:13.000
Verantwortung zu simulieren zumindest, was das dann von, das ist das, worüber wir gerade eben gesprochen haben,

43:13.000 --> 43:19.000
inwiefern man nachgucken kann, ob sie es nun wirklich verantwortlich sich irgendwann mal fühlen können, wissen wir nicht so genau.

43:19.000 --> 43:25.000
Ja, und dass Menschen absehbar erst mal besser für Verantwortungsübernahme qualifiziert sind, überrascht jetzt auch.

43:25.000 --> 43:33.000
Verantwortung ist dabei noch ein relativ modernes Konzept im Prinzip, was irgendwie so mit komplexen Gesellschaften entstanden ist.

43:33.000 --> 43:41.000
Also, weil man irgendwie nicht mehr direkt zuordnen konnte, wer welches Vergehen eigentlich ausgemacht hat,

43:41.000 --> 43:46.000
muss man darauf umstellen, quasi gesellschaftlich zu sagen, verhaltet euch mal verantwortlich,

43:46.000 --> 43:51.000
weil ansonsten haben wir hier irgendwie Dynamiken, die wir nicht mehr so richtig überblicken können.

43:51.000 --> 43:58.000
Aber auch da ist es doch wieder eigentlich extrem ähnlich wie die Haftbarkeit, weil bei der Haftbarkeit geht es ja auch wieder genau darum, wer ist denn jetzt haftbar dafür.

43:58.000 --> 43:59.000
Ja, total.

43:59.000 --> 44:02.000
Also für mich fällt dieses Verantwortung und Haftbarkeit extrem zusammen.

44:02.000 --> 44:05.000
Das hat sich mir noch nicht erschlossen, wie das unterschiedlich ist.

44:05.000 --> 44:14.000
Ja, ich glaube, das ist tatsächlich eine philosophische Unterscheidung, weil mit Verantwortung definitiv mehr einhergeht.

44:14.000 --> 44:22.000
Das wäre glaube ich ein bisschen die These, da hängt halt irgendwie moralisch gut und böse mit dran und an Haftbarkeit eben nicht so sehr.

44:23.000 --> 44:31.000
Aber Lo weist auch darauf hin, na ja, bei Verantwortung geht es eigentlich schon genuin darum, dass man ein konkretes Subjekt der Verantwortungsübernahme immer wieder ausmacht.

44:31.000 --> 44:32.000
Okay.

44:32.000 --> 44:42.000
Ich hatte als Gedanken dazu im Kopf, die Klagen in den USA gegen Volkswagen im Zuge dieses ganzen Abgasskandals und so weiter,

44:42.000 --> 44:48.000
weil da ja dann tatsächlich am Ende auch nicht, also die Firma Volkswagen kann nicht ins Gefängnis gehen.

44:48.000 --> 44:54.000
Die kann irgendwie Geld strafen oder so verantworten, aber das ist vielleicht auch gar nicht das, was manche Leute sehen wollen.

44:54.000 --> 45:00.000
Und das heißt, dass am Ende tatsächlich konkrete Manager und Managerinnen ins Gefängnis mussten.

45:00.000 --> 45:05.000
Also man probiert dann irgendwie auszumachen, die und die Person ist es jetzt gewesen.

45:05.000 --> 45:16.000
Auch wenn glaube ich relativ klar ist, dass das, was Lo für ein Verantwortungsnetzwerk ist, eigentlich das wäre, was viel Treffener beschreibt, wo da die Verantwortlichkeiten liegen.

45:16.000 --> 45:23.000
Das ist ja nicht der eine Manager, der jetzt irgendwie eigentlich verantwortlich ist, aber er wird zumindest haftbar gemacht.

45:23.000 --> 45:24.000
Ja klar.

45:24.000 --> 45:32.000
Ja, genau. Die Idee von Verantwortungsnetzwerken ist dann auch, dass alle beteiligten Parteien in einer Situation Verantwortung tragen.

45:32.000 --> 45:41.000
Das Problem ist, dass sowas entwickelt wurde, um irgendwie solche Situationen, wie ich es hier gerade eben beschrieben habe, zu bewältigen.

45:41.000 --> 45:51.000
Also rein philosophisch jetzt erstmal nicht justiziabel quasi, aber Lo weist darauf hin, dass wenn man probiert, eben Verantwortungsnetzwerke zu begreifen,

45:51.000 --> 45:56.000
gerade im Kontext von autonomen Systemen, also wie der Beispiel Autounfall, wer ist jetzt eigentlich verantwortlich?

45:56.000 --> 46:00.000
Das Auto kann es nicht so richtig sein, der, die Fahrerin vielleicht auch nicht.

46:00.000 --> 46:07.000
Und welchen Anteil hat die Person, der die Unfall zugestoßen ist, eigentlich selbst?

46:07.000 --> 46:13.000
Naja, und was haben die Ingenieure damit zu tun und die Programmierer, die das Programm geschrieben haben und so weiter?

46:13.000 --> 46:20.000
Also so Verantwortungsnetzwerke, wenn man wirklich erfassen will, wer eigentlich schuld an irgendetwas ist, werden riesig groß.

46:20.000 --> 46:27.000
Damit torpediert man quasi die Idee hinter Verantwortung wieder, nämlich dass man eine Person hat, die man haftbar machen kann.

46:27.000 --> 46:29.000
Ja, da fällt es nämlich doch wieder zusammen.

46:29.000 --> 46:38.000
Ja, total, ja. Und da unterscheidet man dann eben vor Gericht quasi zwischen Verantwortung und Haftbarkeit, würde ich sagen.

46:38.000 --> 46:43.000
Gehen wir über zu Verantwortungsobjekten so ein bisschen.

46:43.000 --> 46:50.000
Ja, da ist die Idee dahinter, dass selbst die komplexesten Maschinen nicht mehr als bloße Instrumente menschlichen handeln sind.

46:50.000 --> 46:59.000
Man kennt das von der National Rifle Association aus den USA, also nicht Waffen töten Menschen, sondern Menschen töten Menschen.

46:59.000 --> 47:02.000
Und da sagt Lou, naja, so einfach ist das nicht.

47:02.000 --> 47:10.000
Also eine Person mit einer Pistole ist ein Schütze, eine Schützin und eine Person ohne eine Pistole ist eine Person.

47:10.000 --> 47:15.000
Also sie denkt das schon eher zusammen. Und das Gleiche geht dann eben auch für Roboter.

47:15.000 --> 47:21.000
Sie nennt da einen Philosophen, der sich eigentlich mit Computersystemen beschäftigt und das schon relativ lange.

47:21.000 --> 47:34.000
Und der sieht Probleme im Verantwortungsbereich, in der Philosophie dann, wenn Computersysteme schneller und akkurater als Menschen in der Lage dazu sind, Entscheidungen zu treffen.

47:34.000 --> 47:43.000
Und man kann sich das durchaus vorstellen, gerade in, so nennt sie das, Delikaten Kontexten wie militärischen Frühwarnsystem, wo es dann darum geht,

47:43.000 --> 47:52.000
okay, ein Frühwarnsystem sieht, dass eine Rakete oder so auf einen zufliegt und muss dann darauf reagieren.

47:52.000 --> 47:59.000
Und das kann dann kein Mensch mehr machen, sondern das muss dann eben, ja, muss dann ein Computersystem machen.

47:59.000 --> 48:11.000
Da sieht dieser Hans Lenk eben ein Beleg dafür, dass die Abhängigkeit von Technologien dafür sorgt, dass Computersysteme auch heute schon Verantwortung und Verantwortung übernehmen.

48:11.000 --> 48:20.000
Die Frage ist aber, wenn man so ein Computersystem nicht haftbar machen kann, wer steht dann am Ende Rede und Antwort?

48:20.000 --> 48:21.000
Ja, genau.

48:21.000 --> 48:26.000
Also die Frage nach Haftbarkeit, Verantwortung kommt da dann eben wieder auf.

48:26.000 --> 48:35.000
Zusammenfassend kann man sagen, dass den verantwortlichen Subjekten generell eine moralische Verantwortung für die Produkte ihres Schaffens zugeschrieben wird.

48:35.000 --> 48:44.000
Also auch heute ist es noch so, dass Computersysteme, Roboter und so weiter quasi als verlängerte Arm ihrer ErschafferInnen gelten.

48:44.000 --> 48:45.000
Ja.

48:45.000 --> 48:54.000
Was ich gerade eben schon gesagt hatte, ist, dass es so Ideen gibt von relationalen Konzepten von Moral und die gibt es eben auch im Bereich der Verantwortung.

48:54.000 --> 49:02.000
Das heißt, inklusiven Ansätzen aufgemacht, dass Verantwortung sich ausschließlich in der Interaktion zwischen Wesen abspielt.

49:02.000 --> 49:09.000
Die Idee dahinter ist, dass das Narrativ vom autarken Handlungssubjekt eine gesellschaftliche, rechtliche und politische Illusion ist.

49:09.000 --> 49:11.000
Das ist wieder diese Haraway, die sie da zitiert.

49:11.000 --> 49:19.000
Also dass wir alle so wunderbar autark und autonom sind, ist vielleicht notwendig, dass wir uns das so denken.

49:19.000 --> 49:29.000
Aber faktisch ist das nicht so, weil wir natürlich irgendwie gesellschaftlich eingebunden sind und leben und manchmal gar nicht anders können, als die Situation es irgendwie zulässt.

49:29.000 --> 49:32.000
Ich glaube, wir als Soziologen können das ganz gut nachvollziehen.

49:32.000 --> 49:33.000
Ja.

49:33.000 --> 49:45.000
Ja, dabei muss man irgendwie immer wieder daran erinnert werden, finde ich zumindest, dass die Idee der Individualität dahinter und der entsprechenden Verantwortung und Verantwortungsübernahme noch relativ neu ist.

49:45.000 --> 49:48.000
Also Losak, die ist erst im Mittelalter entstanden.

49:48.000 --> 49:52.000
Das Gleiche gilt übrigens für Begriffe wie Identität.

49:52.000 --> 49:58.000
Also dass jeder Mensch eine eigene Identität hat, ist auch noch eine relativ moderne Erfindung.

49:58.000 --> 50:03.000
Und dass auch solche Individualitätsideen ziemlich kulturraum explizit sind.

50:03.000 --> 50:13.000
Sie meinen zum Beispiel, dass es im asiatischen Kulturraum gar nicht so verbreitet ist, dass man irgendwie die Idee hat, dass man irgendwie ein Individuum ist.

50:13.000 --> 50:19.000
Ein bisschen die Unterscheidung zwischen individualistischen und kollektivistischen Kulturen.

50:19.000 --> 50:22.000
Ich bin da nie in die Materie tief eingestiegen.

50:22.000 --> 50:27.000
Ich weiß nicht, wie gut sich das tatsächlich aufrechterhalten lässt.

50:27.000 --> 50:30.000
Also ich gebe das einfach nur erst mal so wieder.

50:31.000 --> 50:40.000
Aus der Idee von dieser interaktionsbasierten Verantwortung gibt es dann das Konzept der Extended Agency aus.

50:40.000 --> 50:47.000
Dass Interaktionen eben aus einer Menge von menschlichen und nichtmenschlichen Beteiligten bestehen.

50:47.000 --> 50:52.000
Und eine Handlung sich eben nicht mehr nur auf die menschlichen Komponenten bezieht.

50:53.000 --> 50:58.000
Fand ich auch ganz interessant für alle NichtsoziologInnen da draußen.

50:58.000 --> 51:09.000
Die klassische Definition von Handlung ist nämlich nach Max Weber in der Soziologie eigentlich eben, dass sie mit Intention passiert und genuin menschlich ist.

51:09.000 --> 51:13.000
Also Tiere können sich zum Beispiel verhalten, aber nur Menschen können handeln.

51:13.000 --> 51:19.000
Da gibt es eben offenbar Ansätze, die probieren das ein bisschen anders zu sehen.

51:20.000 --> 51:28.000
Wenn da diese Begriffe so das Gegenstände handeln können, das erinnert mich ein bisschen an Latours aktuellen Netzwerktheorie.

51:28.000 --> 51:29.000
Ja ja witzig.

51:29.000 --> 51:34.000
Das Latour taucht auf, da ich Latour nicht kenne, gebe ich dir hier nicht so richtig wieder.

51:34.000 --> 51:39.000
Aber genau darauf wird offenbar viel auch referenziert in der Literatur.

51:40.000 --> 51:49.000
Ja, so wie ich das jetzt gerade eben mal probiert habe, irgendwie knapp zusammenzufassen, gibt es einige Schwierigkeiten, die da auftauchen.

51:49.000 --> 51:52.000
Und ich finde, die liegen auch relativ deutlich klar.

51:52.000 --> 51:59.000
Autonomer werdende Systeme machen es zunehmend schwieriger, Verantwortlichkeiten zuzurechnen.

51:59.000 --> 52:08.000
Also wenn wir mehr Systeme haben, die irgendwie in einem Zwischenraum zwischen ich mache so ein bisschen, was mir einprogrammiert wurde, ein bisschen, was ich irgendwie autonom gelernt habe.

52:08.000 --> 52:16.000
Und aber es sind auch noch Menschen involviert, macht es irgendwie ganz schwierig zu sagen, wer ist denn jetzt eigentlich schuldig, haftbar, verantwortlich für irgendwelche Handlungen.

52:18.000 --> 52:32.000
Und auch in diesen Relationskonstrukten von Verantwortung, wo man sagt, ok, wenn Dinge, Tiere, Roboter mit Menschen agieren oder auch Dinge mit Dingen oder Menschen mit Menschen, nur da entsteht Verantwortung,

52:32.000 --> 52:42.000
ist, dass man da durch diese neuen technischen Systeme auch so Verantwortungslücken in den normativen Kriterien hat, weil man irgendwie für diese technischen Systeme gar nicht so richtig weiß, wie man das alles bewerten soll.

52:42.000 --> 52:43.000
Ja, stimmt.

52:43.000 --> 52:52.000
Ja, ja. Und worauf Lo noch hinweist, das fand ich noch ganz spannend, ist, dass sie sagt, wir erleben auch sowas wie neue Raum-Zeit-Dimensionen im Kontext.

52:52.000 --> 53:02.000
Also sie sagt, die Rückbindung, globale Ereignisse, sie nennt Fluchtbewegungen oder auch den Klimawandel als Beispiele an einzelne Gruppen von Akteurinnen wird schwieriger.

53:02.000 --> 53:09.000
Also man kann nicht mehr so richtig sagen, wer ist denn jetzt beim Klimawandel eigentlich, wer ist daran jetzt schuld, wem ziehen wir dafür zu Rechenschaften.

53:09.000 --> 53:21.000
Oder wenn wir uns komplexe Migrations- und Fluchtbewegungen angucken, wie kann man sie einerseits vielleicht besser koordinieren, wie kann man das steuern, wie kann man dafür sorgen,

53:21.000 --> 53:26.000
dass die Bedingungen deren Wegen geflohen wird, dass die geändert werden.

53:26.000 --> 53:34.000
Auch das ist total schwierig da irgendwie zuzurechnen, wer da zur Verantwortung gezogen werden kann.

53:34.000 --> 53:39.000
Wobei ich da sagen würde, das Genuin hängt jetzt nicht mit Robotern oder auch mit artificiellen Systemen zusammen.

53:39.000 --> 53:41.000
Nur einfach mit unglaublich komplexen Wirkketten.

53:41.000 --> 53:49.000
Ja, genau. Die Frage, die ich mir dabei jetzt am Ende gestellt habe, ist, wenn man jetzt so Verantwortungskonzepte irgendwie erweitert,

53:49.000 --> 53:54.000
probiert in Netzwerke zu integrieren oder auch Roboter zur Verantwortung ziehen möchte.

53:54.000 --> 54:04.000
Ich frage mich ein bisschen, was dadurch gewonnen werden kann, weil ich das Gefühl habe, dass Verantwortungsaufforderungen oder so,

54:04.000 --> 54:11.000
oder die sich eben gerade dadurch da ausdrücken, dass sie nicht immer mit einer rechtlichen Keule um die Ecke kommen,

54:11.000 --> 54:17.000
sondern sagen, verhaltet euch mal verantwortlich, weil es kommen noch Leute nach euch, wir müssen an unsere Kinder denken,

54:17.000 --> 54:23.000
beim Klimawandel oder was auch immer. Und ich frage mich so ein bisschen, gewinnt man eigentlich praktisch irgendwas dadurch,

54:23.000 --> 54:26.000
wenn man das alles so weit zieht quasi?

54:26.000 --> 54:32.000
Ja, das habe ich auch gerade mal nachgedacht, weil ich mir diesen Begriff der Verantwortung überlegt habe,

54:32.000 --> 54:37.000
gerade wenn sie ihn von der Haftbarkeit trennen so stark. Was heißt das überhaupt?

54:37.000 --> 54:43.000
Ich meine, Verantwortung wirkt ja im Grunde als Handlungsprägen, kann das ja nur über mögliche Konsequenzen wirken.

54:43.000 --> 54:49.000
Es können auf der einen Seite sein die Haftbarkeit, die klassische rechtliche, das kann aber auch so etwas sein wie sozialer Status,

54:49.000 --> 54:58.000
wie irgendwie so ein, das bin nicht ich, so ein Selbstachtungsverlust oder Scham oder sowas in der Art.

54:58.000 --> 55:04.000
Aber das kann ja immer nur über diese Wirkmechanismen sozusagen nach außen treten.

55:04.000 --> 55:10.000
Dann ist natürlich die Frage, wie weit die bei irgendwelchen robotischen Systemen sozusagen wirken können.

55:10.000 --> 55:12.000
Kann ein Roboter Scham empfinden?

55:12.000 --> 55:19.000
Ja, das finde ich sind auch so die spannenden Fragestellungen. Das ist mir nicht so richtig klar.

55:19.000 --> 55:25.000
Ich finde, das, was ich gelesen habe, weist eigentlich eher in die Richtung von, es ist zumindest erstmal nicht abzusehen,

55:25.000 --> 55:31.000
dass da sowas wie genuines Bewusstsein wirklich ist. Und dann ist für mich ein bisschen die Frage, warum das alles?

55:31.000 --> 55:39.000
Weil Roboter haben auch absehbar kein eigenes Konto, von dem du irgendwelche Dinge bezahlen kannst.

55:39.000 --> 55:43.000
Das gleiche gilt auch für autonome Fahrsysteme.

55:43.000 --> 55:49.000
Wem ist damit geholfen, wenn du ein Auto für einen Unfall zur Rechenschaft ziehen kannst?

55:49.000 --> 55:53.000
Genau, was heißt das überhaupt ein Auto zur Rechenschaft zu ziehen?

55:53.000 --> 55:56.000
Dann darf das dann drei Monate nicht fahren, weil es ihm ja Spaß macht zu fahren.

55:56.000 --> 55:59.000
Ja, das finde ich auch, das ist irgendwie total schwierig.

55:59.000 --> 56:04.000
Kriegt das irgendwie ein fiktives Gehalt für jeden Kilometer, den es fährt, und muss dann davon wieder was...

56:04.000 --> 56:07.000
Also irgendwie so richtig erschließt sich mir das nicht.

56:07.000 --> 56:11.000
Diese Wirkmechanismen, die fehlen da irgendwie komplett, so Sanktionsmechanismen im Grunde.

56:11.000 --> 56:13.000
Ja, das stimmt.

56:13.000 --> 56:14.000
Auf welcher Ebene auch immer.

56:14.000 --> 56:20.000
Vielleicht muss man das Ganze auch eher als wirklich, ja Gedankenspiel finde ich, ist immer so ein bisschen respektierlich,

56:20.000 --> 56:28.000
aber vielleicht muss man es einfach als wirklich philosophische Überlegung erstmal auch für sich stehen lassen und das als das begreifen.

56:28.000 --> 56:34.000
Also zu sagen, okay, die philosophische Überlegung darüber, was bedeutet Verantwortung, was bedeutet Moral,

56:34.000 --> 56:40.000
in quasi außerhumanen Kontexten ist ein Erkenntnisgewinn für sich.

56:40.000 --> 56:47.000
Ich glaube, wenn man so darauf guckt, wird man dem vielleicht eher gerecht.

56:47.000 --> 56:53.000
Wobei es mir gerade so spontan einfällt, wenn man so Sanktionen irgendwie auch als Lernreiz sozusagen versteht.

56:53.000 --> 56:57.000
Also man sagt, man verheckt eine Sanktion, damit die Person das nicht wieder tut.

56:57.000 --> 57:02.000
Oder im Vorhinein als abschreckend. Das kann man ja wiederum in Computersystemen sehr gut abbilden.

57:02.000 --> 57:07.000
Gerade wenn du so ein selbstlernendes System hast und das bringt irgendwie jemand um, ihm dann ganz böse auf den Finger zu hauen und sagen,

57:07.000 --> 57:12.000
ne, ne, ne, das machst du nie wieder. Das könnte man ja sogar technisch relativ gut abbilden.

57:12.000 --> 57:16.000
Ja, vielleicht ist, ja, das stimmt. Das könnte ganz gut funktionieren.

57:16.000 --> 57:25.000
Gerade wenn die These ist, dass eben Top-Darren-Programmierung offenbar, weil das alles so zwischen den abstrakten Regeln und dem konkreten Verhalten,

57:25.000 --> 57:30.000
dass da so eine große Diskrepanz ist, dass eben Top-Darren nicht dauerhaft gut funktioniert.

57:30.000 --> 57:37.000
Ja, also dann müsste man eben als Regel einprogrammieren, dass sich XY nicht gut anfühlt für das Auto.

57:37.000 --> 57:40.000
Genau, was auch immer nicht gut anfühlen dann wieder heißt.

57:40.000 --> 57:47.000
Genau, aber das kann man ja, das wäre die These dahinter. Und das wissen wir aber eigentlich schon für andere Leute nicht.

57:47.000 --> 57:53.000
Also wir glauben halt nur zu wissen, dass die uns ähnlich, dass die so ähnlich fühlen wie wir. Spannend, spannend.

57:53.000 --> 58:04.000
Naja, kritische Zwischenbilanz zu dem ganzen Verantwortungsthema ist, naja, Akteurschaft ist irgendwie notwendig, um überhaupt Verantwortung zurechnen zu können.

58:04.000 --> 58:10.000
Hinter dem Ganzen steht irgendwie die Befürchtung, dass wir Roboter irgendwann nicht mehr kontrollieren können.

58:10.000 --> 58:18.000
Also wir vertrauen unseren eigenen Geschöpfen quasi nicht mehr so richtig. Und man kann feststellen, dass Menschen zweierlei Verantwortung haben.

58:19.000 --> 58:27.000
Also einmal sehr individuell als Designer innen von autonomen Systemen oder Robotern oder Programmiererinnen und so weiter.

58:27.000 --> 58:33.000
Aber auch kollektiv als Unternehmen, die bestimmte Roboter oder nicht Roboter einsetzen.

58:33.000 --> 58:41.000
Oder auch in Form von Ethikgremien, die irgendwas in Bezug auf diese ganze Roboter-Thematik entscheiden oder nicht entscheiden.

58:42.000 --> 58:54.000
Und ja, was sie sagt, was bei dem ganzen klar wird, ist, Bedürfnis der modernen Gesellschaft, alles immer kontrollieren zu können, wird auch bei Robotern offenbar.

58:54.000 --> 59:02.000
Warum sie das so hervorhebt, weiß ich nicht so ganz genau, weil ich das Gefühl habe, es gibt schon sehr, ich weiß nicht, vielleicht ist es einfach nur eine philosophische Feststellung.

59:02.000 --> 59:12.000
Ich hatte erst mal das Gefühl, na ja, es gibt schon gute Gründe, die die die Resultate des eigenen Schaffens, die Ergebnisse kontrollieren können zu wollen.

59:15.000 --> 59:19.000
Genau, dann schließt sie das Buch mit klassischen abschließenden Bemerkungen.

59:19.000 --> 59:26.000
Das gibt es ja häufiger mal, dass dann eben so ein bisschen nochmal quasi in die Zukunft gedacht werden soll oder ein bisschen praktischer.

59:26.000 --> 59:30.000
Hier in dem Fall ist jetzt so ein bisschen politische Implikation des Ganzen.

59:30.000 --> 59:36.000
Sie wiederholt ihre Aussage, dass Produkte des menschlichen Handelns moralisch nie neutral sind.

59:36.000 --> 59:45.000
Das gibt es eben im Bereich der Technik-Philosophie offenbar diese Position, na ja, es ist halt Technik, die kann man gut oder schlecht verwenden.

59:45.000 --> 59:57.000
Und wenn sie eben schlecht verwendet wird, ist das zwar problematisch, aber die problematische Verwendung liegt eben bei Menschen und nicht in der Technik.

59:57.000 --> 01:00:04.000
Und sie sagt, na ja, Vorsicht, Dinge sind eben moralisch immer aufgeladen, weil wir eben nach gewissen Normen agieren.

01:00:04.000 --> 01:00:16.000
Das gibt es offenbar auch schon länger, das Konzept, also so Gedanken wie zum Beispiel, wenn wir uns einen Tisch vorstellen, der ein schmales Kopfende hat und dann eine lange Tafel hat,

01:00:16.000 --> 01:00:22.000
dann drückt das im Prinzip schon eine hierarchische Struktur aus, die Vorsicht, dass jemand am Kopfende sitzt und Chef ist.

01:00:22.000 --> 01:00:29.000
Ja, es ist ja auch bei den diplomatischen Verhandlungen, dass dann die kreisrunden Tische eingesetzt werden und solche Sachen, die genau das eben abbilden.

01:00:29.000 --> 01:00:35.000
Na ja, Forderungen, die sie am Ende auf jeden Fall stellt, sind mehr Ethik- und Informationsunterricht in Schulen.

01:00:35.000 --> 01:00:44.000
Sie sagt, Ingenieurswissenschaftliche Ausbildung sollte mit Ethikpflichtkursen behaftet sein, weil sie, also sie nimmt da das Beispiel der Medizin.

01:00:44.000 --> 01:00:56.000
Man stelle sich nur mal vor, bei Dingen wie der Präimplantationsdiagnostik würden Ärzte und Ärztinnen irgendwie beraten, ohne je einen Ethikpflichtkurs im Bereich gemacht zu haben.

01:00:56.000 --> 01:01:02.000
Und sie meint, na ja, wenn jetzt irgendwie Roboter entwickelt werden, muss man auch die ethischen Hintergründe kennen.

01:01:02.000 --> 01:01:10.000
Und sie sagt, auch Unternehmen sollten Weiterbildungskurse im Bereich der Technik- und Roboterethik anbieten oder verpflichtend machen.

01:01:10.000 --> 01:01:19.000
Und sie meint, wir brauchen auch zunehmend öffentliche Ethikgremien, die zum Thema Roboter irgendwie beschlagen und bewandert sind.

01:01:19.000 --> 01:01:31.000
Und sie spricht sich als, ich glaube, kann man durchaus so sagen, linke Philosophin für eine Diskursöffnung aus, für die Vermittlung von Sachverstand, Reflektions- und Urteilskraften auf allen Ebenen, für alle Mitglieder einer Gesellschaft.

01:01:32.000 --> 01:01:46.000
Was natürlich demokratischen Idealen absolut entspricht, dass man, wenn man sagt, okay, wir müssen darüber reden, wie wir mit Robotern umgehen, was sie dürfen, was sie nicht dürfen, was sie ersetzen sollen oder auch nicht, dann sollten da auch möglichst viele an diesem Diskurs beteiligt sein.

01:01:46.000 --> 01:02:01.000
Sie hat auch bei Rahel Yagi promoviert, glaube ich, und das ist auf jeden Fall auch eine ziemlich interessante Person und auch auf jeden Fall aber linke Philosophin, die ursprünglich, glaube ich, so aus der Hausbesetzer-Szene kommt.

01:02:01.000 --> 01:02:30.000
Genau. Was sie sagt, was manchmal ein bisschen schwierig ist in dieser ganzen Diskussion, gerade so im Bereich, wenn es um Superintelligenzen und so geht, ist, dass sie sagt, okay, es wird häufig so getan, als sei gesetzt, dass sowas wie eine Superintelligenz irgendwann entsteht, die deutlich cleverer, schneller, ethischer, besser wie auch immer ist als der Mensch oder halt völlig dystopisch.

01:02:30.000 --> 01:02:40.000
Die alles kaputt macht und den Menschen überholt und uns braucht es dann gar nicht mehr. Und sie meint, das sind alles keine Naturgesetze, also es muss nicht alles passieren, was möglich ist.

01:02:40.000 --> 01:02:57.000
Wir kennen das gerade in Deutschland, dass einige Technologien, die theoretisch möglich sind, wie zum Beispiel das Klonen oder so oder das, was immer wieder unter so Designer-Baby oder so filmiert, sowas wäre ja im Prinzip technisch möglich, zumindest grundlegend.

01:02:57.000 --> 01:03:04.000
Das ist mit guten Gründen verboten und das kann man auch im Bereich der Roboter- und Computersystem-Ethik und so weiter auch fortführen.

01:03:04.000 --> 01:03:11.000
Also es ist nicht ausgemacht, dass bestimmte Technologien irgendwann real werden und sie bittet darum, dass man sich das mal vor Augen führt.

01:03:11.000 --> 01:03:16.000
Und das fand ich einen ganz schönen Hinweis, weil ich das tatsächlich bislang auch, glaube ich, wenig reflektiert habe.

01:03:16.000 --> 01:03:30.000
Also mir schien das auch immer so wie ausgemacht, naja, irgendwann gibt es halt so eine, ja, wird es den Punkt geben, wo Computer allumfassend besser und klüger handeln können als wir.

01:03:30.000 --> 01:03:36.000
Mal gucken, nur wann der kommt. Und sie meint, naja, das muss nicht so kommen. Wir können uns auch schon dagegen entscheiden, mit guten Gründen.

01:03:36.000 --> 01:03:37.000
Stimmt.

01:03:37.000 --> 01:03:42.000
Ja, das wäre es von mir zu diesem Buch. Hast du Fragen, Anmerkungen, Kritik?

01:03:42.000 --> 01:03:45.000
Ja, ich glaube, wir haben ja schon währenddessen ziemlich viel diskutiert.

01:03:45.000 --> 01:03:52.000
Deswegen, da ist, glaube ich, genug Anknüpfungspunkte auch für euch da draußen zum auch mal drüber nachdenken.

01:03:52.000 --> 01:03:55.000
Mich hat es auf jeden Fall viel zu denken gebracht. Spannend.

01:03:55.000 --> 01:03:56.000
Schön.

01:04:00.000 --> 01:04:06.000
Bist du zwischendurch schon auf Bücher, Vorschläge des Weitermachens gestoßen oder soll ich das eben noch machen und du übernehmst da nach?

01:04:06.000 --> 01:04:13.000
Ja, ich habe tatsächlich schon Ideen gehabt, auch kurz davor, als ich mir vor der Episode mal kurz zwei Minuten angeguckt hatte, worum es überhaupt geht.

01:04:13.000 --> 01:04:23.000
Das ist einmal tatsächlich das Sachbuch, was ich gerade vor dem Zettelkastenprinzip zu Ende gelesen habe, nämlich Life in Code von Ellen Ullman.

01:04:23.000 --> 01:04:31.000
Und das ist im Grunde so eine Essay-hafte Autobiografie von einer Frau, die halt irgendwie seit den 80er Jahren in der Softwareentwicklung gearbeitet hat.

01:04:31.000 --> 01:04:39.000
Und im Grunde so dann auch als erfahrene Programmiererin so ein bisschen in die Start-up-Kultur reingerutscht ist und die beobachtet hat.

01:04:39.000 --> 01:04:50.000
Und sie hat an einer Stelle, gibt es eine Stelle, wo sie eben drüber nachdenkt, wie es die Welt mit Robotern sozusagen aussieht, wenn man die irgendwie noch menschlicher behandelt.

01:04:50.000 --> 01:04:55.000
Und da überlegt sie dann, was denn eigentlich für Roboter heißt, dass sie genießen.

01:04:56.000 --> 01:05:03.000
Dass sie ein Essen oder irgendetwas genießen, dass es dann irgendwie darum geht, wie das Kühlmittel ihre Rohre durchzieht oder so.

01:05:03.000 --> 01:05:05.000
Das fand ich ein sehr, sehr schönes Bild.

01:05:05.000 --> 01:05:09.000
Und sie widmet sich eben auch so dieser Perspektive, Technik macht die Welt besser.

01:05:09.000 --> 01:05:12.000
Das ist das, was du auch dahin, was du auch so angesprochen hast.

01:05:12.000 --> 01:05:17.000
Das ist jetzt kein hardcoreiges Sachbuch mit irgendwelchen durchargumentierten Punkten oder so.

01:05:17.000 --> 01:05:23.000
Aber vielen interessanten Gedanken über das, was wir so seit 20 Jahren über Technik denken.

01:05:23.000 --> 01:05:27.000
Und wie Technik unsere Gesellschaft geprägt und verändert hat.

01:05:27.000 --> 01:05:29.000
Mit vielen persönlichen Anekdoten und so weiter drin.

01:05:29.000 --> 01:05:31.000
Aber es ist wirklich ein echt spannendes Buch.

01:05:31.000 --> 01:05:33.000
Life in Code von Alan Ullman.

01:05:33.000 --> 01:05:34.000
Und was natürlich...

01:05:34.000 --> 01:05:35.000
Entschuldigung.

01:05:35.000 --> 01:05:37.000
Nee, wirklich.

01:05:37.000 --> 01:05:38.000
Das klingt sehr inspirierend, finde ich.

01:05:38.000 --> 01:05:39.000
Ja, schön.

01:05:39.000 --> 01:05:45.000
Und was natürlich, wenn es um Roboter geht, rührt das an meiner Science-Fiction-Seele.

01:05:45.000 --> 01:05:47.000
Also Science-Fiction-Tipps, Weltenflüstern und so.

01:05:47.000 --> 01:05:49.000
Mein zweiter Podcast.

01:05:49.000 --> 01:05:51.000
Zwei Bücher, die mir da spontan eingefallen sind.

01:05:51.000 --> 01:05:53.000
Das ist einmal ein Klassiker.

01:05:53.000 --> 01:05:54.000
Der 200-Jährige.

01:05:54.000 --> 01:05:57.000
Ursprünglich mal von Isaac Asimov als Kurzgeschichte geschrieben.

01:05:57.000 --> 01:06:00.000
Sowieso Isaac Asimov, der große Roboter, Science-Fiction-Autor.

01:06:00.000 --> 01:06:03.000
Und dann von Robert Silverberg zum Roman ausgearbeitet.

01:06:03.000 --> 01:06:07.000
Und auch mit, äh, nicht doch mit Dustin Hoffman verfilmt.

01:06:07.000 --> 01:06:12.000
Da geht es im Grunde um einen Roboter, der als Mensch anerkannt werden will.

01:06:12.000 --> 01:06:15.000
Mit relativ klassischem Motiv so in den 70er Jahren.

01:06:15.000 --> 01:06:17.000
In der Science-Fiction.

01:06:17.000 --> 01:06:20.000
Und dann gibt es einen Roman zwischen zwei Sternen von Becky Chambers.

01:06:20.000 --> 01:06:24.000
Sowieso eine ganz tolle Autorin, von der lohnen sich im Grunde alle Romane.

01:06:24.000 --> 01:06:27.000
Aber inzwischen zwei Sternen geht es im Grunde um genau das gegenteilige Problem.

01:06:27.000 --> 01:06:31.000
Um einen Roboter, der sich als Mensch ausgeben muss, das aber eigentlich nicht will.

01:06:31.000 --> 01:06:33.000
Oh, das ist auch gut.

01:06:33.000 --> 01:06:36.000
Also es ist ein schöner Twist zu diesem sehr klassischen Motiv.

01:06:36.000 --> 01:06:39.000
Also diese beiden Romane, der 200-Jährige und zwischen zwei Sternen,

01:06:39.000 --> 01:06:41.000
die kann man glaube ich auch mal direkt hintereinander lesen,

01:06:41.000 --> 01:06:43.000
um diesen Unterschied sich ein bisschen deutlich zu machen.

01:06:43.000 --> 01:06:47.000
Auch wie sich die Science-Fiction als Genre verändert hat, lustigerweise.

01:06:47.000 --> 01:06:48.000
Das ist gut.

01:06:48.000 --> 01:06:54.000
Das kommt tatsächlich in dem Buch, also Roboter-Ethik auch, zwischendurch eben mal vor.

01:06:54.000 --> 01:06:57.000
Ich glaube bei dieser Haraway eben, wo so ein bisschen aufgemacht wird,

01:06:57.000 --> 01:07:01.000
dass so die Grenzen zwischen Science-Fiction und Science irgendwie

01:07:01.000 --> 01:07:07.000
und der Wirklichkeit absolut verwischen und gar nicht so klar sind,

01:07:07.000 --> 01:07:10.000
wie man das vielleicht sich manchmal vorstellt.

01:07:10.000 --> 01:07:13.000
Ich finde, man kann das jetzt auch gerade, merkt man das wieder,

01:07:13.000 --> 01:07:26.000
dass so die Vorstellungen von Dystopien auch massiv von Filmen und Büchern geprägt sind.

01:07:26.000 --> 01:07:30.000
Schönen Artikel zu, den ich auch gerne den Jones verlinken kann.

01:07:30.000 --> 01:07:31.000
Ja, sehr gut.

01:07:31.000 --> 01:07:34.000
Was mir noch gerade spontan einfällt, weil ich gerade Isaac Asimov erwähnt habe,

01:07:34.000 --> 01:07:39.000
tauchen in dem Buch irgendwo seine drei Robotik-Gesetze auf,

01:07:39.000 --> 01:07:41.000
zumindest mal in der Fußnote oder in einem Nebensatz?

01:07:41.000 --> 01:07:49.000
Warte, ich gehe mal ganz kurz hinten ins Literaturverzeichnis, aber zitiert ist

01:07:49.000 --> 01:07:53.000
The Complete Robot, The Definitive Collection of Robot Stories.

01:07:53.000 --> 01:07:55.000
Okay, da wird es definitiv drin sein.

01:07:55.000 --> 01:07:59.000
Ja, aber mir ist das jetzt zumindest nicht aufgefallen.

01:07:59.000 --> 01:08:06.000
Weil es gibt irgendwie drei Roboter-Gesetze, ich weiß nicht, ob ich sie jetzt gerade auswendig hinkriege.

01:08:06.000 --> 01:08:10.000
Das erste Gesetz ist auf jeden Fall, Roboter dürfen Menschen niemals schaden.

01:08:10.000 --> 01:08:11.000
Ja.

01:08:11.000 --> 01:08:15.000
Das zweite Gesetz ist, glaube ich, Roboter müssen Menschen immer helfen.

01:08:15.000 --> 01:08:21.000
Und das dritte Gesetz ist, Roboter müssen immer die Befehle ihres Inhabers oder wie auch immer gehorchen.

01:08:21.000 --> 01:08:26.000
Jeweils darf dadurch das vorherige Gesetz aber nicht verletzt werden.

01:08:26.000 --> 01:08:31.000
Also du musst zwar Befehle befolgen, darfst dadurch aber niemandem schaden, sozusagen.

01:08:31.000 --> 01:08:37.000
Das ist so ein Versuch, damit umzugehen, aber da verlinken wir euch auch die korrekten Sachen in den Show Notes.

01:08:37.000 --> 01:08:38.000
Sehr gut.

01:08:38.000 --> 01:08:44.000
Ich finde auch da merkt man, wie das so, also Asimov eigentlich als Science Fiction Autor,

01:08:44.000 --> 01:08:49.000
wie das dann aber vermutlich jetzt auch in die Wirklichkeit von Roboter-Programmierung zurück spielt.

01:08:49.000 --> 01:08:50.000
Natürlich.

01:08:50.000 --> 01:08:56.000
Und das halt eben nicht irgendwie der Science Fiction Autor bleibt, sondern halt in die Realität einfach zurückgeht.

01:08:56.000 --> 01:09:02.000
Naja, im Vorlauf zu diesem Buch oder während ich es gelesen habe, habe ich mit Jennifer,

01:09:02.000 --> 01:09:06.000
mit der ich auch das Soziologische Kaffeekränzchen zusammen mache und die auch einen eigenen Podcast hat,

01:09:06.000 --> 01:09:09.000
den Föllefanzcast, auch darüber gesprochen ein bisschen.

01:09:09.000 --> 01:09:13.000
Und sie meinte, naja, wenn du hier diese Bücher da empfiehlst und vorschlägst,

01:09:13.000 --> 01:09:17.000
schlag doch mal mit vor Maschinen wie ich von Ian McEwan.

01:09:17.000 --> 01:09:24.000
Habe ich jetzt selbst nicht gelesen, scheint momentan aber irgendwie ganz schön viel Bass quasi drum zu sein.

01:09:24.000 --> 01:09:27.000
Und ich weiß nicht, du weißt da glaube ich ein bisschen mehr.

01:09:27.000 --> 01:09:29.000
Ich habe den Roman selber auch nicht gelesen.

01:09:29.000 --> 01:09:34.000
Er wird so in Science Fiction Kreisen so ein bisschen kritisch gesehen, weil das jetzt wieder so ein Fall ist,

01:09:34.000 --> 01:09:41.000
in Anführungszeichen literarischer Autor macht ein Thema auf, was die Science Fiction seit 30, 40 Jahren diskutiert.

01:09:41.000 --> 01:09:45.000
Und bei der Science Fiction hört irgendwie nie jemand zu und jetzt springen auf einmal alle auf diesen Autoren.

01:09:45.000 --> 01:09:48.000
Und er ist so der große Experte für das Thema.

01:09:48.000 --> 01:09:53.000
Dadurch wird das da so ein bisschen kritisch gesehen. Der Roman an sich soll aber tatsächlich relativ gut sein.

01:09:53.000 --> 01:09:56.000
Okay, das ist ja schon mal ganz schön.

01:09:56.000 --> 01:10:01.000
Was ich vor einigen Jahren gelesen habe, ist Germany 2064 von Martin Walker,

01:10:01.000 --> 01:10:05.000
der seinerseits schotte ist, wenn ich es jetzt vorher richtig nachgeschlagen habe,

01:10:05.000 --> 01:10:11.000
finde ich einen ganz schönen Zukunfts Roman oder Thriller oder wie man es nennen möchte.

01:10:11.000 --> 01:10:16.000
Das ist im Prinzip ein Kriminalfall. Ich habe aber vergessen, worum es um eigentlichen Krimi geht, also was der eigentliche Kriminalfall ist.

01:10:16.000 --> 01:10:25.000
Aber es gibt eben einen Inspector, der 2064 an seiner Seite als engsten Mitarbeiter einen Roboter hat,

01:10:26.000 --> 01:10:34.000
der, glaube ich, direkt zu Beginn quasi einen Upgrade erfährt, weil er in einem vorherigen Kriminalfall zu Bruch gegangen ist

01:10:34.000 --> 01:10:37.000
und danach deutlich intelligenter ist, als er es vorher ist.

01:10:37.000 --> 01:10:41.000
Und die beiden sind auch gut befreundet miteinander, also der Roboter und der Inspector.

01:10:41.000 --> 01:10:44.000
Und die Welt, in der das Ganze stattfindet, ist eben Deutschland.

01:10:44.000 --> 01:10:50.000
Und Deutschland ist so ein bisschen zweigeteilt, einerseits ein Hochtechnologieland mit allem, was dazugehört,

01:10:50.000 --> 01:10:56.000
irgendwie fliegende Autos, selbstfahrende Autos und kluge Maschinen wie eben auch diese Roboter.

01:10:56.000 --> 01:11:00.000
Es gibt aber parallel dazu auch Kommunen, die dem Ganzen entsagt haben.

01:11:00.000 --> 01:11:04.000
Das muss man sich, glaube ich, vorstellen wie so ein paar Enklaven quasi in Deutschland.

01:11:04.000 --> 01:11:10.000
Nicht wie richtig zweigeteilt, es gibt ein Land, das ist so, ein Land, das ist so, sondern es gibt eben so ein paar Kommunen,

01:11:10.000 --> 01:11:15.000
die machen mal dem ganzen Kram nicht mit, die sind irgendwie wieder ziemlich zurück auf ganz normale Landwirtschaft,

01:11:16.000 --> 01:11:20.000
haben, glaube ich, auch so rudimentäre Technik. Ich glaube, die haben sich irgendeinen Stichtag gesetzt,

01:11:20.000 --> 01:11:25.000
irgendwas in den 90er Jahren oder so. Bis dahin benutzen sie alles und danach nicht mehr.

01:11:25.000 --> 01:11:30.000
Also es ist aber gar kein großes Gegeneinander, aber es spielt eben in beiden Welten so ein bisschen.

01:11:30.000 --> 01:11:35.000
Und ich glaube, der Kommissar muss eben in beiden Bereichen ein bisschen ermitteln.

01:11:35.000 --> 01:11:38.000
Das habe ich damals auf jeden Fall gerne gelesen.

01:11:38.000 --> 01:11:41.000
Und was ich noch mit empfehlen würde, ist ein Interview.

01:11:41.000 --> 01:11:44.000
Ich weiß nicht, ich glaube, es ist ungefähr eine halbe Stunde oder ein bisschen mehr.

01:11:44.000 --> 01:11:53.000
Janina Loh war im Zuge des Buches bei Sein und Streit einer empfehlenswerten Sendung ganz generell vom Deutschlandfunk Kultur.

01:11:53.000 --> 01:11:55.000
Und das verlinke ich euch auch noch.

01:11:55.000 --> 01:12:01.000
Grundsätzlich Deutschlandfunk hat eigentlich so mittlerweile gefühlt die besten Radiosendungen.

01:12:01.000 --> 01:12:06.000
Ich höre fast nur noch Deutschlandfunk, das hätte ich mir vor einem halben Jahr noch nicht vorstellen können.

01:12:06.000 --> 01:12:11.000
Genau, ja, das war doch mal wieder ein echt spannendes Buch mit viel Diskussionsansatz.

01:12:11.000 --> 01:12:16.000
Habt ihr, glaube ich, auch gemerkt, dass wir ein bisschen mehr ins Diskutieren gekommen sind als sonst oft bei den Büchern.

01:12:16.000 --> 01:12:18.000
Sehr schön und super ausgesucht.

01:12:22.000 --> 01:12:26.000
Genau, jetzt bleibt uns im Grunde nur noch, euch wieder aufzurufen,

01:12:26.000 --> 01:12:32.000
dass vielleicht jetzt die kontroversen Diskussionen als Anlass zu nutzen, uns irgendwie einen Kommentarbeitrag,

01:12:32.000 --> 01:12:39.000
eine Frage oder irgendwas zuzuschicken, die ihr mal hier im Podcast aufgegriffen haben möchtet zu diesem Buch.

01:12:39.000 --> 01:12:41.000
Das würden wir am Anfang der nächsten Episode machen.

01:12:41.000 --> 01:12:44.000
Also habt ihr jetzt so zwei Wochen Zeit oder so vielleicht was einzuschicken.

01:12:44.000 --> 01:12:48.000
Macht das im Idealfall natürlich einfach als Audiophile.

01:12:48.000 --> 01:12:51.000
Es reicht, wenn ihr die kurz mit dem Handy aufnehmt, irgendwo in der ruhigen Ecke.

01:12:51.000 --> 01:12:55.000
Dann können wir das direkt in den Podcast einbinden, was natürlich ganz toll ist.

01:12:55.000 --> 01:13:00.000
Aber ihr könnt es auch einfach als Kommentar unter dem Beitrag auf der Webseite machen.

01:13:00.000 --> 01:13:04.000
Den erreicht ihr unter zwischenzweideckeln.de

01:13:08.000 --> 01:13:10.000
Da findet ihr die aktuelle Episode.

01:13:10.000 --> 01:13:15.000
Und da findet ihr auch Links zu allen Möglichkeiten, wie ihr zwischen zwei Deckeln abonnieren könnt.

01:13:15.000 --> 01:13:21.000
Im Grunde auf der Plattform eurer Wahl, mittlerweile auch bei Spotify, zu hören.

01:13:21.000 --> 01:13:24.000
Da findet ihr auch Links zu unseren Social Media Aktivitäten.

01:13:24.000 --> 01:13:29.000
Wir sind auf Instagram und Twitter jeweils als atdeckeln zu finden.

01:13:29.000 --> 01:13:32.000
Bei Facebook gibt es zwischen zwei Deckeln eine Fanseite.

01:13:32.000 --> 01:13:37.000
Da freuen wir uns auch über Likes, Kommentare, Shares und was da alles so geht.

01:13:37.000 --> 01:13:42.000
Und wenn ihr jetzt diese Episode und auch vielleicht unsere anderen Episoden toll fandet,

01:13:42.000 --> 01:13:49.000
dann freuen wir uns natürlich auch riesig, wenn ihr auf den diversen Plattformen vielleicht eine Rezension hinterlasst oder zumindest ein paar Sterne.

01:13:49.000 --> 01:13:54.000
Das ist auch ganz toll, macht uns Motivation, hier noch weiter zu machen.

01:13:54.000 --> 01:13:57.000
Aber auch keine Angst, wir haben jetzt nicht vor aufzuhören.

01:13:57.000 --> 01:14:02.000
Das motiviert uns nun noch mehr und gibt uns natürlich auch nochmal ein gutes Gefühl, dass wir hier irgendwie was tun,

01:14:02.000 --> 01:14:06.000
was andere Leute gerne hören und auch vielleicht irgendwie unterhält oder informiert.

01:14:07.000 --> 01:14:10.000
Genau, hast du noch irgendwelche Sachen, die du ansprechen wolltest, Christoph?

01:14:10.000 --> 01:14:15.000
Ich glaube, damit hast du vollumfänglich abgedeckt, was wir so am Ende zu sagen haben.

01:14:15.000 --> 01:14:17.000
Von daher bleibt mir nur danke zu sagen, dass sie zugehört habt.

01:14:17.000 --> 01:14:22.000
Und ja, bis zur nächsten Folge quasi. Und tschüss!

01:14:27.000 --> 01:14:29.000
Untertitel im Auftrag des ZDF, 2021

