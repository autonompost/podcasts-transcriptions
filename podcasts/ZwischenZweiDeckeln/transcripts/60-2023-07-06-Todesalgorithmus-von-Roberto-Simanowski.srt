1
00:00:00,000 --> 00:00:23,660
Hallo und herzlich willkommen zu Episode 59 von Zwischen zwei Deckeln, eurem 3-wöchentlichen

2
00:00:23,660 --> 00:00:31,340
Sachbuchpodcast. Mein Name ist Nils und ich habe heute die Amanda dabei. Hallo Amanda. Hallo zusammen.

3
00:00:31,340 --> 00:00:40,180
Ja und bevor wir uns deinem Buch und deinem Themen sozusagen widmen, vielleicht ganz kurz so ein

4
00:00:40,180 --> 00:00:46,300
bisschen was, was uns beschäftigt. Mich beschäftigt gerade ganz stark. Ich lese gerade eine Biografie von

5
00:00:46,300 --> 00:00:52,420
einem Bauhausarchitekten. Ich habe wieder vergessen, ob er Franz oder Fritz ehrlich heißt. Ich kann es

6
00:00:52,420 --> 00:00:59,460
mir nicht merken und das so ganz spannend ist. Es ist also kein ganz berühmter Architekt aus dem

7
00:00:59,460 --> 00:01:04,580
Kontext. Ich hatte den Namen vorher auch noch nie gehört, aber er zeigt sehr schön so diese

8
00:01:04,580 --> 00:01:08,900
diese Entwicklungen und wie sich das Bauhaus mit seiner Technikoffenheit und seinem Blick so

9
00:01:08,900 --> 00:01:16,100
auf Funktionen und Moderne, wie sich das irgendwie ja auch dann auf die Architektur der Nazis und

10
00:01:16,100 --> 00:01:24,260
auf die Architektur der DDR ausgewirkt hat. Und da eben auch mal zu sehen, dass das Bauhaus als

11
00:01:24,260 --> 00:01:28,340
eine Kunst und Design und Gestaltungsrichtung halt nicht nur irgendwie so einen so einen positiv

12
00:01:28,340 --> 00:01:35,140
westlichen Twist hat, sondern auch einen ganz starken Einfluss oder ähnlichen Grundlagen unterlag,

13
00:01:35,140 --> 00:01:41,100
irgendwie wie auch zum Beispiel die Naziarchitektur beispielsweise in den KZs. Das ist wirklich extrem

14
00:01:41,100 --> 00:01:45,740
spannend zu lesen. Das ist eben ein Beispiel dieses Herrn Ehrlich, der eben sowohl ein Bauhaus

15
00:01:45,740 --> 00:01:50,500
lehrte, aber auch Architektur in Buchenwald betrieben hat. Er ist als Häftling und dann als

16
00:01:50,500 --> 00:01:59,060
Angestellter. Ganz, ganz spannend zu lesen. Wie bist du drauf gekommen? Das ist eine gute Frage.

17
00:01:59,060 --> 00:02:03,180
Also Bauhaus ist ein Thema, was mich schon länger beschäftigt. Das ist jetzt tatsächlich für mich

18
00:02:03,180 --> 00:02:07,660
gar nicht neu. Ich weiß nicht, wo ich das Buch zum ersten Mal gesehen habe und dann habe ich

19
00:02:07,660 --> 00:02:13,260
irgendwo in Karlsruhe in der Buchhandlung bei 2001 auf einen halben Preis reduziert liegen sehen.

20
00:02:13,260 --> 00:02:18,260
Und da habe ich es mitgenommen. Das ist gefangen in der Titotalitätsmaschine von Friedrich von

21
00:02:18,260 --> 00:02:23,540
Borjes und Jens-Uwe Fischer. Ich weiß gerade nicht, wie der Herr Fischer mit Vornamen heißt,

22
00:02:23,540 --> 00:02:30,860
aber irgendwas in der Art. Das ist ein Surkampband. Ja, also wenn man sich so für Bauhausarchitektur,

23
00:02:30,860 --> 00:02:35,980
auch gerade moderne Architektur, ersten Hälfte des 20. Jahrhunderts interessiert, ist das wirklich

24
00:02:35,980 --> 00:02:41,220
ein äußerst empfehlenswertes Buch. Klingt spannend. Wahrscheinlich nicht gehaltvoll genug,

25
00:02:41,220 --> 00:02:47,820
um es jetzt hier in den Podcast zu bringen, aber als Lesetipp vielleicht ganz cool. Ja,

26
00:02:47,820 --> 00:02:55,700
was beschäftigt dich gerade so? Ich hatte nicht so viel Zeit zu lesen in letzter Zeit. Ich musste

27
00:02:55,700 --> 00:03:05,740
so ein paar fachliche Dinge zwangslesen, aber ich habe ein schönes kleines Sammelbändchen gefunden.

28
00:03:05,740 --> 00:03:14,180
Das heißt Ungerechte Ungleichheiten und wurde herausgegeben von Steffen Mao. Und ja, es ist ganz

29
00:03:14,180 --> 00:03:20,220
cool, weil das so verschiedene Themen in kurzen Essays von unterschiedlichen AutorInnen behandelt.

30
00:03:20,220 --> 00:03:26,300
Also es geht da um Bildung, es geht um Markt, um den Sozialstaat, Geschlecht, Erben und so weiter.

31
00:03:26,300 --> 00:03:33,980
Und es sind ganz kurze Beiträge, also ein paar Seiten und auch von AutorInnen, die wir hier

32
00:03:34,020 --> 00:03:40,300
schon im Podcast gehört haben. Steffen Mao hatten wir schon. Hartmut Rosa, was haben wir noch?

33
00:03:40,300 --> 00:03:47,260
Stefan Lesse nicht, da habe ich mal was vorgestellt von dem Philosophikum Lech. Also auf jeden Fall

34
00:03:47,260 --> 00:03:54,060
lesenswert und es geht ganz gut so. Ein Beitrag vor dem Schlafen. Wie hieß das Buch Ungleiche

35
00:03:54,060 --> 00:04:01,260
Ungleichheiten? Ungerechte Ungleichheiten und das Un ist so un eingeklammert bei beiden. Auch vom

36
00:04:01,260 --> 00:04:11,340
Sohrkampf. Spannend. Gut, das Thema, was du uns heute mitgebracht hast, ist ein bisschen ein

37
00:04:11,340 --> 00:04:18,180
anderes. Stimmt. Da geht es nicht um Ungleichheiten, zumindest nicht auf den ersten Blick. Du hast ein

38
00:04:18,180 --> 00:04:24,860
Buch mit einem sehr provokanten Titel mitgebracht. Ich bin mal sehr gespannt, was der hält, weil es

39
00:04:24,860 --> 00:04:28,660
auch ein Thema ist, was ja sehr aktuell ist. Und ich weiß jetzt gerade nicht, wie aktuell ist das

40
00:04:28,660 --> 00:04:35,140
Buch. Wann ist das erschienen? Das ist, ich muss schnell drinnen nachschauen. Ich weiß es nicht

41
00:04:35,140 --> 00:04:41,100
aus, wenn nicht. Das ist 2020. Okay, also tatsächlich schon vor dem aktuellen Hype sozusagen. Es geht

42
00:04:41,100 --> 00:04:46,860
nämlich ums Thema künstliche Intelligenz. Und das Buch heißt Todesalgorithmus, das Dilemma der

43
00:04:46,860 --> 00:04:51,980
künstlichen Intelligenz von Roberto Simonowski. Und der Gute ist, wenn ich das richtig gesehen habe,

44
00:04:51,980 --> 00:04:57,300
Kultur- und Medienwissenschaftler. Irgendwie viele im englischsprachigen Ausland unterwegs gewesen,

45
00:04:57,300 --> 00:05:03,780
jetzt aber mittlerweile in Berlin. Ja, ich bin sehr gespannt. Magst du uns das TLDL geben? Ja.

46
00:05:03,780 --> 00:05:13,220
In dem Essay Todesalgorithmus, das Dilemma der künstlichen Intelligenz, geht Roberto Simonowski

47
00:05:13,220 --> 00:05:17,780
den Themen und Fragen auf den Grund, die heute und zukünftig in Zusammenhang mit künstlicher

48
00:05:17,780 --> 00:05:26,420
Intelligenz relevant sind. Er nimmt dazu als roten Faden den Algorithmus in selbstfahrenden Autos.

49
00:05:26,420 --> 00:05:39,860
Das ist ja so das Standardbeispiel der KI-Debatte vor 2022 sozusagen. Ich bin gespannt.

50
00:05:39,860 --> 00:05:47,620
Ja, es ist so, es ist ein Essay. Das Büchlein ist ein bisschen mehr als 100

51
00:05:47,620 --> 00:05:53,660
Seiten lang. Und auch in der Einleitung sagt er, er macht das im Stil von einem

52
00:05:53,660 --> 00:05:58,220
vagabundierenden Denken. Und das ist auch so aufgebaut. Also das ist irgendwie nicht so

53
00:05:58,220 --> 00:06:02,780
eine systematische Abhandlung von dem Thema, sondern er macht viele Beispiele, macht viele

54
00:06:02,780 --> 00:06:11,380
aktuell popkulturelle Referenzen, kann man schon fast sagen. Aber im letzten Teil geht es auch

55
00:06:11,380 --> 00:06:16,620
sehr viel dann um philosophische Themen. Das werde ich wahrscheinlich gar nicht so hier behandeln,

56
00:06:16,620 --> 00:06:21,660
sondern es geht auch einfach ein bisschen darum, ja, um die spannenden Gedanken und Gedankenexperimente,

57
00:06:21,660 --> 00:06:30,180
die halt dieses Thema auch mit sich bringt. Es ist, vielleicht muss man zu Beginn schnell den

58
00:06:30,180 --> 00:06:34,980
Begriff klären von schwacher und starken künstlichen Intelligenz. Ich finde das im

59
00:06:34,980 --> 00:06:40,700
Deutschen nicht so ganz gelungen. Im Englischen heißt das Narrow und General Intelligence.

60
00:06:40,700 --> 00:06:48,460
Und die schwache künstliche Intelligenz, das ist eigentlich das, womit wir auch heute tatsächlich

61
00:06:48,460 --> 00:06:52,780
zu tun haben. Also das ist das, was bei uns in den Smartphones steckt. Das ist aber auch eben

62
00:06:52,780 --> 00:07:00,580
das dieses Autonomofahren. Das ist also diese Art von Technologie, die eine ganz spezifische

63
00:07:00,580 --> 00:07:07,180
Aufgabe lösen kann. Und natürlich eventuell besser oder schneller oder was auch immer als Menschen,

64
00:07:07,180 --> 00:07:13,300
deswegen der Begriff Intelligenz, ist aber ganz was anderes von dieser starken, von dieser General

65
00:07:13,300 --> 00:07:20,700
Intelligence, Artificial Intelligence, wo es dann eher darum geht, dass man halt dieses Problemlösungsverhalten

66
00:07:20,700 --> 00:07:25,980
auch auf andere Kontexte anwenden kann. Und das gibt es im Moment nicht. Also diese starke

67
00:07:25,980 --> 00:07:32,820
künstliche Intelligenz, die existiert zurzeit nicht. Ja, wobei diese aktuelle Debatte um jetzt

68
00:07:32,820 --> 00:07:38,940
LGBT und Co., da fällt das ja immer mal wieder, dass das jetzt so in die Richtung ginge. Genau,

69
00:07:39,260 --> 00:07:48,540
das ist natürlich, aber auch das ist natürlich trotzdem ein sehr begrenztes Feld. Der Output ist

70
00:07:48,540 --> 00:07:56,300
immer noch sehr klar umschrieben. Das ist einfach eigentlich eine probabilistische Ausgabe von

71
00:07:56,300 --> 00:08:01,220
Wörtern. Was könnte jetzt als nächstes kommen für ein bestimmtes Thema? Es ist halt sehr geschickt,

72
00:08:01,220 --> 00:08:06,540
darin so zu tun, als wäre es mehr. Es erweckt halt den Eindruck, als könnte es mehr, als einfach nur

73
00:08:06,540 --> 00:08:11,300
Texte generieren, wie du sagtest, so eine bessere Autovervollständigung. Aber faktisch ist es dann

74
00:08:11,300 --> 00:08:17,020
halt doch nur das und das wird oft nicht so ganz gesehen. Das stimmt, das stimmt. Das ist aber,

75
00:08:17,020 --> 00:08:23,980
das ist auch das Problem. Also die Debatten sind auch immer, also das Spannende oder das,

76
00:08:23,980 --> 00:08:27,820
was halt sexy ist an dieser Diskussion, ist natürlich diese starke künstliche Intelligenz und die

77
00:08:27,820 --> 00:08:31,860
Gefahren, die damit dann hergehen und so weiter. Aber man muss sich schon bewusst daneben. Das gibt

78
00:08:31,860 --> 00:08:38,620
es im Moment nicht und auch der Zeitpunkt, wann es diese Art von Technologie geben wird, ist nicht

79
00:08:38,620 --> 00:08:43,020
klar. Da gibt es Stimmen, die sagen in zehn Jahren oder in fünf Jahren und dann gibt es andere,

80
00:08:43,020 --> 00:08:49,980
die sagen ja das vielleicht gar nie und so weiter. Also das ist noch nicht, steht nicht direkt vor

81
00:08:49,980 --> 00:08:53,980
unserer Tür. Das ist glaube ich auch ein Thema, wo es seit irgendwie 30 Jahren heißt, das gibt es in

82
00:08:53,980 --> 00:09:00,740
zehn Jahren. Ist das so? Ich weiß nicht. Ich habe so ein bisschen das Gefühl, dass das immer zehn

83
00:09:00,740 --> 00:09:06,380
Jahre weg ist, egal wann man fragt. Ja, das kann gut sein. Also in der Science Fiction Literatur

84
00:09:06,380 --> 00:09:12,140
wurde das natürlich schon sehr früh aufgegriffen. Also das ist schon, in den Köpfen von den Menschen

85
00:09:12,140 --> 00:09:18,900
steckt das schon drin. Ja, aber auf jeden Fall, also diese Unterscheidung gliedert so ein bisschen

86
00:09:18,900 --> 00:09:26,020
auch diesen Essay. Also es beginnt am Anfang eben dieser schwachen Intelligenz und diesem Todesalgorithmus

87
00:09:26,020 --> 00:09:31,060
im Auto und erweitert das dann sozusagen immer schrittweise aus. Dann in so einem Mittelteil geht

88
00:09:31,060 --> 00:09:39,540
es dann um die starke Intelligenz und im letzten Teil dann eher so um eine Integration und wie man

89
00:09:39,540 --> 00:09:43,860
das dann mit Hegel verbinden kann und so weiter. Das ist dann sehr hypothetisch und ich habe auch

90
00:09:43,860 --> 00:09:51,860
nicht alles verstanden wahrscheinlich. So oft, wenn es um Hegel geht, ne? Genau. Ja, also beginnen

91
00:09:51,860 --> 00:10:00,140
du das Ganze mit einem Beispiel, mit einem Werbespot. Vielleicht kennst du den. Es ist ein Werbespot,

92
00:10:00,140 --> 00:10:06,500
der von einer Kunstakademie, glaube ich, herausgegeben wurde. Es ist ein Fake-Werbespot und da sieht man

93
00:10:06,500 --> 00:10:14,460
ein Auto, das fährt auf einer Straße in Braunau und das ist ein selbstfahrendes Auto und dann

94
00:10:14,460 --> 00:10:19,820
kommt ein Kind vor das Auto. Also es läuft, es spielt mit einem Ball, läuft vor das Auto und

95
00:10:19,860 --> 00:10:27,420
wird von dem Auto umgefahren. Oha. Und dann zoomt das so raus und man sieht dann so die Gliedmaßen

96
00:10:27,420 --> 00:10:34,460
des Kindes formen dann so ein Hakenkreuz und die Quintessenz ist eigentlich, dass dieses

97
00:10:34,460 --> 00:10:41,300
selbstfahrende Auto die Gefahr erkannt hat, die von diesem Kind ausgeht und dieses Kind sollte

98
00:10:41,300 --> 00:10:48,420
Adolf Hitler darstellen. Ah, okay. Sag dir das was, dass du davon hast, das mal gesehen. Ah, ganz,

99
00:10:48,660 --> 00:10:52,900
ich erinnere mich ganz dunkel an irgendwie sowas, aber ich habe keine Bilder dazu im Kopf. Also ich

100
00:10:52,900 --> 00:10:57,460
habe gerade dieses Hakenkreuzbild müsste ja wahrscheinlich eigentlich relativ einprägsam

101
00:10:57,460 --> 00:11:02,380
sein. Insofern vermute ich mal, dass ich diesen konkreten Spot nicht gesehen habe. Aber so dieses

102
00:11:02,380 --> 00:11:06,620
Motiv, dass irgendwie auch Zeitreisende irgendwie Hitler als Kind umbringen oder so, das ist

103
00:11:06,620 --> 00:11:15,340
natürlich nicht ganz fremd. Genau, ja. Also es ist, ich finde es sehr ins Pferd dieses Video einfach,

104
00:11:15,340 --> 00:11:21,340
weil es macht es schon, also es bringt halt das ziemlich gut auf den Punkt. Also diese Frage nach,

105
00:11:21,340 --> 00:11:26,980
unter welchen Umständen darf man eigentlich töten, um Leben zu retten? Ganz grundsätzlich,

106
00:11:26,980 --> 00:11:33,460
aber auch ganz spezifisch, was bedeutet das, wenn wir Maschinen haben, die all das über uns wissen?

107
00:11:33,460 --> 00:11:42,220
Also man sieht im Spot nun ein Kind, das mit einem Ball spielt. Und im Essay geht es eben zuerst

108
00:11:42,420 --> 00:11:52,940
um diese Frage, was bedeutet das? Darf man töten, um Leben zu retten? Und aktuell gibt es das schon

109
00:11:52,940 --> 00:11:58,060
so ein bisschen auf eine Art und Weise was Ähnliches, was man so beim Predictive Policing

110
00:11:58,060 --> 00:12:04,060
hat. Also diese Vorhersagen der Analyse, was man im Moment auch, basierend auf Daten vor

111
00:12:04,060 --> 00:12:11,540
allen Dingen, versucht vorauszusehen, wann jemand kriminell werden könnte. Oder diese

112
00:12:11,580 --> 00:12:18,300
Täterprofile erstellt. Und also das gibt es schon so ein bisschen. Ich glaube, das wird auch ganz

113
00:12:18,300 --> 00:12:22,700
viel mit Räumlichkeit gemacht. Also ich kenne das irgendwie mehrfach gelesen, dass man so versucht,

114
00:12:22,700 --> 00:12:26,700
gar nicht mal herauszufinden, wer begeht als nächstes ein Verbrechen, sondern wo wird als

115
00:12:26,700 --> 00:12:31,740
nächstes ein Verbrechen begangen. Was natürlich wesentlich unproblematischer ist, weil mal eben

116
00:12:31,740 --> 00:12:35,420
eine Streife irgendwo lang fahren zu lassen, um zu gucken, ob wirklich was passiert. Das ist

117
00:12:35,420 --> 00:12:40,020
natürlich wesentlich weniger invasiv, als mal in Anführungszeichen mal einfach so irgendjemanden

118
00:12:40,020 --> 00:12:46,060
festzunehmen. Ja, und da hast du schon gleich wieder dieses inhärente Problem. Wenn du das dann

119
00:12:46,060 --> 00:12:52,100
machst, dann hast du eine Intervention und die Daten. Also du weißt ja dann nicht, ob das

120
00:12:52,100 --> 00:13:00,180
tatsächlich passiert wäre. Also was ist jetzt, was hat das verhindert? Die Streife oder war die

121
00:13:00,180 --> 00:13:08,780
Vorhersage falsch? Das ist ein anderes Thema. Basierend von dieser Frage, bringt er dann das

122
00:13:08,780 --> 00:13:13,300
Beispiel von dem Buch und vor allen Dingen auch von dem Film, ich glaube ich ist es,

123
00:13:13,300 --> 00:13:22,020
ein Theaterstück von Chirach, das heißt Terror. Und das war von ein paar Jahren, wurde das

124
00:13:22,020 --> 00:13:29,340
aufgeführt, auch im deutschen Fernsehen. Und da geht es darum, dass eigentlich ein Flugzeug mit

125
00:13:29,340 --> 00:13:36,260
160 Menschen an Bord wird von einem Terroristen oder einer Terroristengruppe gekapert und auf die

126
00:13:36,260 --> 00:13:41,140
Allianz Arena in München zugesteuert. Die ist voll, die hat 70.000 Menschen da drin und dann geht

127
00:13:41,140 --> 00:13:46,980
es eigentlich darum, was soll man machen, weil es gibt die Möglichkeit, dieses Flugzeug abzuschießen.

128
00:13:46,980 --> 00:13:53,500
Und man hat dann eigentlich das Ganze so aufgebaut, dass dann das Publikum partizipativ das entscheiden

129
00:13:53,500 --> 00:14:02,060
kann, was man macht. Genau, und das wurde an vielen Orten aufgeführt und gemacht. Das ist

130
00:14:02,540 --> 00:14:14,660
wie so eine empirische Studie schon fast. Also was machen die Menschen? Und interessant ist,

131
00:14:14,660 --> 00:14:22,420
dass wenn man jetzt diesen Outcome hat, dass das Flugzeug abgeschossen wurde, also dass eigentlich

132
00:14:22,420 --> 00:14:28,020
diese 160 Menschen im Flugzeug geopfert wurden, zugunsten dieser Tausenden in der Arena,

133
00:14:28,020 --> 00:14:36,700
dass das eigentlich für die meisten in Deutschland jetzt okay ist. So im Sinne von die Personen,

134
00:14:36,700 --> 00:14:43,500
also der General in diesem Fall wurde freigesprochen. Und das steht aber eigentlich

135
00:14:43,500 --> 00:14:50,540
nicht, also das ist unvereinbar mit dem deutschen Grundrecht. Also Menschenleben dürfen nicht

136
00:14:50,540 --> 00:14:55,980
aufgerechnet werden. Und spannend ist natürlich, dass eigentlich intuitiv dann die Mehrheit von

137
00:14:56,060 --> 00:15:04,900
diesen Personen verfassungswidrig fühlt sozusagen. Und das ist so ein bisschen eines dieser

138
00:15:04,900 --> 00:15:10,940
Grundprobleme. Und er sagt dann auch, und das finde ich interessant, dass Deutschland ganz besonders

139
00:15:10,940 --> 00:15:20,660
damit oder mit diesen Technologien oder mit dieser Frage ein Sonderfall darstellt, weil hier diese

140
00:15:20,660 --> 00:15:26,340
Pflichtenethik, wir kennen das von Kant, also dieses deontologische, sehr eigentlich tiefer

141
00:15:26,340 --> 00:15:32,060
ankert ist. Im Gegensatz jetzt das zu der konsequenzialistischen Ethik, also dem

142
00:15:32,060 --> 00:15:36,740
Utilitarismus klassischerweise, kennt man das, was zum Beispiel im angelsächsischen Raum viel

143
00:15:36,740 --> 00:15:47,300
verbreiteter ist als jetzt bei uns. Und das ist, ja finde ich spannend und ich finde es auch

144
00:15:47,300 --> 00:15:55,620
interessant, dass weil 2006 offenbar das Luftsicherheitsgesetz, so heißt es, hat genau

145
00:15:55,620 --> 00:16:02,300
einen Absatz drin, wo es um dieses Abschießen von Flugzeugen bei Terrorgefahr geht. Und das

146
00:16:02,300 --> 00:16:08,700
wurde aber vom Bundesverfassungsgericht als nicht verfassungswidrig eingestuft. Also wurde

147
00:16:08,700 --> 00:16:18,900
wie abgekanzelt, aber die Menschen scheinen das eigentlich gut zu heißen. Ich habe dann so ein

148
00:16:18,900 --> 00:16:24,300
bisschen geschaut bezüglich der Menschenwürde, das ist auch nämlich ein Konzept, was nicht in

149
00:16:24,300 --> 00:16:30,420
allen Verfassungen steht, also Deutschland hat das drin, ein paar andere EU-Länder hat das auch,

150
00:16:30,420 --> 00:16:36,180
die Schweiz hat das auch drin in der Bundesverfassung und dann noch irgendwie Südafrika und Kenia,

151
00:16:36,180 --> 00:16:42,220
aber sonst ist das gar nicht oft explizit erwähnt. Das ist diese Menschenwürde, sondern eher dann

152
00:16:42,220 --> 00:16:49,620
so Freiheit und was man sonst so darunter subsumieren könnte, aber die Menschenwürde als

153
00:16:49,620 --> 00:16:59,900
Prinzip so erwähnt ist, da ist auch Deutschland so ein bisschen ein Sonderfall. Wenn man das

154
00:16:59,900 --> 00:17:08,180
philosophisch betrachtet, dann ist dieses Problem von dieser Aufwiegung von Menschenleben das

155
00:17:08,180 --> 00:17:15,380
klassische Trolley-Problem, dieses Weichensteller-Problem, das kennen die meisten. Straßenbahn auf der einen

156
00:17:15,380 --> 00:17:21,980
Seite tötet sie fünf Menschen, auf der anderen tötet sie eine Person, ist es okay, wenn man da die Weiche

157
00:17:21,980 --> 00:17:28,860
umstellt. Dann gibt es so die ziemlich provokante Variante mit dem fetten Mann-Problem, hast du die

158
00:17:28,860 --> 00:17:35,180
von schon gehört? Ich kann mir, glaube ich, grob was darunter vorstellen. Ja, geht einfach darum,

159
00:17:35,180 --> 00:17:40,820
dass man die Straßenbahn stoppen kann, indem man einen fetten Mann davor wirft und die dann so

160
00:17:40,820 --> 00:17:46,380
gestoppt wird. Also ist natürlich ganz provokativ und absichtlich so doof formuliert, aber das ist

161
00:17:46,380 --> 00:17:52,580
so die nächste Variante und geht einfach darum, dass man natürlich Menschen nicht so im kantischen

162
00:17:52,580 --> 00:18:00,260
Sinne als Zweck dafür verwenden bzw. als Mittel verwenden kann, sondern sie immer als Zweck sehen

163
00:18:00,260 --> 00:18:09,740
sollte. Und wenn man jetzt das weiter, also ein anderes Beispiel von dieser empirischen Ethik,

164
00:18:09,740 --> 00:18:16,860
eigentlich wie wir das bei diesem partizipativen Theaterstück sehen, gibt es auch sowas vom MIT,

165
00:18:16,860 --> 00:18:23,180
das heißt Moral Machine. Das ist auch ganz spannend, das kann man sich noch anschauen,

166
00:18:23,180 --> 00:18:33,060
das lief von 2016 bis 2020 und nimmt eigentlich genau dieses Weichenstellerproblem und man kann

167
00:18:33,060 --> 00:18:38,140
dann, man konnte dann einfach so sagen, ja es gibt immer zwei Varianten, man konnte sich dann

168
00:18:38,140 --> 00:18:44,100
für eine von beiden entscheiden und die wurden so konstruiert, dass da noch andere Dinge drin sind,

169
00:18:44,100 --> 00:18:50,820
also man kann da entweder von jungen und alten Menschen, also man unterscheidet zwischen jungen

170
00:18:50,820 --> 00:18:57,460
und alten z.B. oder tatsächlich es hat irgendwie übergewichtige Menschen da drin, also das ist

171
00:18:57,460 --> 00:19:04,220
auch eine Kategorie oder halt eine Schwangere oder eine Obdachlose und dann gibt es so abstruse

172
00:19:04,220 --> 00:19:12,060
Szenarien wie eben zwei Obdachlose fahren in einem selbstfahrenden Auto und sollten die eine

173
00:19:12,060 --> 00:19:16,740
schwangere Frau überfahren, die aber über Rot, also als die Ampel auf Rot ist, die Straße überquert.

174
00:19:16,740 --> 00:19:22,940
Also es ist irgendwie noch mehrschichtig und es kommt dann eben auch dieser Gesetzesverstoß

175
00:19:22,940 --> 00:19:30,460
jetzt hier im Sinne von man geht bei Rot über die Straße, kommt dann auch noch dazu und der

176
00:19:30,460 --> 00:19:37,460
Output davon ist auch, das wurde mit mehr als zwei Millionen Menschen haben das ausgefüllt auf der

177
00:19:37,460 --> 00:19:44,060
ganzen Welt und nachher gibt es sehr starke regionale Unterschiede, also man hat dann gesehen,

178
00:19:44,060 --> 00:19:52,740
dass in Lateinamerika und in Afrika beispielsweise Personen tendenziell eha die Kinder favorisieren,

179
00:19:52,740 --> 00:19:59,260
also die Kinder nicht sterben lassen, dann in asiatischen Kulturen oder im asiatischen Raum

180
00:19:59,260 --> 00:20:10,300
eher die Älteren bevorzugt werden und so, also dass es da wirklich ja auch regionale Unterschiede

181
00:20:10,300 --> 00:20:16,060
gibt, macht irgendwo durch auch Sinn, aber stellt natürlich dann die Frage, ja wenn es jetzt

182
00:20:16,060 --> 00:20:20,700
tatsächlich darum geht, das in einem Auto zu verbauen, wie soll das denn gehandhabt werden?

183
00:20:20,700 --> 00:20:26,900
Ja, insbesondere auch eben wenn es dann um diese Regeln, also Südostasien und arabische Staaten

184
00:20:26,900 --> 00:20:31,100
haben dann eher die verschont, die die Regeln befolgen beispielsweise.

185
00:20:31,100 --> 00:20:35,300
Was hätte ich jetzt mir vorgestellt, dass das so im deutschsprachigen Raum oder zumindest in

186
00:20:35,300 --> 00:20:39,660
Deutschland auch eine große Rolle gespielt hat, so wer die Regeln befolgt ist erstmal sicherer als

187
00:20:39,660 --> 00:20:44,620
jemand der über die Straße geht, aber soweit geht unser deutscher Michel dann vielleicht doch nicht.

188
00:20:44,620 --> 00:20:51,580
Weiß ich nicht, also müsste man die Auswertung genau anschauen, das ist jetzt nur so ganz grob

189
00:20:51,580 --> 00:20:55,940
abgebrochen, übers Knie gebrochen, was für Tendenzen es gibt, aber man kann das auch,

190
00:20:55,940 --> 00:21:02,180
ich glaube es gibt ein Buch dazu, das heißt The Car That Knew Too Much von Jean-Francois Bonnefond,

191
00:21:02,180 --> 00:21:08,780
ich glaube das Buch fasst die Ergebnisse von diesem Moral Machine zusammen, also falls man

192
00:21:08,780 --> 00:21:15,900
das da sich anschauen möchte. Auf jeden Fall ist also dieses Grundproblem ein bisschen auch an

193
00:21:15,900 --> 00:21:24,300
diesen an diesen Beispielen, dass man hat wie oder das ist auch was Siemanowsky kritisiert,

194
00:21:24,300 --> 00:21:30,260
man kann sich gar nicht dagegen entscheiden, du kannst gar nicht dieses, hier ist es ja ganz

195
00:21:30,260 --> 00:21:35,140
klar utilitaristisch, man muss wie zwischen zwei Dingen abwägen, man kann sich dagegen gar nicht

196
00:21:35,140 --> 00:21:42,020
entscheiden, du musst eine Entscheidung treffen. Nur schon wie das dieses Experiment aufgestellt

197
00:21:42,020 --> 00:21:48,820
ist und das ist auch so ein bisschen das Problem mit diesem Todesalgorithmus, wie er den nennt im

198
00:21:48,820 --> 00:21:54,860
Auto, dass der kann nicht nicht programmiert werden, oder? Man muss den irgendwie da rein

199
00:21:54,860 --> 00:22:00,700
packen und jetzt in unserem Verständnis liegt der Gesetzesverstoß eigentlich in dieser

200
00:22:00,700 --> 00:22:07,020
Vorentscheidung, also es ist nicht, es ist eben nicht diese Reaktion oder dieses reflexhafte,

201
00:22:07,020 --> 00:22:12,460
was zu der Entscheidung führt, sondern dass man das so kaltblütig sag ich mal im Vornherein

202
00:22:12,460 --> 00:22:19,660
programmiert. Ja und auch irgendwie formalisiert und allgemeingültig, also es ist ja, wenn jetzt

203
00:22:19,660 --> 00:22:23,820
irgendwie eine Person fahren würde, die würde halt, also nehmen wir mal an, sie hätte Zeit zu

204
00:22:23,820 --> 00:22:29,220
entscheiden in der Situation, was jetzt irgendwie nicht wirklich wahrscheinlich ist, aber sie hätte

205
00:22:29,220 --> 00:22:33,260
Zeit zu entscheiden, dann würde sie halt nach ihren eigenen Biases und ihren eigenen Verzerrungen

206
00:22:33,260 --> 00:22:38,380
irgendwie halt entscheiden und dann wäre das halt für diesen einen Fall, aber wenn du es, wenn du es

207
00:22:38,380 --> 00:22:44,620
einem automatischen Auto gibst, dann ist es ja wahrscheinlich, also es kommt immer die Frage,

208
00:22:44,620 --> 00:22:48,780
wie weit das vorprogrammiert ist oder sich quasi im Lernprozess ergibt, aber dann ist es halt

209
00:22:48,780 --> 00:22:54,580
wahrscheinlich so, dass es halt immer dasselbe, die selbe Entscheidung treffen wird. Genau, ja und

210
00:22:54,580 --> 00:23:00,900
das ist, damit sprichst du auch so ein bisschen mit einem Kernproblem an, was ich, also ich fand,

211
00:23:00,900 --> 00:23:05,580
das bringt ein bisschen spät, aber ich finde halt den Gedanken interessant, wenn er sagt, wenn wir

212
00:23:05,580 --> 00:23:12,380
von den Daten sprechen, wenn das jetzt, wenn jetzt alle Daten zur Verfügung stehen, dann ist es ja

213
00:23:12,380 --> 00:23:18,740
nicht mehr dieses Partikulare, das fällt dann ja wie weg und es ist auch nicht per se die gute

214
00:23:18,740 --> 00:23:23,900
Entscheidung, die dann getroffen wird, sondern einfach das Mittel aus allem, oder? Also siehst du,

215
00:23:23,900 --> 00:23:31,020
dieses, dieses Gut und Schlecht, das fällt sowieso ein bisschen weg beim Algorithmus, aber es ist auch

216
00:23:31,020 --> 00:23:35,660
nicht mehr, also es ist einfach so ein Mischmasch von der Mehrheit eigentlich, also die Mehrheit

217
00:23:35,660 --> 00:23:41,260
gewinnt sozusagen in den Daten, weil das ist halt das, was die Überhand nimmt. Ja, wobei ich glaube,

218
00:23:41,260 --> 00:23:45,300
da muss man noch mal gucken, ob es ein Mischmasch ist oder ob es sowas wie, wie du gerade sagtest,

219
00:23:45,300 --> 00:23:48,980
die Mehrheit ist, also ein Mischmasch wäre ja irgendwie so, auch wieder vielleicht eine

220
00:23:48,980 --> 00:23:52,580
probabilistische Entscheidung, in 60 Prozent der Fälle macht das so, in 40 Prozent der Fälle

221
00:23:52,580 --> 00:23:57,740
macht das anders, die andere Variante wäre halt, die 51 Prozent würden es so machen, also macht

222
00:23:57,740 --> 00:24:02,580
es das Auto immer so, das ist ja auch noch mal eher so eine Median, also man nimmt die Median als

223
00:24:02,580 --> 00:24:07,340
absolut wert sozusagen, vielleicht eine Entscheidung wäre, dass sie da mal zwei Perspektiven. Stimmt,

224
00:24:07,340 --> 00:24:15,140
das stimmt, aber das ist natürlich die Frage, ob der Median, das ist was, also ich meine,

225
00:24:15,140 --> 00:24:21,540
ob das überhaupt so umgesetzt wird oder ob auch das natürlich das noch das Erstrebenswerte ist,

226
00:24:21,540 --> 00:24:25,580
aber du hast recht, ja, also Mittelwert und Median können hier in dem Fall natürlich ganz

227
00:24:25,580 --> 00:24:30,860
weit auseinander liegen, je nachdem, wie man es sich anschaut. Auf jeden Fall geht es dann

228
00:24:30,860 --> 00:24:36,340
natürlich darum, wenn man diesen Todesalgorithmus, wie er den nennt, programmiert, stellt sich

229
00:24:36,340 --> 00:24:44,060
natürlich die Frage, ja, wer hat den Vorrang, also wie kann man sowas entscheiden. Und was

230
00:24:44,060 --> 00:24:55,220
ein spannender Gedanke ist, ist natürlich, ja, die AutoherstellerInnen haben natürlich, die würden

231
00:24:55,220 --> 00:25:02,540
es am liebsten so gestalten, dass die Insassen immer Vorrang haben, weil klar, also bringt

232
00:25:02,540 --> 00:25:09,460
natürlich einen Wettbewerbsvorteil für dein Auto. Du bist geschützt? Genau, du bist geschützt und

233
00:25:09,460 --> 00:25:15,780
es ist so, dass offenbar empirische Studien gezeigt haben, er hat die nicht genau zitiert,

234
00:25:15,780 --> 00:25:22,300
ich weiß nicht, wie die zustande gekommen sind, aber es ist wohl so, dass viele Personen eher

235
00:25:22,300 --> 00:25:27,340
dazu neigen zu sagen, ja, eigentlich würde ich mich lieber opfern, also wenn es um so eine

236
00:25:27,340 --> 00:25:35,860
Entscheidung geht, aber ich würde nicht so ein Auto kaufen, das mich opfert. Im abstrakten sind

237
00:25:35,860 --> 00:25:44,900
wir gut, im konkreten dann vielleicht nicht so. Und hier eben, also diese, wer hat Vorrang, diese

238
00:25:44,900 --> 00:25:50,780
Frage ist natürlich dann, die kann man ganz ad absurdum führen, wenn es eben darum geht,

239
00:25:50,780 --> 00:25:56,220
was soll alles berücksichtigt werden. Wir haben jetzt hier ganz, ganz wenige Proxies, die uns so,

240
00:25:56,220 --> 00:26:01,100
die naheliegen, das ist irgendwie das Alter, das kann ich mir vorstellen, es hat kulturell vielleicht

241
00:26:01,100 --> 00:26:05,740
einen Einfluss eben, wie es schon erwähnt, aber nur schon wenn es dann darum geht, irgendwie Tiere,

242
00:26:05,740 --> 00:26:14,060
was machen wir da, oder wenn es dann eben heißt, ja, diese KI, die hat dann plötzlich Zugang zu

243
00:26:14,100 --> 00:26:20,060
allen möglichen Daten und sieht dann ja, okay, da hat es eine Fahrradfahrerin, aber die ist auch

244
00:26:20,060 --> 00:26:25,220
Vielfliegerin und dann hat es hier eine Schwangere, aber die ist des Todes geweiht und dann hat es

245
00:26:25,220 --> 00:26:29,940
noch ein älterer Herr, der hat schon das und das Gutes getan und wenn es dann zu einer Aufwiegung

246
00:26:29,940 --> 00:26:41,340
kommt von, also das ist natürlich dann wirklich schwierig. Ja, und eben, also dieses, dieses

247
00:26:41,340 --> 00:26:46,140
auswählen, also es ist klar, man kann das nicht einfach den Herstellern und Herstellenden von

248
00:26:46,140 --> 00:26:51,380
Autos überlassen, diesen Algorithmus zu programmieren, aber es stellt sich ja auch die Frage,

249
00:26:51,380 --> 00:26:56,780
ja, soll ich den selber auswählen dürfen, wenn ich ein Auto kaufe? Also, was mache ich denn so,

250
00:26:56,780 --> 00:27:01,500
irgendwie Sonntagnachmittag im Park und dann klicke ich mir hier mein Todesalgorithmus zusammen für

251
00:27:01,500 --> 00:27:07,340
mein nächstes Auto? Seltsam, oder? Ja, definitiv. Und wie macht man das? Also, gibt's, sollte man

252
00:27:07,340 --> 00:27:13,260
dann verschiedene Möglichkeiten erlauben, soll man irgendwie die Leute dazu zwingen, keine Ahnung,

253
00:27:13,260 --> 00:27:19,820
mit einem Ethikbeauftragten zu sprechen, bevor man so ein Auto kauft, gibt's dann irgendwie Autos auf

254
00:27:19,820 --> 00:27:26,380
dem Schwarzmarkt, die dann eben einen anderen Algorithmus drin haben, den wir nicht so verwenden.

255
00:27:26,380 --> 00:27:29,820
Also, das sind alles Fragen und das fand ich mega spannend, weil ich bin da nicht drauf gekommen und

256
00:27:29,820 --> 00:27:36,380
er hat das dann so aufgebracht, ja, gibt natürlich keine Lösung dafür, aber auf jeden Fall muss man

257
00:27:36,380 --> 00:27:49,620
darüber nachdenken. Ja, ganz grundsätzlich gibt es als Gegenargument dann so dieses statistische

258
00:27:49,620 --> 00:27:55,740
Argument, das auch heißt, man darf, also, es klingt ja alles so ein bisschen, ja, sollte man das gar

259
00:27:55,740 --> 00:28:02,820
nicht benutzen oder gar nicht erst anfangen, aber so, das Konterargument ist ja immer, ja, auch wenn

260
00:28:02,900 --> 00:28:06,700
wir dann selbstfahrende Autos haben, wir können das natürlich nicht verhindern, dass das zu

261
00:28:06,700 --> 00:28:13,740
Unfällen kommt, aber insgesamt retten wir trotzdem sehr viele Menschenleben dadurch, dass man davon

262
00:28:13,740 --> 00:28:19,540
ausgeht, dass Unfälle sehr viel weniger werden und wir eigentlich eine Verpflichtung dazu haben,

263
00:28:19,540 --> 00:28:29,100
dass diese Technologie auch zustande kommt deswegen. Und was ich auch ein sehr spannender und ein guter

264
00:28:29,100 --> 00:28:36,020
Gedanke fand, ist, dass er sagt, das Risiko jetzt im Straßenverkehr, das geht ja nicht von Kindern

265
00:28:36,020 --> 00:28:43,340
und Fahrradfahrern aus, sondern das geht vom Autoverkehr aus, der per se, also der Autoverkehr,

266
00:28:43,340 --> 00:28:47,780
der sagt dann ja, das ist der, der schwere Gegenstände so schnell durch den Raum bewegt,

267
00:28:47,780 --> 00:28:52,540
dass ein Zusammenstoß tödlich sein kann, das ist das Problem. Unfallopfer können schon gesenkt

268
00:28:52,540 --> 00:28:58,460
werden mit selbstfahrenden Autos, aber die Opfer an sich sind Folge vom Autoverkehr. Ja, stimmt.

269
00:28:58,460 --> 00:29:09,820
Finde ich auch. Kann man natürlich dann auf andere Dankendinge anwenden, das kann man sehr

270
00:29:09,820 --> 00:29:16,260
weiter spinnen, aber trotzdem finde ich das ein sehr valides Argument und auch für die Konsequenz dann,

271
00:29:16,260 --> 00:29:21,380
dass man sagt, ja, es kann natürlich schon angemessen sein, dass man dann sagt, die Opfer

272
00:29:21,380 --> 00:29:29,540
sind die NutzerInnen dieser Mobilität. Ja, ich finde, da wird noch was anderes schön deutlich.

273
00:29:29,540 --> 00:29:34,260
Da wird eben deutlich, dass wir diese Abwägung, nur halten ihn nicht ganz so plakativ, aber doch

274
00:29:34,260 --> 00:29:38,460
auch täglich treffen. Ich meine, auch jetzt schon haben wir ja die Entscheidung getroffen,

275
00:29:38,460 --> 00:29:43,180
dass, ich weiß nicht, wie viele es sind, irgendwie mehrere hundert Verkehrstote jedes Jahr, die

276
00:29:43,180 --> 00:29:48,780
Bequemlichkeit und die schnelle Bewegung anderer Menschen wert sind. Sprich, anscheinend rechnen

277
00:29:48,780 --> 00:29:54,220
wir Menschenleben sogar gegen etwas so Abstraktes und erstmal, na ja, maximal vielleicht ökonomisch

278
00:29:54,220 --> 00:29:59,660
und irgendwie komfortmäßig relevantes, die Bewegungsgeschwindigkeit auf. Also wir tun es

279
00:29:59,660 --> 00:30:06,420
da ja sogar schon. Naja, das ist recht. Eben per Gesetz. Und wenn man weiterdenkt, jetzt irgendwie

280
00:30:06,420 --> 00:30:12,820
Klimaschutz oder Radverkehr in Berlin ist ja gerade großes Thema, da wird das mit jeder Entscheidung

281
00:30:12,820 --> 00:30:18,260
wird das neu getroffen. Wer sozusagen ein Risiko eingeht, wessen Leben in Gefahr gebracht wird und

282
00:30:18,340 --> 00:30:22,580
wem dadurch welcher Nutzen entsteht. Und da ist genau diese Abwägung, nur dass sie halt nicht so auf

283
00:30:22,580 --> 00:30:28,860
diesen einen Punkt fokussiert ist, der uns allen dann sofort irgendwie in sich widersprüchlich und

284
00:30:28,860 --> 00:30:36,580
konfliktär erscheint. Ja, ja, ja, das ist absolut recht. Ist auch irgendwie noch, man kann das dann

285
00:30:36,580 --> 00:30:41,860
so ein bisschen ins Positive drehen, indem man dann sagt, ja, jetzt in dem Fall von selbstfahrenden

286
00:30:41,860 --> 00:30:47,900
Autos, ja, ich nehme sozusagen dieses kleine Risiko eines Unfalls auf mich für das größere Gut,

287
00:30:47,900 --> 00:30:53,020
dass selbstfahrende Autos insgesamt Unfälle vermindern. Ja, also es ist so wie das. Ich

288
00:30:53,020 --> 00:30:59,380
opfere mich für das Gesamtwohl. Das würde ja dann dafür sprechen, dass im Grunde der Fahrer die

289
00:30:59,380 --> 00:31:07,740
letzte Priorität in der Entscheidung sein dürfte. Genau, ja, je nachdem. Also es kann ja auch

290
00:31:07,740 --> 00:31:14,340
prinzipiell. Ja, genau. Also wenn man jetzt das so programmiert, dass die Insassen vom Auto nicht

291
00:31:14,340 --> 00:31:18,780
die höchste Priorität haben. Ja, das wäre ja, ich als Fahrer, der ich mich in so ein gefährliches

292
00:31:18,780 --> 00:31:22,660
Gefährt setze, weiß das und nehme dann den Großteil des Risikos auf mich. Wobei dann

293
00:31:22,660 --> 00:31:28,020
natürlich wieder so Konflikte entstehen. Bringe ich den Fahrer jetzt um oder breche ich den Passagier

294
00:31:28,020 --> 00:31:33,020
beide Beine, dass er danach im Rollstuhl sitzt? Da wird ja noch mehr Differenzierung,

295
00:31:33,020 --> 00:31:39,300
da kommt ja noch mehr Differenzierung rein. Ja, also ich finde eben, die Differenzierung,

296
00:31:39,300 --> 00:31:44,620
die hört ja nie auf. Und ich finde das schon auch das Beängstigende, insbesondere,

297
00:31:44,620 --> 00:31:52,820
wenn es eben um diese Datenmacht geht, weil das ist ja schon ein bisschen so der Trend,

298
00:31:52,820 --> 00:32:02,100
dass wir uns immer mehr dieser Datenmaschine hingeben. Und das kann ich mir vorstellen,

299
00:32:02,100 --> 00:32:05,700
dass es tatsächlich irgendwann mal zu einem Problem führen wird für solche Entscheidungen.

300
00:32:05,700 --> 00:32:10,580
Weil ich da auch, ich bin da immer so ein bisschen hin und her gerissen bei den Problemen dieser

301
00:32:10,580 --> 00:32:15,260
Entscheidungen. Weil es ist ja nicht so, als würden diese Entscheidungen jetzt nicht getroffen. Also

302
00:32:15,260 --> 00:32:18,100
ich habe da auch keine Antwort. Ich will auch nicht sagen, dass die dem jetzt völlig gegen

303
00:32:18,100 --> 00:32:22,180
argumentieren. Aber muss ich halt auch vor Augen halten, wie so Entscheidungen jetzt getroffen

304
00:32:22,180 --> 00:32:26,980
werden. Wenn es jetzt um was kleineres geht als ein Autounfall umgebracht zu werden, sondern

305
00:32:26,980 --> 00:32:32,780
vielleicht um eine Kreditvergabe oder so. Wenn ich da vorher einen Bankberater, vorher vor eine

306
00:32:32,780 --> 00:32:36,860
Bankberater gesessen habe, da hat es vielleicht noch mehr eine Rolle gespielt, welche Hautfarbe

307
00:32:36,860 --> 00:32:42,020
ich denn habe, als jetzt, wo es halt mein Kreditscore bei der Schufa ist. Gut, der wird

308
00:32:42,020 --> 00:32:44,540
wahrscheinlich auch wieder von der Hautfarbe ein bisschen beeinflusst an den ein, zwei Stellen.

309
00:32:44,540 --> 00:32:50,220
Aber dessen Einfluss geht wahrscheinlich erstmal ein bisschen runter. Zumindest solange du,

310
00:32:50,220 --> 00:32:54,540
naja, vielleicht dann doch irgendwie eine Rassisten oder eine Rassisten vor dir sitzen hast bei der

311
00:32:54,540 --> 00:32:58,980
Bank. Wenn du da jemanden hast, der das für sich reflektiert und bewusst versucht irgendwie

312
00:32:58,980 --> 00:33:02,300
auszublenden, dann hast du vielleicht an der Stelle mehr Glück. Aber das heißt ja auch nicht,

313
00:33:02,980 --> 00:33:07,860
dass die menschliche Entscheidung immer besser sein muss als die der künstlichen Intelligenz oder

314
00:33:07,860 --> 00:33:13,180
fairer oder weniger verzerrt. Sie hat halt nur eine wesentlich größere Streuung gefühlt.

315
00:33:13,180 --> 00:33:18,540
Ja, einverstanden. Aber ich sehe das Problem ein bisschen weitergehen als dass man halt,

316
00:33:18,540 --> 00:33:23,500
also wir haben ja so, wir hinterlassen so eine Datenspur, oder? Und ich höre von ganz vielen

317
00:33:23,500 --> 00:33:28,180
Menschen, mit denen ich darüber spreche, ich habe ja nichts zu verbergen. Ja, okay, würde ich auch

318
00:33:28,180 --> 00:33:33,820
von mir sagen, ich mache ja nichts Illegales und so weiter. Aber das Problem liegt ja darin,

319
00:33:33,820 --> 00:33:39,500
dass diese Daten, die gibt es auch noch, nachdem wir, nachdem es uns nicht mehr gibt. Potenziell.

320
00:33:39,500 --> 00:33:44,380
Also die bleiben erhalten, aber die bleiben auch einfach noch in zehn Jahren erhalten und wir

321
00:33:44,380 --> 00:33:53,420
wissen nicht, was dann zum Beispiel plötzlich als Norm gilt. Das ist richtig. Sagen wir, wenn es jetzt

322
00:33:53,420 --> 00:33:59,980
zusammengeht, ich mache das Beispiel dann meistens mit genetischer Sequenzierung. Klar kann ich das

323
00:33:59,980 --> 00:34:05,180
rausgeben, weil ich bin gesund und so, ich habe nichts zu verbergen. Aber vielleicht kann man dann

324
00:34:05,180 --> 00:34:10,940
in zehn Jahren aus meinem Genom rauslesen, dass ich so und so eine Störung oder ein Persönlichkeitsprofil

325
00:34:10,940 --> 00:34:14,380
habe, das vielleicht dann nicht mehr so ganz zu meiner Versicherung passt oder zu was auch immer.

326
00:34:14,380 --> 00:34:21,700
Und dann gibt es ganz neue Diskriminierungspotenziale und das sehe ich halt schon problematisch mit Daten.

327
00:34:21,700 --> 00:34:29,340
Und jetzt haben wir halt so ganz klare Proxys oder man nennt das auch diese schützenswerten

328
00:34:29,340 --> 00:34:41,260
Elemente, die man festgelegt hat. Genau, genau, richtig. Und gegen die man nicht diskriminieren

329
00:34:41,260 --> 00:34:46,100
darf. Achso, das meinst du, ja, okay. Das meine ich ja. Also Gesundheitszahlen, die sind schützenswert,

330
00:34:46,100 --> 00:34:51,060
auf jeden Fall. Aber wir sprechen mit Hautfarben, von Religionen, von solchen Dingen. Gegen die darf

331
00:34:51,060 --> 00:34:57,460
man per Gesetz nicht diskriminiert werden, wegen diesen Dingen. Aber wohin das dann irgendwann

332
00:34:57,460 --> 00:35:03,060
führen wird, das wissen wir nicht. Ja, das stimmt. Das finde ich schon problematisch, weil eben diese

333
00:35:03,060 --> 00:35:07,380
KI potenziell Zugang zu diesen Daten haben wird. Ja, wobei das, da braucht es ja nicht mal eine KI,

334
00:35:07,380 --> 00:35:11,940
das sieht man ja gerade in den USA ganz schön, also schön in Anführungszeichen, in den Bundesstaaten,

335
00:35:11,940 --> 00:35:17,780
wo es jetzt um die zielische Versorgung von Transidenten, Kindern zum Beispiel geht, die

336
00:35:17,780 --> 00:35:21,340
halt wollen tatsächlich, ich weiß nicht mehr was es war, aber wirklich rückliegend irgendwie

337
00:35:21,340 --> 00:35:27,020
Chatverläufe oder sowas ausgewertet werden oder Kommunikation zwischen Ärzten und PatientInnen und

338
00:35:27,020 --> 00:35:31,580
dann daraus irgendwie auch geschlussfolgert werden kann, wo irgendwie eine medizinische

339
00:35:31,580 --> 00:35:39,460
Behandlung stattfindet, die nach dem neuen, nach der neuen Doctrin nicht mehr opportun ist.

340
00:35:39,460 --> 00:35:47,100
Naja, du hast recht. Und da braucht man doch nicht mal eine KI für. Ja, das stimmt, das gibt es

341
00:35:47,100 --> 00:35:54,180
schon jetzt und es gibt auch genügend Beispiele wie, also eben für algorithmischen Ungerechtigkeiten,

342
00:35:54,180 --> 00:36:00,700
also das haben wir schon jetzt, da braucht es wirklich keine KI für. Aber eben, also wer diese

343
00:36:00,700 --> 00:36:05,620
Merkmale festhält, ich weiß nicht, ich habe schon das Gefühl, manchmal hängen wir da schon ein

344
00:36:05,620 --> 00:36:11,580
bisschen so hinterher, wenn es um diese Diskriminierungsmerkmale geht, aber ja,

345
00:36:11,580 --> 00:36:18,020
meinen Standpunkt ist halt, wir wissen nicht, was es in Zukunft geben wird, wogegen oder wofür wir

346
00:36:18,020 --> 00:36:23,500
diskriminiert werden können und deswegen müssen wir eigentlich jetzt schon damit beginnen, dass

347
00:36:23,500 --> 00:36:28,140
wir unsere Daten schützen, aber ja. Wobei ja auch dann die Frage ist, wie dann in der KI kannst du

348
00:36:28,140 --> 00:36:33,460
es dann auch viel weniger festlegen, du kannst halt viel weniger sagen, an welchem Prozentsatz oder an

349
00:36:33,460 --> 00:36:39,100
welchem Faktor hängt, weil ja auch die internen Parameter und sowas, die in den Entscheidungen

350
00:36:39,100 --> 00:36:43,540
einfließen, ja auch gar nicht mehr menschlich irgendwie interpretierbar sind. Das ist ja auch

351
00:36:43,540 --> 00:36:49,220
wieder noch so ein Punkt. Stimmt, ja, stimmt, aber es gibt natürlich schon auch hier Möglichkeiten,

352
00:36:49,220 --> 00:36:53,700
also zu dieser Interpretability von, Interpretierbarkeit von KI, das ist schon,

353
00:36:53,700 --> 00:37:02,700
das ist auch gefordert, also auch diese EU-AI-Legal Act und so weiter, also das ist schon mit bedacht

354
00:37:02,700 --> 00:37:07,700
und da gibt es schon auch Tools, aber das ist recht, also ganz, ganz nachvollziehbar wird das

355
00:37:07,700 --> 00:37:13,100
nicht mehr sein irgendwann. Also so, dass man, dass ein einziger Mensch das verstehen kann bis,

356
00:37:13,100 --> 00:37:23,500
bis in alle Details. Ja, also eben, also dieses, diese ganze, diese ganze Thematik führt ein bisschen

357
00:37:23,500 --> 00:37:28,340
auch zu der Frage, ja, können wir überhaupt einen universellen Konsens dafür finden? Das ist so die

358
00:37:28,340 --> 00:37:37,180
Frage, die er auch aufwirft. Führt Technik zu einem transkulturellen Einvernehmen? Kann die

359
00:37:37,180 --> 00:37:44,780
Technik eigentlich, ja, die Werte, bringt die sie selbst mit, die ihre Operationsweise bestimmt?

360
00:37:44,780 --> 00:37:52,340
Das ist ein bisschen die Frage, wenn es auch um die starke KI geht. Und was ich noch spannend finde,

361
00:37:52,340 --> 00:37:56,780
ist eben diese Ambivalenz. Ich kann es noch nie so ganz einordnen, aber ich finde es interessant,

362
00:37:56,860 --> 00:38:02,340
weil dieser Individualismus, in dem ja eigentlich alle so leben und das auch, was ein bisschen immer

363
00:38:02,340 --> 00:38:09,780
zum Thema wird, das wird einfach relativiert. Also die, es kommt dann, es ist so ein starkes

364
00:38:09,780 --> 00:38:17,820
utilitaristisches oder so ein starkes Gruppenelement oder Moment da in all diesen Argumentationen,

365
00:38:17,820 --> 00:38:22,540
dass das Individuum da wirklich untergeht. Und das finde ich, finde ich irgendwie eine Spannung,

366
00:38:22,540 --> 00:38:29,940
so eine Spannung zu dieser Gesellschaftsidee, die wir, wie ich sie wahrnehme und wie das dann

367
00:38:29,940 --> 00:38:35,860
vielleicht irgendwann, ja, sich entwickelt. Ja, es ist irgendwie so beides. Es hat einerseits

368
00:38:35,860 --> 00:38:41,580
diesen individualistischen Charakter, weil es eben, sagst du, diese Sakrosantheit des Individuums,

369
00:38:41,580 --> 00:38:47,980
die setzt es ja schon irgendwie an. Aber gleichzeitig, der ich jetzt auch in keiner Weise widersprechen

370
00:38:47,980 --> 00:38:53,940
will, aber gleichzeitig macht es eben so diese, ja, diese verbindliche Festlegung von tatsächlichem

371
00:38:53,940 --> 00:38:57,580
Handeln. Also das ist ja im Grunde, es gibt ja auch diesen schönen Satz von Code is Law, also

372
00:38:57,580 --> 00:39:02,380
Programmiercode ist, sind Gesetze in gewisser Weise, weil das ist halt nicht nur ein Gesetz,

373
00:39:02,380 --> 00:39:06,300
wo irgendwo steht und falls du dagegen verhandelst, falls du dagegen handelst, wirst du vielleicht

374
00:39:06,300 --> 00:39:12,140
bestraft, sondern das wird halt immer so passieren, weil Code da einfach sehr deterministisch ist in

375
00:39:12,140 --> 00:39:17,700
dem Moment. Und ja, wenn das da einmal so drin steht, dann wird das auch immer so passieren.

376
00:39:17,700 --> 00:39:21,220
Dann kann nicht danach in einen Richter sagen, ja, das war vielleicht gerechtfertigt, dass die

377
00:39:21,220 --> 00:39:25,020
Person in der Situation anders gehandelt hat. Es wird einfach nie anders gehandelt werden.

378
00:39:25,020 --> 00:39:38,980
Ja, ja, hast du recht. Ja, ich finde halt dieses Deterministische ist halt irgendwie,

379
00:39:38,980 --> 00:39:46,700
ist halt irgendwie noch, noch, noch schwierig, weil ich, ich würde so spontan dazu trainieren,

380
00:39:46,700 --> 00:39:51,340
man könnte das doch einfach lösen, indem man jetzt ganz konkret auf diesen Todesalgorithmus,

381
00:39:51,340 --> 00:39:58,780
indem man da die, den Zufall entscheiden lässt. Ja. Oder? Und ich muss sagen, ich finde das sehr

382
00:39:58,780 --> 00:40:03,180
unbefriedigend, wie er das abgehackt hat. Das ist da irgendwie so zwei Sätze zu und irgendwie

383
00:40:03,180 --> 00:40:09,940
trotzdem, das ändert dann nichts daran, sagt er, auch das degradiert die Person im Straßenverkehr

384
00:40:09,940 --> 00:40:14,460
zu einem bloßen Objekt. Und ich hätte mir da ein bisschen mehr erhofft zu diesem Argument, weil

385
00:40:14,500 --> 00:40:21,100
für mich scheint das nicht, gar nicht so unplausibel zu sein. Mit dem Zufallgenerator. Ja, oder? Ich

386
00:40:21,100 --> 00:40:24,780
weiß nicht. Also so intuitiv finde ich das gar keine schlechte Idee. Ja, die Idee hatte ich

387
00:40:24,780 --> 00:40:30,060
tatsächlich auch. Die Frage ist dann erstens, wie gewichtest du den Zufall? Also wirfst du eine

388
00:40:30,060 --> 00:40:35,340
Münze oder machst du vorher irgendeine Gewichtung? Dann stellt sich das Problem im Grunde gleich

389
00:40:35,340 --> 00:40:42,540
genau wieder. Was gewicht ich jetzt höher, was gewicht ich niedriger, das, ja. Trotzdem ist

390
00:40:42,540 --> 00:40:46,500
es irgendwie eine Bewertung von Menschen. Andererseits ist es aber auch genau der Punkt,

391
00:40:46,500 --> 00:40:51,820
wir machen das ständig implizit. Und ich glaube, es herrscht da irgendwie das, was ich auch gerade

392
00:40:51,820 --> 00:40:56,820
sagte, so ein bisschen so ein Bild vom Menschen als jemanden, der irgendwie automatisch richtig

393
00:40:56,820 --> 00:41:02,820
oder neutraler handelt oder dass eine menschliche Entscheidung in irgendeiner Form inherent an sich

394
00:41:02,820 --> 00:41:09,940
wertvoller wäre als eine KI-Entscheidung. Also das ist ganz, ganz schwer irgendwie zu fassen,

395
00:41:10,100 --> 00:41:14,020
was sagt das eigentlich über unsere Sicht auf menschliche Entscheidungen aus,

396
00:41:14,020 --> 00:41:17,580
wie wir über Entscheidungen künstlicher Intelligenz, ich setze das ganz gerne in

397
00:41:17,580 --> 00:41:22,180
Anführungszeichen, wie wir darüber denken. Das sagt ja auch was darüber aus, wie wir über

398
00:41:22,180 --> 00:41:30,780
Entscheidungen echter Intelligenz denken. Total, aber da kann natürlich das Argument ins Spiel

399
00:41:30,780 --> 00:41:34,660
gebracht werden, das sagt ja, also wir sprechen jetzt von Willensfreiheit in den Menschen

400
00:41:34,660 --> 00:41:39,220
beispielsweise, die gibt es ja gar nicht. Der Mensch ist eigentlich auch nur Algorithmus,

401
00:41:39,220 --> 00:41:44,100
das ist ja auch eine philosophische Standpunkt, die ganz aktuell ist. Aber da kannst du sagen,

402
00:41:44,100 --> 00:41:47,940
ja gut, also dann ist es... Meistens von den gleichen Menschen, die sagen,

403
00:41:47,940 --> 00:41:57,500
dass KI das doch genauso gut entscheiden kann, logischerweise. Ja, also generell mit dieser

404
00:41:57,500 --> 00:42:02,140
Anthropomorphisierung der KI, das ist auch so etwas, das mich persönlich nervt das immer so ein

405
00:42:02,140 --> 00:42:07,460
bisschen, ja KI übernimmt dann die Herrschaft, weil und Gier und Triebe und so weiter, wie der

406
00:42:07,460 --> 00:42:13,660
Mensch. Ich kann das nicht so ganz nachvollziehen, weshalb man das immer so sieht. Und auch

407
00:42:13,660 --> 00:42:20,620
Simonowski macht das eben, er sagt das auch, also es ist nicht klar, dass sich die KI tendenziell

408
00:42:20,620 --> 00:42:28,300
grundsätzlich wie ein Mensch verhält. Aber er sagt dann einfach, wenn man davon ausgeht,

409
00:42:28,300 --> 00:42:36,460
dass die Intelligenz so darin vorhanden oder so operiert, wie wir die im Menschen kennen,

410
00:42:36,460 --> 00:42:40,780
dann kann es sein, dass sie tatsächlich auch so wie diese Triebe entfaltet, die auch zu

411
00:42:40,780 --> 00:42:50,020
unserer menschlichen Intelligenz gehören. Und wie kann man das wie so ein bisschen verhindern? Da

412
00:42:50,020 --> 00:42:57,340
gibt es so eine Möglichkeit oder eine, ja das heißt so eine KI-Nanny, also dass man wie eigentlich

413
00:42:57,340 --> 00:43:04,300
die KI, die Erfindung dieser, sagen wir mal potentiell bösen KI überwachen lässt durch

414
00:43:04,380 --> 00:43:11,260
eine andere KI, die wir weltumspannend einsetzen, die dann auch Zugang auf alle Daten hat und dann

415
00:43:11,260 --> 00:43:18,500
verhindert, dass es eine KI gibt, die sich gegen uns richtet. Bester Plan ever, not. Ja ich bin

416
00:43:18,500 --> 00:43:26,460
auch KI-Nanny, so der Begriff, irgendwie schräg. Aber ja, er hat recht, also es zeigt sich dann

417
00:43:26,460 --> 00:43:36,420
auch, also wir haben nur schon Schwierigkeiten, wenn es darum geht, irgendwie eine Atomwaffen zu

418
00:43:36,420 --> 00:43:41,420
beschränken, also das schaffen wir schon gar nicht irgendwie weltweit, geschweige denn Klimakrise und

419
00:43:41,420 --> 00:43:52,060
so weiter. Und wenn es dann darum geht, eben wir sollten, also die KI soll ja eigentlich das tun,

420
00:43:52,060 --> 00:44:00,420
was wir ihnen vorschreiben, dann stellt sich eben die Frage, wer ist dieses Wesen? Haben wir

421
00:44:00,420 --> 00:44:06,060
überhaupt eine gemeinsame Vorstellung, was wir wollen? Also man geht dann immer so ein bisschen

422
00:44:06,060 --> 00:44:10,860
davon aus, dass unser gemeinsames Interesse darin liegt, dass unsere Gattung überlebt,

423
00:44:10,860 --> 00:44:19,180
oder? Das ist so das, was man uns als Menschheit unterstellt. Tut man das? Muss man ja wissen,

424
00:44:19,300 --> 00:44:25,780
also das muss man ja wie als Vorbedingungen? Also wenn wir das täten zumindest, also wenn

425
00:44:25,780 --> 00:44:29,260
man das empirisch überprüfen würde, machen wir gerade einen verdammt schlechten Job, was das

426
00:44:29,260 --> 00:44:35,620
angeht. Sprichst du jetzt in Bezug auf Klima? Ja, genau, ja. Und das ist auch das Argument,

427
00:44:35,620 --> 00:44:40,900
das er dann sagt oder verwendet, es geht jetzt ein bisschen um diese Öko-KI, also die ist ja

428
00:44:40,900 --> 00:44:48,020
allwissende KI, wo es darum geht, ja, wenn gehen wir gesetzt, das Überleben unserer eigenen Gattung

429
00:44:48,020 --> 00:44:57,940
ist unser gemeinsames Interesse, können wir davon profitieren, wenn wir eine KI installieren,

430
00:44:57,940 --> 00:45:06,460
die uns eigentlich, also die uns regiert, also dass wir uns selbst entmächtigen durch diese KI.

431
00:45:06,460 --> 00:45:13,900
Also eine KI, die unsere Aktionsmacht einschränkt, indem sie oder nachdem sie von uns dazu beauftragt

432
00:45:13,900 --> 00:45:20,020
sind. Hast du deinen Hobbs gelesen, Leviathan? Das klingelt gerade bei mir, weil der Leviathan

433
00:45:20,020 --> 00:45:26,580
ist ja im Grunde genau dieses Instrument mit einer unbeschränkten Macht, das einmal per Vertrag mit

434
00:45:26,580 --> 00:45:31,980
dieser Macht ausgestattet wird und danach unser soziales Leben kontrolliert. Das stuppert doch

435
00:45:31,980 --> 00:45:37,940
sehr danach. Du hast recht, ja, daran habe ich nicht gedacht, aber es ist genau das,

436
00:45:37,940 --> 00:45:47,020
und Leviathan, das passt dann auch vom Namen zu so einem System, oder? Ja, genau, also es ist

437
00:45:47,020 --> 00:45:53,660
halt so die Frage, wieso soll der Mensch seiner eigenen Bevorzugung zustimmen und ich glaube,

438
00:45:53,660 --> 00:46:01,060
oder er sagt tendenziell, ist das gar nicht mehr so unvorstellbar, weil wir Menschen sind im Denken

439
00:46:01,060 --> 00:46:06,140
besser als im Handeln und wir haben diese Probleme, wir haben diesen Present-Bias,

440
00:46:06,140 --> 00:46:15,780
wir bevorzugen die Gegenwart gegenüber der Zukunft, wir haben diesen Regional-Bias, also wir sind

441
00:46:15,780 --> 00:46:23,820
eigentlich so eine Zentralinstanz, die ist zwar unpersönlich, aber das wollen wir im Prinzip ja

442
00:46:23,820 --> 00:46:29,140
genau, es soll eben nicht diese Partikularinteressen berücksichtigt werden, wenn es jetzt darum geht,

443
00:46:29,460 --> 00:46:39,340
Öko-Richtlinien durchzusetzen. Er nennt das, du hast Hobson, er nennt das dann, das wäre dann

444
00:46:39,340 --> 00:46:46,620
der Robespierre des 21. Jahrhunderts. Okay, wie Robespierre geendet ist, wissen wir auch. Genau,

445
00:46:46,620 --> 00:46:56,300
ja, also es ist, man kann da viele Analogien anführen, aber ja, es ist wie, es läuft so ein

446
00:46:56,300 --> 00:47:00,940
bisschen drauf hinaus, dass man auch natürlich in Frage stellen kann, ist die Demokratie überhaupt

447
00:47:00,940 --> 00:47:10,460
dafür geeignet, also wenn wir von unseren eigenen Schwächen da, wie wir sie jetzt in dieser Form haben,

448
00:47:10,460 --> 00:47:15,420
ausgehen, dann ist die Demokratie möglicherweise tatsächlich die schlechteste Form, um unser

449
00:47:15,420 --> 00:47:22,140
Überleben zu sichern. Wobei ich dann jetzt so die Perspektive einer KI-basierten Expertokratie,

450
00:47:22,140 --> 00:47:27,660
also einer Kaiokratie sozusagen, ist jetzt auch nicht unbedingt das, was ich unglaublich

451
00:47:27,660 --> 00:47:35,340
erstrebenswert finde. Und warum nicht? Ja, genau, aus einerseits dem Grund, dass es halt wirklich

452
00:47:35,340 --> 00:47:39,940
im Grunde eine komplette Selbstentmündigung sozusagen wäre, was ja im Grunde, wenn man

453
00:47:39,940 --> 00:47:49,940
jetzt mal weiterdenkt, ist das, naja, auch eine Entmenschlichung sozusagen. Und ja, weil du eben

454
00:47:49,940 --> 00:47:54,700
auch wieder nicht weißt, ich meine, was dich jetzt irgendwie an den eigenen Interessen entwickelt

455
00:47:54,700 --> 00:47:59,380
oder so was, soweit will ich jetzt eigentlich gar nicht spinnen, aber wie vollständig irgendwie

456
00:47:59,380 --> 00:48:02,820
so Informationen sind, die man glaubt zu haben, und dann lernt man irgendwann, dass die Welt doch

457
00:48:02,820 --> 00:48:07,460
anders funktioniert. Das habe ich jetzt gerade in diesem Bauhausbuch gelesen, wo es halt auch

458
00:48:07,460 --> 00:48:14,780
stark darum ging, dass am Anfang in der frühen DDR es durchaus Initiativen gab, also alte Innenstädte,

459
00:48:14,780 --> 00:48:18,980
alte Innenstädte wie in Dresden oder so was, komplett abzureißen und die Städte komplett

460
00:48:18,980 --> 00:48:23,100
neu zu planen nach einem Vorbild dessen, was wir heute aus den USA kennen mit irgendwie den

461
00:48:23,100 --> 00:48:33,620
weitläufigen autozentrierten Städten und so. Und so sehr man das viel in der DDR verachten kann,

462
00:48:33,620 --> 00:48:38,980
bin ich zumindest an dem Punkt ganz froh, dass sie es nicht gemacht haben. Dass sie, auch wenn es in dem

463
00:48:38,980 --> 00:48:44,260
Moment nationalistisch motiviert war, gesagt haben, nee, wir rekonstruieren die Städte mal eher

464
00:48:44,260 --> 00:48:49,300
wieder ein bisschen so, wie wir sie mal hatten. Das hat dann alles so seine Schwierigkeiten gekriegt,

465
00:48:49,300 --> 00:48:53,780
aber das war die Grundidee dahinter, zumindest am Anfang. Und da bin ich dann auch wieder ganz

466
00:48:53,780 --> 00:48:57,180
ganz froh, weil mittlerweile wissen wir, dass das die bessere Idee ist. Damals wussten wir das

467
00:48:57,180 --> 00:49:02,140
vielleicht nicht unbedingt und sowas würde sich halt auch in einer KI in irgendeiner Form verankern

468
00:49:02,140 --> 00:49:08,180
und festschlagen, weil alles so, die kann über alle Daten verfügen und völlig neutral, das ist

469
00:49:08,180 --> 00:49:12,140
halt auch Fiktion. Das ist halt ein Gedankenexperiment und nicht mehr.

470
00:49:12,140 --> 00:49:19,220
Total, das stimmt. Ich finde es spannend, wie du das, ich werde nachher noch gleich die Pünke

471
00:49:19,220 --> 00:49:25,260
dazu schlagen, was du jetzt gesagt hast, wenn es um diese Willensfreiheit geht. Aber trotzdem ist

472
00:49:25,260 --> 00:49:29,380
natürlich diese Demokratie, also es ist wieder so sehr, ich sage jetzt mal sehr deutsch, ganz doof

473
00:49:29,380 --> 00:49:36,020
gesagt, was dieses Empfinden, ich teile das mit dir. Aber es zeigt ja schon auch ein bisschen,

474
00:49:36,020 --> 00:49:45,100
wenn man jetzt nach China schaut zum Beispiel, so die Stabilität und Wohlstand, das kann ja schon

475
00:49:45,100 --> 00:49:50,740
auch für die Menschen erstrebenswert sein. Also dass der Staat das wie sich erstellt und die

476
00:49:50,740 --> 00:49:58,260
Würde wird dann nicht mehr individualistisch gedacht, sondern kollektiv. Dann hat man so

477
00:49:58,260 --> 00:50:04,180
diesen pragmatischen Autoritarismus, wie man den eben in China beispielsweise sieht. Also man kann

478
00:50:04,180 --> 00:50:12,540
schon auch dafür argumentieren und offenbar ist es auch so, dass immer mehr Menschen sich da gar

479
00:50:12,540 --> 00:50:17,980
nicht mehr so abgeneigt sind. Wir haben da schon so ein bisschen eine Sonderposition, wie wir das

480
00:50:17,980 --> 00:50:25,620
wahrnehmen. Und ich weiß nicht, ob sich das nicht irgendwann ein bisschen verschieben wird.

481
00:50:25,620 --> 00:50:30,820
Ja, also ich finde das spannend, weil klar, wenn du sagst, das Überleben der Menschheit geht über

482
00:50:30,820 --> 00:50:37,260
alles, dann wäre wahrscheinlich das schöne Begriff vom wohlwollenden Diktator. Der hätte

483
00:50:37,260 --> 00:50:41,780
natürlich einen Vorteil, aber wer stellt bitte sicher, dass der in irgendeiner Form wohlwollend

484
00:50:41,780 --> 00:50:47,500
ist? Wer kontrolliert den wohlwollenden Diktator? Und dann läuft man wieder in das Problem rein.

485
00:50:47,500 --> 00:50:52,500
Also du sagst jetzt China Stabilität, da kann man schon für argumentieren, da ist natürlich was

486
00:50:52,500 --> 00:50:58,900
dran, aber frag mal die Uiguren. Die sehen das Problem dann vielleicht aus einer anderen

487
00:50:58,900 --> 00:51:02,620
Perspektive. Und das ist glaube ich wirklich schwierig. Ich meine, unsere demokratisch-kapitalistischen

488
00:51:02,620 --> 00:51:08,260
Systeme haben das auch. Du findest ja auch viele Leute, die von dem System komplett vergessen und

489
00:51:08,260 --> 00:51:14,380
abgehängt werden und sich dann irgendwie auch aus dem demokratischen Diskurs verabschieden. Damit

490
00:51:14,380 --> 00:51:19,340
meine ich jetzt nicht AfD wählen, sondern nicht wählen. Ich will nicht das Narrativ vorantreiben,

491
00:51:19,340 --> 00:51:25,140
dass das irgendwie AfD-Wähler die Armen abgehängt werden. Das ist nämlich leider nicht so. Aber

492
00:51:25,460 --> 00:51:30,700
ich tue mich damit schwer, aber das Schöne ist, ich muss darauf keine definitive Antwort finden.

493
00:51:30,700 --> 00:51:40,340
Ja, genau. Ich verstehe natürlich deine Ambivalenz, Gemi, genauso. Ich finde auch,

494
00:51:40,340 --> 00:51:50,420
wie du das vorhin gesagt hast, diese KI-Okratie. Das ist auch so ein bisschen das, wenn es darum

495
00:51:50,420 --> 00:51:57,500
geht. Man kann ja sagen, diese Willensfreiheit des Menschen wird so ein bisschen infrage gestellt

496
00:51:57,500 --> 00:52:05,300
durch diese KI. Ein Aspekt davon ist eigentlich die Beschränkung der Freiheit, dass man anders

497
00:52:05,300 --> 00:52:13,340
kann. Und zwar im Sinne von, wir gehen weg von dieser Disziplinargesellschaft, wie wir sie jetzt

498
00:52:13,340 --> 00:52:20,580
kennen. Also wir werden dafür bestraft, wenn wir was falsch machen, hin zu eigentlich einer

499
00:52:20,580 --> 00:52:28,980
Kontrollgesellschaft. Wir dürfen gar nicht erst mehr was falsch machen. Es wird so designed,

500
00:52:28,980 --> 00:52:32,580
sagen wir mal, unser Leben, dass wir das gar nicht mehr können. Wir können nicht, genau. Das ist

501
00:52:32,580 --> 00:52:37,740
noch mal was anderes als, wir dürfen gar nicht mehr. Genau. Und das ist das, was wir uns doch so ein

502
00:52:37,740 --> 00:52:42,340
bisschen intuitiv, das ist so ein bisschen Kant und Leib, jetzt in ein gutes Handeln, ist nur dann

503
00:52:42,340 --> 00:52:47,780
moralisch, wenn es freiwillig ist, so Kant, aus Pflichtgefühl muss das passieren, dann ist es gut,

504
00:52:47,780 --> 00:52:54,300
moralisch gut. Und Leibniz hat mit seiner Theorie C, das ist die beste aller Welten, man kennt

505
00:52:54,300 --> 00:52:58,900
so ein bisschen davon, es muss so wie das Üben der Welt geben, damit sich das Gute beweisen kann.

506
00:52:58,900 --> 00:53:08,940
Ja. Das ist so, was ich auch fühle, sage ich mal. Und ganz lustig war dann, ich glaube,

507
00:53:08,940 --> 00:53:17,660
es war ehemaliger Bundesverfassungsrichter, sah unter anderem ein Problem in dieser Auto-KI,

508
00:53:17,660 --> 00:53:26,420
dass die FahrerInnen durch die KI an der Missachtung der Verkehrsregeln gehindert

509
00:53:26,420 --> 00:53:32,340
werden können. Ja. Das war wie ein Argument. Die Menschen, die dürfen ja dann gar nicht mehr,

510
00:53:32,340 --> 00:53:37,540
die können gar nicht mehr die Verkehrsregeln missachten. Und bei uns gibt es halt wie kein

511
00:53:37,540 --> 00:53:41,980
Gesetz, das jetzt zum Beispiel Sicherheit immer vor Freiheit stellt. Und das ist so ein bisschen

512
00:53:41,980 --> 00:53:47,260
auch was, was ich mir vorstellen kann, was in deinem Argument vorhin mitspielt. Was ist denn gut oder

513
00:53:47,260 --> 00:53:53,300
wer stellt das sicher? Aber klar kann man sagen, wenn jetzt bei uns ökonomisches Wachstum und

514
00:53:53,300 --> 00:53:59,740
Wohlstand und so und so als das Gute in dem Sinne definiert wird, dann kann man schon eine KI kriegen,

515
00:53:59,740 --> 00:54:05,460
die das sicherstellt. Ja gut, wenn man das vielleicht wieder genau genug definiert

516
00:54:05,460 --> 00:54:10,420
kriegt. Du kommst natürlich wieder darauf hin, dass du dein Ziel so konkret wie es irgendwie

517
00:54:10,420 --> 00:54:14,540
geht nur definieren musst. Das erlebt man ja auch gerade bei, nehmen wir mal das einfache

518
00:54:14,540 --> 00:54:21,060
Beispiel, ChatGPT. Schreib mir jetzt mal einen Text, der genau das tut. So und dann musst du

519
00:54:21,060 --> 00:54:25,420
fünffach korrigieren und auf Vorschläge davon reagieren und antworten. Nee, mach das mal ein

520
00:54:25,420 --> 00:54:30,660
bisschen größer. Oder nein, diese Zahl ist falsch. Bist du dann irgendwann nach einer Stunde im

521
00:54:30,660 --> 00:54:33,700
Gespräch mit ChatGPT einen guten Text? Das in der Zeit hättest du ihn wahrscheinlich auch selber

522
00:54:33,700 --> 00:54:40,380
schreiben können. Und das ist glaube ich noch mal so dieser Gedanke, der hinterlässt, die Präzision

523
00:54:40,380 --> 00:54:46,380
des Ziels zu kennen. Die Funktion, die man da reinschreibt oder die Bewertungsfunktion, nach

524
00:54:46,380 --> 00:54:51,020
der man einen Zustand bewertet, dass das eben auch einfach ein unglaublich schwieriger Aspekt ist,

525
00:54:51,020 --> 00:54:54,660
weil du dann natürlich voll in so eine Reduktionismus-Falle reinläufst. Wie sagst du,

526
00:54:54,660 --> 00:55:01,860
ja, ich bewerte alles in Geld. Zum Beispiel, wie es die Ökonomie ja gerne mal tut. Naja,

527
00:55:01,940 --> 00:55:09,980
guter Punkt, das stimmt. Ja, aber das ist klar, man kann das natürlich dann auch wieder abtun,

528
00:55:09,980 --> 00:55:17,580
das ist ein ganz spezifisches Implementierungsproblem. Ja, nee. Ja, nee, natürlich nicht,

529
00:55:17,580 --> 00:55:23,940
weil das dann gleichzeitig eine riesige Frage ist, wie man dieses Ziel festhalten möchte.

530
00:55:23,940 --> 00:55:33,780
Absolut. Es ist so ein bisschen ein Traum, der Traum der Technokraten, Technokratinnen. Auch so

531
00:55:33,780 --> 00:55:37,500
ein bisschen nach Habermas, nicht der Bildungsbildungsprozess, sondern eigentlich

532
00:55:37,500 --> 00:55:42,860
nur noch die Regelungsoptimierung ist relevant. Und da ist natürlich die Frage, was optimiert

533
00:55:42,860 --> 00:55:51,060
man nämlich? Was will man denn am Schluss überhaupt erreichen? Und das geht dann über

534
00:55:51,100 --> 00:55:57,140
in diesen zweiten Punkt, wenn wir davon ausgehen, dass eben diese Willensfreiheit in Frage gestellt

535
00:55:57,140 --> 00:56:03,300
wird. Das ist eigentlich, dass der Mensch dadurch verlärmt, durch die KI etwas zu wollen,

536
00:56:03,300 --> 00:56:10,460
im Sinne von seinen freien Willen überhaupt zu gebrauchen. Das geht so ein bisschen da in das

537
00:56:10,460 --> 00:56:18,580
Argument, wir müssen das auch trainieren. Also was wir als wichtig oder richtig oder nicht richtig

538
00:56:18,700 --> 00:56:25,700
empfinden oder machen oder und so weiter, das haben wir vielleicht nicht einfach in uns drin,

539
00:56:25,700 --> 00:56:30,500
sondern das muss gelernt und trainiert werden. Das schließt wunderbar, also das, was du sagst,

540
00:56:30,500 --> 00:56:35,380
das schließt an die nächste Folge an, die schon aufgezeichnet ist, aber natürlich noch nicht

541
00:56:35,380 --> 00:56:39,820
veröffentlicht, wenn ihr diese hört, vermutlich. Da wird es um ein anderes Buch gehen, nämlich

542
00:56:39,820 --> 00:56:46,540
Truckpoint Capitalism, wo es unter anderem, dass das Argument auftaucht, warum Dienste wie Spotify

543
00:56:46,540 --> 00:56:50,820
ein großes Interesse daran haben, dass wir unsere Musik, die wir hören, nicht mehr selber aussuchen,

544
00:56:50,820 --> 00:56:56,740
sondern uns einfach von Spotify das aussuchen lassen im Rahmen von Playlists. Das spielt ein

545
00:56:56,740 --> 00:57:00,580
ganz ähnliches Element rein, dass man irgendwann dann verlärmt, sozusagen nochmal zu gucken,

546
00:57:00,580 --> 00:57:05,180
was gibt es denn an KünstlerInnen, was gefällt mir denn an Musik eigentlich wirklich und wo sind

547
00:57:05,180 --> 00:57:09,940
denn jetzt die feinen Unterschiede und wo bin ich jetzt bereit, eventuell Zeit oder Geld in einen

548
00:57:09,940 --> 00:57:14,900
Künstler zu investieren und wo vielleicht auch nicht. Das finde ich einen ganz spannenden Punkt,

549
00:57:15,060 --> 00:57:19,780
es gibt auch bei dem bei dem Chat-GPT-Thema so dieses Argument, ja, das übernimmt halt so die

550
00:57:19,780 --> 00:57:26,780
langweiligen Arbeiten, also die standardisierten Arbeiten und so. Da sagt man, ja, das mag das mag

551
00:57:26,780 --> 00:57:31,860
es sogar in manchen Bereichen können, aber woran übt man denn dann eigentlich, um die schwierigen

552
00:57:31,860 --> 00:57:36,220
und die nicht so standardisierten Sachen überhaupt machen zu können? Weil jetzt nutzt man ja die

553
00:57:36,220 --> 00:57:40,900
einfachen standardisierten Fälle, um für die schwierigen komplizierten Fälle zu üben. Aber

554
00:57:40,900 --> 00:57:44,020
wenn man das nicht mehr macht, wer lernt denn dann noch die schwierigen komplizierten Fälle

555
00:57:44,020 --> 00:57:49,380
zu bearbeiten? Also ganz spannende Argumente, die sich daraus ergeben. Ja, das ist auch ein

556
00:57:49,380 --> 00:57:55,180
Argument, das ich ganz oft bringe, wenn es um in der Medizin darum geht. Also wenn ich mit meinen

557
00:57:55,180 --> 00:58:00,340
Ärztinnen und Kollegen darüber spreche und dann kommt es ja, da wird uns dann das abgenommen,

558
00:58:00,340 --> 00:58:05,740
zum Beispiel diese banalen Fälle auf dem Notfall, der Schnupfen, Husten und so, das ist dann weg und

559
00:58:05,740 --> 00:58:11,860
das ist für mich ein totales Problem, weil mein Mensch, auch mein statistisches Empfinden einer

560
00:58:11,860 --> 00:58:16,820
Krankheit, ist er ganz krass davon geprägt, dass sich neun von zehn Fälle sind banal und dann

561
00:58:16,820 --> 00:58:23,580
erkenne ich den einen, der nicht banal ist. Und wenn irgendeine KI oder was auch immer mir diese

562
00:58:23,580 --> 00:58:30,340
neuen banalen Fälle abnimmt, dann habe ich gar keine Relation mehr. Das spielt also ein bisschen mit

563
00:58:30,340 --> 00:58:37,740
rein und deswegen, ich sehe das auch ganz und gar nicht positiv. Ein gutes Beispiel, um das von

564
00:58:37,740 --> 00:58:44,980
Spotify aufzunehmen, das bringt er auch im Buch, das ist dieses Video, das von Google geleakt wurde

565
00:58:44,980 --> 00:58:53,540
oder es ist nicht ganz klar, ob es einfach geleakt wurde oder absichtlich, wo es um diesen Selfish

566
00:58:53,540 --> 00:58:59,900
Letcher geht. Ich weiß nicht, ob du davon gehört hast, da geht es darum, es ist ein internes Video,

567
00:58:59,900 --> 00:59:08,420
soll auch das Thought Provoking sein. Also es geht darum, dass sie das so konstruieren,

568
00:59:08,420 --> 00:59:20,300
dass der Mensch eigentlich ein Konto hat, wo die Daten drin sind und das geht dann so weit,

569
00:59:20,300 --> 00:59:26,620
dass man das dann weitergeben kann. Dieses Mein-Daten-Konto wird dann irgendwann für die

570
00:59:26,620 --> 00:59:34,900
ganze Menschheit zu Verfügung gestellt. Und es ist sehr unangenehm, das Video, wenn man da schaut,

571
00:59:34,900 --> 00:59:42,900
weil es ist ein Beispiel, es geht dann so darauf hin, dass du hast eine App, dann kannst du da

572
00:59:42,900 --> 00:59:48,860
einstellen, was dein Lebensziel sozusagen sein soll und alles wird dann danach ausgerichtet. Also

573
00:59:48,860 --> 00:59:56,020
wenn du jetzt nur öko-bewusst, umweltbewusst leben möchtest, dann, wenn du eine Banane kaufst,

574
00:59:56,020 --> 01:00:01,980
dann wird dir angezeigt, ja, aber du kannst hier auch die lokale, gewachsene Banane kaufen. Und das

575
01:00:01,980 --> 01:00:06,940
ist ja noch okay, das ist so ein Nudging, so ein bisschen okay, man hat eine Alternative, aber so

576
01:00:06,940 --> 01:00:11,580
der zweite Teil vom Video und so weiter geht dann darum, ja, irgendwann kann der Algorithmus auch

577
01:00:11,580 --> 01:00:16,740
für dich entscheiden. Du hast dann gar nicht mehr die Wahl und es geht dann auch so weit, dass irgendwann

578
01:00:16,740 --> 01:00:21,620
merkt dann diese Daten, ja, ich weiß jetzt gar nicht, wie schwer du bist, das heißt, ich gehe

579
01:00:21,620 --> 01:00:26,780
mal auf die Suche nach einer Waage und präsentiere die dann so, als dass du die brauchst, damit ich

580
01:00:26,780 --> 01:00:34,460
dann dein Gewicht kenne. Also schon ein bisschen verstörend, aber es ist neun Minuten lang, wer Lust

581
01:00:34,460 --> 01:00:46,220
hat, ich kann das dann nicht schauen, es ist schon noch interessant. Ja, das wären so ungefähr die

582
01:00:46,220 --> 01:00:53,300
Gedanken, die ich auch hier präsentieren wollte. Wie gesagt, der letzte Teil geht dann so ein bisschen

583
01:00:53,300 --> 01:01:00,020
ins Philosophische auch interessant, wer dann so ein bisschen Heidegger und Kittler und eben Hegel,

584
01:01:00,020 --> 01:01:08,220
ja, wer diese Argumente darin finden möchte, dem sei das nagelegt auch zu lesen. Ich habe das Buch

585
01:01:08,220 --> 01:01:13,620
jetzt schon ein paar Mal durchgelesen, ich finde es wirklich, ja, es gibt viel, es liest sich ringen

586
01:01:13,620 --> 01:01:25,260
und wer das Thema spannend findet, dem kann ich das echt empfehlen. Ja, super, danke dir. Ja,

587
01:01:25,260 --> 01:01:29,820
viel gedacht, viel diskutiert, haben wir jetzt auch schon in der Vorstellung. Ich habe noch

588
01:01:29,820 --> 01:01:34,580
so ein paar Anschlusspunkte, wo man noch mal drüber nachdenken könnte. Also ein Thema,

589
01:01:34,580 --> 01:01:39,780
was ich gerade bei diesem Thema KI, was man immer so ein bisschen Blick drauf haben muss,

590
01:01:39,780 --> 01:01:46,100
ich weiß nicht, ich sagte den Begriff Critihype, was? Nein. Das ist ein Begriff, den hat Lee Wenzel,

591
01:01:46,100 --> 01:01:51,660
der auch schon mal hier im Podcast aufgetaucht ist, geprägt. Da geht es darum, dass gerade so diese

592
01:01:51,660 --> 01:01:56,580
Kritik vor dem, was KI mal alles können wird und dieses Warnen davor, was das alles können wird,

593
01:01:56,580 --> 01:02:03,700
auch eine Strategie sein kann, den Hype darum zu pushen. Einfach weil man eben, dass es das mal

594
01:02:03,700 --> 01:02:10,140
können wird, ist gesetzt. Die Frage ist nur noch, wie gehen wir damit um? Das ist so ein schöner

595
01:02:10,140 --> 01:02:15,820
Mechanismus, hat man jetzt bei JetGPT gesehen, dass so jemand wie der CEO von OpenAI, also den

596
01:02:15,820 --> 01:02:20,100
Machern von JetGPT, dass der auf einmal sagt, man sollte sechs Monate alle Forschung an solchen

597
01:02:20,100 --> 01:02:26,100
Modellen verbieten, weil die könnten ja irgendwann die Weltherrschaft übernehmen. Ja, das pusht

598
01:02:26,100 --> 01:02:32,380
vor allen Dingen den Wert seines Produkts. Die Diskussion darum sorgt halt dafür, wie das kann

599
01:02:32,380 --> 01:02:36,140
die Weltherrschaft übernehmen. Das muss ja was sein. Und so weiter und so fort. Auf einmal ist das

600
01:02:36,140 --> 01:02:40,860
wieder in aller Munde. Also das ist so ein Punkt, den man immer so mit Blick haben muss. Das hatte

601
01:02:40,860 --> 01:02:45,340
ich jetzt deiner Darstellung nach bei dem Buch nicht unbedingt, aber es ist ein gutes Korrektiv,

602
01:02:45,340 --> 01:02:49,420
wenn man sich den Artikel von Lee Wenzel dazu vielleicht mal durchliest. Den packe ich logischerweise

603
01:02:49,420 --> 01:02:53,660
in die Show Notes. Also das ist jetzt als Leseempfehlung etwas aus fühliger gemeint gewesen.

604
01:02:53,660 --> 01:03:02,100
Mir sind noch zwei weitere Bücher über den Weg gelaufen, die ich ganz spannend finde. Also

605
01:03:02,100 --> 01:03:05,820
Sachbücher. Einmal Choke Point Capitalism. Das werden wir euch in der nächsten Episode

606
01:03:05,820 --> 01:03:13,100
vorstellen von Rebecca Giblin und Cory Doctorow. Und dann habe ich noch von Stefan Kühl. Das ist

607
01:03:13,100 --> 01:03:19,900
ein deutscher Organisationssoziologe. Da gibt es das Buch Brauchbare Illegalität vom Nutzen des

608
01:03:19,900 --> 01:03:24,020
Regelbruchs in Organisationen. Da musst dich dran denken, als du das gerade ansprachst mit dem,

609
01:03:24,020 --> 01:03:29,020
man verhindert, dass die Leute Regeln brechen. Aber da muss ich genau an so eine Situation denken,

610
01:03:29,020 --> 01:03:33,740
wie Autofahrer, das ist eine rote Ampel, Autofahrer sieht aber, dass da irgendwie,

611
01:03:33,740 --> 01:03:37,540
dass er jetzt Leben retten kann, wenn er über die rote Ampel fährt, aber die rote Ampel lässt ihn

612
01:03:37,540 --> 01:03:41,700
halt nicht. So, ne? Also manchmal kann es dann einfach Sinn machen, Regeln zu brechen. In

613
01:03:41,700 --> 01:03:45,540
Organisationen ist das noch viel relevanter. Jeder, der irgendwie in einem Unternehmen oder in einem

614
01:03:45,540 --> 01:03:49,340
größeren Kontext arbeitet, weiß, warum es manchmal auch sinnvoll sein kann, sich nicht ganz an die

615
01:03:49,340 --> 01:03:54,540
Regeln zu halten. Aber das im Großen und Ganzen auch im Sinne der Regeln dann ist, wenn man das

616
01:03:54,540 --> 01:03:59,260
in dem Fall nicht tut. Also da muss ich dran denken, als du den Aspekt ansprachst. Das waren

617
01:03:59,260 --> 01:04:06,260
die zwei Sachbücher und dann noch zwei Fiktionen, fiktive Texte oder auch Filme. Also ein Film ist

618
01:04:06,260 --> 01:04:11,020
sicherlich nicht unbekannt, das Minority Report. Das ist ja eine Kurzgeschichte, ich glaube,

619
01:04:11,020 --> 01:04:18,060
von Philip K. Dick war sie oder war sie von, Gott, jetzt blamier ich mich gerade. Ja, aber ich hab's

620
01:04:18,060 --> 01:04:22,780
auch so irgendwie abgespeichert. Und natürlich als Film, da ist sie halt bekannt und berühmt

621
01:04:22,780 --> 01:04:26,900
geworden. Da geht es eben um dieses Predictive Policing, also dieses Leute festnehmen,

622
01:04:26,900 --> 01:04:31,220
bevor sie ein Verbrechen vergehen, um das Verbrechen eben zu verhindern. Und dann gibt's

623
01:04:31,220 --> 01:04:37,780
von Andreas Eschbach den Roman NSA, Nationales Sicherheitsamt. Ist ein bisschen kontrovers,

624
01:04:37,780 --> 01:04:43,860
weil er halt so das Nazi-Judenverfolgungs-Setting als so ein bisschen Hintergrund für eine spannende

625
01:04:43,860 --> 01:04:49,940
Roman-Geschichte nimmt, eine fiktive, weil er da eben das Nationale Sicherheitsamt, Abkürzung NSA,

626
01:04:49,940 --> 01:04:56,020
was ein Zufall, zeigt, die irgendwie sämtliche Datenbanken und so was haben und daraus so Dinge

627
01:04:56,020 --> 01:05:00,700
nutzen, wie die Leute kaufen mehr als für vier Personen notwendig ist, die beherbergen wahrscheinlich

628
01:05:00,700 --> 01:05:05,660
irgendwie jemanden, den wir nicht finden sollen oder so. Und erzählt dann eben auf dieser Grundlage

629
01:05:05,660 --> 01:05:12,620
eine Geschichte, was mit so vorgeblich unschuldigen Daten, wie zum Beispiel ein Einkaufszettel, was

630
01:05:12,620 --> 01:05:16,740
daraus irgendwie geschlussfolgert werden kann und wie das, wenn man jetzt sich in einem autoritären

631
01:05:16,740 --> 01:05:22,820
Regime sieht, dann auch genutzt werden kann und auf einen zurück schlagen kann. Ist in der Hinsicht

632
01:05:22,820 --> 01:05:25,660
ganz spannend, ich fand ihn sehr spannend zu lesen, aber wie gesagt, das hat so ein bisschen

633
01:05:25,660 --> 01:05:30,580
dieses, ja so ein bisschen, ich will nicht sagen, Nazi-Romantik, weil es die auch extrem kritisch

634
01:05:30,580 --> 01:05:37,220
sieht, aber da kann man ein bisschen draufgucken. Genau, das waren meine Lesetipps über das Thema

635
01:05:37,220 --> 01:05:44,300
hinaus. Cool, ja, vielen Dank. Das klingt nach Sommerferienlektüre, so das Letzte. Das könnte

636
01:05:44,300 --> 01:05:47,780
ich mir vorstellen, dass ich das mit... Also es macht nicht unbedingt Spaß, es ist eher so dunkel,

637
01:05:47,780 --> 01:05:52,740
es ist nicht gute Laune. Dafür ist das Thema auch zu ernst, aber man kann es gut, Espar kann

638
01:05:52,740 --> 01:06:01,780
einfach sehr gut schreiben, sehr fluffig, sehr gefällig. Okay, ja, sehr gut. Ich habe so als

639
01:06:01,780 --> 01:06:06,820
weitergehende Lektüre, es gibt ein anderes Buch von Szymanowski, also es gibt mehrere natürlich,

640
01:06:06,820 --> 01:06:12,020
aber ein anderes, das heißt Data Love, das ist bei Mathes und Salz erschienen, wo es so eben

641
01:06:12,420 --> 01:06:18,580
um die ganze Daten-Thematik geht. Ich finde, er schreibt einfach auch sehr ansprechend,

642
01:06:18,580 --> 01:06:23,060
ich finde auch dieses Buch, das hat sehr viele Referenzen und es geht ihm auch Filme und konkrete

643
01:06:23,060 --> 01:06:29,940
Beispiele ein und man sieht dann aber eben im letzten Teil, dass es trotzdem auch irgendwie

644
01:06:29,940 --> 01:06:35,900
inhaltlich nicht einfach so dahergesagt ist, also wenn er dann diese Beispiele oder diese

645
01:06:35,900 --> 01:06:42,340
Philosophen ins Spiel bringt, deswegen noch ein anderes Buch von ihm. Dann eines, das habe ich

646
01:06:42,340 --> 01:06:47,100
jetzt angefangen, ich bin noch nicht durch, aber es passt ganz gut zum Thema, das heißt Was das

647
01:06:47,100 --> 01:06:55,540
Valley Denken nennt von Adrian Daub, wo es eben darum geht, natürlich auch bei dieser KI, das ist

648
01:06:55,540 --> 01:07:00,860
ja eine ganz kleine Gruppe von Personen, die eigentlich an der Entwicklung davon überhaupt

649
01:07:00,860 --> 01:07:09,980
beteiligt sind und ja das Silicon Valley ist natürlich ein Brennpunkt für Daphne und Adrian Daub

650
01:07:09,980 --> 01:07:16,700
ist Adrenalin, ist er Deutscher sogar? Ich glaube ja, ich glaube er ist tatsächlich, also er lebt und

651
01:07:16,700 --> 01:07:20,340
arbeitet in den USA, aber ich habe ihn auch schon in deutschen Interviews gehört, also er ist auf

652
01:07:20,340 --> 01:07:29,620
jeden Fall deutsch auf hohem Sprachlevel und ich meine tatsächlich auch, er wäre gebürtig in

653
01:07:29,620 --> 01:07:36,100
Deutschland, aber ich kann es dir jetzt nicht garantieren. Ja okay, auf jeden Fall, das würde

654
01:07:36,100 --> 01:07:44,500
dazu passen und von einer deutschen Professorin Katrin Misselhorn ist das kleine Büchlein KI und

655
01:07:44,500 --> 01:07:51,500
Empathie, das beim Reclam erschienen ist, auch sehr empfehlenswert meiner Meinung nach, geht es auch

656
01:07:51,500 --> 01:07:57,660
so ein bisschen darum, eben Empathie ist auch was man trainieren kann, soll, muss, wie wird das in

657
01:07:57,660 --> 01:08:02,940
Frage gestellt oder beeinflusst durch Roboter, durch KI und so weiter. Also Adrian Daub ist

658
01:08:02,940 --> 01:08:09,420
tatsächlich in Köln geboren, aber wenn ich das richtig sehe, akademisch komplett in den USA oder

659
01:08:09,420 --> 01:08:13,700
Großbritannien, in den USA sozialisiert, also in den USA sogar seinen Bachelor schon gemacht.

660
01:08:13,700 --> 01:08:22,620
Ach so, okay. Ja, wir haben auch so ein paar passende Folgen natürlich zum Thema, ich habe

661
01:08:22,620 --> 01:08:28,500
jetzt einfach zwei rausgepickt, das ist einerseits das metrische Wir von Steffen Mau, das wird auch

662
01:08:28,500 --> 01:08:37,100
explizit im Buch erwähnt, das Buch, was auch dazu passt, ist Roboterethik von Janina Loh, wo es

663
01:08:37,100 --> 01:08:44,580
generell also ein bisschen Maschinenethik auch geht. Ja, also es gibt natürlich ganz ganz vieles, was

664
01:08:44,580 --> 01:08:50,780
passt, aber das könnte man sich dazu anhören und so Filmtipps, auch ein Film, das den eher erwähnt

665
01:08:50,780 --> 01:08:59,580
ist, ein neuerer Film ist Ex Machina. Ja. Wieso zögerst du? Also ich kenne Ex Machina,

666
01:08:59,580 --> 01:09:05,460
den Film, ist ein sehr sehr guter Film, mir war nur der Bezug zu seinem Thema nicht sofort klar. Ach so,

667
01:09:05,460 --> 01:09:16,460
ja, es geht ein bisschen darum, dass, um diesen Sprung davon, dass Technik eigentlich wie dann

668
01:09:16,460 --> 01:09:22,820
in die Welt rausgeht und sich selbst, um die Welt zu entdecken, also man hat wie in, ich glaube,

669
01:09:22,820 --> 01:09:27,940
Hell nimmt als anderes Beispiel von dieser starken KI, die zwar dann diese Astronaut, das ist Space

670
01:09:27,940 --> 01:09:32,660
Odyssey, oder? Ja, genau. Space Odyssey, wo die Astronauten dann nicht mehr zurück ins Raumschiff

671
01:09:32,660 --> 01:09:40,580
dürfen, aber weil Hell sozusagen die Menschheit oder seinen Auftrag erfüllen möchte, also es ist

672
01:09:40,580 --> 01:09:48,100
wie noch, er macht das noch im Sinne von seinem Auftrag, der von den Menschen vorgegeben ist und

673
01:09:48,100 --> 01:09:52,900
bei Ex Machina gibt es dann wie diesen Schritt, dass diese Roboter in diesem Fall, diese starke KI

674
01:09:52,900 --> 01:10:00,460
eigentlich sich davon löst. Es ist wie wirklich diese eigene Welt dann und so, das ist wie er das

675
01:10:00,460 --> 01:10:05,780
im Buch beschreibt. Aber ich finde es auch generell so eigentlich ein guter Film, der das Thema so,

676
01:10:05,780 --> 01:10:13,940
wenn es um Roboter und Maschinen geht, gut darstellt. Ja, das wäre es von meinen Tipps.

677
01:10:13,940 --> 01:10:19,100
Sehr schön, ja. Vielen Dank dir, Amanda, für die Buchvorstellung. Vielen Dank euch an den

678
01:10:19,100 --> 01:10:27,500
Kopfhörern oder Lautsprechern fürs Zuhören und dann bleibt mir nur zu hoffen, euch gewünscht zu

679
01:10:27,500 --> 01:10:32,540
haben, dass ihr viel Spaß hattet, keine Ahnung, ich hab mich verhaspelt, es ist spät, dass ihr was

680
01:10:32,540 --> 01:10:38,140
mitgenommen habt, dass ihr viel Spaß hattet beim Hören dieser Episode und wenn ihr uns auf den

681
01:10:38,140 --> 01:10:43,500
sozialen Medien folgen wollt, könnt ihr das tun. Wir sind noch Fragezeichen, Stand heute ist es

682
01:10:43,500 --> 01:10:48,340
gerade ein bisschen problematisch mit Twitter, bei Twitter als atdeckelen unter dem gleichen Handel

683
01:10:48,340 --> 01:10:57,580
auch bei Instagram zu finden. Neben Fidiversum sind wir atzzd, atpodcasts.social und auf Facebook

684
01:10:57,580 --> 01:11:01,420
haben wir nämlich auch noch eine Seite, da findet ihr uns als zwischen zwei Deckeln. Ihr könnt aber

685
01:11:01,460 --> 01:11:06,780
auch ganz klassisch das alte web10 bemühen und auf zwischenzweideckel.de gehen, da findet ihr

686
01:11:06,780 --> 01:11:13,060
alle Episoden genauso wie im Podcatcher eurer Wahl. Ihr könnt uns auf Spotify hören, aber wenn

687
01:11:13,060 --> 01:11:18,660
ihr andere Tools nutzt, nutzt doch lieber die anderen, wenn es irgendwie geht. Auch wieder im

688
01:11:18,660 --> 01:11:23,100
Anschluss an das Thema heute. Jetzt bleibt mir nur euch, bis zum nächsten Mal, viel Spaß beim

689
01:11:23,100 --> 01:11:27,420
Lesen zu wünschen, macht es gut und bis bald. Bis dann, tschüss zusammen.

