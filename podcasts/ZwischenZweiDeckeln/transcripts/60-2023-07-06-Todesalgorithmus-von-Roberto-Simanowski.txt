Hallo und herzlich willkommen zu Episode 59 von Zwischen zwei Deckeln, eurem 3-wöchentlichen
Sachbuchpodcast. Mein Name ist Nils und ich habe heute die Amanda dabei. Hallo Amanda. Hallo zusammen.
Ja und bevor wir uns deinem Buch und deinem Themen sozusagen widmen, vielleicht ganz kurz so ein
bisschen was, was uns beschäftigt. Mich beschäftigt gerade ganz stark. Ich lese gerade eine Biografie von
einem Bauhausarchitekten. Ich habe wieder vergessen, ob er Franz oder Fritz ehrlich heißt. Ich kann es
mir nicht merken und das so ganz spannend ist. Es ist also kein ganz berühmter Architekt aus dem
Kontext. Ich hatte den Namen vorher auch noch nie gehört, aber er zeigt sehr schön so diese
diese Entwicklungen und wie sich das Bauhaus mit seiner Technikoffenheit und seinem Blick so
auf Funktionen und Moderne, wie sich das irgendwie ja auch dann auf die Architektur der Nazis und
auf die Architektur der DDR ausgewirkt hat. Und da eben auch mal zu sehen, dass das Bauhaus als
eine Kunst und Design und Gestaltungsrichtung halt nicht nur irgendwie so einen so einen positiv
westlichen Twist hat, sondern auch einen ganz starken Einfluss oder ähnlichen Grundlagen unterlag,
irgendwie wie auch zum Beispiel die Naziarchitektur beispielsweise in den KZs. Das ist wirklich extrem
spannend zu lesen. Das ist eben ein Beispiel dieses Herrn Ehrlich, der eben sowohl ein Bauhaus
lehrte, aber auch Architektur in Buchenwald betrieben hat. Er ist als Häftling und dann als
Angestellter. Ganz, ganz spannend zu lesen. Wie bist du drauf gekommen? Das ist eine gute Frage.
Also Bauhaus ist ein Thema, was mich schon länger beschäftigt. Das ist jetzt tatsächlich für mich
gar nicht neu. Ich weiß nicht, wo ich das Buch zum ersten Mal gesehen habe und dann habe ich
irgendwo in Karlsruhe in der Buchhandlung bei 2001 auf einen halben Preis reduziert liegen sehen.
Und da habe ich es mitgenommen. Das ist gefangen in der Titotalitätsmaschine von Friedrich von
Borjes und Jens-Uwe Fischer. Ich weiß gerade nicht, wie der Herr Fischer mit Vornamen heißt,
aber irgendwas in der Art. Das ist ein Surkampband. Ja, also wenn man sich so für Bauhausarchitektur,
auch gerade moderne Architektur, ersten Hälfte des 20. Jahrhunderts interessiert, ist das wirklich
ein äußerst empfehlenswertes Buch. Klingt spannend. Wahrscheinlich nicht gehaltvoll genug,
um es jetzt hier in den Podcast zu bringen, aber als Lesetipp vielleicht ganz cool. Ja,
was beschäftigt dich gerade so? Ich hatte nicht so viel Zeit zu lesen in letzter Zeit. Ich musste
so ein paar fachliche Dinge zwangslesen, aber ich habe ein schönes kleines Sammelbändchen gefunden.
Das heißt Ungerechte Ungleichheiten und wurde herausgegeben von Steffen Mao. Und ja, es ist ganz
cool, weil das so verschiedene Themen in kurzen Essays von unterschiedlichen AutorInnen behandelt.
Also es geht da um Bildung, es geht um Markt, um den Sozialstaat, Geschlecht, Erben und so weiter.
Und es sind ganz kurze Beiträge, also ein paar Seiten und auch von AutorInnen, die wir hier
schon im Podcast gehört haben. Steffen Mao hatten wir schon. Hartmut Rosa, was haben wir noch?
Stefan Lesse nicht, da habe ich mal was vorgestellt von dem Philosophikum Lech. Also auf jeden Fall
lesenswert und es geht ganz gut so. Ein Beitrag vor dem Schlafen. Wie hieß das Buch Ungleiche
Ungleichheiten? Ungerechte Ungleichheiten und das Un ist so un eingeklammert bei beiden. Auch vom
Sohrkampf. Spannend. Gut, das Thema, was du uns heute mitgebracht hast, ist ein bisschen ein
anderes. Stimmt. Da geht es nicht um Ungleichheiten, zumindest nicht auf den ersten Blick. Du hast ein
Buch mit einem sehr provokanten Titel mitgebracht. Ich bin mal sehr gespannt, was der hält, weil es
auch ein Thema ist, was ja sehr aktuell ist. Und ich weiß jetzt gerade nicht, wie aktuell ist das
Buch. Wann ist das erschienen? Das ist, ich muss schnell drinnen nachschauen. Ich weiß es nicht
aus, wenn nicht. Das ist 2020. Okay, also tatsächlich schon vor dem aktuellen Hype sozusagen. Es geht
nämlich ums Thema künstliche Intelligenz. Und das Buch heißt Todesalgorithmus, das Dilemma der
künstlichen Intelligenz von Roberto Simonowski. Und der Gute ist, wenn ich das richtig gesehen habe,
Kultur- und Medienwissenschaftler. Irgendwie viele im englischsprachigen Ausland unterwegs gewesen,
jetzt aber mittlerweile in Berlin. Ja, ich bin sehr gespannt. Magst du uns das TLDL geben? Ja.
In dem Essay Todesalgorithmus, das Dilemma der künstlichen Intelligenz, geht Roberto Simonowski
den Themen und Fragen auf den Grund, die heute und zukünftig in Zusammenhang mit künstlicher
Intelligenz relevant sind. Er nimmt dazu als roten Faden den Algorithmus in selbstfahrenden Autos.
Das ist ja so das Standardbeispiel der KI-Debatte vor 2022 sozusagen. Ich bin gespannt.
Ja, es ist so, es ist ein Essay. Das Büchlein ist ein bisschen mehr als 100
Seiten lang. Und auch in der Einleitung sagt er, er macht das im Stil von einem
vagabundierenden Denken. Und das ist auch so aufgebaut. Also das ist irgendwie nicht so
eine systematische Abhandlung von dem Thema, sondern er macht viele Beispiele, macht viele
aktuell popkulturelle Referenzen, kann man schon fast sagen. Aber im letzten Teil geht es auch
sehr viel dann um philosophische Themen. Das werde ich wahrscheinlich gar nicht so hier behandeln,
sondern es geht auch einfach ein bisschen darum, ja, um die spannenden Gedanken und Gedankenexperimente,
die halt dieses Thema auch mit sich bringt. Es ist, vielleicht muss man zu Beginn schnell den
Begriff klären von schwacher und starken künstlichen Intelligenz. Ich finde das im
Deutschen nicht so ganz gelungen. Im Englischen heißt das Narrow und General Intelligence.
Und die schwache künstliche Intelligenz, das ist eigentlich das, womit wir auch heute tatsächlich
zu tun haben. Also das ist das, was bei uns in den Smartphones steckt. Das ist aber auch eben
das dieses Autonomofahren. Das ist also diese Art von Technologie, die eine ganz spezifische
Aufgabe lösen kann. Und natürlich eventuell besser oder schneller oder was auch immer als Menschen,
deswegen der Begriff Intelligenz, ist aber ganz was anderes von dieser starken, von dieser General
Intelligence, Artificial Intelligence, wo es dann eher darum geht, dass man halt dieses Problemlösungsverhalten
auch auf andere Kontexte anwenden kann. Und das gibt es im Moment nicht. Also diese starke
künstliche Intelligenz, die existiert zurzeit nicht. Ja, wobei diese aktuelle Debatte um jetzt
LGBT und Co., da fällt das ja immer mal wieder, dass das jetzt so in die Richtung ginge. Genau,
das ist natürlich, aber auch das ist natürlich trotzdem ein sehr begrenztes Feld. Der Output ist
immer noch sehr klar umschrieben. Das ist einfach eigentlich eine probabilistische Ausgabe von
Wörtern. Was könnte jetzt als nächstes kommen für ein bestimmtes Thema? Es ist halt sehr geschickt,
darin so zu tun, als wäre es mehr. Es erweckt halt den Eindruck, als könnte es mehr, als einfach nur
Texte generieren, wie du sagtest, so eine bessere Autovervollständigung. Aber faktisch ist es dann
halt doch nur das und das wird oft nicht so ganz gesehen. Das stimmt, das stimmt. Das ist aber,
das ist auch das Problem. Also die Debatten sind auch immer, also das Spannende oder das,
was halt sexy ist an dieser Diskussion, ist natürlich diese starke künstliche Intelligenz und die
Gefahren, die damit dann hergehen und so weiter. Aber man muss sich schon bewusst daneben. Das gibt
es im Moment nicht und auch der Zeitpunkt, wann es diese Art von Technologie geben wird, ist nicht
klar. Da gibt es Stimmen, die sagen in zehn Jahren oder in fünf Jahren und dann gibt es andere,
die sagen ja das vielleicht gar nie und so weiter. Also das ist noch nicht, steht nicht direkt vor
unserer Tür. Das ist glaube ich auch ein Thema, wo es seit irgendwie 30 Jahren heißt, das gibt es in
zehn Jahren. Ist das so? Ich weiß nicht. Ich habe so ein bisschen das Gefühl, dass das immer zehn
Jahre weg ist, egal wann man fragt. Ja, das kann gut sein. Also in der Science Fiction Literatur
wurde das natürlich schon sehr früh aufgegriffen. Also das ist schon, in den Köpfen von den Menschen
steckt das schon drin. Ja, aber auf jeden Fall, also diese Unterscheidung gliedert so ein bisschen
auch diesen Essay. Also es beginnt am Anfang eben dieser schwachen Intelligenz und diesem Todesalgorithmus
im Auto und erweitert das dann sozusagen immer schrittweise aus. Dann in so einem Mittelteil geht
es dann um die starke Intelligenz und im letzten Teil dann eher so um eine Integration und wie man
das dann mit Hegel verbinden kann und so weiter. Das ist dann sehr hypothetisch und ich habe auch
nicht alles verstanden wahrscheinlich. So oft, wenn es um Hegel geht, ne? Genau. Ja, also beginnen
du das Ganze mit einem Beispiel, mit einem Werbespot. Vielleicht kennst du den. Es ist ein Werbespot,
der von einer Kunstakademie, glaube ich, herausgegeben wurde. Es ist ein Fake-Werbespot und da sieht man
ein Auto, das fährt auf einer Straße in Braunau und das ist ein selbstfahrendes Auto und dann
kommt ein Kind vor das Auto. Also es läuft, es spielt mit einem Ball, läuft vor das Auto und
wird von dem Auto umgefahren. Oha. Und dann zoomt das so raus und man sieht dann so die Gliedmaßen
des Kindes formen dann so ein Hakenkreuz und die Quintessenz ist eigentlich, dass dieses
selbstfahrende Auto die Gefahr erkannt hat, die von diesem Kind ausgeht und dieses Kind sollte
Adolf Hitler darstellen. Ah, okay. Sag dir das was, dass du davon hast, das mal gesehen. Ah, ganz,
ich erinnere mich ganz dunkel an irgendwie sowas, aber ich habe keine Bilder dazu im Kopf. Also ich
habe gerade dieses Hakenkreuzbild müsste ja wahrscheinlich eigentlich relativ einprägsam
sein. Insofern vermute ich mal, dass ich diesen konkreten Spot nicht gesehen habe. Aber so dieses
Motiv, dass irgendwie auch Zeitreisende irgendwie Hitler als Kind umbringen oder so, das ist
natürlich nicht ganz fremd. Genau, ja. Also es ist, ich finde es sehr ins Pferd dieses Video einfach,
weil es macht es schon, also es bringt halt das ziemlich gut auf den Punkt. Also diese Frage nach,
unter welchen Umständen darf man eigentlich töten, um Leben zu retten? Ganz grundsätzlich,
aber auch ganz spezifisch, was bedeutet das, wenn wir Maschinen haben, die all das über uns wissen?
Also man sieht im Spot nun ein Kind, das mit einem Ball spielt. Und im Essay geht es eben zuerst
um diese Frage, was bedeutet das? Darf man töten, um Leben zu retten? Und aktuell gibt es das schon
so ein bisschen auf eine Art und Weise was Ähnliches, was man so beim Predictive Policing
hat. Also diese Vorhersagen der Analyse, was man im Moment auch, basierend auf Daten vor
allen Dingen, versucht vorauszusehen, wann jemand kriminell werden könnte. Oder diese
Täterprofile erstellt. Und also das gibt es schon so ein bisschen. Ich glaube, das wird auch ganz
viel mit Räumlichkeit gemacht. Also ich kenne das irgendwie mehrfach gelesen, dass man so versucht,
gar nicht mal herauszufinden, wer begeht als nächstes ein Verbrechen, sondern wo wird als
nächstes ein Verbrechen begangen. Was natürlich wesentlich unproblematischer ist, weil mal eben
eine Streife irgendwo lang fahren zu lassen, um zu gucken, ob wirklich was passiert. Das ist
natürlich wesentlich weniger invasiv, als mal in Anführungszeichen mal einfach so irgendjemanden
festzunehmen. Ja, und da hast du schon gleich wieder dieses inhärente Problem. Wenn du das dann
machst, dann hast du eine Intervention und die Daten. Also du weißt ja dann nicht, ob das
tatsächlich passiert wäre. Also was ist jetzt, was hat das verhindert? Die Streife oder war die
Vorhersage falsch? Das ist ein anderes Thema. Basierend von dieser Frage, bringt er dann das
Beispiel von dem Buch und vor allen Dingen auch von dem Film, ich glaube ich ist es,
ein Theaterstück von Chirach, das heißt Terror. Und das war von ein paar Jahren, wurde das
aufgeführt, auch im deutschen Fernsehen. Und da geht es darum, dass eigentlich ein Flugzeug mit
160 Menschen an Bord wird von einem Terroristen oder einer Terroristengruppe gekapert und auf die
Allianz Arena in München zugesteuert. Die ist voll, die hat 70.000 Menschen da drin und dann geht
es eigentlich darum, was soll man machen, weil es gibt die Möglichkeit, dieses Flugzeug abzuschießen.
Und man hat dann eigentlich das Ganze so aufgebaut, dass dann das Publikum partizipativ das entscheiden
kann, was man macht. Genau, und das wurde an vielen Orten aufgeführt und gemacht. Das ist
wie so eine empirische Studie schon fast. Also was machen die Menschen? Und interessant ist,
dass wenn man jetzt diesen Outcome hat, dass das Flugzeug abgeschossen wurde, also dass eigentlich
diese 160 Menschen im Flugzeug geopfert wurden, zugunsten dieser Tausenden in der Arena,
dass das eigentlich für die meisten in Deutschland jetzt okay ist. So im Sinne von die Personen,
also der General in diesem Fall wurde freigesprochen. Und das steht aber eigentlich
nicht, also das ist unvereinbar mit dem deutschen Grundrecht. Also Menschenleben dürfen nicht
aufgerechnet werden. Und spannend ist natürlich, dass eigentlich intuitiv dann die Mehrheit von
diesen Personen verfassungswidrig fühlt sozusagen. Und das ist so ein bisschen eines dieser
Grundprobleme. Und er sagt dann auch, und das finde ich interessant, dass Deutschland ganz besonders
damit oder mit diesen Technologien oder mit dieser Frage ein Sonderfall darstellt, weil hier diese
Pflichtenethik, wir kennen das von Kant, also dieses deontologische, sehr eigentlich tiefer
ankert ist. Im Gegensatz jetzt das zu der konsequenzialistischen Ethik, also dem
Utilitarismus klassischerweise, kennt man das, was zum Beispiel im angelsächsischen Raum viel
verbreiteter ist als jetzt bei uns. Und das ist, ja finde ich spannend und ich finde es auch
interessant, dass weil 2006 offenbar das Luftsicherheitsgesetz, so heißt es, hat genau
einen Absatz drin, wo es um dieses Abschießen von Flugzeugen bei Terrorgefahr geht. Und das
wurde aber vom Bundesverfassungsgericht als nicht verfassungswidrig eingestuft. Also wurde
wie abgekanzelt, aber die Menschen scheinen das eigentlich gut zu heißen. Ich habe dann so ein
bisschen geschaut bezüglich der Menschenwürde, das ist auch nämlich ein Konzept, was nicht in
allen Verfassungen steht, also Deutschland hat das drin, ein paar andere EU-Länder hat das auch,
die Schweiz hat das auch drin in der Bundesverfassung und dann noch irgendwie Südafrika und Kenia,
aber sonst ist das gar nicht oft explizit erwähnt. Das ist diese Menschenwürde, sondern eher dann
so Freiheit und was man sonst so darunter subsumieren könnte, aber die Menschenwürde als
Prinzip so erwähnt ist, da ist auch Deutschland so ein bisschen ein Sonderfall. Wenn man das
philosophisch betrachtet, dann ist dieses Problem von dieser Aufwiegung von Menschenleben das
klassische Trolley-Problem, dieses Weichensteller-Problem, das kennen die meisten. Straßenbahn auf der einen
Seite tötet sie fünf Menschen, auf der anderen tötet sie eine Person, ist es okay, wenn man da die Weiche
umstellt. Dann gibt es so die ziemlich provokante Variante mit dem fetten Mann-Problem, hast du die
von schon gehört? Ich kann mir, glaube ich, grob was darunter vorstellen. Ja, geht einfach darum,
dass man die Straßenbahn stoppen kann, indem man einen fetten Mann davor wirft und die dann so
gestoppt wird. Also ist natürlich ganz provokativ und absichtlich so doof formuliert, aber das ist
so die nächste Variante und geht einfach darum, dass man natürlich Menschen nicht so im kantischen
Sinne als Zweck dafür verwenden bzw. als Mittel verwenden kann, sondern sie immer als Zweck sehen
sollte. Und wenn man jetzt das weiter, also ein anderes Beispiel von dieser empirischen Ethik,
eigentlich wie wir das bei diesem partizipativen Theaterstück sehen, gibt es auch sowas vom MIT,
das heißt Moral Machine. Das ist auch ganz spannend, das kann man sich noch anschauen,
das lief von 2016 bis 2020 und nimmt eigentlich genau dieses Weichenstellerproblem und man kann
dann, man konnte dann einfach so sagen, ja es gibt immer zwei Varianten, man konnte sich dann
für eine von beiden entscheiden und die wurden so konstruiert, dass da noch andere Dinge drin sind,
also man kann da entweder von jungen und alten Menschen, also man unterscheidet zwischen jungen
und alten z.B. oder tatsächlich es hat irgendwie übergewichtige Menschen da drin, also das ist
auch eine Kategorie oder halt eine Schwangere oder eine Obdachlose und dann gibt es so abstruse
Szenarien wie eben zwei Obdachlose fahren in einem selbstfahrenden Auto und sollten die eine
schwangere Frau überfahren, die aber über Rot, also als die Ampel auf Rot ist, die Straße überquert.
Also es ist irgendwie noch mehrschichtig und es kommt dann eben auch dieser Gesetzesverstoß
jetzt hier im Sinne von man geht bei Rot über die Straße, kommt dann auch noch dazu und der
Output davon ist auch, das wurde mit mehr als zwei Millionen Menschen haben das ausgefüllt auf der
ganzen Welt und nachher gibt es sehr starke regionale Unterschiede, also man hat dann gesehen,
dass in Lateinamerika und in Afrika beispielsweise Personen tendenziell eha die Kinder favorisieren,
also die Kinder nicht sterben lassen, dann in asiatischen Kulturen oder im asiatischen Raum
eher die Älteren bevorzugt werden und so, also dass es da wirklich ja auch regionale Unterschiede
gibt, macht irgendwo durch auch Sinn, aber stellt natürlich dann die Frage, ja wenn es jetzt
tatsächlich darum geht, das in einem Auto zu verbauen, wie soll das denn gehandhabt werden?
Ja, insbesondere auch eben wenn es dann um diese Regeln, also Südostasien und arabische Staaten
haben dann eher die verschont, die die Regeln befolgen beispielsweise.
Was hätte ich jetzt mir vorgestellt, dass das so im deutschsprachigen Raum oder zumindest in
Deutschland auch eine große Rolle gespielt hat, so wer die Regeln befolgt ist erstmal sicherer als
jemand der über die Straße geht, aber soweit geht unser deutscher Michel dann vielleicht doch nicht.
Weiß ich nicht, also müsste man die Auswertung genau anschauen, das ist jetzt nur so ganz grob
abgebrochen, übers Knie gebrochen, was für Tendenzen es gibt, aber man kann das auch,
ich glaube es gibt ein Buch dazu, das heißt The Car That Knew Too Much von Jean-Francois Bonnefond,
ich glaube das Buch fasst die Ergebnisse von diesem Moral Machine zusammen, also falls man
das da sich anschauen möchte. Auf jeden Fall ist also dieses Grundproblem ein bisschen auch an
diesen an diesen Beispielen, dass man hat wie oder das ist auch was Siemanowsky kritisiert,
man kann sich gar nicht dagegen entscheiden, du kannst gar nicht dieses, hier ist es ja ganz
klar utilitaristisch, man muss wie zwischen zwei Dingen abwägen, man kann sich dagegen gar nicht
entscheiden, du musst eine Entscheidung treffen. Nur schon wie das dieses Experiment aufgestellt
ist und das ist auch so ein bisschen das Problem mit diesem Todesalgorithmus, wie er den nennt im
Auto, dass der kann nicht nicht programmiert werden, oder? Man muss den irgendwie da rein
packen und jetzt in unserem Verständnis liegt der Gesetzesverstoß eigentlich in dieser
Vorentscheidung, also es ist nicht, es ist eben nicht diese Reaktion oder dieses reflexhafte,
was zu der Entscheidung führt, sondern dass man das so kaltblütig sag ich mal im Vornherein
programmiert. Ja und auch irgendwie formalisiert und allgemeingültig, also es ist ja, wenn jetzt
irgendwie eine Person fahren würde, die würde halt, also nehmen wir mal an, sie hätte Zeit zu
entscheiden in der Situation, was jetzt irgendwie nicht wirklich wahrscheinlich ist, aber sie hätte
Zeit zu entscheiden, dann würde sie halt nach ihren eigenen Biases und ihren eigenen Verzerrungen
irgendwie halt entscheiden und dann wäre das halt für diesen einen Fall, aber wenn du es, wenn du es
einem automatischen Auto gibst, dann ist es ja wahrscheinlich, also es kommt immer die Frage,
wie weit das vorprogrammiert ist oder sich quasi im Lernprozess ergibt, aber dann ist es halt
wahrscheinlich so, dass es halt immer dasselbe, die selbe Entscheidung treffen wird. Genau, ja und
das ist, damit sprichst du auch so ein bisschen mit einem Kernproblem an, was ich, also ich fand,
das bringt ein bisschen spät, aber ich finde halt den Gedanken interessant, wenn er sagt, wenn wir
von den Daten sprechen, wenn das jetzt, wenn jetzt alle Daten zur Verfügung stehen, dann ist es ja
nicht mehr dieses Partikulare, das fällt dann ja wie weg und es ist auch nicht per se die gute
Entscheidung, die dann getroffen wird, sondern einfach das Mittel aus allem, oder? Also siehst du,
dieses, dieses Gut und Schlecht, das fällt sowieso ein bisschen weg beim Algorithmus, aber es ist auch
nicht mehr, also es ist einfach so ein Mischmasch von der Mehrheit eigentlich, also die Mehrheit
gewinnt sozusagen in den Daten, weil das ist halt das, was die Überhand nimmt. Ja, wobei ich glaube,
da muss man noch mal gucken, ob es ein Mischmasch ist oder ob es sowas wie, wie du gerade sagtest,
die Mehrheit ist, also ein Mischmasch wäre ja irgendwie so, auch wieder vielleicht eine
probabilistische Entscheidung, in 60 Prozent der Fälle macht das so, in 40 Prozent der Fälle
macht das anders, die andere Variante wäre halt, die 51 Prozent würden es so machen, also macht
es das Auto immer so, das ist ja auch noch mal eher so eine Median, also man nimmt die Median als
absolut wert sozusagen, vielleicht eine Entscheidung wäre, dass sie da mal zwei Perspektiven. Stimmt,
das stimmt, aber das ist natürlich die Frage, ob der Median, das ist was, also ich meine,
ob das überhaupt so umgesetzt wird oder ob auch das natürlich das noch das Erstrebenswerte ist,
aber du hast recht, ja, also Mittelwert und Median können hier in dem Fall natürlich ganz
weit auseinander liegen, je nachdem, wie man es sich anschaut. Auf jeden Fall geht es dann
natürlich darum, wenn man diesen Todesalgorithmus, wie er den nennt, programmiert, stellt sich
natürlich die Frage, ja, wer hat den Vorrang, also wie kann man sowas entscheiden. Und was
ein spannender Gedanke ist, ist natürlich, ja, die AutoherstellerInnen haben natürlich, die würden
es am liebsten so gestalten, dass die Insassen immer Vorrang haben, weil klar, also bringt
natürlich einen Wettbewerbsvorteil für dein Auto. Du bist geschützt? Genau, du bist geschützt und
es ist so, dass offenbar empirische Studien gezeigt haben, er hat die nicht genau zitiert,
ich weiß nicht, wie die zustande gekommen sind, aber es ist wohl so, dass viele Personen eher
dazu neigen zu sagen, ja, eigentlich würde ich mich lieber opfern, also wenn es um so eine
Entscheidung geht, aber ich würde nicht so ein Auto kaufen, das mich opfert. Im abstrakten sind
wir gut, im konkreten dann vielleicht nicht so. Und hier eben, also diese, wer hat Vorrang, diese
Frage ist natürlich dann, die kann man ganz ad absurdum führen, wenn es eben darum geht,
was soll alles berücksichtigt werden. Wir haben jetzt hier ganz, ganz wenige Proxies, die uns so,
die naheliegen, das ist irgendwie das Alter, das kann ich mir vorstellen, es hat kulturell vielleicht
einen Einfluss eben, wie es schon erwähnt, aber nur schon wenn es dann darum geht, irgendwie Tiere,
was machen wir da, oder wenn es dann eben heißt, ja, diese KI, die hat dann plötzlich Zugang zu
allen möglichen Daten und sieht dann ja, okay, da hat es eine Fahrradfahrerin, aber die ist auch
Vielfliegerin und dann hat es hier eine Schwangere, aber die ist des Todes geweiht und dann hat es
noch ein älterer Herr, der hat schon das und das Gutes getan und wenn es dann zu einer Aufwiegung
kommt von, also das ist natürlich dann wirklich schwierig. Ja, und eben, also dieses, dieses
auswählen, also es ist klar, man kann das nicht einfach den Herstellern und Herstellenden von
Autos überlassen, diesen Algorithmus zu programmieren, aber es stellt sich ja auch die Frage,
ja, soll ich den selber auswählen dürfen, wenn ich ein Auto kaufe? Also, was mache ich denn so,
irgendwie Sonntagnachmittag im Park und dann klicke ich mir hier mein Todesalgorithmus zusammen für
mein nächstes Auto? Seltsam, oder? Ja, definitiv. Und wie macht man das? Also, gibt's, sollte man
dann verschiedene Möglichkeiten erlauben, soll man irgendwie die Leute dazu zwingen, keine Ahnung,
mit einem Ethikbeauftragten zu sprechen, bevor man so ein Auto kauft, gibt's dann irgendwie Autos auf
dem Schwarzmarkt, die dann eben einen anderen Algorithmus drin haben, den wir nicht so verwenden.
Also, das sind alles Fragen und das fand ich mega spannend, weil ich bin da nicht drauf gekommen und
er hat das dann so aufgebracht, ja, gibt natürlich keine Lösung dafür, aber auf jeden Fall muss man
darüber nachdenken. Ja, ganz grundsätzlich gibt es als Gegenargument dann so dieses statistische
Argument, das auch heißt, man darf, also, es klingt ja alles so ein bisschen, ja, sollte man das gar
nicht benutzen oder gar nicht erst anfangen, aber so, das Konterargument ist ja immer, ja, auch wenn
wir dann selbstfahrende Autos haben, wir können das natürlich nicht verhindern, dass das zu
Unfällen kommt, aber insgesamt retten wir trotzdem sehr viele Menschenleben dadurch, dass man davon
ausgeht, dass Unfälle sehr viel weniger werden und wir eigentlich eine Verpflichtung dazu haben,
dass diese Technologie auch zustande kommt deswegen. Und was ich auch ein sehr spannender und ein guter
Gedanke fand, ist, dass er sagt, das Risiko jetzt im Straßenverkehr, das geht ja nicht von Kindern
und Fahrradfahrern aus, sondern das geht vom Autoverkehr aus, der per se, also der Autoverkehr,
der sagt dann ja, das ist der, der schwere Gegenstände so schnell durch den Raum bewegt,
dass ein Zusammenstoß tödlich sein kann, das ist das Problem. Unfallopfer können schon gesenkt
werden mit selbstfahrenden Autos, aber die Opfer an sich sind Folge vom Autoverkehr. Ja, stimmt.
Finde ich auch. Kann man natürlich dann auf andere Dankendinge anwenden, das kann man sehr
weiter spinnen, aber trotzdem finde ich das ein sehr valides Argument und auch für die Konsequenz dann,
dass man sagt, ja, es kann natürlich schon angemessen sein, dass man dann sagt, die Opfer
sind die NutzerInnen dieser Mobilität. Ja, ich finde, da wird noch was anderes schön deutlich.
Da wird eben deutlich, dass wir diese Abwägung, nur halten ihn nicht ganz so plakativ, aber doch
auch täglich treffen. Ich meine, auch jetzt schon haben wir ja die Entscheidung getroffen,
dass, ich weiß nicht, wie viele es sind, irgendwie mehrere hundert Verkehrstote jedes Jahr, die
Bequemlichkeit und die schnelle Bewegung anderer Menschen wert sind. Sprich, anscheinend rechnen
wir Menschenleben sogar gegen etwas so Abstraktes und erstmal, na ja, maximal vielleicht ökonomisch
und irgendwie komfortmäßig relevantes, die Bewegungsgeschwindigkeit auf. Also wir tun es
da ja sogar schon. Naja, das ist recht. Eben per Gesetz. Und wenn man weiterdenkt, jetzt irgendwie
Klimaschutz oder Radverkehr in Berlin ist ja gerade großes Thema, da wird das mit jeder Entscheidung
wird das neu getroffen. Wer sozusagen ein Risiko eingeht, wessen Leben in Gefahr gebracht wird und
wem dadurch welcher Nutzen entsteht. Und da ist genau diese Abwägung, nur dass sie halt nicht so auf
diesen einen Punkt fokussiert ist, der uns allen dann sofort irgendwie in sich widersprüchlich und
konfliktär erscheint. Ja, ja, ja, das ist absolut recht. Ist auch irgendwie noch, man kann das dann
so ein bisschen ins Positive drehen, indem man dann sagt, ja, jetzt in dem Fall von selbstfahrenden
Autos, ja, ich nehme sozusagen dieses kleine Risiko eines Unfalls auf mich für das größere Gut,
dass selbstfahrende Autos insgesamt Unfälle vermindern. Ja, also es ist so wie das. Ich
opfere mich für das Gesamtwohl. Das würde ja dann dafür sprechen, dass im Grunde der Fahrer die
letzte Priorität in der Entscheidung sein dürfte. Genau, ja, je nachdem. Also es kann ja auch
prinzipiell. Ja, genau. Also wenn man jetzt das so programmiert, dass die Insassen vom Auto nicht
die höchste Priorität haben. Ja, das wäre ja, ich als Fahrer, der ich mich in so ein gefährliches
Gefährt setze, weiß das und nehme dann den Großteil des Risikos auf mich. Wobei dann
natürlich wieder so Konflikte entstehen. Bringe ich den Fahrer jetzt um oder breche ich den Passagier
beide Beine, dass er danach im Rollstuhl sitzt? Da wird ja noch mehr Differenzierung,
da kommt ja noch mehr Differenzierung rein. Ja, also ich finde eben, die Differenzierung,
die hört ja nie auf. Und ich finde das schon auch das Beängstigende, insbesondere,
wenn es eben um diese Datenmacht geht, weil das ist ja schon ein bisschen so der Trend,
dass wir uns immer mehr dieser Datenmaschine hingeben. Und das kann ich mir vorstellen,
dass es tatsächlich irgendwann mal zu einem Problem führen wird für solche Entscheidungen.
Weil ich da auch, ich bin da immer so ein bisschen hin und her gerissen bei den Problemen dieser
Entscheidungen. Weil es ist ja nicht so, als würden diese Entscheidungen jetzt nicht getroffen. Also
ich habe da auch keine Antwort. Ich will auch nicht sagen, dass die dem jetzt völlig gegen
argumentieren. Aber muss ich halt auch vor Augen halten, wie so Entscheidungen jetzt getroffen
werden. Wenn es jetzt um was kleineres geht als ein Autounfall umgebracht zu werden, sondern
vielleicht um eine Kreditvergabe oder so. Wenn ich da vorher einen Bankberater, vorher vor eine
Bankberater gesessen habe, da hat es vielleicht noch mehr eine Rolle gespielt, welche Hautfarbe
ich denn habe, als jetzt, wo es halt mein Kreditscore bei der Schufa ist. Gut, der wird
wahrscheinlich auch wieder von der Hautfarbe ein bisschen beeinflusst an den ein, zwei Stellen.
Aber dessen Einfluss geht wahrscheinlich erstmal ein bisschen runter. Zumindest solange du,
naja, vielleicht dann doch irgendwie eine Rassisten oder eine Rassisten vor dir sitzen hast bei der
Bank. Wenn du da jemanden hast, der das für sich reflektiert und bewusst versucht irgendwie
auszublenden, dann hast du vielleicht an der Stelle mehr Glück. Aber das heißt ja auch nicht,
dass die menschliche Entscheidung immer besser sein muss als die der künstlichen Intelligenz oder
fairer oder weniger verzerrt. Sie hat halt nur eine wesentlich größere Streuung gefühlt.
Ja, einverstanden. Aber ich sehe das Problem ein bisschen weitergehen als dass man halt,
also wir haben ja so, wir hinterlassen so eine Datenspur, oder? Und ich höre von ganz vielen
Menschen, mit denen ich darüber spreche, ich habe ja nichts zu verbergen. Ja, okay, würde ich auch
von mir sagen, ich mache ja nichts Illegales und so weiter. Aber das Problem liegt ja darin,
dass diese Daten, die gibt es auch noch, nachdem wir, nachdem es uns nicht mehr gibt. Potenziell.
Also die bleiben erhalten, aber die bleiben auch einfach noch in zehn Jahren erhalten und wir
wissen nicht, was dann zum Beispiel plötzlich als Norm gilt. Das ist richtig. Sagen wir, wenn es jetzt
zusammengeht, ich mache das Beispiel dann meistens mit genetischer Sequenzierung. Klar kann ich das
rausgeben, weil ich bin gesund und so, ich habe nichts zu verbergen. Aber vielleicht kann man dann
in zehn Jahren aus meinem Genom rauslesen, dass ich so und so eine Störung oder ein Persönlichkeitsprofil
habe, das vielleicht dann nicht mehr so ganz zu meiner Versicherung passt oder zu was auch immer.
Und dann gibt es ganz neue Diskriminierungspotenziale und das sehe ich halt schon problematisch mit Daten.
Und jetzt haben wir halt so ganz klare Proxys oder man nennt das auch diese schützenswerten
Elemente, die man festgelegt hat. Genau, genau, richtig. Und gegen die man nicht diskriminieren
darf. Achso, das meinst du, ja, okay. Das meine ich ja. Also Gesundheitszahlen, die sind schützenswert,
auf jeden Fall. Aber wir sprechen mit Hautfarben, von Religionen, von solchen Dingen. Gegen die darf
man per Gesetz nicht diskriminiert werden, wegen diesen Dingen. Aber wohin das dann irgendwann
führen wird, das wissen wir nicht. Ja, das stimmt. Das finde ich schon problematisch, weil eben diese
KI potenziell Zugang zu diesen Daten haben wird. Ja, wobei das, da braucht es ja nicht mal eine KI,
das sieht man ja gerade in den USA ganz schön, also schön in Anführungszeichen, in den Bundesstaaten,
wo es jetzt um die zielische Versorgung von Transidenten, Kindern zum Beispiel geht, die
halt wollen tatsächlich, ich weiß nicht mehr was es war, aber wirklich rückliegend irgendwie
Chatverläufe oder sowas ausgewertet werden oder Kommunikation zwischen Ärzten und PatientInnen und
dann daraus irgendwie auch geschlussfolgert werden kann, wo irgendwie eine medizinische
Behandlung stattfindet, die nach dem neuen, nach der neuen Doctrin nicht mehr opportun ist.
Naja, du hast recht. Und da braucht man doch nicht mal eine KI für. Ja, das stimmt, das gibt es
schon jetzt und es gibt auch genügend Beispiele wie, also eben für algorithmischen Ungerechtigkeiten,
also das haben wir schon jetzt, da braucht es wirklich keine KI für. Aber eben, also wer diese
Merkmale festhält, ich weiß nicht, ich habe schon das Gefühl, manchmal hängen wir da schon ein
bisschen so hinterher, wenn es um diese Diskriminierungsmerkmale geht, aber ja,
meinen Standpunkt ist halt, wir wissen nicht, was es in Zukunft geben wird, wogegen oder wofür wir
diskriminiert werden können und deswegen müssen wir eigentlich jetzt schon damit beginnen, dass
wir unsere Daten schützen, aber ja. Wobei ja auch dann die Frage ist, wie dann in der KI kannst du
es dann auch viel weniger festlegen, du kannst halt viel weniger sagen, an welchem Prozentsatz oder an
welchem Faktor hängt, weil ja auch die internen Parameter und sowas, die in den Entscheidungen
einfließen, ja auch gar nicht mehr menschlich irgendwie interpretierbar sind. Das ist ja auch
wieder noch so ein Punkt. Stimmt, ja, stimmt, aber es gibt natürlich schon auch hier Möglichkeiten,
also zu dieser Interpretability von, Interpretierbarkeit von KI, das ist schon,
das ist auch gefordert, also auch diese EU-AI-Legal Act und so weiter, also das ist schon mit bedacht
und da gibt es schon auch Tools, aber das ist recht, also ganz, ganz nachvollziehbar wird das
nicht mehr sein irgendwann. Also so, dass man, dass ein einziger Mensch das verstehen kann bis,
bis in alle Details. Ja, also eben, also dieses, diese ganze, diese ganze Thematik führt ein bisschen
auch zu der Frage, ja, können wir überhaupt einen universellen Konsens dafür finden? Das ist so die
Frage, die er auch aufwirft. Führt Technik zu einem transkulturellen Einvernehmen? Kann die
Technik eigentlich, ja, die Werte, bringt die sie selbst mit, die ihre Operationsweise bestimmt?
Das ist ein bisschen die Frage, wenn es auch um die starke KI geht. Und was ich noch spannend finde,
ist eben diese Ambivalenz. Ich kann es noch nie so ganz einordnen, aber ich finde es interessant,
weil dieser Individualismus, in dem ja eigentlich alle so leben und das auch, was ein bisschen immer
zum Thema wird, das wird einfach relativiert. Also die, es kommt dann, es ist so ein starkes
utilitaristisches oder so ein starkes Gruppenelement oder Moment da in all diesen Argumentationen,
dass das Individuum da wirklich untergeht. Und das finde ich, finde ich irgendwie eine Spannung,
so eine Spannung zu dieser Gesellschaftsidee, die wir, wie ich sie wahrnehme und wie das dann
vielleicht irgendwann, ja, sich entwickelt. Ja, es ist irgendwie so beides. Es hat einerseits
diesen individualistischen Charakter, weil es eben, sagst du, diese Sakrosantheit des Individuums,
die setzt es ja schon irgendwie an. Aber gleichzeitig, der ich jetzt auch in keiner Weise widersprechen
will, aber gleichzeitig macht es eben so diese, ja, diese verbindliche Festlegung von tatsächlichem
Handeln. Also das ist ja im Grunde, es gibt ja auch diesen schönen Satz von Code is Law, also
Programmiercode ist, sind Gesetze in gewisser Weise, weil das ist halt nicht nur ein Gesetz,
wo irgendwo steht und falls du dagegen verhandelst, falls du dagegen handelst, wirst du vielleicht
bestraft, sondern das wird halt immer so passieren, weil Code da einfach sehr deterministisch ist in
dem Moment. Und ja, wenn das da einmal so drin steht, dann wird das auch immer so passieren.
Dann kann nicht danach in einen Richter sagen, ja, das war vielleicht gerechtfertigt, dass die
Person in der Situation anders gehandelt hat. Es wird einfach nie anders gehandelt werden.
Ja, ja, hast du recht. Ja, ich finde halt dieses Deterministische ist halt irgendwie,
ist halt irgendwie noch, noch, noch schwierig, weil ich, ich würde so spontan dazu trainieren,
man könnte das doch einfach lösen, indem man jetzt ganz konkret auf diesen Todesalgorithmus,
indem man da die, den Zufall entscheiden lässt. Ja. Oder? Und ich muss sagen, ich finde das sehr
unbefriedigend, wie er das abgehackt hat. Das ist da irgendwie so zwei Sätze zu und irgendwie
trotzdem, das ändert dann nichts daran, sagt er, auch das degradiert die Person im Straßenverkehr
zu einem bloßen Objekt. Und ich hätte mir da ein bisschen mehr erhofft zu diesem Argument, weil
für mich scheint das nicht, gar nicht so unplausibel zu sein. Mit dem Zufallgenerator. Ja, oder? Ich
weiß nicht. Also so intuitiv finde ich das gar keine schlechte Idee. Ja, die Idee hatte ich
tatsächlich auch. Die Frage ist dann erstens, wie gewichtest du den Zufall? Also wirfst du eine
Münze oder machst du vorher irgendeine Gewichtung? Dann stellt sich das Problem im Grunde gleich
genau wieder. Was gewicht ich jetzt höher, was gewicht ich niedriger, das, ja. Trotzdem ist
es irgendwie eine Bewertung von Menschen. Andererseits ist es aber auch genau der Punkt,
wir machen das ständig implizit. Und ich glaube, es herrscht da irgendwie das, was ich auch gerade
sagte, so ein bisschen so ein Bild vom Menschen als jemanden, der irgendwie automatisch richtig
oder neutraler handelt oder dass eine menschliche Entscheidung in irgendeiner Form inherent an sich
wertvoller wäre als eine KI-Entscheidung. Also das ist ganz, ganz schwer irgendwie zu fassen,
was sagt das eigentlich über unsere Sicht auf menschliche Entscheidungen aus,
wie wir über Entscheidungen künstlicher Intelligenz, ich setze das ganz gerne in
Anführungszeichen, wie wir darüber denken. Das sagt ja auch was darüber aus, wie wir über
Entscheidungen echter Intelligenz denken. Total, aber da kann natürlich das Argument ins Spiel
gebracht werden, das sagt ja, also wir sprechen jetzt von Willensfreiheit in den Menschen
beispielsweise, die gibt es ja gar nicht. Der Mensch ist eigentlich auch nur Algorithmus,
das ist ja auch eine philosophische Standpunkt, die ganz aktuell ist. Aber da kannst du sagen,
ja gut, also dann ist es... Meistens von den gleichen Menschen, die sagen,
dass KI das doch genauso gut entscheiden kann, logischerweise. Ja, also generell mit dieser
Anthropomorphisierung der KI, das ist auch so etwas, das mich persönlich nervt das immer so ein
bisschen, ja KI übernimmt dann die Herrschaft, weil und Gier und Triebe und so weiter, wie der
Mensch. Ich kann das nicht so ganz nachvollziehen, weshalb man das immer so sieht. Und auch
Simonowski macht das eben, er sagt das auch, also es ist nicht klar, dass sich die KI tendenziell
grundsätzlich wie ein Mensch verhält. Aber er sagt dann einfach, wenn man davon ausgeht,
dass die Intelligenz so darin vorhanden oder so operiert, wie wir die im Menschen kennen,
dann kann es sein, dass sie tatsächlich auch so wie diese Triebe entfaltet, die auch zu
unserer menschlichen Intelligenz gehören. Und wie kann man das wie so ein bisschen verhindern? Da
gibt es so eine Möglichkeit oder eine, ja das heißt so eine KI-Nanny, also dass man wie eigentlich
die KI, die Erfindung dieser, sagen wir mal potentiell bösen KI überwachen lässt durch
eine andere KI, die wir weltumspannend einsetzen, die dann auch Zugang auf alle Daten hat und dann
verhindert, dass es eine KI gibt, die sich gegen uns richtet. Bester Plan ever, not. Ja ich bin
auch KI-Nanny, so der Begriff, irgendwie schräg. Aber ja, er hat recht, also es zeigt sich dann
auch, also wir haben nur schon Schwierigkeiten, wenn es darum geht, irgendwie eine Atomwaffen zu
beschränken, also das schaffen wir schon gar nicht irgendwie weltweit, geschweige denn Klimakrise und
so weiter. Und wenn es dann darum geht, eben wir sollten, also die KI soll ja eigentlich das tun,
was wir ihnen vorschreiben, dann stellt sich eben die Frage, wer ist dieses Wesen? Haben wir
überhaupt eine gemeinsame Vorstellung, was wir wollen? Also man geht dann immer so ein bisschen
davon aus, dass unser gemeinsames Interesse darin liegt, dass unsere Gattung überlebt,
oder? Das ist so das, was man uns als Menschheit unterstellt. Tut man das? Muss man ja wissen,
also das muss man ja wie als Vorbedingungen? Also wenn wir das täten zumindest, also wenn
man das empirisch überprüfen würde, machen wir gerade einen verdammt schlechten Job, was das
angeht. Sprichst du jetzt in Bezug auf Klima? Ja, genau, ja. Und das ist auch das Argument,
das er dann sagt oder verwendet, es geht jetzt ein bisschen um diese Öko-KI, also die ist ja
allwissende KI, wo es darum geht, ja, wenn gehen wir gesetzt, das Überleben unserer eigenen Gattung
ist unser gemeinsames Interesse, können wir davon profitieren, wenn wir eine KI installieren,
die uns eigentlich, also die uns regiert, also dass wir uns selbst entmächtigen durch diese KI.
Also eine KI, die unsere Aktionsmacht einschränkt, indem sie oder nachdem sie von uns dazu beauftragt
sind. Hast du deinen Hobbs gelesen, Leviathan? Das klingelt gerade bei mir, weil der Leviathan
ist ja im Grunde genau dieses Instrument mit einer unbeschränkten Macht, das einmal per Vertrag mit
dieser Macht ausgestattet wird und danach unser soziales Leben kontrolliert. Das stuppert doch
sehr danach. Du hast recht, ja, daran habe ich nicht gedacht, aber es ist genau das,
und Leviathan, das passt dann auch vom Namen zu so einem System, oder? Ja, genau, also es ist
halt so die Frage, wieso soll der Mensch seiner eigenen Bevorzugung zustimmen und ich glaube,
oder er sagt tendenziell, ist das gar nicht mehr so unvorstellbar, weil wir Menschen sind im Denken
besser als im Handeln und wir haben diese Probleme, wir haben diesen Present-Bias,
wir bevorzugen die Gegenwart gegenüber der Zukunft, wir haben diesen Regional-Bias, also wir sind
eigentlich so eine Zentralinstanz, die ist zwar unpersönlich, aber das wollen wir im Prinzip ja
genau, es soll eben nicht diese Partikularinteressen berücksichtigt werden, wenn es jetzt darum geht,
Öko-Richtlinien durchzusetzen. Er nennt das, du hast Hobson, er nennt das dann, das wäre dann
der Robespierre des 21. Jahrhunderts. Okay, wie Robespierre geendet ist, wissen wir auch. Genau,
ja, also es ist, man kann da viele Analogien anführen, aber ja, es ist wie, es läuft so ein
bisschen drauf hinaus, dass man auch natürlich in Frage stellen kann, ist die Demokratie überhaupt
dafür geeignet, also wenn wir von unseren eigenen Schwächen da, wie wir sie jetzt in dieser Form haben,
ausgehen, dann ist die Demokratie möglicherweise tatsächlich die schlechteste Form, um unser
Überleben zu sichern. Wobei ich dann jetzt so die Perspektive einer KI-basierten Expertokratie,
also einer Kaiokratie sozusagen, ist jetzt auch nicht unbedingt das, was ich unglaublich
erstrebenswert finde. Und warum nicht? Ja, genau, aus einerseits dem Grund, dass es halt wirklich
im Grunde eine komplette Selbstentmündigung sozusagen wäre, was ja im Grunde, wenn man
jetzt mal weiterdenkt, ist das, naja, auch eine Entmenschlichung sozusagen. Und ja, weil du eben
auch wieder nicht weißt, ich meine, was dich jetzt irgendwie an den eigenen Interessen entwickelt
oder so was, soweit will ich jetzt eigentlich gar nicht spinnen, aber wie vollständig irgendwie
so Informationen sind, die man glaubt zu haben, und dann lernt man irgendwann, dass die Welt doch
anders funktioniert. Das habe ich jetzt gerade in diesem Bauhausbuch gelesen, wo es halt auch
stark darum ging, dass am Anfang in der frühen DDR es durchaus Initiativen gab, also alte Innenstädte,
alte Innenstädte wie in Dresden oder so was, komplett abzureißen und die Städte komplett
neu zu planen nach einem Vorbild dessen, was wir heute aus den USA kennen mit irgendwie den
weitläufigen autozentrierten Städten und so. Und so sehr man das viel in der DDR verachten kann,
bin ich zumindest an dem Punkt ganz froh, dass sie es nicht gemacht haben. Dass sie, auch wenn es in dem
Moment nationalistisch motiviert war, gesagt haben, nee, wir rekonstruieren die Städte mal eher
wieder ein bisschen so, wie wir sie mal hatten. Das hat dann alles so seine Schwierigkeiten gekriegt,
aber das war die Grundidee dahinter, zumindest am Anfang. Und da bin ich dann auch wieder ganz
ganz froh, weil mittlerweile wissen wir, dass das die bessere Idee ist. Damals wussten wir das
vielleicht nicht unbedingt und sowas würde sich halt auch in einer KI in irgendeiner Form verankern
und festschlagen, weil alles so, die kann über alle Daten verfügen und völlig neutral, das ist
halt auch Fiktion. Das ist halt ein Gedankenexperiment und nicht mehr.
Total, das stimmt. Ich finde es spannend, wie du das, ich werde nachher noch gleich die Pünke
dazu schlagen, was du jetzt gesagt hast, wenn es um diese Willensfreiheit geht. Aber trotzdem ist
natürlich diese Demokratie, also es ist wieder so sehr, ich sage jetzt mal sehr deutsch, ganz doof
gesagt, was dieses Empfinden, ich teile das mit dir. Aber es zeigt ja schon auch ein bisschen,
wenn man jetzt nach China schaut zum Beispiel, so die Stabilität und Wohlstand, das kann ja schon
auch für die Menschen erstrebenswert sein. Also dass der Staat das wie sich erstellt und die
Würde wird dann nicht mehr individualistisch gedacht, sondern kollektiv. Dann hat man so
diesen pragmatischen Autoritarismus, wie man den eben in China beispielsweise sieht. Also man kann
schon auch dafür argumentieren und offenbar ist es auch so, dass immer mehr Menschen sich da gar
nicht mehr so abgeneigt sind. Wir haben da schon so ein bisschen eine Sonderposition, wie wir das
wahrnehmen. Und ich weiß nicht, ob sich das nicht irgendwann ein bisschen verschieben wird.
Ja, also ich finde das spannend, weil klar, wenn du sagst, das Überleben der Menschheit geht über
alles, dann wäre wahrscheinlich das schöne Begriff vom wohlwollenden Diktator. Der hätte
natürlich einen Vorteil, aber wer stellt bitte sicher, dass der in irgendeiner Form wohlwollend
ist? Wer kontrolliert den wohlwollenden Diktator? Und dann läuft man wieder in das Problem rein.
Also du sagst jetzt China Stabilität, da kann man schon für argumentieren, da ist natürlich was
dran, aber frag mal die Uiguren. Die sehen das Problem dann vielleicht aus einer anderen
Perspektive. Und das ist glaube ich wirklich schwierig. Ich meine, unsere demokratisch-kapitalistischen
Systeme haben das auch. Du findest ja auch viele Leute, die von dem System komplett vergessen und
abgehängt werden und sich dann irgendwie auch aus dem demokratischen Diskurs verabschieden. Damit
meine ich jetzt nicht AfD wählen, sondern nicht wählen. Ich will nicht das Narrativ vorantreiben,
dass das irgendwie AfD-Wähler die Armen abgehängt werden. Das ist nämlich leider nicht so. Aber
ich tue mich damit schwer, aber das Schöne ist, ich muss darauf keine definitive Antwort finden.
Ja, genau. Ich verstehe natürlich deine Ambivalenz, Gemi, genauso. Ich finde auch,
wie du das vorhin gesagt hast, diese KI-Okratie. Das ist auch so ein bisschen das, wenn es darum
geht. Man kann ja sagen, diese Willensfreiheit des Menschen wird so ein bisschen infrage gestellt
durch diese KI. Ein Aspekt davon ist eigentlich die Beschränkung der Freiheit, dass man anders
kann. Und zwar im Sinne von, wir gehen weg von dieser Disziplinargesellschaft, wie wir sie jetzt
kennen. Also wir werden dafür bestraft, wenn wir was falsch machen, hin zu eigentlich einer
Kontrollgesellschaft. Wir dürfen gar nicht erst mehr was falsch machen. Es wird so designed,
sagen wir mal, unser Leben, dass wir das gar nicht mehr können. Wir können nicht, genau. Das ist
noch mal was anderes als, wir dürfen gar nicht mehr. Genau. Und das ist das, was wir uns doch so ein
bisschen intuitiv, das ist so ein bisschen Kant und Leib, jetzt in ein gutes Handeln, ist nur dann
moralisch, wenn es freiwillig ist, so Kant, aus Pflichtgefühl muss das passieren, dann ist es gut,
moralisch gut. Und Leibniz hat mit seiner Theorie C, das ist die beste aller Welten, man kennt
so ein bisschen davon, es muss so wie das Üben der Welt geben, damit sich das Gute beweisen kann.
Ja. Das ist so, was ich auch fühle, sage ich mal. Und ganz lustig war dann, ich glaube,
es war ehemaliger Bundesverfassungsrichter, sah unter anderem ein Problem in dieser Auto-KI,
dass die FahrerInnen durch die KI an der Missachtung der Verkehrsregeln gehindert
werden können. Ja. Das war wie ein Argument. Die Menschen, die dürfen ja dann gar nicht mehr,
die können gar nicht mehr die Verkehrsregeln missachten. Und bei uns gibt es halt wie kein
Gesetz, das jetzt zum Beispiel Sicherheit immer vor Freiheit stellt. Und das ist so ein bisschen
auch was, was ich mir vorstellen kann, was in deinem Argument vorhin mitspielt. Was ist denn gut oder
wer stellt das sicher? Aber klar kann man sagen, wenn jetzt bei uns ökonomisches Wachstum und
Wohlstand und so und so als das Gute in dem Sinne definiert wird, dann kann man schon eine KI kriegen,
die das sicherstellt. Ja gut, wenn man das vielleicht wieder genau genug definiert
kriegt. Du kommst natürlich wieder darauf hin, dass du dein Ziel so konkret wie es irgendwie
geht nur definieren musst. Das erlebt man ja auch gerade bei, nehmen wir mal das einfache
Beispiel, ChatGPT. Schreib mir jetzt mal einen Text, der genau das tut. So und dann musst du
fünffach korrigieren und auf Vorschläge davon reagieren und antworten. Nee, mach das mal ein
bisschen größer. Oder nein, diese Zahl ist falsch. Bist du dann irgendwann nach einer Stunde im
Gespräch mit ChatGPT einen guten Text? Das in der Zeit hättest du ihn wahrscheinlich auch selber
schreiben können. Und das ist glaube ich noch mal so dieser Gedanke, der hinterlässt, die Präzision
des Ziels zu kennen. Die Funktion, die man da reinschreibt oder die Bewertungsfunktion, nach
der man einen Zustand bewertet, dass das eben auch einfach ein unglaublich schwieriger Aspekt ist,
weil du dann natürlich voll in so eine Reduktionismus-Falle reinläufst. Wie sagst du,
ja, ich bewerte alles in Geld. Zum Beispiel, wie es die Ökonomie ja gerne mal tut. Naja,
guter Punkt, das stimmt. Ja, aber das ist klar, man kann das natürlich dann auch wieder abtun,
das ist ein ganz spezifisches Implementierungsproblem. Ja, nee. Ja, nee, natürlich nicht,
weil das dann gleichzeitig eine riesige Frage ist, wie man dieses Ziel festhalten möchte.
Absolut. Es ist so ein bisschen ein Traum, der Traum der Technokraten, Technokratinnen. Auch so
ein bisschen nach Habermas, nicht der Bildungsbildungsprozess, sondern eigentlich
nur noch die Regelungsoptimierung ist relevant. Und da ist natürlich die Frage, was optimiert
man nämlich? Was will man denn am Schluss überhaupt erreichen? Und das geht dann über
in diesen zweiten Punkt, wenn wir davon ausgehen, dass eben diese Willensfreiheit in Frage gestellt
wird. Das ist eigentlich, dass der Mensch dadurch verlärmt, durch die KI etwas zu wollen,
im Sinne von seinen freien Willen überhaupt zu gebrauchen. Das geht so ein bisschen da in das
Argument, wir müssen das auch trainieren. Also was wir als wichtig oder richtig oder nicht richtig
empfinden oder machen oder und so weiter, das haben wir vielleicht nicht einfach in uns drin,
sondern das muss gelernt und trainiert werden. Das schließt wunderbar, also das, was du sagst,
das schließt an die nächste Folge an, die schon aufgezeichnet ist, aber natürlich noch nicht
veröffentlicht, wenn ihr diese hört, vermutlich. Da wird es um ein anderes Buch gehen, nämlich
Truckpoint Capitalism, wo es unter anderem, dass das Argument auftaucht, warum Dienste wie Spotify
ein großes Interesse daran haben, dass wir unsere Musik, die wir hören, nicht mehr selber aussuchen,
sondern uns einfach von Spotify das aussuchen lassen im Rahmen von Playlists. Das spielt ein
ganz ähnliches Element rein, dass man irgendwann dann verlärmt, sozusagen nochmal zu gucken,
was gibt es denn an KünstlerInnen, was gefällt mir denn an Musik eigentlich wirklich und wo sind
denn jetzt die feinen Unterschiede und wo bin ich jetzt bereit, eventuell Zeit oder Geld in einen
Künstler zu investieren und wo vielleicht auch nicht. Das finde ich einen ganz spannenden Punkt,
es gibt auch bei dem bei dem Chat-GPT-Thema so dieses Argument, ja, das übernimmt halt so die
langweiligen Arbeiten, also die standardisierten Arbeiten und so. Da sagt man, ja, das mag das mag
es sogar in manchen Bereichen können, aber woran übt man denn dann eigentlich, um die schwierigen
und die nicht so standardisierten Sachen überhaupt machen zu können? Weil jetzt nutzt man ja die
einfachen standardisierten Fälle, um für die schwierigen komplizierten Fälle zu üben. Aber
wenn man das nicht mehr macht, wer lernt denn dann noch die schwierigen komplizierten Fälle
zu bearbeiten? Also ganz spannende Argumente, die sich daraus ergeben. Ja, das ist auch ein
Argument, das ich ganz oft bringe, wenn es um in der Medizin darum geht. Also wenn ich mit meinen
Ärztinnen und Kollegen darüber spreche und dann kommt es ja, da wird uns dann das abgenommen,
zum Beispiel diese banalen Fälle auf dem Notfall, der Schnupfen, Husten und so, das ist dann weg und
das ist für mich ein totales Problem, weil mein Mensch, auch mein statistisches Empfinden einer
Krankheit, ist er ganz krass davon geprägt, dass sich neun von zehn Fälle sind banal und dann
erkenne ich den einen, der nicht banal ist. Und wenn irgendeine KI oder was auch immer mir diese
neuen banalen Fälle abnimmt, dann habe ich gar keine Relation mehr. Das spielt also ein bisschen mit
rein und deswegen, ich sehe das auch ganz und gar nicht positiv. Ein gutes Beispiel, um das von
Spotify aufzunehmen, das bringt er auch im Buch, das ist dieses Video, das von Google geleakt wurde
oder es ist nicht ganz klar, ob es einfach geleakt wurde oder absichtlich, wo es um diesen Selfish
Letcher geht. Ich weiß nicht, ob du davon gehört hast, da geht es darum, es ist ein internes Video,
soll auch das Thought Provoking sein. Also es geht darum, dass sie das so konstruieren,
dass der Mensch eigentlich ein Konto hat, wo die Daten drin sind und das geht dann so weit,
dass man das dann weitergeben kann. Dieses Mein-Daten-Konto wird dann irgendwann für die
ganze Menschheit zu Verfügung gestellt. Und es ist sehr unangenehm, das Video, wenn man da schaut,
weil es ist ein Beispiel, es geht dann so darauf hin, dass du hast eine App, dann kannst du da
einstellen, was dein Lebensziel sozusagen sein soll und alles wird dann danach ausgerichtet. Also
wenn du jetzt nur öko-bewusst, umweltbewusst leben möchtest, dann, wenn du eine Banane kaufst,
dann wird dir angezeigt, ja, aber du kannst hier auch die lokale, gewachsene Banane kaufen. Und das
ist ja noch okay, das ist so ein Nudging, so ein bisschen okay, man hat eine Alternative, aber so
der zweite Teil vom Video und so weiter geht dann darum, ja, irgendwann kann der Algorithmus auch
für dich entscheiden. Du hast dann gar nicht mehr die Wahl und es geht dann auch so weit, dass irgendwann
merkt dann diese Daten, ja, ich weiß jetzt gar nicht, wie schwer du bist, das heißt, ich gehe
mal auf die Suche nach einer Waage und präsentiere die dann so, als dass du die brauchst, damit ich
dann dein Gewicht kenne. Also schon ein bisschen verstörend, aber es ist neun Minuten lang, wer Lust
hat, ich kann das dann nicht schauen, es ist schon noch interessant. Ja, das wären so ungefähr die
Gedanken, die ich auch hier präsentieren wollte. Wie gesagt, der letzte Teil geht dann so ein bisschen
ins Philosophische auch interessant, wer dann so ein bisschen Heidegger und Kittler und eben Hegel,
ja, wer diese Argumente darin finden möchte, dem sei das nagelegt auch zu lesen. Ich habe das Buch
jetzt schon ein paar Mal durchgelesen, ich finde es wirklich, ja, es gibt viel, es liest sich ringen
und wer das Thema spannend findet, dem kann ich das echt empfehlen. Ja, super, danke dir. Ja,
viel gedacht, viel diskutiert, haben wir jetzt auch schon in der Vorstellung. Ich habe noch
so ein paar Anschlusspunkte, wo man noch mal drüber nachdenken könnte. Also ein Thema,
was ich gerade bei diesem Thema KI, was man immer so ein bisschen Blick drauf haben muss,
ich weiß nicht, ich sagte den Begriff Critihype, was? Nein. Das ist ein Begriff, den hat Lee Wenzel,
der auch schon mal hier im Podcast aufgetaucht ist, geprägt. Da geht es darum, dass gerade so diese
Kritik vor dem, was KI mal alles können wird und dieses Warnen davor, was das alles können wird,
auch eine Strategie sein kann, den Hype darum zu pushen. Einfach weil man eben, dass es das mal
können wird, ist gesetzt. Die Frage ist nur noch, wie gehen wir damit um? Das ist so ein schöner
Mechanismus, hat man jetzt bei JetGPT gesehen, dass so jemand wie der CEO von OpenAI, also den
Machern von JetGPT, dass der auf einmal sagt, man sollte sechs Monate alle Forschung an solchen
Modellen verbieten, weil die könnten ja irgendwann die Weltherrschaft übernehmen. Ja, das pusht
vor allen Dingen den Wert seines Produkts. Die Diskussion darum sorgt halt dafür, wie das kann
die Weltherrschaft übernehmen. Das muss ja was sein. Und so weiter und so fort. Auf einmal ist das
wieder in aller Munde. Also das ist so ein Punkt, den man immer so mit Blick haben muss. Das hatte
ich jetzt deiner Darstellung nach bei dem Buch nicht unbedingt, aber es ist ein gutes Korrektiv,
wenn man sich den Artikel von Lee Wenzel dazu vielleicht mal durchliest. Den packe ich logischerweise
in die Show Notes. Also das ist jetzt als Leseempfehlung etwas aus fühliger gemeint gewesen.
Mir sind noch zwei weitere Bücher über den Weg gelaufen, die ich ganz spannend finde. Also
Sachbücher. Einmal Choke Point Capitalism. Das werden wir euch in der nächsten Episode
vorstellen von Rebecca Giblin und Cory Doctorow. Und dann habe ich noch von Stefan Kühl. Das ist
ein deutscher Organisationssoziologe. Da gibt es das Buch Brauchbare Illegalität vom Nutzen des
Regelbruchs in Organisationen. Da musst dich dran denken, als du das gerade ansprachst mit dem,
man verhindert, dass die Leute Regeln brechen. Aber da muss ich genau an so eine Situation denken,
wie Autofahrer, das ist eine rote Ampel, Autofahrer sieht aber, dass da irgendwie,
dass er jetzt Leben retten kann, wenn er über die rote Ampel fährt, aber die rote Ampel lässt ihn
halt nicht. So, ne? Also manchmal kann es dann einfach Sinn machen, Regeln zu brechen. In
Organisationen ist das noch viel relevanter. Jeder, der irgendwie in einem Unternehmen oder in einem
größeren Kontext arbeitet, weiß, warum es manchmal auch sinnvoll sein kann, sich nicht ganz an die
Regeln zu halten. Aber das im Großen und Ganzen auch im Sinne der Regeln dann ist, wenn man das
in dem Fall nicht tut. Also da muss ich dran denken, als du den Aspekt ansprachst. Das waren
die zwei Sachbücher und dann noch zwei Fiktionen, fiktive Texte oder auch Filme. Also ein Film ist
sicherlich nicht unbekannt, das Minority Report. Das ist ja eine Kurzgeschichte, ich glaube,
von Philip K. Dick war sie oder war sie von, Gott, jetzt blamier ich mich gerade. Ja, aber ich hab's
auch so irgendwie abgespeichert. Und natürlich als Film, da ist sie halt bekannt und berühmt
geworden. Da geht es eben um dieses Predictive Policing, also dieses Leute festnehmen,
bevor sie ein Verbrechen vergehen, um das Verbrechen eben zu verhindern. Und dann gibt's
von Andreas Eschbach den Roman NSA, Nationales Sicherheitsamt. Ist ein bisschen kontrovers,
weil er halt so das Nazi-Judenverfolgungs-Setting als so ein bisschen Hintergrund für eine spannende
Roman-Geschichte nimmt, eine fiktive, weil er da eben das Nationale Sicherheitsamt, Abkürzung NSA,
was ein Zufall, zeigt, die irgendwie sämtliche Datenbanken und so was haben und daraus so Dinge
nutzen, wie die Leute kaufen mehr als für vier Personen notwendig ist, die beherbergen wahrscheinlich
irgendwie jemanden, den wir nicht finden sollen oder so. Und erzählt dann eben auf dieser Grundlage
eine Geschichte, was mit so vorgeblich unschuldigen Daten, wie zum Beispiel ein Einkaufszettel, was
daraus irgendwie geschlussfolgert werden kann und wie das, wenn man jetzt sich in einem autoritären
Regime sieht, dann auch genutzt werden kann und auf einen zurück schlagen kann. Ist in der Hinsicht
ganz spannend, ich fand ihn sehr spannend zu lesen, aber wie gesagt, das hat so ein bisschen
dieses, ja so ein bisschen, ich will nicht sagen, Nazi-Romantik, weil es die auch extrem kritisch
sieht, aber da kann man ein bisschen draufgucken. Genau, das waren meine Lesetipps über das Thema
hinaus. Cool, ja, vielen Dank. Das klingt nach Sommerferienlektüre, so das Letzte. Das könnte
ich mir vorstellen, dass ich das mit... Also es macht nicht unbedingt Spaß, es ist eher so dunkel,
es ist nicht gute Laune. Dafür ist das Thema auch zu ernst, aber man kann es gut, Espar kann
einfach sehr gut schreiben, sehr fluffig, sehr gefällig. Okay, ja, sehr gut. Ich habe so als
weitergehende Lektüre, es gibt ein anderes Buch von Szymanowski, also es gibt mehrere natürlich,
aber ein anderes, das heißt Data Love, das ist bei Mathes und Salz erschienen, wo es so eben
um die ganze Daten-Thematik geht. Ich finde, er schreibt einfach auch sehr ansprechend,
ich finde auch dieses Buch, das hat sehr viele Referenzen und es geht ihm auch Filme und konkrete
Beispiele ein und man sieht dann aber eben im letzten Teil, dass es trotzdem auch irgendwie
inhaltlich nicht einfach so dahergesagt ist, also wenn er dann diese Beispiele oder diese
Philosophen ins Spiel bringt, deswegen noch ein anderes Buch von ihm. Dann eines, das habe ich
jetzt angefangen, ich bin noch nicht durch, aber es passt ganz gut zum Thema, das heißt Was das
Valley Denken nennt von Adrian Daub, wo es eben darum geht, natürlich auch bei dieser KI, das ist
ja eine ganz kleine Gruppe von Personen, die eigentlich an der Entwicklung davon überhaupt
beteiligt sind und ja das Silicon Valley ist natürlich ein Brennpunkt für Daphne und Adrian Daub
ist Adrenalin, ist er Deutscher sogar? Ich glaube ja, ich glaube er ist tatsächlich, also er lebt und
arbeitet in den USA, aber ich habe ihn auch schon in deutschen Interviews gehört, also er ist auf
jeden Fall deutsch auf hohem Sprachlevel und ich meine tatsächlich auch, er wäre gebürtig in
Deutschland, aber ich kann es dir jetzt nicht garantieren. Ja okay, auf jeden Fall, das würde
dazu passen und von einer deutschen Professorin Katrin Misselhorn ist das kleine Büchlein KI und
Empathie, das beim Reclam erschienen ist, auch sehr empfehlenswert meiner Meinung nach, geht es auch
so ein bisschen darum, eben Empathie ist auch was man trainieren kann, soll, muss, wie wird das in
Frage gestellt oder beeinflusst durch Roboter, durch KI und so weiter. Also Adrian Daub ist
tatsächlich in Köln geboren, aber wenn ich das richtig sehe, akademisch komplett in den USA oder
Großbritannien, in den USA sozialisiert, also in den USA sogar seinen Bachelor schon gemacht.
Ach so, okay. Ja, wir haben auch so ein paar passende Folgen natürlich zum Thema, ich habe
jetzt einfach zwei rausgepickt, das ist einerseits das metrische Wir von Steffen Mau, das wird auch
explizit im Buch erwähnt, das Buch, was auch dazu passt, ist Roboterethik von Janina Loh, wo es
generell also ein bisschen Maschinenethik auch geht. Ja, also es gibt natürlich ganz ganz vieles, was
passt, aber das könnte man sich dazu anhören und so Filmtipps, auch ein Film, das den eher erwähnt
ist, ein neuerer Film ist Ex Machina. Ja. Wieso zögerst du? Also ich kenne Ex Machina,
den Film, ist ein sehr sehr guter Film, mir war nur der Bezug zu seinem Thema nicht sofort klar. Ach so,
ja, es geht ein bisschen darum, dass, um diesen Sprung davon, dass Technik eigentlich wie dann
in die Welt rausgeht und sich selbst, um die Welt zu entdecken, also man hat wie in, ich glaube,
Hell nimmt als anderes Beispiel von dieser starken KI, die zwar dann diese Astronaut, das ist Space
Odyssey, oder? Ja, genau. Space Odyssey, wo die Astronauten dann nicht mehr zurück ins Raumschiff
dürfen, aber weil Hell sozusagen die Menschheit oder seinen Auftrag erfüllen möchte, also es ist
wie noch, er macht das noch im Sinne von seinem Auftrag, der von den Menschen vorgegeben ist und
bei Ex Machina gibt es dann wie diesen Schritt, dass diese Roboter in diesem Fall, diese starke KI
eigentlich sich davon löst. Es ist wie wirklich diese eigene Welt dann und so, das ist wie er das
im Buch beschreibt. Aber ich finde es auch generell so eigentlich ein guter Film, der das Thema so,
wenn es um Roboter und Maschinen geht, gut darstellt. Ja, das wäre es von meinen Tipps.
Sehr schön, ja. Vielen Dank dir, Amanda, für die Buchvorstellung. Vielen Dank euch an den
Kopfhörern oder Lautsprechern fürs Zuhören und dann bleibt mir nur zu hoffen, euch gewünscht zu
haben, dass ihr viel Spaß hattet, keine Ahnung, ich hab mich verhaspelt, es ist spät, dass ihr was
mitgenommen habt, dass ihr viel Spaß hattet beim Hören dieser Episode und wenn ihr uns auf den
sozialen Medien folgen wollt, könnt ihr das tun. Wir sind noch Fragezeichen, Stand heute ist es
gerade ein bisschen problematisch mit Twitter, bei Twitter als atdeckelen unter dem gleichen Handel
auch bei Instagram zu finden. Neben Fidiversum sind wir atzzd, atpodcasts.social und auf Facebook
haben wir nämlich auch noch eine Seite, da findet ihr uns als zwischen zwei Deckeln. Ihr könnt aber
auch ganz klassisch das alte web10 bemühen und auf zwischenzweideckel.de gehen, da findet ihr
alle Episoden genauso wie im Podcatcher eurer Wahl. Ihr könnt uns auf Spotify hören, aber wenn
ihr andere Tools nutzt, nutzt doch lieber die anderen, wenn es irgendwie geht. Auch wieder im
Anschluss an das Thema heute. Jetzt bleibt mir nur euch, bis zum nächsten Mal, viel Spaß beim
Lesen zu wünschen, macht es gut und bis bald. Bis dann, tschüss zusammen.
