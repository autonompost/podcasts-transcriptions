start	end	text
0	21840	Hallo und herzlich willkommen zu Episode 15 von Zwischen zwei Deckeln, dem Sachbuchpodcast.
21840	27000	Ich bin wie immer der Nils und mit mir ist heute wieder der Christoph dabei.
27000	28000	Hallo zusammen.
28000	37000	Ja, schön, dass ihr da seid, dass ihr wieder eingeschaltet habt, wenn ihr jetzt zuhause sitzt oder irgendwie auf dem Abenteuer einkaufen seid oder zu denen gehört, die arbeiten müssen da draußen.
37000	40000	Danke an euch, dass ihr irgendwie den Laden am Laufen haltet.
40000	45000	Genau, ja. Wie sieht es aus bei dir? Irgendetwas Relevantes, Neues?
45000	53000	Nichts Spektakuläres auch. Ich bin überwiegend zuhause, kaufe einige Menschen mit ein und habe Zeit zum Lesen gefunden.
53000	55000	Was liest du denn gerade?
55000	63000	Also jetzt gerade tatsächlich wirklich für die Episode, die wir heute aufnehmen, habe ich das Buch von heute in den letzten anderthalb, zwei Wochen eben gelesen.
63000	75000	Also ihr habt es ja schon im Titel gesehen, es geht um Roboterethik von Janina Lo und ansonsten habe ich nochmal was bei der BBB bestellt und habe ein, zwei Krimis mir jetzt bei Twitter empfehlen lassen, die ich angehen möchte.
75000	82000	So ist mein Ziel. Aber sonst muss ich sagen, habe ich nicht viel mehr Zeit als vorher. Ich weiß nicht, wie es bei dir ist.
82000	87000	Ja, bei mir auch nicht so wirklich, weil meine Arbeit sich dann doch irgendwie eins zu eins ins Homeoffice übertragen lässt.
87000	92000	Und dadurch, dass die Pendelzeit wegfällt, sogar die klassische Lesezeit irgendwie ein bisschen kürzer wird.
92000	99000	Aber ich komme so ein bisschen zum Lesen. Ich habe jetzt gerade fertig gelesen das Zettelkastenprinzip von Sönke Ahrens.
99000	111000	Tatsächlich mal so ein bisschen der Versuch, diesen mysteriösen, ominösen, lumenschen Zettelkasten für so privates Notizenmanagement, Wissensmanagement aufzubereiten.
111000	118000	Und das fand ich echt total spannend. Da wird es zunächst wahrscheinlich auch den ein oder anderen Blogbeitrag zu geben bei mir.
118000	123000	Hast du Kommunikation mit Zettelkästen von Luman dazu gelesen?
123000	125000	Das ist ein Buch, ein Essay?
125000	128000	Nee, das ist ein Essay. Ich glaube, das sind nur so zehn Seiten oder so.
128000	130000	Ja, ich glaube, habe ich zumindest mal angefangen zu lesen.
130000	140000	Vielleicht, wenn wir dran denken, können wir es ja verlinken. Ich muss sagen, das ist relativ amüsant, weil Luman da eben klarmacht, wie egal es ihm ist, mit wem er eigentlich da irgendwie zu tun hat
140000	146000	und mit wem er zusammenarbeitet, also seinen Zettelkasten quasi als Assistenten begreift.
146000	148000	Und das ist ganz, ganz gut.
148000	152000	Aber das kann tatsächlich funktionieren. Das ist das Krasse dabei.
154000	157000	Genau, da können wir vielleicht auch eine Episode hier mal zumachen. Mal gucken.
157000	158000	Vielleicht auch das.
158000	164000	Ja, von euch da draußen ist irgendwie nichts an Fragen oder Kommentaren oder so zu der letzten Episode gekommen.
164000	167000	Fühlt euch da nochmal eingeladen, wenn ihr irgendwas diskutieren wollt.
167000	175000	Oder so uns einfach ein Audiophile zu schicken, einfach am Handy aufgenommen, vielleicht in der ruhigen Ecke oder irgendwie unter Blogbeitrag zu kommentieren.
175000	178000	Dann können wir das hier im Zweifel aufgreifen.
178000	181000	Also fühlt euch da nochmal herzlichst so eingeladen.
181000	185000	Apropos letzte Episode. Christoph, erinnerst du dich noch, worum ging es?
185000	190000	Ja, also du hast uns vorgestellt, Die Economist's Hour von Benjamin Applebaum.
191000	204000	Und da ging es, wenn ich mich jetzt recht im Sinne darum, wie im Prinzip so liberale, marktliberale Ökonomen Einfluss auf die Politik in den USA genommen haben.
204000	215000	Und ja, es zeigt sich, dass viel von dem, was so prognostiziert wurde, in dem Bereich gar nicht so eingetreten ist, wie es gesagt wurde.
215000	218000	So würde ich es in zwei, drei Sätzen zusammenfassen.
218000	220000	Genau, also wie es von den Ökonomen selbst vorher gesagt wurde.
220000	221000	Ja, genau, das meine ich.
221000	226000	Andere haben dann durchaus zumindest mal geahnt, was passieren könnte.
226000	230000	Ok, ja, genau, darum ging es letzte Woche, letzten Monat.
230000	232000	Diese Episode hast du ja schon gesagt, worum es geht.
232000	241000	Wir werden ein bisschen technikphilosophisch, wenn ich das richtig sehe, mit dem Buch Roboterethik eine Einführung von Janina Loh.
241000	249000	Die Autorin ist Philosophin und arbeitet an der Uni Wien im Bereich Technik und Medienphilosophie, sagst du.
249000	255000	Und sie habilitiert zu posthumanistischen Elementen in Hannah Arendts Werk und Denken.
255000	264000	Also ja, da merkt man schon ein bisschen, dass sowohl das klassischphilosophische als auch das technikphilosophische taucht ja schon in dem Titel auf.
264000	269000	Möchtest du anfangen, uns das Buch in der Kurzfassung kurz vorzustellen?
272000	274000	Ja, das mache ich gerne.
274000	285000	Also Janina Loh untersucht in ihrer Studie Roboterethik verschiedene Ansätze der Einordnung der Fähigkeiten und Zugeständnisse von und an Roboter durch EthikerInnen und PhilosophInnen.
285000	292000	Es zeigt sich, dass Begriffe wie Moral und Verantwortung traditionell dem Menschen vorbehalten sind und einzelnen Individuen zugeschrieben werden.
292000	299000	Loh stellt Gegenpositionen dar und macht Vorschläge, wie und warum wir unsere tradierten Denkmuster durchbrechen können und vielleicht auch sollten.
302000	306000	Okay, das klingt ja doch potentiell kontrovers.
306000	309000	Möglich, ja, mal gucken, wie kontrovers es wirklich wird.
309000	314000	Und wie danach so was wie haben Tiere auch Menschenrechte sozusagen.
314000	317000	So ein bisschen haben Roboter auch Menschenrechte.
317000	321000	Scheint wieder so ein bisschen das zu sein, was mir spontan einfällt.
321000	329000	Tatsächlich gibt es da auch einige Referenzen und auch Vergleiche zu dem, was eben Robotern zugetraut wird und Tieren oder nicht Tieren.
329000	339000	Genau, einige Fragestellen aus der Fragestellung aus der Roboterethik wurden eben offenbar in der Tierethik schon behandelt oder werden es immer noch oder man kann sie auch zusammen behandeln und so.
339000	347000	Genau, so ein bisschen die Frage nach dem Status von Nichtmenschen im Prinzip, mit denen wir aber irgendwie zu tun haben.
347000	356000	Im Prinzip ist das Buch doch, ja, ich glaube, ich weiß nicht, wie philosophische Studien sonst so aufgebaut sind, aber ziemlich stringent.
356000	362000	Und dadurch, ja, manchmal so ein bisschen habe ich das Gefühl, wird einiges einfach abgearbeitet.
362000	371000	Also es geht erstmal los mit einer Einleitung und dann stellt sich Loh die Frage, welche Bereiche der Robotik und der ethischen Fragen es überhaupt gibt.
371000	380000	Und dann geht es einmal die Arbeitsfelder der Robotik durch und dann geht es darum, inwiefern Roboter moralische Handlungssubjekte oder Objekte sein können, was das bedeutet.
380000	382000	Darüber sprechen wir noch.
382000	388000	Und dann geht es im dritten Kapitel um Verantwortungzuschreibung in der Interaktion zwischen Menschen und Robotern.
388000	394000	Und dann wird das wieder durchexerziert als Roboter als Verantwortungssubjekte und Roboter als Verantwortungsobjekte.
394000	403000	Und dann gibt es am Ende noch ein bisschen neuen Seiten, einmal ein bisschen Breitseite gegen alles und jeden quasi.
403000	406000	Also ein bisschen kritischer Ausblick kennt man ja auch aus vielen Büchern.
406000	412000	Aber ja, so ist das Buch erstmal aufgebaut und ich glaube, ich fange einfach mal an.
412000	426000	Loh steigt damit ein, dass sie erstmal sagt, dass wenn wir über Ethik und Roboter oder Technik sprechen, man immer davon ausgehen muss, dass Technik von Menschen erschaffen ist und dementsprechend von Normen geprägt ist.
426000	433000	Also egal, was wir tun, ist irgendwie zumindest implizit mit gesellschaftlichen Ideen, Normen und so weiter verknüpft.
433000	448000	Das heißt, so die Idee einer völlig neutralen Technik, die ja so wie objektiv vor uns liegt und quasi gar keinen, in sich keine ethischen Implikationen oder so hat, das verwirft sie direkt zum Anfang.
448000	457000	Dabei sagt sie, dass Roboterethik eben ein Teilbereich der Maschinenethik ist, was bedeutet, dass alle Roboter Maschinen sind, aber nicht alle Maschinen sind Roboter.
457000	458000	Ja, macht Sinn.
458000	464000	Und auch dann weist sie direkt darauf hin, dass die Fragestellungen durchaus der der Tierethik ähneln.
464000	468000	Also was kann ein Tier, was kann ein Roboter, was fühlt es, was fühlt er.
468000	475000	Ja, weil ich gerade gesagt habe, dass Roboter keine Maschinen sind, ist vielleicht einmal wichtig zu definieren, was Roboter überhaupt sind.
475000	479000	Und damit starten wir in die Definition von Maschinen.
479000	488000	Das sind nämlich künstliche Gebilde, die aus einem Antriebssystem durch Motor, Wind oder Wasser bewegten Teilen besteht und die Energie umsetzen.
488000	490000	Das ist eine Maschine, so ganz allgemein.
490000	493000	Roboter hingegen sind spezielle Maschinen.
493000	497000	Das Wort geht auf Roboter aus dem Tschechischen zurück.
497000	499000	Das kommt auch aus irgendeinem Science Fiction Roman.
499000	501000	Das ist ja so dein Spezialgebiet.
501000	506000	Ja, ich könnte Stanislav Lem sein, aber ich bin mir da nicht ganz sicher.
506000	508000	Ja, ich glaube, es war nicht Stanislav Lem.
508000	512000	Dann hätte ich nämlich, glaube ich, dazu geschrieben, weil den, den kenne ich quasi.
512000	519000	Naja, und das Wort geht, steht offen, also Roboter im Tschechischen steht offenbar für Arbeit, frohendienst oder Zwangsarbeit.
519000	524000	Das fand ich ganz interessant, gerade mit Blick auf eben ethische Implikation.
524000	527000	Und ja, also jetzt die Definition von Roboter.
527000	542000	Das ist eine elektromechanische Maschine, die A. über einen eigenständigen Körper und B. über mindestens einen Prozessor verfügt, C. Sensoren hat, D. über Effektoren oder Aktoren verfügt, die Signale in mechanische Abläufe übersetzen.
542000	548000	Roboter erscheinen zumindest autonom und können physisch Einfluss auf ihre Umwelt nehmen.
548000	555000	Genau, also relativ komplex irgendwie, aber man kann sich merken, gut, die haben einen eigenständigen Körper.
555000	563000	Sie haben irgendwas, womit sie irgendwie Dinge prozessieren können, also einen Prozessor, also das, was bei Menschen das Gehirn vielleicht ist.
563000	566000	Und sie haben irgendwie Sensoren, um die Umwelt wahrzunehmen.
566000	577000	Und sie haben irgendwas, wie sie um die Umwelt eingreifen können, also irgendwelche Roboterarme oder sie haben einen kleinen Staubsauger, wie diese Staubsaugerroboter und so weiter.
577000	585000	Aber danach macht Loh die These auf, dass es heute eigentlich keinen Bereich im menschlichen Alltag mehr gibt, in dem Roboter noch keinen Einzug gehalten hätten.
585000	588000	Kann man erst mal so auf sich wirken lassen und sich dann überlegen, ob das stimmt.
588000	593000	Ich habe erst gedacht, das klingt ja irgendwie relativ steil als These, aber ich glaube, sie hat schon recht.
593000	602000	Egal, wo wir hingucken, wir wissen, dass es zumindest Roboter gibt, auch wenn wir sie nicht tagtäglich sehen und mit ihnen zu tun haben sollten.
603000	614000	Ja, dann geht es im ersten Kapitel darum zu klären, worum es in diesem Buch eigentlich geht, also welche Bereiche aus der Robotik zur Sprache kommen oder in der Roboterethik eine Rolle spielen.
614000	625000	Das ist einmal die Industrierobotik, also die Frage nach Ersetzung von Arbeitsplätzen hat man sicherlich schon gehört und daran anschließend, wir hatten es ja gerade, sie habilitiert zu Hannah Arendt auch,
625000	633000	die Frage danach, was der Mensch ist, wenn er nicht mehr arbeitet, wenn er nichts mehr schafft oder nicht mehr Lohn arbeitet oder wie man es nennen möchte.
633000	640000	Also das sind Fragen, die daran anschließen an das Problem von, wenn Arbeitsplätze ersetzt werden, was resultiert eigentlich daraus.
640000	647000	Dann, was natürlich glaube ich auch ein verbreitetes Beispiel ist, das auch in dem Buch immer wieder vorkommt, ist die Frage nach dem autonomen Fahren.
648000	662000	Also wenn wir jetzt Automobile haben, die irgendwie intelligente Fahrsysteme haben, ist die Frage, wie diese Autos in Unfallsituationen reagieren sollen, wer bei Unfällen dann die Verantwortung trägt,
662000	673000	also kann das Auto selbst irgendwie verantwortlich gemacht werden, ist es der Konzern, der das Auto gebaut hat, ist es der Fahrer, der eigentlich noch hinterm Lenkrad sitzt, aber nicht mehr eingreifen muss und so weiter.
673000	677000	Dann kriegt das Auto drei Monate Fahrverbot, stelle ich mir auch lustig vor.
677000	686000	Ja, das ist irgendwie ganz interessant. Da geht später auch noch drum die Unterscheidung zwischen Verantwortung und Haftbarkeit, die so ein bisschen getrennt wird.
686000	687000	Macht Sinn.
687000	698000	Und sie weiß, glaube ich, drei oder viermal in dem Buch darauf hin, dass in der EU oder vom Europäischen Parlament gerade die Idee von elektronischen Personen erarbeitet wird,
698000	711000	also quasi analog zu juristischen Personen, die wir aus dem Recht schon kennen, wo dann irgendwelche Unternehmen vor Gericht wie Personen behandelt werden können, könnte man das eben auch für Maschinen oder Roboter vielleicht einsetzen.
711000	718000	Was aus diesem Projekt der Erarbeitung da genauer resultiert ist, das schreibt sie aber nicht, nur dass das eben im Prozess ist.
718000	726000	In der Medizin oder Therapie- und Pflege-Robotik stellt sich, wenn man da Roboter einsetzt, und das ist ja mittlerweile auch schon so,
726000	735000	ich persönlich habe das auch schon erlebt, also ich habe mal in einem alten Zentrum ein Praktikum gemacht, in dem diese Roboter-Robbe Paro zum Einsatz kam.
735000	736000	Ich weiß nicht, ob du die kennst.
736000	737000	Ja, sag mir was.
737000	753000	Also für die, die es nicht kennen, das ist quasi sowas wie ein großes Kuscheltier in Form einer Robbe, die es weiß und die hat verschiedenste Sensoren eingebracht und reagiert auf Berührung und Ansprache und so weiter ein Stück weit.
753000	756000	Und es ist auch ganz schön teuer, die kostet, glaube ich, um die 5000 Euro oder mehr.
756000	761000	Gibt es ganz viele Videos zu, wenn einem das interessiert.
761000	771000	Und im Zuge dieser Pflege-Robotik, und man kann sich da ja auch Sachen vorstellen, wie Roboter, die Menschen aus Betten heben oder auf die Toilette begleiten und so weiter,
771000	776000	stellt sich eben die Frage nach der Autonomie der Personen, die dort behandelt werden,
776000	785000	und inwiefern die Emotionen, die da vielleicht auch aufkommen gegenüber einer Roboter-Robbe oder einem Roboter als Gesprächspartner, kann man sich ja alles Mögliche vorstellen,
785000	792000	inwiefern diese Emotionen in gewisser Maße echt sind, inwiefern die Personen, die da beteiligt sind, getäuscht werden, sowas alles.
792000	797000	Ich nehme es gleich vorweg, Janina Loh sieht das alles nicht so problematisch.
797000	798000	Okay.
798000	809000	Sie sagt, dass eben Beziehungen zu Objekten oder Robotern durchaus echte Emotionen sein können und dass das keine Täuschung in dem Sinne darstellen muss.
809000	810000	Okay.
810000	817000	Vielleicht hat man das jetzt auch schon gehört, Sexroboter sind ja auch ein Ding, also die gibt es schon, kann man schon kaufen, kosten, glaube ich, immer noch relativ viel Geld.
817000	823000	Und was da in dem Buch augenfällig wird, da geht es dann häufig in dem Kontext um feministische Fragestellung.
823000	824000	Ja.
824000	833000	Also es gibt da irgendwie teilweise die Möglichkeit, eben Roboter zu vergewaltigen und dass das irgendwie im Kern problematisch sein kann.
833000	840000	Auf der anderen Seite vielleicht aber auch irgendwie befreiend, weil Leute, die so etwas haben, das dann ausleben können, ohne eben Menschen zu schädigen.
840000	841000	Ja.
841000	845000	Ja, da gibt es viele ethische Ansätze und viele Diskussionen darum.
845000	854000	Und darum geht dann natürlich auch um Objektifizierung von, gerade von Frauen, weil diese Sexroboter eben sich häufig an heterosexuelle Männer wenden.
854000	860000	Ja, also auch ein Bereich, in dem der Feminismus eine große Rolle spielt und auch spielen sollte.
860000	869000	Der letzte Bereich, den Lo aufzählt und das liegt, glaube ich, auf der Hand, dass das irgendwie ethisch relevant ist, ist eben die Frage nach Militärrobotern.
869000	870000	Ja.
870000	886000	Und wie wird auch unbefugten der Zugang zu Entscheidungen verwehrt und wie viel Autonomie dürfen artificielle Systeme eigentlich haben, wenn sie eben konkret über Leben und Tod entscheiden?
886000	890000	Da ist dann auch wieder die Schnittstelle zu dem autonomen Auto, das Entscheidende, auf wen es rettet.
890000	892000	Ja, absolut.
893000	899000	Ja, in der Roboterethik gibt es dann zwei klassische Arbeitsfelder, mit denen man sich beschäftigen kann.
899000	907000	Also einmal Roboter als moralische Akteurinnen, also inwiefern sie selbst irgendwie moralfähig sind und moralisch handeln.
907000	910000	Ein bisschen, das wir gerade eben schon besprochen haben.
910000	918000	Und die andere Frage ist danach, inwiefern Roboter als Objekte moralischen Handelns irgendwie relevant werden, wie zum Beispiel bei den Sexrobotern.
918000	919000	Ja.
919000	930000	Es ist auffällig, dass die Deutungshoheit über das ganze erste Arbeitsfeld, also inwiefern Roboter moralische Subjekte sein können, ziemlich anthropozentrisch sind.
930000	934000	Anthropozentrisch meint damit, der Mensch steht im Mittelpunkt und es wird von Menschen ausgedacht.
934000	944000	Also unser Bild davon, was irgendwie moral ist und sein kann, ist immer davon geleitet, was wir von Menschen können, wie wir glauben, dass Menschen sind.
944000	954000	Und das bedeutet am Ende auch, dass durchweg Menschen entscheiden, ob Roboter moralisch handeln können, ob sie sowas wie Willensfreiheit besitzen können und so weiter.
954000	955000	Ja.
955000	957000	Leuchtet, glaube ich, auch ein.
957000	964000	Ich glaube, was Lo damit aufzumachen, probiert es erstmal, einfach einen Reflektionspunkt, einfach mal darüber nachzudenken.
964000	978000	Okay, wir denken immer von uns aus, was vermutlich auch gar nicht anders möglich ist, aber wir nehmen uns damit eben auch die Freiheit und Hoheit darüber raus, zu entscheiden, wer eigentlich Teil von dem ist, was wir uns eigentlich nur zubelegen.
978000	979000	Genau, ja klar.
979000	983000	Sie sagt dabei, dass wir eigentlich gar nicht wissen, wie es überhaupt ist, frei zu sein.
983000	984000	Also Zitat jetzt.
984000	998000	Ja, zum zweiten so ein bisschen in die Richtung.
998000	1005000	Roboter als Moralobjekte, da was ich gerade eben schon hatte, waren eben die Beispiele aus der Sexrobotik.
1005000	1012000	Sexroboter reproduzieren eben heteronormative, patriarchale und diskriminierende Strukturen.
1012000	1014000	Das ist durchaus ein Problem.
1014000	1024000	Würde man sie anders konzipieren, könnte man aber durchaus auch sich andere Sexroboter vorstellen, die vielleicht auch andere Möglichkeiten bieten.
1024000	1041000	Ich könnte mir vorstellen, dass gerade wenn es um Sexualtherapie geht und so die Begleitung von Menschen, die vielleicht zuerst Sexualpraktiken körperlich nicht eigenständig in der Lage sind, dass da eigentlich ein großes Feld der Emanzipation auch offen steht.
1041000	1044000	Also ein bisschen die Schnittstelle wieder zu der Therapie.
1044000	1045000	Ja, genau.
1045000	1050000	Dabei stellt sich immer wieder die Frage, wie Moral überhaupt in eine Maschine gelangen kann.
1050000	1056000	Also wenn Sie irgendwie Moral entwickeln können sollten, wie funktioniert das?
1056000	1061000	Und da gibt es eben verschiedene Ansätze, entweder Top-Down, Bottom-Up oder Hybrid-Ansätze.
1061000	1071000	Also entweder man programmiert halt sowas wie moralische Regeln ein oder man setzt darauf, dass artificielle Systeme selbst lernen können.
1071000	1075000	Also sowas, was unter KI verstanden wird oder man kombiniert ein bisschen.
1075000	1081000	Also einerseits gibt man irgendwelche Regeln vor, andererseits hat man irgendwie selbst der andere Ansätze.
1081000	1093000	Was auffällig ist, ist, dass jeder Ansatz irgendwie Top-Down, das zu bewältigen, total schwierig ist, weil Moral eben abstrakt ist und die Entscheidungen, die aber gefällt werden müssen, sehr konkret sind.
1094000	1102000	Die Rettung von Menschenleben ist irgendwie sehr abstrakt, aber die Entscheidung, in welche Richtung ein Auto ausweicht, ist eben sehr konkret.
1102000	1109000	Das heißt, man kann sich nicht einfach darauf verlassen zu sagen, rette möglichst viele Menschen zu einem Auto und das wird schon irgendwie funktionieren.
1109000	1121000	Wobei ich das ganz spannend finde, weil wir ja gerade auch jetzt in der aktuellen Situation mit der drohenden Überlastung oder in manchen anderen Ländern auch schon der Überlastung von Intensivstationen und Ähnlichem,
1121000	1126000	wo ja im Grunde genau diese moralischen Entscheidungen getroffen werden, wen können wir jetzt behandeln und wen nicht.
1126000	1140000	Und da eben danach gerufen wird, wir brauchen eine klare Triage-Regeln sozusagen, nach welchen Kriterien entscheiden wir, dass das eben nicht das Individuum machen muss, der einzelne Arzt oder die einzelne Ärztin oder der einzelne Pfleger,
1140000	1149000	die halt sagen, der kriegt jetzt die Beatmung und der nicht, sondern dass wir da im Grunde nach genau diesen abstrakten Regeln verlangen, die du jetzt sagst, die eigentlich schwer zu machen sind.
1149000	1158000	Das finde ich auch ziemlich interessant, weil mein Gefühl auch immer wäre, man kann probieren, das durchzureglementieren, wie so eine Triage stattfinden soll und wie die organisiert werden möchte.
1158000	1163000	Aber ich habe das Gefühl, das würde sich im Konkreten sowieso ergeben.
1163000	1171000	Ich habe das Gefühl, die eigentliche Funktion dahinter ist, eben Ärztinnen zu entlasten von der von der konkreten Entscheidung.
1171000	1184000	Aber ich glaube, ich vermute, dass die Entscheidung, die Ärztinnen treffen würden, ohne Triage-Regelung, ziemlich genau das widerspiegeln würden, was sie auch mit Regelungen jetzt entscheiden können, sollen oder dürfen.
1184000	1205000	Da ist ja gerade diese Diskussion, die auch gerade so aus der Rassismusdiskussion sozusagen ein bisschen kommt, ob man nicht mit objektiveren Triageregeln solche im Zweifel auch erst mal unbewussten Verzerrungen und wie eben rassistische Vorurteile und Ähnliches nicht auch ein bisschen ausgleichen kann.
1205000	1214000	Wenn man eben sagt, ich überlasse das nicht dem Einzelnen, der eventuell unbewusst irgendwas internalisiert hat, sondern ich sage halt, ne, stopp hier, das Alter muss so sein.
1214000	1217000	Und ja, wenn der jetzt zwei Jahre jünger ist, dann kriege ich das halt trotzdem.
1217000	1220000	Aber wenn du vielleicht gesagt hast, und so weiter und so fort.
1220000	1230000	Ich glaube, da kommt diese Objektivierung ins Spiel, die da vielleicht auch wirklich was nochmal verändern kann gegenüber Einzelnen.
1230000	1235000	Ja, stimmt, das ist nochmal ein ganz gelungener Gedankenanstoß da.
1235000	1240000	Und ich finde auch einfach die moralische Entlastung quasi von Ernst, den ist ja auch ein Eigenwert.
1240000	1242000	Das ist ein totaler Wert natürlich, gar keine Frage.
1242000	1254000	Inwiefern Roboter jetzt wirklich sowas wie, ja, also richtig Subjekte im moralischen Sinne sein können, hängt natürlich auch ganz stark davon ab, was moralische Akteurinnen überhaupt benötigen.
1254000	1264000	Und da jetzt einfach mal so, um darzustellen, wie dieses Buch geschrieben ist, nimmt Janina Losig eben immer verschiedene Autorinnen raus, sagt XY hat das und das geschrieben.
1264000	1269000	Und dann wird ein anderer Text quasi behandelt und da wird das und das gesagt.
1269000	1275000	Und das stellt sie eben da, was es manchmal ein bisschen schwierig macht, dieses Buch jetzt eben so ganz doll zusammenzufassen.
1275000	1281000	Im Prinzip müsste ich sagen, okay, es gibt sechs Studien, die untersucht wurden und die kommen jeweils zu dem Schluss.
1281000	1283000	Deswegen probiere ich das jetzt hier mal zusammenzufassen.
1283000	1288000	Manche Studienergebnisse lasse ich dann aber auch quasi hinten runterfallen und stelle sie nicht da.
1288000	1290000	Also manche Ethikerinnen kommen dann hier nicht vor.
1290000	1306000	Und am Ende eines jeden Kapitels bezieht Janina Loh das, was sie quasi gelesen hat, auf verschiedene Robotersysteme und probiert einzuordnen, inwiefern jetzt eben Subjekt, Objektverantwortungs möglich wären im Sinne der gelesenen Autorinnen.
1306000	1314000	Auch das lasse ich hinten noch rüber, weil ich glaube, wenn ich jetzt sechs oder sieben Roboter immer wieder aufzähle und sage, möglicherweise können die dies, das oder das können sie auch nicht.
1314000	1317000	Je nachdem, wem man fragt, das geht ein bisschen zu weit.
1317000	1322000	Wenn euch das im Konkreten interessiert, müsst ihr das Buch dann tatsächlich vielleicht einfach kaufen und lesen.
1322000	1330000	Naja, auffällig ist, dass bei Moral sich auch die Philosophinnen nicht so richtig einig sind, was denn eigentlich Moral ist und was Gut ist.
1330000	1336000	Also es gibt einerseits so Ideen von Gut ist, wer Gutes tut, so im Sinne von Forrest Gump quasi.
1336000	1343000	Also man guckt einfach nur darauf, was irgendwie der Output von einer Handlung ist.
1343000	1353000	Und wenn der gut ist, dann sind auch Roboter befähigt, quasi moralisch zu agieren oder auch wenn es schlecht ist.
1353000	1363000	Also man guckt einfach nur, was der Output einer Aktion ist und auf der anderen Seite gibt es natürlich irgendwie hochkomplexe Anforderungsprofile,
1363000	1372000	wo auch ein Eigenverständnis von Moral durch die handelnde Person, den handelnden Roboter oder das handelnde Tier irgendwie vorliegen muss.
1372000	1381000	Die Frage, die im Kern so ein bisschen da ist, ist, wann sind Roboter eigentlich wirklich moralfähig und wann nutzen sie nur Funktionsequivalente zu Moral?
1381000	1386000	Und tun dann quasi nur so, als wären sie moralfähig?
1386000	1390000	Und da hängt dann natürlich die große Frage nach dem Bewusstsein im Kern da dran.
1390000	1395000	Das erinnert mich an dieses Gedankenexperiment von dem Chinese Room, das chinesische Zimmer.
1395000	1396000	Ich weiß nicht, ob du das kennst.
1396000	1397000	Ah, erzähl mal.
1397000	1401000	Ja, da geht es da um Sprachverstehen im Grunde, da kommt das ursprünglich her.
1401000	1410000	Und das nimmt halt an, ich weiß gar nicht mehr von wem es kommt, dass du ein Zimmer hast, wo sämtliche Regeln der chinesischen Sprache drin stehen, in Büchern.
1410000	1420000	Und ganz ausgefeilte Regeln, wenn dieses Zeichen kommt, dann antwortest du mit diesem Zeichen und wenn diese Zeichen kommen, dann antwortest du mit dem Zeichen und so weiter und so fort.
1420000	1430000	Und jetzt ist halt eine Person in diesem Zimmer, die bekommt per Zettel oder was, eine Reihe chinesischer Schriftzeichen reingereicht, befolgt die Regeln und gibt eine Antwort.
1430000	1439000	So, und diese Antwort ist natürlich sinnvoll, ohne dass diese Person verstanden hätte, was sie da tut inhaltlich, also dass die Sprache sozusagen verstanden hätte.
1439000	1445000	Und jetzt gibt es die Frage, dieses Zimmer, ist das Verstehen, was da passiert?
1445000	1448000	Wenn es von außen so aussieht, als wäre es Verstehen?
1448000	1449000	Ja.
1449000	1452000	Das schließt dann wahrscheinlich an den Turing-Test im Grunde an.
1452000	1454000	Das klingt so, als wäre das das.
1454000	1455000	Ja.
1455000	1466000	Ja, in einem Interview weiß, das verlinke ich ja auch nachher, bei Sein und Streit war sie bei Deutschland von Kultur, da weiß Janina Loh natürlich auch darauf hin, dass diese ganze Frage nach dem Bewusstsein total schwierig ist.
1466000	1478000	Und halt sowas wie eine, sie nennt es, glaube ich, philosophisches Zusatzargument, also einfach die absolute Annahme, dass Menschen Bewusstsein haben, ist empirisch nicht beweisbar.
1478000	1490000	Man kann quasi nicht den Kopf aufschneiden, ins Gehirn gucken und sagen, da ist es, das Bewusstsein, sondern auch da sind ja nur irgendwie Verkettungen von Nerven und Datenströmen im Prinzip irgendwie.
1490000	1505000	Und das heißt, ja, es gibt einfach Dinge, die kann man nicht so richtig nachvollziehen und das wird man also vermutlich absehbar zumindest auch nicht beantworten können, inwiefern irgendwas Bewusstsein erlangen kann, außer dass wir es Menschen halt auf jeden Fall zubelegen.
1505000	1506000	Ja.
1506000	1519000	Was auffällig ist, wenn es darum geht, ob Dinge, Tiere, Roboter, wie auch immer moralische Akteure sein können, ist, dass Autonomie irgendwie im Zentrum von moralischer Akteurschaft steht.
1519000	1533000	Das heißt, diese Top-Down-Ansätze, wenn einfach nur einprogrammiert ist, tu dies, wenn das, realisieren diese Moralfähigkeit nicht, weil eben komplett determiniert ist, wie sich der Roboter verhält.
1534000	1544000	Okay, aber jetzt trifft sie ja im Grunde eine Entscheidung. Sie hat ja gesagt, ist gut, wer Gutes tut oder ist gut, wer weiß, dass er Gutes tut sozusagen.
1544000	1545000	Ja.
1545000	1547000	Und hier sagt sie jetzt ja, nee, es ist gut, wer weiß, dass er Gutes tut.
1547000	1556000	Ja, genau, also diesem, quasi diesem, sie nennt das moralischem Schwellenwert, diesen Ansatz, den ich da vorgestellt habe, dem steht sie durchaus kritisch gegenüber.
1556000	1557000	Okay.
1557000	1558000	Ja, das merkt man auch. Genau.
1559000	1566000	Ja, und Autonomie und so weiter referenzieren dann immer weiter eigentlich auf sowas wie Intentionalität.
1566000	1572000	Also, wenn Roboter wirklich moralfähig sein sollten, müssten sie sowieso etwas wie Gründe für ihr Handeln anführen können.
1572000	1573000	Ja.
1573000	1581000	Man muss irgendwie sagen können, warum man getan hat, was man getan hat. Man kann nicht einfach nur sagen, ich hab das getan, weil das wurde mir halt so gesagt oder beigebracht.
1581000	1582000	Mhm.
1582000	1591000	Ja, dann geht es darum, inwiefern Roboter moralische Handlungsobjekte sein können und da gibt es so etwas wie eine Standardposition.
1591000	1605000	Also Handlungsobjekt heißt, in dem Sinne moralisches Handlungsobjekt, müssen wir uns eigentlich, also unabhängig davon, ob Roboter selbst moralisch als Subjekt handeln können, müssen wir uns denen eigentlich gegenüber vernünftig verhalten auf eine Art.
1605000	1606000	Ja.
1606000	1618000	Und die philosophische Standardposition, so wie sie das nennt, ist, dass eigentlich nur Dinge einen entsprechenden Status zugebilligt bekommen, die selbst auch Subjekte sind.
1618000	1619000	Okay.
1619000	1620000	Ja.
1620000	1622000	Das finde ich krass eigentlich.
1622000	1626000	Ja, wer kein Moralsubjekt ist, wird auch kein Moralobjekt.
1626000	1633000	Also, wenn man das weiterdenkt, bedeutet das, man kann mit Robotern verfahren, wie man will, das ist moralisch nicht problematisch.
1633000	1639000	Ja, zumindest nicht, zumindest wenn man bei der Frage nach dem Moralsubjekt zu der Antwort kommt, nein, ist es nicht.
1639000	1642000	Genau, dann kann man eigentlich mit denen machen, was man möchte.
1642000	1646000	Das ist eine krasse, gerade weil du sagst, dass das die Standardposition ist.
1646000	1650000	Ich würde jetzt dazu kommen, meine erste Intuition wäre, dass die total umstritten sein muss.
1650000	1651000	Ja.
1651000	1654000	Weil man das ja gerade genau auf die Tiere wieder wunderbar übertragen kann.
1654000	1662000	Das hat mich da auch ein bisschen gewundert, weil ich das Gefühl hatte, das kam nicht so richtig vor quasi als Vergleich.
1662000	1668000	Weil ich auch dachte, naja, dann könnten wir ja auch, also wenn das die Idee ist, dann könnte man ja auch mit Tieren tun und lassen, was man möchte.
1668000	1671000	Und das scheint mir keine Standardposition zu sein.
1671000	1672000	Genau.
1672000	1678000	Ich meine, das steht ja sogar in unserem Grundgesetz drin quasi, dass das eben nicht in Ordnung ist.
1678000	1679000	Ja.
1679000	1683000	Und das ist ja auch irgendwie philosophisch abgeleitet.
1683000	1685000	Das hat mich auch ein bisschen irritiert.
1685000	1689000	Aber gut, sie sagt das erstmal so, dass das eben so die allgemeine Position ist.
1689000	1694000	Ich weiß nicht, ob das jetzt vielleicht auch langsam ins Rutschen kommt.
1694000	1702000	Ich meine, wenn man an irgendwelche Roboter in der industriellen Fertigung oder so denkt, dann weiß ich nicht.
1702000	1710000	Da entspinnt sich bei mir auch nicht sofort das Bild der Notwendigkeit des quasi verantwortungsvollen Umgangs.
1710000	1713000	Da würde ich auch sagen, naja, wenn du kaputt bist, kommst du auf den Müll.
1713000	1717000	Und wenn du mich nervst, dann schreite ich auch an, ist mir egal.
1717000	1718000	So.
1718000	1719000	Interessant.
1719000	1723000	Das wird dann wieder interessant, wenn man jetzt die Sexroboter oder so ins Spiel bringt zum Beispiel.
1723000	1724000	Ja, total.
1724000	1729000	Wo dann auf einmal eine andere, die Roboter sind ja immer noch die gleichen.
1729000	1732000	Aber es kommt irgendwie eine ganz andere moralische Wertung ins Spiel.
1732000	1736000	Sie macht da noch ein ganz spannendes Beispiel auf, was mich irgendwie so ein bisschen ins Denken gebracht hat.
1736000	1738000	Da komme ich gleich noch zu.
1738000	1744000	Na ja, was ihr erstmal feststellt, ist, dass Roboter häufig in Aussehen als auch im Verhalten vermenschlicht werden.
1744000	1749000	Also man nennt das dann Anthropomorphisierung.
1749000	1752000	Sie entwickeln sich dann irgendwie zu sowas wie Gefährten von Menschen.
1752000	1758000	Und diese Anthropomorphisierung wird von der Psychologie meistens traditionell negativ gesehen.
1758000	1762000	Also als eine Voreingenommenheit oder einen sogenannten Kategorienfehler.
1762000	1763000	Ja.
1763000	1769000	Das, was ich am Anfang als Betrug quasi betitelt habe, bei dieser Parorobbe zum Beispiel.
1769000	1776000	Und da macht die Psychologie den Vorwurf, dass es eben die Illusion von Beziehungen irgendwie entsteht,
1776000	1783000	wenn man Roboter quasi so moralzentriert oder auch menschlich behandelt.
1783000	1784000	Ja.
1784000	1788000	Lo macht dann die Frage danach auf, wann ist eine Beziehung eigentlich eine Beziehung?
1788000	1796000	Also wie unterscheiden sich Roboter von Tieren oder auch Beziehungen oder Bezugsnamen zu Dingen im Allgemeinen?
1796000	1800000	Es gibt ja durchaus Menschen, die objektsexuell sind.
1800000	1805000	Das sind wenige Menschen, aber die gibt es halt, die führen Beziehungen zu Dingen.
1805000	1815000	Und wenn ich so einfach daran denke, jede Person, die ich kenne, hat definitiv auch Dinge, zu denen sie irgendwie eine emotionale Bindung zum Beispiel hat.
1815000	1821000	Und das wird im Allgemeinen auch nicht besonders als völlig abstruse oder verwerflich angesehen.
1821000	1831000	Was man da dann bei Robotern sehen kann, ist, dass sie so klassische Kategorisierungen von Subjekt und Objekt oder Belebt und Unbelebt so ein bisschen aufweichen.
1831000	1843000	Also die kommen so zunehmend, gerade wenn sie eben so vermenschlicht auftreten und Menschen auch sehr emotional oder menschlich eben auf sie reagieren, kommt es in so eine Grenzregion.
1843000	1854000	Was sie am Ende sagt, ist, dass eine Definition von Beziehung eigentlich ist, dass eine Beziehung bis hin zu einer Freundschaft kann man desto eher zu einem Gegenüber eingehen,
1854000	1859000	je mehr es eine befriedigende Antwort auf die eigenen Bedürfnisse zu geben, imstande ist.
1859000	1869000	Da spielt glaube ich ein bisschen rein, was wir gerade eben schon hatten, dass uns eigentlich auch menschliche Gegenüber schon ziemlich intransparent sind und wir gar nicht so genau hinter deren Kopf gucken können.
1869000	1879000	Und uns häufig auch egal ist, ob, keine Ahnung, stellen wir uns vor, wir sind an der Supermarktkasse und wünschen uns einen freundlichen Umgang mit der Person, die da kassiert.
1879000	1886000	Dann geht es uns um den freundlichen Umgang und nicht darum, ob uns die Person wirklich wohlgesonnen ist.
1887000	1897000	Dann gibt es in der Philosophie offenbar die Position und die finde ich eigentlich ganz überzeugend ist, dass für Roboter als Objekte der Moral die These spricht,
1897000	1903000	dass es Menschen besser gelingt menschlich zu bleiben, wenn sie Roboter und andere human behandeln.
1903000	1913000	Also das heißt, wenn wir mit Dingen und Robotern eben vernünftig menschlich und nicht völlig gewaltvoll oder so umgehen,
1913000	1928000	dass wir dann eben auch menschselbst bleiben und eben ja nicht sonst wo unsere Gewalt ausleben und die dann vielleicht wieder zu Menschen hin zurück transformieren und sie auch dafür legitim auf einmal erachten.
1928000	1935000	Da gibt es doch diese Diskussion, die ich letztes Mal so am Rande mitgekriegt habe, zum Thema wie Kinder mit so Alexa und Siri und so weiter umgehen,
1935000	1946000	dass sie halt gewöhnt sind so nach dem Motto Alexa spiel XY und dann eventuell auch anfangen im Alltag mit Leuten so umzugehen und einfach nur aufzufordern, ohne da wirklich eine soziale Interaktion einzutreten.
1946000	1954000	Da geht es also ein bisschen um dieses Vorbildhafte sozusagen. Wenn ich mich da mal dran gewöhnt habe, dann bin ich auch Menschenpampiger.
1954000	1964000	Ja, ich hatte neulich Besuch von einem guten Freund und habe mir einen Tee aufgegossen und wenn ich mir einen Tee aufgegieße, dann schrei ich im Normalfall Siri an.
1964000	1969000	Also nein, ich sage zu Siri, Siri stelle einen Timer auf drei Minuten oder so.
1969000	1976000	Und naja, ich hatte mich gerade mit dem Freund unterhalten, habe mir parallel meinen Tee aufgegossen und dann eben zu meinem Handy gesagt, stelle einen Timer auf drei Minuten.
1976000	1984000	Aber halt dann zu einem sehr neutralen Ton und ich wurde dann von meinem Freund so ein bisschen irritiert angeguckt.
1984000	1987000	Aber er hat dann sein Handy rausgeholt und wollte eben Timer stellen.
1987000	2003000	Aber ja, das war definitiv irritierend und von daher kann ich ein Beispiel gut nachvollziehen, weil eben meine Ansprache gegenüber dieser Technik Siri offenbar eine ganz andere war, die ich gegenüber einem guten Freund so nie an den Tag legen würde.
2003000	2009000	So würde ich nie einen Freund darum bitten, doch mal einen Kurzzeitbäcker zu stellen.
2009000	2022000	Naja, und die Frage danach, ob das jetzt aber am Ende irgendwie richtig oder falsch ist, da macht Lo die Frage auf, naja, aber wie verhalten wir uns zum Beispiel gegenüber Kunstwerken oder auch Romanfiguren?
2022000	2025000	Ist es wirklich egal, wie wir ihnen begegnen?
2025000	2042000	Da würden wir vermutlich nicht zu dem Schluss kommen zu sagen, naja, wie ich auf ein Kunstwerk reagiere, ob ich da in einem Museum völlig, keine Ahnung, gefühlskalt und so ohne jeden Respekt vor der Kunst dem gegenüber trete.
2042000	2047000	Naja, da würde niemand sagen, dass das so ganz egal ist, wie wir uns da eigentlich verhalten.
2047000	2048000	Ja, stimmt.
2049000	2053000	Und die Frage ist, warum sollte es dann bei Robotern eben egal sein, ne?
2053000	2055000	Spannend, spannend.
2055000	2057000	Das fand ich auch ganz spannend, naja.
2057000	2067000	Dann gibt es zu dem Ganzen sowas, was sie inklusiver Ansätze nimmt, also die probieren so ein bisschen weiterzudenken, das macht sie dann für den zweiten Teil des Buches auch nochmal.
2067000	2078000	Und diese inklusiven Ansätze bemühen sich eben um so eine Loslösung von dem, was sie Anthropozentrismus nennt, also von diesem Ausgang von, wir gucken nur vom Menschen aus, wie das eigentlich alles beurteilt werden kann.
2078000	2088000	Den Menschen soll dabei an sich nichts abgesprochen werden, also es geht dann nicht darum, den Menschen quasi kleiner zu machen, als er es traditioneller Philosophie gemacht wird.
2088000	2094000	Aber ja, Janina Loh macht auch ganz viel zum Thema Posthumanismus und Transhumanismus.
2094000	2107000	Und da fasst sich so zusammen, dass der kritische Posthumanismus die tradierten und zumeist humanistischen Dichotomien, wie etwa Frau-Mann, Natur-Kultur oder Subjekt-Objekt hinterfragt.
2107000	2109000	Also darum geht es so ein bisschen.
2109000	2121000	Eine Autorin, auf die sie viel referenziert ist, Donna Haraway, die, glaube ich, Biologin und Verhaltensforscherin und Philosophin ist und die ganz lange schon zu Cyborgs und Tieren schreibt.
2121000	2131000	Und genau, sie hat seit neuerem wohl mehr über Tiere und Tierethik und die auch einen eigenen Hund hat, den aber nicht als Haustier bezeichnet, sondern als Forschungsbegleiterin.
2131000	2134000	Aber früher hat sie ja halt über Cyborgs offenbar geschrieben.
2134000	2144000	Und die weiß daraufhin, dass Cyborgs natürlich irgendwie sind so kybernetische Organismen und ja, hybride aus Maschine und Organismus.
2144000	2149000	Und die gibt es sowohl in der gesellschaftlichen Wirklichkeit als auch in der Fiktion.
2149000	2158000	Also ja, und die These, die Haraway offenbar aufmacht und die Loh, glaube ich, nicht unsympathisch findet, ist, dass wir alle schon Cyborgs sind.
2158000	2172000	Also wir benutzen irgendwie schon unsere Handys. Im Prinzip benutzen wir auch alle schon künstliche Dinge wie Pullover und Hosen und fahren eigentlich auch mit Straßenbahnen durch die Gegend und so.
2172000	2177000	Also wenn man das ein bisschen weiter denkt, dann ist die Maschine schon Teil von uns.
2177000	2185000	Nur um mal aufzuzeigen, man kann über diese Differenz von Maschine und Mensch anders denken, als es klassischerweise unterm Alltag getan wird.
2185000	2192000	Ja, dann geht Loh noch so ein paar Beispiele durch. Und das will ich auch ein bisschen zumindest machen.
2192000	2198000	Sie zitiert eine Person, die heißt Suchmann. Ich weiß nicht genau, wie man sie ausspricht.
2198000	2210000	Die sagt zum Beispiel, na ja, man kann auch über Materie zum Beispiel anders denken und als man es gemeinhin tut, also die Differenz zwischen belebt und unbelebt ein bisschen auflösen.
2210000	2216000	Das kam mir erst ein bisschen merkwürdig vor, weil Suchmann offenbar sagt, dass Materie diskursiv ist.
2216000	2224000	Aber sie bringt als Beispiel eine kalifornische Rosine, die, wenn ich sie esse, mehr ist als eine kalifornische Rosine.
2224000	2236000	Sondern wenn ich so eine Rosine esse, dann beschäftige ich mich oder habe Einfluss oder nehme in mich auch so Konzepte wie Kapitalismus einerseits,
2236000	2243000	weil die Rosine wurde ja irgendwo erwirtschaftet und ich habe sie gekauft und so weiter, als auch Kolonialismus, also gerade in den USA.
2243000	2248000	Und vielleicht auch Rassismus, denn wer hat diese Rosine zum Beispiel gepflückt?
2248000	2260000	Also die Idee ist einfach das, was da aufgezeigt werden soll, ist, man kann anders über das Verhältnis von Materie und Mensch und unbelebt und belebt denken.
2260000	2265000	Wobei da natürlich mal schnell bei dieser Ebene sind einfach kulturelle Überformungen sozusagen von Materie zu fassen.
2265000	2268000	Das ist jetzt ja gar nicht so neu, darüber nachzudenken.
2268000	2270000	Absolut, naja.
2270000	2275000	Naja, und daher gibt es dann jetzt eben noch ein paar Mehrbeispiele, die gehe ich jetzt nicht noch alle durch.
2275000	2287000	Genau, wer das Buch liest, sie zitiert häufig eine Person namens Körkelberth, denke ich, also C-O-E-C-K-E-L-B-E-R-G-H geschrieben.
2287000	2291000	Der Person steht sie immer ziemlich positiv gegenüber und diesen Ansätzen.
2291000	2298000	Wenn ich es richtig sehe, ist das ihr Vorgesetzter an der Uni Wien, also quasi ihr Chef.
2298000	2302000	Das sollte man im Hinterkopf haben, wenn man das alles probiert, durchzubewerten.
2302000	2309000	Also ich vermute, dass sie es auch wirklich gut findet, aber nur um zu wissen, dass sie einfach aus der gleichen Kerke herbekommt.
2310000	2317000	Ihre Zwischenbilanz zu dem Ganzen ist dann jetzt erstmal, dass Roboter als moralische Akteure, hatte ich ja schon gesagt,
2317000	2323000	sowas wie eigentlich Autonomie und sowas wie Lernfähigkeit brauchen, eigentlich auch Intentionalität,
2323000	2331000	irgendeine Form von Urteilskraft und Verantwortung und das eben auffällig ist, dass immer von Menschen ausgedacht wird.
2331000	2339000	Als Handlungsobjekte gelten sie im Normalfall eben nur dann, wenn sie auch Handlungsubjekte mal irgendwann sein sollten.
2339000	2347000	Aber man könnte auch sagen, da wo eine emotionale Bindung entsteht zu etwas, zu jemandem, entsteht auch Moralität.
2347000	2349000	Also die These macht sie auch auf.
2349000	2358000	Also es gibt so relationale Moraltheorien, die eben sagen, da wo etwas zwischen zwei Wesen entsteht, sowas wie eine Bindung oder so,
2358000	2366000	da ist eben auch Moral am Werk und das finde ich, kann man zumindest sehr gut an Tieren erkennen, da ist es ja auch nicht egal.
2366000	2371000	Also wenn wir uns an die Binden, empfinden wir sowas wie eine moralische Verantwortung ziemlich schnell
2371000	2382000	und warum sollte das bei Robotern eigentlich anders sein, weil ob die Kapazitäten, die gedanklich in sich so groß unterscheiden, sei mal dahingestellt.
2382000	2387000	Das ist was, was du gerade auch bei diesem Beispiel mit dem Kunstwerk meinst, was mir gerade so ein bisschen reinkommt,
2387000	2396000	dass dieser moralische Bezug gegenüber Objekten meist in erster Linie daran hängt, wie andere Menschen emotional an diese Objekte gebunden sind.
2396000	2399000	Ja, das stimmt.
2399000	2406000	Dass der Roboter, der in der Fabrik irgendwelche Autos zusammenbaut, da ist kein Mensch emotional daran gebunden.
2406000	2409000	Deswegen fällt es uns da leicht zu sagen, ja komm schmeiß den halt auf den Müll.
2409000	2421000	Wenn das jetzt aber der Objekt Liebe oder Objekt Sexuell sozusagen, wenn das irgendwie das, was auch immer das für ein Gegenstand ist,
2421000	2427000	zu dem jemand irgendwie eine sehr enge emotionale Bindung aufgebaut hat, dann ist mit diesem Gegenstand umzugehen,
2427000	2431000	ja einen mit diesen Menschen umgehen und dadurch dann sehr stark moralisch geprägt.
2431000	2436000	Das finde ich gerade spannend, das ist nicht nur die Beziehung, wie gehe ich mit einem Gegenstand um,
2436000	2444000	sondern auch wie sind andere Menschen moralisch an diesen Gegenstand oder emotional in diesen Gegenstand gebunden, die dann da mit ins Spiel laufen.
2444000	2450000	Ich finde dabei durchaus auch zu beachten, gerade wenn es irgendwie um Kunstwerke und das Museumsbeispiel, was sich da gebracht hatte,
2450000	2459000	oder auch um Romanenfiguren geht, ja also die Kunstwerke und Romanenfiguren referenzieren immer auch sehr direkt auf die Person,
2459000	2465000	die diese Dinge erschaffen hat, also das steht ja irgendwie im Hintergrund, wenn ich ein Kunstwerk gegenüber trete,
2465000	2471000	ist das ja auch immer der Person, die es erschaffen hat irgendwie, ja.
2471000	2477000	Von da aus geht es dann eben über zu Robotern als Verantwortungs-Subjekten und ich muss sagen,
2477000	2484000	mit meiner nicht großen philosophischen Vorbildung ist es mir da manchmal ein bisschen schwer gefallen irgendwie nachzuvollziehen,
2484000	2491000	warum diese Trennung so stark gemacht wurde zwischen Moral-Subjekt und Objekt und Verantwortungs-Subjekt und Objekt,
2491000	2499000	aber gut, nehmen wir es erstmal so hin, aber genau, rein von der Zugänglichkeit her für Leute, die vielleicht philosophisch nicht so bewandert sind,
2499000	2505000	hätte ich mich da darüber gefreut, wenn das vielleicht einfach in einem Guss gewesen wäre und nicht so stark getrennt,
2505000	2508000	aber ich probiere mal das irgendwie nachzuziehen.
2508000	2516000	Na ja, da startet sie damit zu sagen, dass Verantwortung und die Möglichkeit zur Verantwortung an Akteurschaft gebunden ist
2516000	2523000	und dass man Verantwortung da von Haftbarkeit unterscheiden sollte, also das ist nicht das Gleiche,
2523000	2531000	also ein bisschen die Unterscheidung zwischen verantwortlichem Handeln einerseits, die irgendwie moralisch total aufgeladen ist
2531000	2540000	und der rein begrenzten Verursachung, also auch, haben wir jetzt Beispiel, wenn ein Kind irgendwas kaputtmacht,
2540000	2547000	dann ist es vielleicht haftbar zu machen dafür oder halt die Eltern, aber man würde nicht unbedingt sagen,
2547000	2554000	dass es in dem Sinne moralisch verantwortlich ist, auch wenn es im Deutschen zumindest von der Sprache her manchmal so ein bisschen verwischt,
2554000	2556000	also Verantwortung und Verursachung quasi.
2556000	2562000	Interessant, da hätte ich jetzt, mein intuitives Begriffsverständnis wäre quasi genau andersrum gewesen,
2562000	2567000	dass gerade derjenige, der verantwortlich ist, dafür haftbar ist, weil die Eltern zum Beispiel nicht genug auf das Kind aufgepasst haben.
2567000	2572000	Genau, das wäre vermutlich das, aber sie haben es nicht verursacht, sondern das Kind hat verursacht.
2572000	2577000	Genau, und deswegen Kind als Verursacher, Eltern als Verantwortliche, vielleicht.
2579000	2587000	Ja, Lo macht ihre eigene Position auch deutlich, also sie sagt, dass manche Roboter die notwendigen Kompetenzen dafür haben,
2587000	2593000	Verantwortung zu simulieren zumindest, was das dann von, das ist das, worüber wir gerade eben gesprochen haben,
2593000	2599000	inwiefern man nachgucken kann, ob sie es nun wirklich verantwortlich sich irgendwann mal fühlen können, wissen wir nicht so genau.
2599000	2605000	Ja, und dass Menschen absehbar erst mal besser für Verantwortungsübernahme qualifiziert sind, überrascht jetzt auch.
2605000	2613000	Verantwortung ist dabei noch ein relativ modernes Konzept im Prinzip, was irgendwie so mit komplexen Gesellschaften entstanden ist.
2613000	2621000	Also, weil man irgendwie nicht mehr direkt zuordnen konnte, wer welches Vergehen eigentlich ausgemacht hat,
2621000	2626000	muss man darauf umstellen, quasi gesellschaftlich zu sagen, verhaltet euch mal verantwortlich,
2626000	2631000	weil ansonsten haben wir hier irgendwie Dynamiken, die wir nicht mehr so richtig überblicken können.
2631000	2638000	Aber auch da ist es doch wieder eigentlich extrem ähnlich wie die Haftbarkeit, weil bei der Haftbarkeit geht es ja auch wieder genau darum, wer ist denn jetzt haftbar dafür.
2638000	2639000	Ja, total.
2639000	2642000	Also für mich fällt dieses Verantwortung und Haftbarkeit extrem zusammen.
2642000	2645000	Das hat sich mir noch nicht erschlossen, wie das unterschiedlich ist.
2645000	2654000	Ja, ich glaube, das ist tatsächlich eine philosophische Unterscheidung, weil mit Verantwortung definitiv mehr einhergeht.
2654000	2662000	Das wäre glaube ich ein bisschen die These, da hängt halt irgendwie moralisch gut und böse mit dran und an Haftbarkeit eben nicht so sehr.
2663000	2671000	Aber Lo weist auch darauf hin, na ja, bei Verantwortung geht es eigentlich schon genuin darum, dass man ein konkretes Subjekt der Verantwortungsübernahme immer wieder ausmacht.
2671000	2672000	Okay.
2672000	2682000	Ich hatte als Gedanken dazu im Kopf, die Klagen in den USA gegen Volkswagen im Zuge dieses ganzen Abgasskandals und so weiter,
2682000	2688000	weil da ja dann tatsächlich am Ende auch nicht, also die Firma Volkswagen kann nicht ins Gefängnis gehen.
2688000	2694000	Die kann irgendwie Geld strafen oder so verantworten, aber das ist vielleicht auch gar nicht das, was manche Leute sehen wollen.
2694000	2700000	Und das heißt, dass am Ende tatsächlich konkrete Manager und Managerinnen ins Gefängnis mussten.
2700000	2705000	Also man probiert dann irgendwie auszumachen, die und die Person ist es jetzt gewesen.
2705000	2716000	Auch wenn glaube ich relativ klar ist, dass das, was Lo für ein Verantwortungsnetzwerk ist, eigentlich das wäre, was viel Treffener beschreibt, wo da die Verantwortlichkeiten liegen.
2716000	2723000	Das ist ja nicht der eine Manager, der jetzt irgendwie eigentlich verantwortlich ist, aber er wird zumindest haftbar gemacht.
2723000	2724000	Ja klar.
2724000	2732000	Ja, genau. Die Idee von Verantwortungsnetzwerken ist dann auch, dass alle beteiligten Parteien in einer Situation Verantwortung tragen.
2732000	2741000	Das Problem ist, dass sowas entwickelt wurde, um irgendwie solche Situationen, wie ich es hier gerade eben beschrieben habe, zu bewältigen.
2741000	2751000	Also rein philosophisch jetzt erstmal nicht justiziabel quasi, aber Lo weist darauf hin, dass wenn man probiert, eben Verantwortungsnetzwerke zu begreifen,
2751000	2756000	gerade im Kontext von autonomen Systemen, also wie der Beispiel Autounfall, wer ist jetzt eigentlich verantwortlich?
2756000	2760000	Das Auto kann es nicht so richtig sein, der, die Fahrerin vielleicht auch nicht.
2760000	2767000	Und welchen Anteil hat die Person, der die Unfall zugestoßen ist, eigentlich selbst?
2767000	2773000	Naja, und was haben die Ingenieure damit zu tun und die Programmierer, die das Programm geschrieben haben und so weiter?
2773000	2780000	Also so Verantwortungsnetzwerke, wenn man wirklich erfassen will, wer eigentlich schuld an irgendetwas ist, werden riesig groß.
2780000	2787000	Damit torpediert man quasi die Idee hinter Verantwortung wieder, nämlich dass man eine Person hat, die man haftbar machen kann.
2787000	2789000	Ja, da fällt es nämlich doch wieder zusammen.
2789000	2798000	Ja, total, ja. Und da unterscheidet man dann eben vor Gericht quasi zwischen Verantwortung und Haftbarkeit, würde ich sagen.
2798000	2803000	Gehen wir über zu Verantwortungsobjekten so ein bisschen.
2803000	2810000	Ja, da ist die Idee dahinter, dass selbst die komplexesten Maschinen nicht mehr als bloße Instrumente menschlichen handeln sind.
2810000	2819000	Man kennt das von der National Rifle Association aus den USA, also nicht Waffen töten Menschen, sondern Menschen töten Menschen.
2819000	2822000	Und da sagt Lou, naja, so einfach ist das nicht.
2822000	2830000	Also eine Person mit einer Pistole ist ein Schütze, eine Schützin und eine Person ohne eine Pistole ist eine Person.
2830000	2835000	Also sie denkt das schon eher zusammen. Und das Gleiche geht dann eben auch für Roboter.
2835000	2841000	Sie nennt da einen Philosophen, der sich eigentlich mit Computersystemen beschäftigt und das schon relativ lange.
2841000	2854000	Und der sieht Probleme im Verantwortungsbereich, in der Philosophie dann, wenn Computersysteme schneller und akkurater als Menschen in der Lage dazu sind, Entscheidungen zu treffen.
2854000	2863000	Und man kann sich das durchaus vorstellen, gerade in, so nennt sie das, Delikaten Kontexten wie militärischen Frühwarnsystem, wo es dann darum geht,
2863000	2872000	okay, ein Frühwarnsystem sieht, dass eine Rakete oder so auf einen zufliegt und muss dann darauf reagieren.
2872000	2879000	Und das kann dann kein Mensch mehr machen, sondern das muss dann eben, ja, muss dann ein Computersystem machen.
2879000	2891000	Da sieht dieser Hans Lenk eben ein Beleg dafür, dass die Abhängigkeit von Technologien dafür sorgt, dass Computersysteme auch heute schon Verantwortung und Verantwortung übernehmen.
2891000	2900000	Die Frage ist aber, wenn man so ein Computersystem nicht haftbar machen kann, wer steht dann am Ende Rede und Antwort?
2900000	2901000	Ja, genau.
2901000	2906000	Also die Frage nach Haftbarkeit, Verantwortung kommt da dann eben wieder auf.
2906000	2915000	Zusammenfassend kann man sagen, dass den verantwortlichen Subjekten generell eine moralische Verantwortung für die Produkte ihres Schaffens zugeschrieben wird.
2915000	2924000	Also auch heute ist es noch so, dass Computersysteme, Roboter und so weiter quasi als verlängerte Arm ihrer ErschafferInnen gelten.
2924000	2925000	Ja.
2925000	2934000	Was ich gerade eben schon gesagt hatte, ist, dass es so Ideen gibt von relationalen Konzepten von Moral und die gibt es eben auch im Bereich der Verantwortung.
2934000	2942000	Das heißt, inklusiven Ansätzen aufgemacht, dass Verantwortung sich ausschließlich in der Interaktion zwischen Wesen abspielt.
2942000	2949000	Die Idee dahinter ist, dass das Narrativ vom autarken Handlungssubjekt eine gesellschaftliche, rechtliche und politische Illusion ist.
2949000	2951000	Das ist wieder diese Haraway, die sie da zitiert.
2951000	2959000	Also dass wir alle so wunderbar autark und autonom sind, ist vielleicht notwendig, dass wir uns das so denken.
2959000	2969000	Aber faktisch ist das nicht so, weil wir natürlich irgendwie gesellschaftlich eingebunden sind und leben und manchmal gar nicht anders können, als die Situation es irgendwie zulässt.
2969000	2972000	Ich glaube, wir als Soziologen können das ganz gut nachvollziehen.
2972000	2973000	Ja.
2973000	2985000	Ja, dabei muss man irgendwie immer wieder daran erinnert werden, finde ich zumindest, dass die Idee der Individualität dahinter und der entsprechenden Verantwortung und Verantwortungsübernahme noch relativ neu ist.
2985000	2988000	Also Losak, die ist erst im Mittelalter entstanden.
2988000	2992000	Das Gleiche gilt übrigens für Begriffe wie Identität.
2992000	2998000	Also dass jeder Mensch eine eigene Identität hat, ist auch noch eine relativ moderne Erfindung.
2998000	3003000	Und dass auch solche Individualitätsideen ziemlich kulturraum explizit sind.
3003000	3013000	Sie meinen zum Beispiel, dass es im asiatischen Kulturraum gar nicht so verbreitet ist, dass man irgendwie die Idee hat, dass man irgendwie ein Individuum ist.
3013000	3019000	Ein bisschen die Unterscheidung zwischen individualistischen und kollektivistischen Kulturen.
3019000	3022000	Ich bin da nie in die Materie tief eingestiegen.
3022000	3027000	Ich weiß nicht, wie gut sich das tatsächlich aufrechterhalten lässt.
3027000	3030000	Also ich gebe das einfach nur erst mal so wieder.
3031000	3040000	Aus der Idee von dieser interaktionsbasierten Verantwortung gibt es dann das Konzept der Extended Agency aus.
3040000	3047000	Dass Interaktionen eben aus einer Menge von menschlichen und nichtmenschlichen Beteiligten bestehen.
3047000	3052000	Und eine Handlung sich eben nicht mehr nur auf die menschlichen Komponenten bezieht.
3053000	3058000	Fand ich auch ganz interessant für alle NichtsoziologInnen da draußen.
3058000	3069000	Die klassische Definition von Handlung ist nämlich nach Max Weber in der Soziologie eigentlich eben, dass sie mit Intention passiert und genuin menschlich ist.
3069000	3073000	Also Tiere können sich zum Beispiel verhalten, aber nur Menschen können handeln.
3073000	3079000	Da gibt es eben offenbar Ansätze, die probieren das ein bisschen anders zu sehen.
3080000	3088000	Wenn da diese Begriffe so das Gegenstände handeln können, das erinnert mich ein bisschen an Latours aktuellen Netzwerktheorie.
3088000	3089000	Ja ja witzig.
3089000	3094000	Das Latour taucht auf, da ich Latour nicht kenne, gebe ich dir hier nicht so richtig wieder.
3094000	3099000	Aber genau darauf wird offenbar viel auch referenziert in der Literatur.
3100000	3109000	Ja, so wie ich das jetzt gerade eben mal probiert habe, irgendwie knapp zusammenzufassen, gibt es einige Schwierigkeiten, die da auftauchen.
3109000	3112000	Und ich finde, die liegen auch relativ deutlich klar.
3112000	3119000	Autonomer werdende Systeme machen es zunehmend schwieriger, Verantwortlichkeiten zuzurechnen.
3119000	3128000	Also wenn wir mehr Systeme haben, die irgendwie in einem Zwischenraum zwischen ich mache so ein bisschen, was mir einprogrammiert wurde, ein bisschen, was ich irgendwie autonom gelernt habe.
3128000	3136000	Und aber es sind auch noch Menschen involviert, macht es irgendwie ganz schwierig zu sagen, wer ist denn jetzt eigentlich schuldig, haftbar, verantwortlich für irgendwelche Handlungen.
3138000	3152000	Und auch in diesen Relationskonstrukten von Verantwortung, wo man sagt, ok, wenn Dinge, Tiere, Roboter mit Menschen agieren oder auch Dinge mit Dingen oder Menschen mit Menschen, nur da entsteht Verantwortung,
3152000	3162000	ist, dass man da durch diese neuen technischen Systeme auch so Verantwortungslücken in den normativen Kriterien hat, weil man irgendwie für diese technischen Systeme gar nicht so richtig weiß, wie man das alles bewerten soll.
3162000	3163000	Ja, stimmt.
3163000	3172000	Ja, ja. Und worauf Lo noch hinweist, das fand ich noch ganz spannend, ist, dass sie sagt, wir erleben auch sowas wie neue Raum-Zeit-Dimensionen im Kontext.
3172000	3182000	Also sie sagt, die Rückbindung, globale Ereignisse, sie nennt Fluchtbewegungen oder auch den Klimawandel als Beispiele an einzelne Gruppen von Akteurinnen wird schwieriger.
3182000	3189000	Also man kann nicht mehr so richtig sagen, wer ist denn jetzt beim Klimawandel eigentlich, wer ist daran jetzt schuld, wem ziehen wir dafür zu Rechenschaften.
3189000	3201000	Oder wenn wir uns komplexe Migrations- und Fluchtbewegungen angucken, wie kann man sie einerseits vielleicht besser koordinieren, wie kann man das steuern, wie kann man dafür sorgen,
3201000	3206000	dass die Bedingungen deren Wegen geflohen wird, dass die geändert werden.
3206000	3214000	Auch das ist total schwierig da irgendwie zuzurechnen, wer da zur Verantwortung gezogen werden kann.
3214000	3219000	Wobei ich da sagen würde, das Genuin hängt jetzt nicht mit Robotern oder auch mit artificiellen Systemen zusammen.
3219000	3221000	Nur einfach mit unglaublich komplexen Wirkketten.
3221000	3229000	Ja, genau. Die Frage, die ich mir dabei jetzt am Ende gestellt habe, ist, wenn man jetzt so Verantwortungskonzepte irgendwie erweitert,
3229000	3234000	probiert in Netzwerke zu integrieren oder auch Roboter zur Verantwortung ziehen möchte.
3234000	3244000	Ich frage mich ein bisschen, was dadurch gewonnen werden kann, weil ich das Gefühl habe, dass Verantwortungsaufforderungen oder so,
3244000	3251000	oder die sich eben gerade dadurch da ausdrücken, dass sie nicht immer mit einer rechtlichen Keule um die Ecke kommen,
3251000	3257000	sondern sagen, verhaltet euch mal verantwortlich, weil es kommen noch Leute nach euch, wir müssen an unsere Kinder denken,
3257000	3263000	beim Klimawandel oder was auch immer. Und ich frage mich so ein bisschen, gewinnt man eigentlich praktisch irgendwas dadurch,
3263000	3266000	wenn man das alles so weit zieht quasi?
3266000	3272000	Ja, das habe ich auch gerade mal nachgedacht, weil ich mir diesen Begriff der Verantwortung überlegt habe,
3272000	3277000	gerade wenn sie ihn von der Haftbarkeit trennen so stark. Was heißt das überhaupt?
3277000	3283000	Ich meine, Verantwortung wirkt ja im Grunde als Handlungsprägen, kann das ja nur über mögliche Konsequenzen wirken.
3283000	3289000	Es können auf der einen Seite sein die Haftbarkeit, die klassische rechtliche, das kann aber auch so etwas sein wie sozialer Status,
3289000	3298000	wie irgendwie so ein, das bin nicht ich, so ein Selbstachtungsverlust oder Scham oder sowas in der Art.
3298000	3304000	Aber das kann ja immer nur über diese Wirkmechanismen sozusagen nach außen treten.
3304000	3310000	Dann ist natürlich die Frage, wie weit die bei irgendwelchen robotischen Systemen sozusagen wirken können.
3310000	3312000	Kann ein Roboter Scham empfinden?
3312000	3319000	Ja, das finde ich sind auch so die spannenden Fragestellungen. Das ist mir nicht so richtig klar.
3319000	3325000	Ich finde, das, was ich gelesen habe, weist eigentlich eher in die Richtung von, es ist zumindest erstmal nicht abzusehen,
3325000	3331000	dass da sowas wie genuines Bewusstsein wirklich ist. Und dann ist für mich ein bisschen die Frage, warum das alles?
3331000	3339000	Weil Roboter haben auch absehbar kein eigenes Konto, von dem du irgendwelche Dinge bezahlen kannst.
3339000	3343000	Das gleiche gilt auch für autonome Fahrsysteme.
3343000	3349000	Wem ist damit geholfen, wenn du ein Auto für einen Unfall zur Rechenschaft ziehen kannst?
3349000	3353000	Genau, was heißt das überhaupt ein Auto zur Rechenschaft zu ziehen?
3353000	3356000	Dann darf das dann drei Monate nicht fahren, weil es ihm ja Spaß macht zu fahren.
3356000	3359000	Ja, das finde ich auch, das ist irgendwie total schwierig.
3359000	3364000	Kriegt das irgendwie ein fiktives Gehalt für jeden Kilometer, den es fährt, und muss dann davon wieder was...
3364000	3367000	Also irgendwie so richtig erschließt sich mir das nicht.
3367000	3371000	Diese Wirkmechanismen, die fehlen da irgendwie komplett, so Sanktionsmechanismen im Grunde.
3371000	3373000	Ja, das stimmt.
3373000	3374000	Auf welcher Ebene auch immer.
3374000	3380000	Vielleicht muss man das Ganze auch eher als wirklich, ja Gedankenspiel finde ich, ist immer so ein bisschen respektierlich,
3380000	3388000	aber vielleicht muss man es einfach als wirklich philosophische Überlegung erstmal auch für sich stehen lassen und das als das begreifen.
3388000	3394000	Also zu sagen, okay, die philosophische Überlegung darüber, was bedeutet Verantwortung, was bedeutet Moral,
3394000	3400000	in quasi außerhumanen Kontexten ist ein Erkenntnisgewinn für sich.
3400000	3407000	Ich glaube, wenn man so darauf guckt, wird man dem vielleicht eher gerecht.
3407000	3413000	Wobei es mir gerade so spontan einfällt, wenn man so Sanktionen irgendwie auch als Lernreiz sozusagen versteht.
3413000	3417000	Also man sagt, man verheckt eine Sanktion, damit die Person das nicht wieder tut.
3417000	3422000	Oder im Vorhinein als abschreckend. Das kann man ja wiederum in Computersystemen sehr gut abbilden.
3422000	3427000	Gerade wenn du so ein selbstlernendes System hast und das bringt irgendwie jemand um, ihm dann ganz böse auf den Finger zu hauen und sagen,
3427000	3432000	ne, ne, ne, das machst du nie wieder. Das könnte man ja sogar technisch relativ gut abbilden.
3432000	3436000	Ja, vielleicht ist, ja, das stimmt. Das könnte ganz gut funktionieren.
3436000	3445000	Gerade wenn die These ist, dass eben Top-Darren-Programmierung offenbar, weil das alles so zwischen den abstrakten Regeln und dem konkreten Verhalten,
3445000	3450000	dass da so eine große Diskrepanz ist, dass eben Top-Darren nicht dauerhaft gut funktioniert.
3450000	3457000	Ja, also dann müsste man eben als Regel einprogrammieren, dass sich XY nicht gut anfühlt für das Auto.
3457000	3460000	Genau, was auch immer nicht gut anfühlen dann wieder heißt.
3460000	3467000	Genau, aber das kann man ja, das wäre die These dahinter. Und das wissen wir aber eigentlich schon für andere Leute nicht.
3467000	3473000	Also wir glauben halt nur zu wissen, dass die uns ähnlich, dass die so ähnlich fühlen wie wir. Spannend, spannend.
3473000	3484000	Naja, kritische Zwischenbilanz zu dem ganzen Verantwortungsthema ist, naja, Akteurschaft ist irgendwie notwendig, um überhaupt Verantwortung zurechnen zu können.
3484000	3490000	Hinter dem Ganzen steht irgendwie die Befürchtung, dass wir Roboter irgendwann nicht mehr kontrollieren können.
3490000	3498000	Also wir vertrauen unseren eigenen Geschöpfen quasi nicht mehr so richtig. Und man kann feststellen, dass Menschen zweierlei Verantwortung haben.
3499000	3507000	Also einmal sehr individuell als Designer innen von autonomen Systemen oder Robotern oder Programmiererinnen und so weiter.
3507000	3513000	Aber auch kollektiv als Unternehmen, die bestimmte Roboter oder nicht Roboter einsetzen.
3513000	3521000	Oder auch in Form von Ethikgremien, die irgendwas in Bezug auf diese ganze Roboter-Thematik entscheiden oder nicht entscheiden.
3522000	3534000	Und ja, was sie sagt, was bei dem ganzen klar wird, ist, Bedürfnis der modernen Gesellschaft, alles immer kontrollieren zu können, wird auch bei Robotern offenbar.
3534000	3542000	Warum sie das so hervorhebt, weiß ich nicht so ganz genau, weil ich das Gefühl habe, es gibt schon sehr, ich weiß nicht, vielleicht ist es einfach nur eine philosophische Feststellung.
3542000	3552000	Ich hatte erst mal das Gefühl, na ja, es gibt schon gute Gründe, die die die Resultate des eigenen Schaffens, die Ergebnisse kontrollieren können zu wollen.
3555000	3559000	Genau, dann schließt sie das Buch mit klassischen abschließenden Bemerkungen.
3559000	3566000	Das gibt es ja häufiger mal, dass dann eben so ein bisschen nochmal quasi in die Zukunft gedacht werden soll oder ein bisschen praktischer.
3566000	3570000	Hier in dem Fall ist jetzt so ein bisschen politische Implikation des Ganzen.
3570000	3576000	Sie wiederholt ihre Aussage, dass Produkte des menschlichen Handelns moralisch nie neutral sind.
3576000	3585000	Das gibt es eben im Bereich der Technik-Philosophie offenbar diese Position, na ja, es ist halt Technik, die kann man gut oder schlecht verwenden.
3585000	3597000	Und wenn sie eben schlecht verwendet wird, ist das zwar problematisch, aber die problematische Verwendung liegt eben bei Menschen und nicht in der Technik.
3597000	3604000	Und sie sagt, na ja, Vorsicht, Dinge sind eben moralisch immer aufgeladen, weil wir eben nach gewissen Normen agieren.
3604000	3616000	Das gibt es offenbar auch schon länger, das Konzept, also so Gedanken wie zum Beispiel, wenn wir uns einen Tisch vorstellen, der ein schmales Kopfende hat und dann eine lange Tafel hat,
3616000	3622000	dann drückt das im Prinzip schon eine hierarchische Struktur aus, die Vorsicht, dass jemand am Kopfende sitzt und Chef ist.
3622000	3629000	Ja, es ist ja auch bei den diplomatischen Verhandlungen, dass dann die kreisrunden Tische eingesetzt werden und solche Sachen, die genau das eben abbilden.
3629000	3635000	Na ja, Forderungen, die sie am Ende auf jeden Fall stellt, sind mehr Ethik- und Informationsunterricht in Schulen.
3635000	3644000	Sie sagt, Ingenieurswissenschaftliche Ausbildung sollte mit Ethikpflichtkursen behaftet sein, weil sie, also sie nimmt da das Beispiel der Medizin.
3644000	3656000	Man stelle sich nur mal vor, bei Dingen wie der Präimplantationsdiagnostik würden Ärzte und Ärztinnen irgendwie beraten, ohne je einen Ethikpflichtkurs im Bereich gemacht zu haben.
3656000	3662000	Und sie meint, na ja, wenn jetzt irgendwie Roboter entwickelt werden, muss man auch die ethischen Hintergründe kennen.
3662000	3670000	Und sie sagt, auch Unternehmen sollten Weiterbildungskurse im Bereich der Technik- und Roboterethik anbieten oder verpflichtend machen.
3670000	3679000	Und sie meint, wir brauchen auch zunehmend öffentliche Ethikgremien, die zum Thema Roboter irgendwie beschlagen und bewandert sind.
3679000	3691000	Und sie spricht sich als, ich glaube, kann man durchaus so sagen, linke Philosophin für eine Diskursöffnung aus, für die Vermittlung von Sachverstand, Reflektions- und Urteilskraften auf allen Ebenen, für alle Mitglieder einer Gesellschaft.
3692000	3706000	Was natürlich demokratischen Idealen absolut entspricht, dass man, wenn man sagt, okay, wir müssen darüber reden, wie wir mit Robotern umgehen, was sie dürfen, was sie nicht dürfen, was sie ersetzen sollen oder auch nicht, dann sollten da auch möglichst viele an diesem Diskurs beteiligt sein.
3706000	3721000	Sie hat auch bei Rahel Yagi promoviert, glaube ich, und das ist auf jeden Fall auch eine ziemlich interessante Person und auch auf jeden Fall aber linke Philosophin, die ursprünglich, glaube ich, so aus der Hausbesetzer-Szene kommt.
3721000	3750000	Genau. Was sie sagt, was manchmal ein bisschen schwierig ist in dieser ganzen Diskussion, gerade so im Bereich, wenn es um Superintelligenzen und so geht, ist, dass sie sagt, okay, es wird häufig so getan, als sei gesetzt, dass sowas wie eine Superintelligenz irgendwann entsteht, die deutlich cleverer, schneller, ethischer, besser wie auch immer ist als der Mensch oder halt völlig dystopisch.
3750000	3760000	Die alles kaputt macht und den Menschen überholt und uns braucht es dann gar nicht mehr. Und sie meint, das sind alles keine Naturgesetze, also es muss nicht alles passieren, was möglich ist.
3760000	3777000	Wir kennen das gerade in Deutschland, dass einige Technologien, die theoretisch möglich sind, wie zum Beispiel das Klonen oder so oder das, was immer wieder unter so Designer-Baby oder so filmiert, sowas wäre ja im Prinzip technisch möglich, zumindest grundlegend.
3777000	3784000	Das ist mit guten Gründen verboten und das kann man auch im Bereich der Roboter- und Computersystem-Ethik und so weiter auch fortführen.
3784000	3791000	Also es ist nicht ausgemacht, dass bestimmte Technologien irgendwann real werden und sie bittet darum, dass man sich das mal vor Augen führt.
3791000	3796000	Und das fand ich einen ganz schönen Hinweis, weil ich das tatsächlich bislang auch, glaube ich, wenig reflektiert habe.
3796000	3810000	Also mir schien das auch immer so wie ausgemacht, naja, irgendwann gibt es halt so eine, ja, wird es den Punkt geben, wo Computer allumfassend besser und klüger handeln können als wir.
3810000	3816000	Mal gucken, nur wann der kommt. Und sie meint, naja, das muss nicht so kommen. Wir können uns auch schon dagegen entscheiden, mit guten Gründen.
3816000	3817000	Stimmt.
3817000	3822000	Ja, das wäre es von mir zu diesem Buch. Hast du Fragen, Anmerkungen, Kritik?
3822000	3825000	Ja, ich glaube, wir haben ja schon währenddessen ziemlich viel diskutiert.
3825000	3832000	Deswegen, da ist, glaube ich, genug Anknüpfungspunkte auch für euch da draußen zum auch mal drüber nachdenken.
3832000	3835000	Mich hat es auf jeden Fall viel zu denken gebracht. Spannend.
3835000	3836000	Schön.
3840000	3846000	Bist du zwischendurch schon auf Bücher, Vorschläge des Weitermachens gestoßen oder soll ich das eben noch machen und du übernehmst da nach?
3846000	3853000	Ja, ich habe tatsächlich schon Ideen gehabt, auch kurz davor, als ich mir vor der Episode mal kurz zwei Minuten angeguckt hatte, worum es überhaupt geht.
3853000	3863000	Das ist einmal tatsächlich das Sachbuch, was ich gerade vor dem Zettelkastenprinzip zu Ende gelesen habe, nämlich Life in Code von Ellen Ullman.
3863000	3871000	Und das ist im Grunde so eine Essay-hafte Autobiografie von einer Frau, die halt irgendwie seit den 80er Jahren in der Softwareentwicklung gearbeitet hat.
3871000	3879000	Und im Grunde so dann auch als erfahrene Programmiererin so ein bisschen in die Start-up-Kultur reingerutscht ist und die beobachtet hat.
3879000	3890000	Und sie hat an einer Stelle, gibt es eine Stelle, wo sie eben drüber nachdenkt, wie es die Welt mit Robotern sozusagen aussieht, wenn man die irgendwie noch menschlicher behandelt.
3890000	3895000	Und da überlegt sie dann, was denn eigentlich für Roboter heißt, dass sie genießen.
3896000	3903000	Dass sie ein Essen oder irgendetwas genießen, dass es dann irgendwie darum geht, wie das Kühlmittel ihre Rohre durchzieht oder so.
3903000	3905000	Das fand ich ein sehr, sehr schönes Bild.
3905000	3909000	Und sie widmet sich eben auch so dieser Perspektive, Technik macht die Welt besser.
3909000	3912000	Das ist das, was du auch dahin, was du auch so angesprochen hast.
3912000	3917000	Das ist jetzt kein hardcoreiges Sachbuch mit irgendwelchen durchargumentierten Punkten oder so.
3917000	3923000	Aber vielen interessanten Gedanken über das, was wir so seit 20 Jahren über Technik denken.
3923000	3927000	Und wie Technik unsere Gesellschaft geprägt und verändert hat.
3927000	3929000	Mit vielen persönlichen Anekdoten und so weiter drin.
3929000	3931000	Aber es ist wirklich ein echt spannendes Buch.
3931000	3933000	Life in Code von Alan Ullman.
3933000	3934000	Und was natürlich...
3934000	3935000	Entschuldigung.
3935000	3937000	Nee, wirklich.
3937000	3938000	Das klingt sehr inspirierend, finde ich.
3938000	3939000	Ja, schön.
3939000	3945000	Und was natürlich, wenn es um Roboter geht, rührt das an meiner Science-Fiction-Seele.
3945000	3947000	Also Science-Fiction-Tipps, Weltenflüstern und so.
3947000	3949000	Mein zweiter Podcast.
3949000	3951000	Zwei Bücher, die mir da spontan eingefallen sind.
3951000	3953000	Das ist einmal ein Klassiker.
3953000	3954000	Der 200-Jährige.
3954000	3957000	Ursprünglich mal von Isaac Asimov als Kurzgeschichte geschrieben.
3957000	3960000	Sowieso Isaac Asimov, der große Roboter, Science-Fiction-Autor.
3960000	3963000	Und dann von Robert Silverberg zum Roman ausgearbeitet.
3963000	3967000	Und auch mit, äh, nicht doch mit Dustin Hoffman verfilmt.
3967000	3972000	Da geht es im Grunde um einen Roboter, der als Mensch anerkannt werden will.
3972000	3975000	Mit relativ klassischem Motiv so in den 70er Jahren.
3975000	3977000	In der Science-Fiction.
3977000	3980000	Und dann gibt es einen Roman zwischen zwei Sternen von Becky Chambers.
3980000	3984000	Sowieso eine ganz tolle Autorin, von der lohnen sich im Grunde alle Romane.
3984000	3987000	Aber inzwischen zwei Sternen geht es im Grunde um genau das gegenteilige Problem.
3987000	3991000	Um einen Roboter, der sich als Mensch ausgeben muss, das aber eigentlich nicht will.
3991000	3993000	Oh, das ist auch gut.
3993000	3996000	Also es ist ein schöner Twist zu diesem sehr klassischen Motiv.
3996000	3999000	Also diese beiden Romane, der 200-Jährige und zwischen zwei Sternen,
3999000	4001000	die kann man glaube ich auch mal direkt hintereinander lesen,
4001000	4003000	um diesen Unterschied sich ein bisschen deutlich zu machen.
4003000	4007000	Auch wie sich die Science-Fiction als Genre verändert hat, lustigerweise.
4007000	4008000	Das ist gut.
4008000	4014000	Das kommt tatsächlich in dem Buch, also Roboter-Ethik auch, zwischendurch eben mal vor.
4014000	4017000	Ich glaube bei dieser Haraway eben, wo so ein bisschen aufgemacht wird,
4017000	4021000	dass so die Grenzen zwischen Science-Fiction und Science irgendwie
4021000	4027000	und der Wirklichkeit absolut verwischen und gar nicht so klar sind,
4027000	4030000	wie man das vielleicht sich manchmal vorstellt.
4030000	4033000	Ich finde, man kann das jetzt auch gerade, merkt man das wieder,
4033000	4046000	dass so die Vorstellungen von Dystopien auch massiv von Filmen und Büchern geprägt sind.
4046000	4050000	Schönen Artikel zu, den ich auch gerne den Jones verlinken kann.
4050000	4051000	Ja, sehr gut.
4051000	4054000	Was mir noch gerade spontan einfällt, weil ich gerade Isaac Asimov erwähnt habe,
4054000	4059000	tauchen in dem Buch irgendwo seine drei Robotik-Gesetze auf,
4059000	4061000	zumindest mal in der Fußnote oder in einem Nebensatz?
4061000	4069000	Warte, ich gehe mal ganz kurz hinten ins Literaturverzeichnis, aber zitiert ist
4069000	4073000	The Complete Robot, The Definitive Collection of Robot Stories.
4073000	4075000	Okay, da wird es definitiv drin sein.
4075000	4079000	Ja, aber mir ist das jetzt zumindest nicht aufgefallen.
4079000	4086000	Weil es gibt irgendwie drei Roboter-Gesetze, ich weiß nicht, ob ich sie jetzt gerade auswendig hinkriege.
4086000	4090000	Das erste Gesetz ist auf jeden Fall, Roboter dürfen Menschen niemals schaden.
4090000	4091000	Ja.
4091000	4095000	Das zweite Gesetz ist, glaube ich, Roboter müssen Menschen immer helfen.
4095000	4101000	Und das dritte Gesetz ist, Roboter müssen immer die Befehle ihres Inhabers oder wie auch immer gehorchen.
4101000	4106000	Jeweils darf dadurch das vorherige Gesetz aber nicht verletzt werden.
4106000	4111000	Also du musst zwar Befehle befolgen, darfst dadurch aber niemandem schaden, sozusagen.
4111000	4117000	Das ist so ein Versuch, damit umzugehen, aber da verlinken wir euch auch die korrekten Sachen in den Show Notes.
4117000	4118000	Sehr gut.
4118000	4124000	Ich finde auch da merkt man, wie das so, also Asimov eigentlich als Science Fiction Autor,
4124000	4129000	wie das dann aber vermutlich jetzt auch in die Wirklichkeit von Roboter-Programmierung zurück spielt.
4129000	4130000	Natürlich.
4130000	4136000	Und das halt eben nicht irgendwie der Science Fiction Autor bleibt, sondern halt in die Realität einfach zurückgeht.
4136000	4142000	Naja, im Vorlauf zu diesem Buch oder während ich es gelesen habe, habe ich mit Jennifer,
4142000	4146000	mit der ich auch das Soziologische Kaffeekränzchen zusammen mache und die auch einen eigenen Podcast hat,
4146000	4149000	den Föllefanzcast, auch darüber gesprochen ein bisschen.
4149000	4153000	Und sie meinte, naja, wenn du hier diese Bücher da empfiehlst und vorschlägst,
4153000	4157000	schlag doch mal mit vor Maschinen wie ich von Ian McEwan.
4157000	4164000	Habe ich jetzt selbst nicht gelesen, scheint momentan aber irgendwie ganz schön viel Bass quasi drum zu sein.
4164000	4167000	Und ich weiß nicht, du weißt da glaube ich ein bisschen mehr.
4167000	4169000	Ich habe den Roman selber auch nicht gelesen.
4169000	4174000	Er wird so in Science Fiction Kreisen so ein bisschen kritisch gesehen, weil das jetzt wieder so ein Fall ist,
4174000	4181000	in Anführungszeichen literarischer Autor macht ein Thema auf, was die Science Fiction seit 30, 40 Jahren diskutiert.
4181000	4185000	Und bei der Science Fiction hört irgendwie nie jemand zu und jetzt springen auf einmal alle auf diesen Autoren.
4185000	4188000	Und er ist so der große Experte für das Thema.
4188000	4193000	Dadurch wird das da so ein bisschen kritisch gesehen. Der Roman an sich soll aber tatsächlich relativ gut sein.
4193000	4196000	Okay, das ist ja schon mal ganz schön.
4196000	4201000	Was ich vor einigen Jahren gelesen habe, ist Germany 2064 von Martin Walker,
4201000	4205000	der seinerseits schotte ist, wenn ich es jetzt vorher richtig nachgeschlagen habe,
4205000	4211000	finde ich einen ganz schönen Zukunfts Roman oder Thriller oder wie man es nennen möchte.
4211000	4216000	Das ist im Prinzip ein Kriminalfall. Ich habe aber vergessen, worum es um eigentlichen Krimi geht, also was der eigentliche Kriminalfall ist.
4216000	4225000	Aber es gibt eben einen Inspector, der 2064 an seiner Seite als engsten Mitarbeiter einen Roboter hat,
4226000	4234000	der, glaube ich, direkt zu Beginn quasi einen Upgrade erfährt, weil er in einem vorherigen Kriminalfall zu Bruch gegangen ist
4234000	4237000	und danach deutlich intelligenter ist, als er es vorher ist.
4237000	4241000	Und die beiden sind auch gut befreundet miteinander, also der Roboter und der Inspector.
4241000	4244000	Und die Welt, in der das Ganze stattfindet, ist eben Deutschland.
4244000	4250000	Und Deutschland ist so ein bisschen zweigeteilt, einerseits ein Hochtechnologieland mit allem, was dazugehört,
4250000	4256000	irgendwie fliegende Autos, selbstfahrende Autos und kluge Maschinen wie eben auch diese Roboter.
4256000	4260000	Es gibt aber parallel dazu auch Kommunen, die dem Ganzen entsagt haben.
4260000	4264000	Das muss man sich, glaube ich, vorstellen wie so ein paar Enklaven quasi in Deutschland.
4264000	4270000	Nicht wie richtig zweigeteilt, es gibt ein Land, das ist so, ein Land, das ist so, sondern es gibt eben so ein paar Kommunen,
4270000	4275000	die machen mal dem ganzen Kram nicht mit, die sind irgendwie wieder ziemlich zurück auf ganz normale Landwirtschaft,
4276000	4280000	haben, glaube ich, auch so rudimentäre Technik. Ich glaube, die haben sich irgendeinen Stichtag gesetzt,
4280000	4285000	irgendwas in den 90er Jahren oder so. Bis dahin benutzen sie alles und danach nicht mehr.
4285000	4290000	Also es ist aber gar kein großes Gegeneinander, aber es spielt eben in beiden Welten so ein bisschen.
4290000	4295000	Und ich glaube, der Kommissar muss eben in beiden Bereichen ein bisschen ermitteln.
4295000	4298000	Das habe ich damals auf jeden Fall gerne gelesen.
4298000	4301000	Und was ich noch mit empfehlen würde, ist ein Interview.
4301000	4304000	Ich weiß nicht, ich glaube, es ist ungefähr eine halbe Stunde oder ein bisschen mehr.
4304000	4313000	Janina Loh war im Zuge des Buches bei Sein und Streit einer empfehlenswerten Sendung ganz generell vom Deutschlandfunk Kultur.
4313000	4315000	Und das verlinke ich euch auch noch.
4315000	4321000	Grundsätzlich Deutschlandfunk hat eigentlich so mittlerweile gefühlt die besten Radiosendungen.
4321000	4326000	Ich höre fast nur noch Deutschlandfunk, das hätte ich mir vor einem halben Jahr noch nicht vorstellen können.
4326000	4331000	Genau, ja, das war doch mal wieder ein echt spannendes Buch mit viel Diskussionsansatz.
4331000	4336000	Habt ihr, glaube ich, auch gemerkt, dass wir ein bisschen mehr ins Diskutieren gekommen sind als sonst oft bei den Büchern.
4336000	4338000	Sehr schön und super ausgesucht.
4342000	4346000	Genau, jetzt bleibt uns im Grunde nur noch, euch wieder aufzurufen,
4346000	4352000	dass vielleicht jetzt die kontroversen Diskussionen als Anlass zu nutzen, uns irgendwie einen Kommentarbeitrag,
4352000	4359000	eine Frage oder irgendwas zuzuschicken, die ihr mal hier im Podcast aufgegriffen haben möchtet zu diesem Buch.
4359000	4361000	Das würden wir am Anfang der nächsten Episode machen.
4361000	4364000	Also habt ihr jetzt so zwei Wochen Zeit oder so vielleicht was einzuschicken.
4364000	4368000	Macht das im Idealfall natürlich einfach als Audiophile.
4368000	4371000	Es reicht, wenn ihr die kurz mit dem Handy aufnehmt, irgendwo in der ruhigen Ecke.
4371000	4375000	Dann können wir das direkt in den Podcast einbinden, was natürlich ganz toll ist.
4375000	4380000	Aber ihr könnt es auch einfach als Kommentar unter dem Beitrag auf der Webseite machen.
4380000	4384000	Den erreicht ihr unter zwischenzweideckeln.de
4388000	4390000	Da findet ihr die aktuelle Episode.
4390000	4395000	Und da findet ihr auch Links zu allen Möglichkeiten, wie ihr zwischen zwei Deckeln abonnieren könnt.
4395000	4401000	Im Grunde auf der Plattform eurer Wahl, mittlerweile auch bei Spotify, zu hören.
4401000	4404000	Da findet ihr auch Links zu unseren Social Media Aktivitäten.
4404000	4409000	Wir sind auf Instagram und Twitter jeweils als atdeckeln zu finden.
4409000	4412000	Bei Facebook gibt es zwischen zwei Deckeln eine Fanseite.
4412000	4417000	Da freuen wir uns auch über Likes, Kommentare, Shares und was da alles so geht.
4417000	4422000	Und wenn ihr jetzt diese Episode und auch vielleicht unsere anderen Episoden toll fandet,
4422000	4429000	dann freuen wir uns natürlich auch riesig, wenn ihr auf den diversen Plattformen vielleicht eine Rezension hinterlasst oder zumindest ein paar Sterne.
4429000	4434000	Das ist auch ganz toll, macht uns Motivation, hier noch weiter zu machen.
4434000	4437000	Aber auch keine Angst, wir haben jetzt nicht vor aufzuhören.
4437000	4442000	Das motiviert uns nun noch mehr und gibt uns natürlich auch nochmal ein gutes Gefühl, dass wir hier irgendwie was tun,
4442000	4446000	was andere Leute gerne hören und auch vielleicht irgendwie unterhält oder informiert.
4447000	4450000	Genau, hast du noch irgendwelche Sachen, die du ansprechen wolltest, Christoph?
4450000	4455000	Ich glaube, damit hast du vollumfänglich abgedeckt, was wir so am Ende zu sagen haben.
4455000	4457000	Von daher bleibt mir nur danke zu sagen, dass sie zugehört habt.
4457000	4462000	Und ja, bis zur nächsten Folge quasi. Und tschüss!
4467000	4469000	Untertitel im Auftrag des ZDF, 2021
