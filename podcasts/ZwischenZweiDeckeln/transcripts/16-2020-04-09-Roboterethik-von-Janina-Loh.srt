1
00:00:00,000 --> 00:00:21,840
Hallo und herzlich willkommen zu Episode 15 von Zwischen zwei Deckeln, dem Sachbuchpodcast.

2
00:00:21,840 --> 00:00:27,000
Ich bin wie immer der Nils und mit mir ist heute wieder der Christoph dabei.

3
00:00:27,000 --> 00:00:28,000
Hallo zusammen.

4
00:00:28,000 --> 00:00:37,000
Ja, schön, dass ihr da seid, dass ihr wieder eingeschaltet habt, wenn ihr jetzt zuhause sitzt oder irgendwie auf dem Abenteuer einkaufen seid oder zu denen gehört, die arbeiten müssen da draußen.

5
00:00:37,000 --> 00:00:40,000
Danke an euch, dass ihr irgendwie den Laden am Laufen haltet.

6
00:00:40,000 --> 00:00:45,000
Genau, ja. Wie sieht es aus bei dir? Irgendetwas Relevantes, Neues?

7
00:00:45,000 --> 00:00:53,000
Nichts Spektakuläres auch. Ich bin überwiegend zuhause, kaufe einige Menschen mit ein und habe Zeit zum Lesen gefunden.

8
00:00:53,000 --> 00:00:55,000
Was liest du denn gerade?

9
00:00:55,000 --> 00:01:03,000
Also jetzt gerade tatsächlich wirklich für die Episode, die wir heute aufnehmen, habe ich das Buch von heute in den letzten anderthalb, zwei Wochen eben gelesen.

10
00:01:03,000 --> 00:01:15,000
Also ihr habt es ja schon im Titel gesehen, es geht um Roboterethik von Janina Lo und ansonsten habe ich nochmal was bei der BBB bestellt und habe ein, zwei Krimis mir jetzt bei Twitter empfehlen lassen, die ich angehen möchte.

11
00:01:15,000 --> 00:01:22,000
So ist mein Ziel. Aber sonst muss ich sagen, habe ich nicht viel mehr Zeit als vorher. Ich weiß nicht, wie es bei dir ist.

12
00:01:22,000 --> 00:01:27,000
Ja, bei mir auch nicht so wirklich, weil meine Arbeit sich dann doch irgendwie eins zu eins ins Homeoffice übertragen lässt.

13
00:01:27,000 --> 00:01:32,000
Und dadurch, dass die Pendelzeit wegfällt, sogar die klassische Lesezeit irgendwie ein bisschen kürzer wird.

14
00:01:32,000 --> 00:01:39,000
Aber ich komme so ein bisschen zum Lesen. Ich habe jetzt gerade fertig gelesen das Zettelkastenprinzip von Sönke Ahrens.

15
00:01:39,000 --> 00:01:51,000
Tatsächlich mal so ein bisschen der Versuch, diesen mysteriösen, ominösen, lumenschen Zettelkasten für so privates Notizenmanagement, Wissensmanagement aufzubereiten.

16
00:01:51,000 --> 00:01:58,000
Und das fand ich echt total spannend. Da wird es zunächst wahrscheinlich auch den ein oder anderen Blogbeitrag zu geben bei mir.

17
00:01:58,000 --> 00:02:03,000
Hast du Kommunikation mit Zettelkästen von Luman dazu gelesen?

18
00:02:03,000 --> 00:02:05,000
Das ist ein Buch, ein Essay?

19
00:02:05,000 --> 00:02:08,000
Nee, das ist ein Essay. Ich glaube, das sind nur so zehn Seiten oder so.

20
00:02:08,000 --> 00:02:10,000
Ja, ich glaube, habe ich zumindest mal angefangen zu lesen.

21
00:02:10,000 --> 00:02:20,000
Vielleicht, wenn wir dran denken, können wir es ja verlinken. Ich muss sagen, das ist relativ amüsant, weil Luman da eben klarmacht, wie egal es ihm ist, mit wem er eigentlich da irgendwie zu tun hat

22
00:02:20,000 --> 00:02:26,000
und mit wem er zusammenarbeitet, also seinen Zettelkasten quasi als Assistenten begreift.

23
00:02:26,000 --> 00:02:28,000
Und das ist ganz, ganz gut.

24
00:02:28,000 --> 00:02:32,000
Aber das kann tatsächlich funktionieren. Das ist das Krasse dabei.

25
00:02:34,000 --> 00:02:37,000
Genau, da können wir vielleicht auch eine Episode hier mal zumachen. Mal gucken.

26
00:02:37,000 --> 00:02:38,000
Vielleicht auch das.

27
00:02:38,000 --> 00:02:44,000
Ja, von euch da draußen ist irgendwie nichts an Fragen oder Kommentaren oder so zu der letzten Episode gekommen.

28
00:02:44,000 --> 00:02:47,000
Fühlt euch da nochmal eingeladen, wenn ihr irgendwas diskutieren wollt.

29
00:02:47,000 --> 00:02:55,000
Oder so uns einfach ein Audiophile zu schicken, einfach am Handy aufgenommen, vielleicht in der ruhigen Ecke oder irgendwie unter Blogbeitrag zu kommentieren.

30
00:02:55,000 --> 00:02:58,000
Dann können wir das hier im Zweifel aufgreifen.

31
00:02:58,000 --> 00:03:01,000
Also fühlt euch da nochmal herzlichst so eingeladen.

32
00:03:01,000 --> 00:03:05,000
Apropos letzte Episode. Christoph, erinnerst du dich noch, worum ging es?

33
00:03:05,000 --> 00:03:10,000
Ja, also du hast uns vorgestellt, Die Economist's Hour von Benjamin Applebaum.

34
00:03:11,000 --> 00:03:24,000
Und da ging es, wenn ich mich jetzt recht im Sinne darum, wie im Prinzip so liberale, marktliberale Ökonomen Einfluss auf die Politik in den USA genommen haben.

35
00:03:24,000 --> 00:03:35,000
Und ja, es zeigt sich, dass viel von dem, was so prognostiziert wurde, in dem Bereich gar nicht so eingetreten ist, wie es gesagt wurde.

36
00:03:35,000 --> 00:03:38,000
So würde ich es in zwei, drei Sätzen zusammenfassen.

37
00:03:38,000 --> 00:03:40,000
Genau, also wie es von den Ökonomen selbst vorher gesagt wurde.

38
00:03:40,000 --> 00:03:41,000
Ja, genau, das meine ich.

39
00:03:41,000 --> 00:03:46,000
Andere haben dann durchaus zumindest mal geahnt, was passieren könnte.

40
00:03:46,000 --> 00:03:50,000
Ok, ja, genau, darum ging es letzte Woche, letzten Monat.

41
00:03:50,000 --> 00:03:52,000
Diese Episode hast du ja schon gesagt, worum es geht.

42
00:03:52,000 --> 00:04:01,000
Wir werden ein bisschen technikphilosophisch, wenn ich das richtig sehe, mit dem Buch Roboterethik eine Einführung von Janina Loh.

43
00:04:01,000 --> 00:04:09,000
Die Autorin ist Philosophin und arbeitet an der Uni Wien im Bereich Technik und Medienphilosophie, sagst du.

44
00:04:09,000 --> 00:04:15,000
Und sie habilitiert zu posthumanistischen Elementen in Hannah Arendts Werk und Denken.

45
00:04:15,000 --> 00:04:24,000
Also ja, da merkt man schon ein bisschen, dass sowohl das klassischphilosophische als auch das technikphilosophische taucht ja schon in dem Titel auf.

46
00:04:24,000 --> 00:04:29,000
Möchtest du anfangen, uns das Buch in der Kurzfassung kurz vorzustellen?

47
00:04:32,000 --> 00:04:34,000
Ja, das mache ich gerne.

48
00:04:34,000 --> 00:04:45,000
Also Janina Loh untersucht in ihrer Studie Roboterethik verschiedene Ansätze der Einordnung der Fähigkeiten und Zugeständnisse von und an Roboter durch EthikerInnen und PhilosophInnen.

49
00:04:45,000 --> 00:04:52,000
Es zeigt sich, dass Begriffe wie Moral und Verantwortung traditionell dem Menschen vorbehalten sind und einzelnen Individuen zugeschrieben werden.

50
00:04:52,000 --> 00:04:59,000
Loh stellt Gegenpositionen dar und macht Vorschläge, wie und warum wir unsere tradierten Denkmuster durchbrechen können und vielleicht auch sollten.

51
00:05:02,000 --> 00:05:06,000
Okay, das klingt ja doch potentiell kontrovers.

52
00:05:06,000 --> 00:05:09,000
Möglich, ja, mal gucken, wie kontrovers es wirklich wird.

53
00:05:09,000 --> 00:05:14,000
Und wie danach so was wie haben Tiere auch Menschenrechte sozusagen.

54
00:05:14,000 --> 00:05:17,000
So ein bisschen haben Roboter auch Menschenrechte.

55
00:05:17,000 --> 00:05:21,000
Scheint wieder so ein bisschen das zu sein, was mir spontan einfällt.

56
00:05:21,000 --> 00:05:29,000
Tatsächlich gibt es da auch einige Referenzen und auch Vergleiche zu dem, was eben Robotern zugetraut wird und Tieren oder nicht Tieren.

57
00:05:29,000 --> 00:05:39,000
Genau, einige Fragestellen aus der Fragestellung aus der Roboterethik wurden eben offenbar in der Tierethik schon behandelt oder werden es immer noch oder man kann sie auch zusammen behandeln und so.

58
00:05:39,000 --> 00:05:47,000
Genau, so ein bisschen die Frage nach dem Status von Nichtmenschen im Prinzip, mit denen wir aber irgendwie zu tun haben.

59
00:05:47,000 --> 00:05:56,000
Im Prinzip ist das Buch doch, ja, ich glaube, ich weiß nicht, wie philosophische Studien sonst so aufgebaut sind, aber ziemlich stringent.

60
00:05:56,000 --> 00:06:02,000
Und dadurch, ja, manchmal so ein bisschen habe ich das Gefühl, wird einiges einfach abgearbeitet.

61
00:06:02,000 --> 00:06:11,000
Also es geht erstmal los mit einer Einleitung und dann stellt sich Loh die Frage, welche Bereiche der Robotik und der ethischen Fragen es überhaupt gibt.

62
00:06:11,000 --> 00:06:20,000
Und dann geht es einmal die Arbeitsfelder der Robotik durch und dann geht es darum, inwiefern Roboter moralische Handlungssubjekte oder Objekte sein können, was das bedeutet.

63
00:06:20,000 --> 00:06:22,000
Darüber sprechen wir noch.

64
00:06:22,000 --> 00:06:28,000
Und dann geht es im dritten Kapitel um Verantwortungzuschreibung in der Interaktion zwischen Menschen und Robotern.

65
00:06:28,000 --> 00:06:34,000
Und dann wird das wieder durchexerziert als Roboter als Verantwortungssubjekte und Roboter als Verantwortungsobjekte.

66
00:06:34,000 --> 00:06:43,000
Und dann gibt es am Ende noch ein bisschen neuen Seiten, einmal ein bisschen Breitseite gegen alles und jeden quasi.

67
00:06:43,000 --> 00:06:46,000
Also ein bisschen kritischer Ausblick kennt man ja auch aus vielen Büchern.

68
00:06:46,000 --> 00:06:52,000
Aber ja, so ist das Buch erstmal aufgebaut und ich glaube, ich fange einfach mal an.

69
00:06:52,000 --> 00:07:06,000
Loh steigt damit ein, dass sie erstmal sagt, dass wenn wir über Ethik und Roboter oder Technik sprechen, man immer davon ausgehen muss, dass Technik von Menschen erschaffen ist und dementsprechend von Normen geprägt ist.

70
00:07:06,000 --> 00:07:13,000
Also egal, was wir tun, ist irgendwie zumindest implizit mit gesellschaftlichen Ideen, Normen und so weiter verknüpft.

71
00:07:13,000 --> 00:07:28,000
Das heißt, so die Idee einer völlig neutralen Technik, die ja so wie objektiv vor uns liegt und quasi gar keinen, in sich keine ethischen Implikationen oder so hat, das verwirft sie direkt zum Anfang.

72
00:07:28,000 --> 00:07:37,000
Dabei sagt sie, dass Roboterethik eben ein Teilbereich der Maschinenethik ist, was bedeutet, dass alle Roboter Maschinen sind, aber nicht alle Maschinen sind Roboter.

73
00:07:37,000 --> 00:07:38,000
Ja, macht Sinn.

74
00:07:38,000 --> 00:07:44,000
Und auch dann weist sie direkt darauf hin, dass die Fragestellungen durchaus der der Tierethik ähneln.

75
00:07:44,000 --> 00:07:48,000
Also was kann ein Tier, was kann ein Roboter, was fühlt es, was fühlt er.

76
00:07:48,000 --> 00:07:55,000
Ja, weil ich gerade gesagt habe, dass Roboter keine Maschinen sind, ist vielleicht einmal wichtig zu definieren, was Roboter überhaupt sind.

77
00:07:55,000 --> 00:07:59,000
Und damit starten wir in die Definition von Maschinen.

78
00:07:59,000 --> 00:08:08,000
Das sind nämlich künstliche Gebilde, die aus einem Antriebssystem durch Motor, Wind oder Wasser bewegten Teilen besteht und die Energie umsetzen.

79
00:08:08,000 --> 00:08:10,000
Das ist eine Maschine, so ganz allgemein.

80
00:08:10,000 --> 00:08:13,000
Roboter hingegen sind spezielle Maschinen.

81
00:08:13,000 --> 00:08:17,000
Das Wort geht auf Roboter aus dem Tschechischen zurück.

82
00:08:17,000 --> 00:08:19,000
Das kommt auch aus irgendeinem Science Fiction Roman.

83
00:08:19,000 --> 00:08:21,000
Das ist ja so dein Spezialgebiet.

84
00:08:21,000 --> 00:08:26,000
Ja, ich könnte Stanislav Lem sein, aber ich bin mir da nicht ganz sicher.

85
00:08:26,000 --> 00:08:28,000
Ja, ich glaube, es war nicht Stanislav Lem.

86
00:08:28,000 --> 00:08:32,000
Dann hätte ich nämlich, glaube ich, dazu geschrieben, weil den, den kenne ich quasi.

87
00:08:32,000 --> 00:08:39,000
Naja, und das Wort geht, steht offen, also Roboter im Tschechischen steht offenbar für Arbeit, frohendienst oder Zwangsarbeit.

88
00:08:39,000 --> 00:08:44,000
Das fand ich ganz interessant, gerade mit Blick auf eben ethische Implikation.

89
00:08:44,000 --> 00:08:47,000
Und ja, also jetzt die Definition von Roboter.

90
00:08:47,000 --> 00:09:02,000
Das ist eine elektromechanische Maschine, die A. über einen eigenständigen Körper und B. über mindestens einen Prozessor verfügt, C. Sensoren hat, D. über Effektoren oder Aktoren verfügt, die Signale in mechanische Abläufe übersetzen.

91
00:09:02,000 --> 00:09:08,000
Roboter erscheinen zumindest autonom und können physisch Einfluss auf ihre Umwelt nehmen.

92
00:09:08,000 --> 00:09:15,000
Genau, also relativ komplex irgendwie, aber man kann sich merken, gut, die haben einen eigenständigen Körper.

93
00:09:15,000 --> 00:09:23,000
Sie haben irgendwas, womit sie irgendwie Dinge prozessieren können, also einen Prozessor, also das, was bei Menschen das Gehirn vielleicht ist.

94
00:09:23,000 --> 00:09:26,000
Und sie haben irgendwie Sensoren, um die Umwelt wahrzunehmen.

95
00:09:26,000 --> 00:09:37,000
Und sie haben irgendwas, wie sie um die Umwelt eingreifen können, also irgendwelche Roboterarme oder sie haben einen kleinen Staubsauger, wie diese Staubsaugerroboter und so weiter.

96
00:09:37,000 --> 00:09:45,000
Aber danach macht Loh die These auf, dass es heute eigentlich keinen Bereich im menschlichen Alltag mehr gibt, in dem Roboter noch keinen Einzug gehalten hätten.

97
00:09:45,000 --> 00:09:48,000
Kann man erst mal so auf sich wirken lassen und sich dann überlegen, ob das stimmt.

98
00:09:48,000 --> 00:09:53,000
Ich habe erst gedacht, das klingt ja irgendwie relativ steil als These, aber ich glaube, sie hat schon recht.

99
00:09:53,000 --> 00:10:02,000
Egal, wo wir hingucken, wir wissen, dass es zumindest Roboter gibt, auch wenn wir sie nicht tagtäglich sehen und mit ihnen zu tun haben sollten.

100
00:10:03,000 --> 00:10:14,000
Ja, dann geht es im ersten Kapitel darum zu klären, worum es in diesem Buch eigentlich geht, also welche Bereiche aus der Robotik zur Sprache kommen oder in der Roboterethik eine Rolle spielen.

101
00:10:14,000 --> 00:10:25,000
Das ist einmal die Industrierobotik, also die Frage nach Ersetzung von Arbeitsplätzen hat man sicherlich schon gehört und daran anschließend, wir hatten es ja gerade, sie habilitiert zu Hannah Arendt auch,

102
00:10:25,000 --> 00:10:33,000
die Frage danach, was der Mensch ist, wenn er nicht mehr arbeitet, wenn er nichts mehr schafft oder nicht mehr Lohn arbeitet oder wie man es nennen möchte.

103
00:10:33,000 --> 00:10:40,000
Also das sind Fragen, die daran anschließen an das Problem von, wenn Arbeitsplätze ersetzt werden, was resultiert eigentlich daraus.

104
00:10:40,000 --> 00:10:47,000
Dann, was natürlich glaube ich auch ein verbreitetes Beispiel ist, das auch in dem Buch immer wieder vorkommt, ist die Frage nach dem autonomen Fahren.

105
00:10:48,000 --> 00:11:02,000
Also wenn wir jetzt Automobile haben, die irgendwie intelligente Fahrsysteme haben, ist die Frage, wie diese Autos in Unfallsituationen reagieren sollen, wer bei Unfällen dann die Verantwortung trägt,

106
00:11:02,000 --> 00:11:13,000
also kann das Auto selbst irgendwie verantwortlich gemacht werden, ist es der Konzern, der das Auto gebaut hat, ist es der Fahrer, der eigentlich noch hinterm Lenkrad sitzt, aber nicht mehr eingreifen muss und so weiter.

107
00:11:13,000 --> 00:11:17,000
Dann kriegt das Auto drei Monate Fahrverbot, stelle ich mir auch lustig vor.

108
00:11:17,000 --> 00:11:26,000
Ja, das ist irgendwie ganz interessant. Da geht später auch noch drum die Unterscheidung zwischen Verantwortung und Haftbarkeit, die so ein bisschen getrennt wird.

109
00:11:26,000 --> 00:11:27,000
Macht Sinn.

110
00:11:27,000 --> 00:11:38,000
Und sie weiß, glaube ich, drei oder viermal in dem Buch darauf hin, dass in der EU oder vom Europäischen Parlament gerade die Idee von elektronischen Personen erarbeitet wird,

111
00:11:38,000 --> 00:11:51,000
also quasi analog zu juristischen Personen, die wir aus dem Recht schon kennen, wo dann irgendwelche Unternehmen vor Gericht wie Personen behandelt werden können, könnte man das eben auch für Maschinen oder Roboter vielleicht einsetzen.

112
00:11:51,000 --> 00:11:58,000
Was aus diesem Projekt der Erarbeitung da genauer resultiert ist, das schreibt sie aber nicht, nur dass das eben im Prozess ist.

113
00:11:58,000 --> 00:12:06,000
In der Medizin oder Therapie- und Pflege-Robotik stellt sich, wenn man da Roboter einsetzt, und das ist ja mittlerweile auch schon so,

114
00:12:06,000 --> 00:12:15,000
ich persönlich habe das auch schon erlebt, also ich habe mal in einem alten Zentrum ein Praktikum gemacht, in dem diese Roboter-Robbe Paro zum Einsatz kam.

115
00:12:15,000 --> 00:12:16,000
Ich weiß nicht, ob du die kennst.

116
00:12:16,000 --> 00:12:17,000
Ja, sag mir was.

117
00:12:17,000 --> 00:12:33,000
Also für die, die es nicht kennen, das ist quasi sowas wie ein großes Kuscheltier in Form einer Robbe, die es weiß und die hat verschiedenste Sensoren eingebracht und reagiert auf Berührung und Ansprache und so weiter ein Stück weit.

118
00:12:33,000 --> 00:12:36,000
Und es ist auch ganz schön teuer, die kostet, glaube ich, um die 5000 Euro oder mehr.

119
00:12:36,000 --> 00:12:41,000
Gibt es ganz viele Videos zu, wenn einem das interessiert.

120
00:12:41,000 --> 00:12:51,000
Und im Zuge dieser Pflege-Robotik, und man kann sich da ja auch Sachen vorstellen, wie Roboter, die Menschen aus Betten heben oder auf die Toilette begleiten und so weiter,

121
00:12:51,000 --> 00:12:56,000
stellt sich eben die Frage nach der Autonomie der Personen, die dort behandelt werden,

122
00:12:56,000 --> 00:13:05,000
und inwiefern die Emotionen, die da vielleicht auch aufkommen gegenüber einer Roboter-Robbe oder einem Roboter als Gesprächspartner, kann man sich ja alles Mögliche vorstellen,

123
00:13:05,000 --> 00:13:12,000
inwiefern diese Emotionen in gewisser Maße echt sind, inwiefern die Personen, die da beteiligt sind, getäuscht werden, sowas alles.

124
00:13:12,000 --> 00:13:17,000
Ich nehme es gleich vorweg, Janina Loh sieht das alles nicht so problematisch.

125
00:13:17,000 --> 00:13:18,000
Okay.

126
00:13:18,000 --> 00:13:29,000
Sie sagt, dass eben Beziehungen zu Objekten oder Robotern durchaus echte Emotionen sein können und dass das keine Täuschung in dem Sinne darstellen muss.

127
00:13:29,000 --> 00:13:30,000
Okay.

128
00:13:30,000 --> 00:13:37,000
Vielleicht hat man das jetzt auch schon gehört, Sexroboter sind ja auch ein Ding, also die gibt es schon, kann man schon kaufen, kosten, glaube ich, immer noch relativ viel Geld.

129
00:13:37,000 --> 00:13:43,000
Und was da in dem Buch augenfällig wird, da geht es dann häufig in dem Kontext um feministische Fragestellung.

130
00:13:43,000 --> 00:13:44,000
Ja.

131
00:13:44,000 --> 00:13:53,000
Also es gibt da irgendwie teilweise die Möglichkeit, eben Roboter zu vergewaltigen und dass das irgendwie im Kern problematisch sein kann.

132
00:13:53,000 --> 00:14:00,000
Auf der anderen Seite vielleicht aber auch irgendwie befreiend, weil Leute, die so etwas haben, das dann ausleben können, ohne eben Menschen zu schädigen.

133
00:14:00,000 --> 00:14:01,000
Ja.

134
00:14:01,000 --> 00:14:05,000
Ja, da gibt es viele ethische Ansätze und viele Diskussionen darum.

135
00:14:05,000 --> 00:14:14,000
Und darum geht dann natürlich auch um Objektifizierung von, gerade von Frauen, weil diese Sexroboter eben sich häufig an heterosexuelle Männer wenden.

136
00:14:14,000 --> 00:14:20,000
Ja, also auch ein Bereich, in dem der Feminismus eine große Rolle spielt und auch spielen sollte.

137
00:14:20,000 --> 00:14:29,000
Der letzte Bereich, den Lo aufzählt und das liegt, glaube ich, auf der Hand, dass das irgendwie ethisch relevant ist, ist eben die Frage nach Militärrobotern.

138
00:14:29,000 --> 00:14:30,000
Ja.

139
00:14:30,000 --> 00:14:46,000
Und wie wird auch unbefugten der Zugang zu Entscheidungen verwehrt und wie viel Autonomie dürfen artificielle Systeme eigentlich haben, wenn sie eben konkret über Leben und Tod entscheiden?

140
00:14:46,000 --> 00:14:50,000
Da ist dann auch wieder die Schnittstelle zu dem autonomen Auto, das Entscheidende, auf wen es rettet.

141
00:14:50,000 --> 00:14:52,000
Ja, absolut.

142
00:14:53,000 --> 00:14:59,000
Ja, in der Roboterethik gibt es dann zwei klassische Arbeitsfelder, mit denen man sich beschäftigen kann.

143
00:14:59,000 --> 00:15:07,000
Also einmal Roboter als moralische Akteurinnen, also inwiefern sie selbst irgendwie moralfähig sind und moralisch handeln.

144
00:15:07,000 --> 00:15:10,000
Ein bisschen, das wir gerade eben schon besprochen haben.

145
00:15:10,000 --> 00:15:18,000
Und die andere Frage ist danach, inwiefern Roboter als Objekte moralischen Handelns irgendwie relevant werden, wie zum Beispiel bei den Sexrobotern.

146
00:15:18,000 --> 00:15:19,000
Ja.

147
00:15:19,000 --> 00:15:30,000
Es ist auffällig, dass die Deutungshoheit über das ganze erste Arbeitsfeld, also inwiefern Roboter moralische Subjekte sein können, ziemlich anthropozentrisch sind.

148
00:15:30,000 --> 00:15:34,000
Anthropozentrisch meint damit, der Mensch steht im Mittelpunkt und es wird von Menschen ausgedacht.

149
00:15:34,000 --> 00:15:44,000
Also unser Bild davon, was irgendwie moral ist und sein kann, ist immer davon geleitet, was wir von Menschen können, wie wir glauben, dass Menschen sind.

150
00:15:44,000 --> 00:15:54,000
Und das bedeutet am Ende auch, dass durchweg Menschen entscheiden, ob Roboter moralisch handeln können, ob sie sowas wie Willensfreiheit besitzen können und so weiter.

151
00:15:54,000 --> 00:15:55,000
Ja.

152
00:15:55,000 --> 00:15:57,000
Leuchtet, glaube ich, auch ein.

153
00:15:57,000 --> 00:16:04,000
Ich glaube, was Lo damit aufzumachen, probiert es erstmal, einfach einen Reflektionspunkt, einfach mal darüber nachzudenken.

154
00:16:04,000 --> 00:16:18,000
Okay, wir denken immer von uns aus, was vermutlich auch gar nicht anders möglich ist, aber wir nehmen uns damit eben auch die Freiheit und Hoheit darüber raus, zu entscheiden, wer eigentlich Teil von dem ist, was wir uns eigentlich nur zubelegen.

155
00:16:18,000 --> 00:16:19,000
Genau, ja klar.

156
00:16:19,000 --> 00:16:23,000
Sie sagt dabei, dass wir eigentlich gar nicht wissen, wie es überhaupt ist, frei zu sein.

157
00:16:23,000 --> 00:16:24,000
Also Zitat jetzt.

158
00:16:24,000 --> 00:16:38,000
Ja, zum zweiten so ein bisschen in die Richtung.

159
00:16:38,000 --> 00:16:45,000
Roboter als Moralobjekte, da was ich gerade eben schon hatte, waren eben die Beispiele aus der Sexrobotik.

160
00:16:45,000 --> 00:16:52,000
Sexroboter reproduzieren eben heteronormative, patriarchale und diskriminierende Strukturen.

161
00:16:52,000 --> 00:16:54,000
Das ist durchaus ein Problem.

162
00:16:54,000 --> 00:17:04,000
Würde man sie anders konzipieren, könnte man aber durchaus auch sich andere Sexroboter vorstellen, die vielleicht auch andere Möglichkeiten bieten.

163
00:17:04,000 --> 00:17:21,000
Ich könnte mir vorstellen, dass gerade wenn es um Sexualtherapie geht und so die Begleitung von Menschen, die vielleicht zuerst Sexualpraktiken körperlich nicht eigenständig in der Lage sind, dass da eigentlich ein großes Feld der Emanzipation auch offen steht.

164
00:17:21,000 --> 00:17:24,000
Also ein bisschen die Schnittstelle wieder zu der Therapie.

165
00:17:24,000 --> 00:17:25,000
Ja, genau.

166
00:17:25,000 --> 00:17:30,000
Dabei stellt sich immer wieder die Frage, wie Moral überhaupt in eine Maschine gelangen kann.

167
00:17:30,000 --> 00:17:36,000
Also wenn Sie irgendwie Moral entwickeln können sollten, wie funktioniert das?

168
00:17:36,000 --> 00:17:41,000
Und da gibt es eben verschiedene Ansätze, entweder Top-Down, Bottom-Up oder Hybrid-Ansätze.

169
00:17:41,000 --> 00:17:51,000
Also entweder man programmiert halt sowas wie moralische Regeln ein oder man setzt darauf, dass artificielle Systeme selbst lernen können.

170
00:17:51,000 --> 00:17:55,000
Also sowas, was unter KI verstanden wird oder man kombiniert ein bisschen.

171
00:17:55,000 --> 00:18:01,000
Also einerseits gibt man irgendwelche Regeln vor, andererseits hat man irgendwie selbst der andere Ansätze.

172
00:18:01,000 --> 00:18:13,000
Was auffällig ist, ist, dass jeder Ansatz irgendwie Top-Down, das zu bewältigen, total schwierig ist, weil Moral eben abstrakt ist und die Entscheidungen, die aber gefällt werden müssen, sehr konkret sind.

173
00:18:14,000 --> 00:18:22,000
Die Rettung von Menschenleben ist irgendwie sehr abstrakt, aber die Entscheidung, in welche Richtung ein Auto ausweicht, ist eben sehr konkret.

174
00:18:22,000 --> 00:18:29,000
Das heißt, man kann sich nicht einfach darauf verlassen zu sagen, rette möglichst viele Menschen zu einem Auto und das wird schon irgendwie funktionieren.

175
00:18:29,000 --> 00:18:41,000
Wobei ich das ganz spannend finde, weil wir ja gerade auch jetzt in der aktuellen Situation mit der drohenden Überlastung oder in manchen anderen Ländern auch schon der Überlastung von Intensivstationen und Ähnlichem,

176
00:18:41,000 --> 00:18:46,000
wo ja im Grunde genau diese moralischen Entscheidungen getroffen werden, wen können wir jetzt behandeln und wen nicht.

177
00:18:46,000 --> 00:19:00,000
Und da eben danach gerufen wird, wir brauchen eine klare Triage-Regeln sozusagen, nach welchen Kriterien entscheiden wir, dass das eben nicht das Individuum machen muss, der einzelne Arzt oder die einzelne Ärztin oder der einzelne Pfleger,

178
00:19:00,000 --> 00:19:09,000
die halt sagen, der kriegt jetzt die Beatmung und der nicht, sondern dass wir da im Grunde nach genau diesen abstrakten Regeln verlangen, die du jetzt sagst, die eigentlich schwer zu machen sind.

179
00:19:09,000 --> 00:19:18,000
Das finde ich auch ziemlich interessant, weil mein Gefühl auch immer wäre, man kann probieren, das durchzureglementieren, wie so eine Triage stattfinden soll und wie die organisiert werden möchte.

180
00:19:18,000 --> 00:19:23,000
Aber ich habe das Gefühl, das würde sich im Konkreten sowieso ergeben.

181
00:19:23,000 --> 00:19:31,000
Ich habe das Gefühl, die eigentliche Funktion dahinter ist, eben Ärztinnen zu entlasten von der von der konkreten Entscheidung.

182
00:19:31,000 --> 00:19:44,000
Aber ich glaube, ich vermute, dass die Entscheidung, die Ärztinnen treffen würden, ohne Triage-Regelung, ziemlich genau das widerspiegeln würden, was sie auch mit Regelungen jetzt entscheiden können, sollen oder dürfen.

183
00:19:44,000 --> 00:20:05,000
Da ist ja gerade diese Diskussion, die auch gerade so aus der Rassismusdiskussion sozusagen ein bisschen kommt, ob man nicht mit objektiveren Triageregeln solche im Zweifel auch erst mal unbewussten Verzerrungen und wie eben rassistische Vorurteile und Ähnliches nicht auch ein bisschen ausgleichen kann.

184
00:20:05,000 --> 00:20:14,000
Wenn man eben sagt, ich überlasse das nicht dem Einzelnen, der eventuell unbewusst irgendwas internalisiert hat, sondern ich sage halt, ne, stopp hier, das Alter muss so sein.

185
00:20:14,000 --> 00:20:17,000
Und ja, wenn der jetzt zwei Jahre jünger ist, dann kriege ich das halt trotzdem.

186
00:20:17,000 --> 00:20:20,000
Aber wenn du vielleicht gesagt hast, und so weiter und so fort.

187
00:20:20,000 --> 00:20:30,000
Ich glaube, da kommt diese Objektivierung ins Spiel, die da vielleicht auch wirklich was nochmal verändern kann gegenüber Einzelnen.

188
00:20:30,000 --> 00:20:35,000
Ja, stimmt, das ist nochmal ein ganz gelungener Gedankenanstoß da.

189
00:20:35,000 --> 00:20:40,000
Und ich finde auch einfach die moralische Entlastung quasi von Ernst, den ist ja auch ein Eigenwert.

190
00:20:40,000 --> 00:20:42,000
Das ist ein totaler Wert natürlich, gar keine Frage.

191
00:20:42,000 --> 00:20:54,000
Inwiefern Roboter jetzt wirklich sowas wie, ja, also richtig Subjekte im moralischen Sinne sein können, hängt natürlich auch ganz stark davon ab, was moralische Akteurinnen überhaupt benötigen.

192
00:20:54,000 --> 00:21:04,000
Und da jetzt einfach mal so, um darzustellen, wie dieses Buch geschrieben ist, nimmt Janina Losig eben immer verschiedene Autorinnen raus, sagt XY hat das und das geschrieben.

193
00:21:04,000 --> 00:21:09,000
Und dann wird ein anderer Text quasi behandelt und da wird das und das gesagt.

194
00:21:09,000 --> 00:21:15,000
Und das stellt sie eben da, was es manchmal ein bisschen schwierig macht, dieses Buch jetzt eben so ganz doll zusammenzufassen.

195
00:21:15,000 --> 00:21:21,000
Im Prinzip müsste ich sagen, okay, es gibt sechs Studien, die untersucht wurden und die kommen jeweils zu dem Schluss.

196
00:21:21,000 --> 00:21:23,000
Deswegen probiere ich das jetzt hier mal zusammenzufassen.

197
00:21:23,000 --> 00:21:28,000
Manche Studienergebnisse lasse ich dann aber auch quasi hinten runterfallen und stelle sie nicht da.

198
00:21:28,000 --> 00:21:30,000
Also manche Ethikerinnen kommen dann hier nicht vor.

199
00:21:30,000 --> 00:21:46,000
Und am Ende eines jeden Kapitels bezieht Janina Loh das, was sie quasi gelesen hat, auf verschiedene Robotersysteme und probiert einzuordnen, inwiefern jetzt eben Subjekt, Objektverantwortungs möglich wären im Sinne der gelesenen Autorinnen.

200
00:21:46,000 --> 00:21:54,000
Auch das lasse ich hinten noch rüber, weil ich glaube, wenn ich jetzt sechs oder sieben Roboter immer wieder aufzähle und sage, möglicherweise können die dies, das oder das können sie auch nicht.

201
00:21:54,000 --> 00:21:57,000
Je nachdem, wem man fragt, das geht ein bisschen zu weit.

202
00:21:57,000 --> 00:22:02,000
Wenn euch das im Konkreten interessiert, müsst ihr das Buch dann tatsächlich vielleicht einfach kaufen und lesen.

203
00:22:02,000 --> 00:22:10,000
Naja, auffällig ist, dass bei Moral sich auch die Philosophinnen nicht so richtig einig sind, was denn eigentlich Moral ist und was Gut ist.

204
00:22:10,000 --> 00:22:16,000
Also es gibt einerseits so Ideen von Gut ist, wer Gutes tut, so im Sinne von Forrest Gump quasi.

205
00:22:16,000 --> 00:22:23,000
Also man guckt einfach nur darauf, was irgendwie der Output von einer Handlung ist.

206
00:22:23,000 --> 00:22:33,000
Und wenn der gut ist, dann sind auch Roboter befähigt, quasi moralisch zu agieren oder auch wenn es schlecht ist.

207
00:22:33,000 --> 00:22:43,000
Also man guckt einfach nur, was der Output einer Aktion ist und auf der anderen Seite gibt es natürlich irgendwie hochkomplexe Anforderungsprofile,

208
00:22:43,000 --> 00:22:52,000
wo auch ein Eigenverständnis von Moral durch die handelnde Person, den handelnden Roboter oder das handelnde Tier irgendwie vorliegen muss.

209
00:22:52,000 --> 00:23:01,000
Die Frage, die im Kern so ein bisschen da ist, ist, wann sind Roboter eigentlich wirklich moralfähig und wann nutzen sie nur Funktionsequivalente zu Moral?

210
00:23:01,000 --> 00:23:06,000
Und tun dann quasi nur so, als wären sie moralfähig?

211
00:23:06,000 --> 00:23:10,000
Und da hängt dann natürlich die große Frage nach dem Bewusstsein im Kern da dran.

212
00:23:10,000 --> 00:23:15,000
Das erinnert mich an dieses Gedankenexperiment von dem Chinese Room, das chinesische Zimmer.

213
00:23:15,000 --> 00:23:16,000
Ich weiß nicht, ob du das kennst.

214
00:23:16,000 --> 00:23:17,000
Ah, erzähl mal.

215
00:23:17,000 --> 00:23:21,000
Ja, da geht es da um Sprachverstehen im Grunde, da kommt das ursprünglich her.

216
00:23:21,000 --> 00:23:30,000
Und das nimmt halt an, ich weiß gar nicht mehr von wem es kommt, dass du ein Zimmer hast, wo sämtliche Regeln der chinesischen Sprache drin stehen, in Büchern.

217
00:23:30,000 --> 00:23:40,000
Und ganz ausgefeilte Regeln, wenn dieses Zeichen kommt, dann antwortest du mit diesem Zeichen und wenn diese Zeichen kommen, dann antwortest du mit dem Zeichen und so weiter und so fort.

218
00:23:40,000 --> 00:23:50,000
Und jetzt ist halt eine Person in diesem Zimmer, die bekommt per Zettel oder was, eine Reihe chinesischer Schriftzeichen reingereicht, befolgt die Regeln und gibt eine Antwort.

219
00:23:50,000 --> 00:23:59,000
So, und diese Antwort ist natürlich sinnvoll, ohne dass diese Person verstanden hätte, was sie da tut inhaltlich, also dass die Sprache sozusagen verstanden hätte.

220
00:23:59,000 --> 00:24:05,000
Und jetzt gibt es die Frage, dieses Zimmer, ist das Verstehen, was da passiert?

221
00:24:05,000 --> 00:24:08,000
Wenn es von außen so aussieht, als wäre es Verstehen?

222
00:24:08,000 --> 00:24:09,000
Ja.

223
00:24:09,000 --> 00:24:12,000
Das schließt dann wahrscheinlich an den Turing-Test im Grunde an.

224
00:24:12,000 --> 00:24:14,000
Das klingt so, als wäre das das.

225
00:24:14,000 --> 00:24:15,000
Ja.

226
00:24:15,000 --> 00:24:26,000
Ja, in einem Interview weiß, das verlinke ich ja auch nachher, bei Sein und Streit war sie bei Deutschland von Kultur, da weiß Janina Loh natürlich auch darauf hin, dass diese ganze Frage nach dem Bewusstsein total schwierig ist.

227
00:24:26,000 --> 00:24:38,000
Und halt sowas wie eine, sie nennt es, glaube ich, philosophisches Zusatzargument, also einfach die absolute Annahme, dass Menschen Bewusstsein haben, ist empirisch nicht beweisbar.

228
00:24:38,000 --> 00:24:50,000
Man kann quasi nicht den Kopf aufschneiden, ins Gehirn gucken und sagen, da ist es, das Bewusstsein, sondern auch da sind ja nur irgendwie Verkettungen von Nerven und Datenströmen im Prinzip irgendwie.

229
00:24:50,000 --> 00:25:05,000
Und das heißt, ja, es gibt einfach Dinge, die kann man nicht so richtig nachvollziehen und das wird man also vermutlich absehbar zumindest auch nicht beantworten können, inwiefern irgendwas Bewusstsein erlangen kann, außer dass wir es Menschen halt auf jeden Fall zubelegen.

230
00:25:05,000 --> 00:25:06,000
Ja.

231
00:25:06,000 --> 00:25:19,000
Was auffällig ist, wenn es darum geht, ob Dinge, Tiere, Roboter, wie auch immer moralische Akteure sein können, ist, dass Autonomie irgendwie im Zentrum von moralischer Akteurschaft steht.

232
00:25:19,000 --> 00:25:33,000
Das heißt, diese Top-Down-Ansätze, wenn einfach nur einprogrammiert ist, tu dies, wenn das, realisieren diese Moralfähigkeit nicht, weil eben komplett determiniert ist, wie sich der Roboter verhält.

233
00:25:34,000 --> 00:25:44,000
Okay, aber jetzt trifft sie ja im Grunde eine Entscheidung. Sie hat ja gesagt, ist gut, wer Gutes tut oder ist gut, wer weiß, dass er Gutes tut sozusagen.

234
00:25:44,000 --> 00:25:45,000
Ja.

235
00:25:45,000 --> 00:25:47,000
Und hier sagt sie jetzt ja, nee, es ist gut, wer weiß, dass er Gutes tut.

236
00:25:47,000 --> 00:25:56,000
Ja, genau, also diesem, quasi diesem, sie nennt das moralischem Schwellenwert, diesen Ansatz, den ich da vorgestellt habe, dem steht sie durchaus kritisch gegenüber.

237
00:25:56,000 --> 00:25:57,000
Okay.

238
00:25:57,000 --> 00:25:58,000
Ja, das merkt man auch. Genau.

239
00:25:59,000 --> 00:26:06,000
Ja, und Autonomie und so weiter referenzieren dann immer weiter eigentlich auf sowas wie Intentionalität.

240
00:26:06,000 --> 00:26:12,000
Also, wenn Roboter wirklich moralfähig sein sollten, müssten sie sowieso etwas wie Gründe für ihr Handeln anführen können.

241
00:26:12,000 --> 00:26:13,000
Ja.

242
00:26:13,000 --> 00:26:21,000
Man muss irgendwie sagen können, warum man getan hat, was man getan hat. Man kann nicht einfach nur sagen, ich hab das getan, weil das wurde mir halt so gesagt oder beigebracht.

243
00:26:21,000 --> 00:26:22,000
Mhm.

244
00:26:22,000 --> 00:26:31,000
Ja, dann geht es darum, inwiefern Roboter moralische Handlungsobjekte sein können und da gibt es so etwas wie eine Standardposition.

245
00:26:31,000 --> 00:26:45,000
Also Handlungsobjekt heißt, in dem Sinne moralisches Handlungsobjekt, müssen wir uns eigentlich, also unabhängig davon, ob Roboter selbst moralisch als Subjekt handeln können, müssen wir uns denen eigentlich gegenüber vernünftig verhalten auf eine Art.

246
00:26:45,000 --> 00:26:46,000
Ja.

247
00:26:46,000 --> 00:26:58,000
Und die philosophische Standardposition, so wie sie das nennt, ist, dass eigentlich nur Dinge einen entsprechenden Status zugebilligt bekommen, die selbst auch Subjekte sind.

248
00:26:58,000 --> 00:26:59,000
Okay.

249
00:26:59,000 --> 00:27:00,000
Ja.

250
00:27:00,000 --> 00:27:02,000
Das finde ich krass eigentlich.

251
00:27:02,000 --> 00:27:06,000
Ja, wer kein Moralsubjekt ist, wird auch kein Moralobjekt.

252
00:27:06,000 --> 00:27:13,000
Also, wenn man das weiterdenkt, bedeutet das, man kann mit Robotern verfahren, wie man will, das ist moralisch nicht problematisch.

253
00:27:13,000 --> 00:27:19,000
Ja, zumindest nicht, zumindest wenn man bei der Frage nach dem Moralsubjekt zu der Antwort kommt, nein, ist es nicht.

254
00:27:19,000 --> 00:27:22,000
Genau, dann kann man eigentlich mit denen machen, was man möchte.

255
00:27:22,000 --> 00:27:26,000
Das ist eine krasse, gerade weil du sagst, dass das die Standardposition ist.

256
00:27:26,000 --> 00:27:30,000
Ich würde jetzt dazu kommen, meine erste Intuition wäre, dass die total umstritten sein muss.

257
00:27:30,000 --> 00:27:31,000
Ja.

258
00:27:31,000 --> 00:27:34,000
Weil man das ja gerade genau auf die Tiere wieder wunderbar übertragen kann.

259
00:27:34,000 --> 00:27:42,000
Das hat mich da auch ein bisschen gewundert, weil ich das Gefühl hatte, das kam nicht so richtig vor quasi als Vergleich.

260
00:27:42,000 --> 00:27:48,000
Weil ich auch dachte, naja, dann könnten wir ja auch, also wenn das die Idee ist, dann könnte man ja auch mit Tieren tun und lassen, was man möchte.

261
00:27:48,000 --> 00:27:51,000
Und das scheint mir keine Standardposition zu sein.

262
00:27:51,000 --> 00:27:52,000
Genau.

263
00:27:52,000 --> 00:27:58,000
Ich meine, das steht ja sogar in unserem Grundgesetz drin quasi, dass das eben nicht in Ordnung ist.

264
00:27:58,000 --> 00:27:59,000
Ja.

265
00:27:59,000 --> 00:28:03,000
Und das ist ja auch irgendwie philosophisch abgeleitet.

266
00:28:03,000 --> 00:28:05,000
Das hat mich auch ein bisschen irritiert.

267
00:28:05,000 --> 00:28:09,000
Aber gut, sie sagt das erstmal so, dass das eben so die allgemeine Position ist.

268
00:28:09,000 --> 00:28:14,000
Ich weiß nicht, ob das jetzt vielleicht auch langsam ins Rutschen kommt.

269
00:28:14,000 --> 00:28:22,000
Ich meine, wenn man an irgendwelche Roboter in der industriellen Fertigung oder so denkt, dann weiß ich nicht.

270
00:28:22,000 --> 00:28:30,000
Da entspinnt sich bei mir auch nicht sofort das Bild der Notwendigkeit des quasi verantwortungsvollen Umgangs.

271
00:28:30,000 --> 00:28:33,000
Da würde ich auch sagen, naja, wenn du kaputt bist, kommst du auf den Müll.

272
00:28:33,000 --> 00:28:37,000
Und wenn du mich nervst, dann schreite ich auch an, ist mir egal.

273
00:28:37,000 --> 00:28:38,000
So.

274
00:28:38,000 --> 00:28:39,000
Interessant.

275
00:28:39,000 --> 00:28:43,000
Das wird dann wieder interessant, wenn man jetzt die Sexroboter oder so ins Spiel bringt zum Beispiel.

276
00:28:43,000 --> 00:28:44,000
Ja, total.

277
00:28:44,000 --> 00:28:49,000
Wo dann auf einmal eine andere, die Roboter sind ja immer noch die gleichen.

278
00:28:49,000 --> 00:28:52,000
Aber es kommt irgendwie eine ganz andere moralische Wertung ins Spiel.

279
00:28:52,000 --> 00:28:56,000
Sie macht da noch ein ganz spannendes Beispiel auf, was mich irgendwie so ein bisschen ins Denken gebracht hat.

280
00:28:56,000 --> 00:28:58,000
Da komme ich gleich noch zu.

281
00:28:58,000 --> 00:29:04,000
Na ja, was ihr erstmal feststellt, ist, dass Roboter häufig in Aussehen als auch im Verhalten vermenschlicht werden.

282
00:29:04,000 --> 00:29:09,000
Also man nennt das dann Anthropomorphisierung.

283
00:29:09,000 --> 00:29:12,000
Sie entwickeln sich dann irgendwie zu sowas wie Gefährten von Menschen.

284
00:29:12,000 --> 00:29:18,000
Und diese Anthropomorphisierung wird von der Psychologie meistens traditionell negativ gesehen.

285
00:29:18,000 --> 00:29:22,000
Also als eine Voreingenommenheit oder einen sogenannten Kategorienfehler.

286
00:29:22,000 --> 00:29:23,000
Ja.

287
00:29:23,000 --> 00:29:29,000
Das, was ich am Anfang als Betrug quasi betitelt habe, bei dieser Parorobbe zum Beispiel.

288
00:29:29,000 --> 00:29:36,000
Und da macht die Psychologie den Vorwurf, dass es eben die Illusion von Beziehungen irgendwie entsteht,

289
00:29:36,000 --> 00:29:43,000
wenn man Roboter quasi so moralzentriert oder auch menschlich behandelt.

290
00:29:43,000 --> 00:29:44,000
Ja.

291
00:29:44,000 --> 00:29:48,000
Lo macht dann die Frage danach auf, wann ist eine Beziehung eigentlich eine Beziehung?

292
00:29:48,000 --> 00:29:56,000
Also wie unterscheiden sich Roboter von Tieren oder auch Beziehungen oder Bezugsnamen zu Dingen im Allgemeinen?

293
00:29:56,000 --> 00:30:00,000
Es gibt ja durchaus Menschen, die objektsexuell sind.

294
00:30:00,000 --> 00:30:05,000
Das sind wenige Menschen, aber die gibt es halt, die führen Beziehungen zu Dingen.

295
00:30:05,000 --> 00:30:15,000
Und wenn ich so einfach daran denke, jede Person, die ich kenne, hat definitiv auch Dinge, zu denen sie irgendwie eine emotionale Bindung zum Beispiel hat.

296
00:30:15,000 --> 00:30:21,000
Und das wird im Allgemeinen auch nicht besonders als völlig abstruse oder verwerflich angesehen.

297
00:30:21,000 --> 00:30:31,000
Was man da dann bei Robotern sehen kann, ist, dass sie so klassische Kategorisierungen von Subjekt und Objekt oder Belebt und Unbelebt so ein bisschen aufweichen.

298
00:30:31,000 --> 00:30:43,000
Also die kommen so zunehmend, gerade wenn sie eben so vermenschlicht auftreten und Menschen auch sehr emotional oder menschlich eben auf sie reagieren, kommt es in so eine Grenzregion.

299
00:30:43,000 --> 00:30:54,000
Was sie am Ende sagt, ist, dass eine Definition von Beziehung eigentlich ist, dass eine Beziehung bis hin zu einer Freundschaft kann man desto eher zu einem Gegenüber eingehen,

300
00:30:54,000 --> 00:30:59,000
je mehr es eine befriedigende Antwort auf die eigenen Bedürfnisse zu geben, imstande ist.

301
00:30:59,000 --> 00:31:09,000
Da spielt glaube ich ein bisschen rein, was wir gerade eben schon hatten, dass uns eigentlich auch menschliche Gegenüber schon ziemlich intransparent sind und wir gar nicht so genau hinter deren Kopf gucken können.

302
00:31:09,000 --> 00:31:19,000
Und uns häufig auch egal ist, ob, keine Ahnung, stellen wir uns vor, wir sind an der Supermarktkasse und wünschen uns einen freundlichen Umgang mit der Person, die da kassiert.

303
00:31:19,000 --> 00:31:26,000
Dann geht es uns um den freundlichen Umgang und nicht darum, ob uns die Person wirklich wohlgesonnen ist.

304
00:31:27,000 --> 00:31:37,000
Dann gibt es in der Philosophie offenbar die Position und die finde ich eigentlich ganz überzeugend ist, dass für Roboter als Objekte der Moral die These spricht,

305
00:31:37,000 --> 00:31:43,000
dass es Menschen besser gelingt menschlich zu bleiben, wenn sie Roboter und andere human behandeln.

306
00:31:43,000 --> 00:31:53,000
Also das heißt, wenn wir mit Dingen und Robotern eben vernünftig menschlich und nicht völlig gewaltvoll oder so umgehen,

307
00:31:53,000 --> 00:32:08,000
dass wir dann eben auch menschselbst bleiben und eben ja nicht sonst wo unsere Gewalt ausleben und die dann vielleicht wieder zu Menschen hin zurück transformieren und sie auch dafür legitim auf einmal erachten.

308
00:32:08,000 --> 00:32:15,000
Da gibt es doch diese Diskussion, die ich letztes Mal so am Rande mitgekriegt habe, zum Thema wie Kinder mit so Alexa und Siri und so weiter umgehen,

309
00:32:15,000 --> 00:32:26,000
dass sie halt gewöhnt sind so nach dem Motto Alexa spiel XY und dann eventuell auch anfangen im Alltag mit Leuten so umzugehen und einfach nur aufzufordern, ohne da wirklich eine soziale Interaktion einzutreten.

310
00:32:26,000 --> 00:32:34,000
Da geht es also ein bisschen um dieses Vorbildhafte sozusagen. Wenn ich mich da mal dran gewöhnt habe, dann bin ich auch Menschenpampiger.

311
00:32:34,000 --> 00:32:44,000
Ja, ich hatte neulich Besuch von einem guten Freund und habe mir einen Tee aufgegossen und wenn ich mir einen Tee aufgegieße, dann schrei ich im Normalfall Siri an.

312
00:32:44,000 --> 00:32:49,000
Also nein, ich sage zu Siri, Siri stelle einen Timer auf drei Minuten oder so.

313
00:32:49,000 --> 00:32:56,000
Und naja, ich hatte mich gerade mit dem Freund unterhalten, habe mir parallel meinen Tee aufgegossen und dann eben zu meinem Handy gesagt, stelle einen Timer auf drei Minuten.

314
00:32:56,000 --> 00:33:04,000
Aber halt dann zu einem sehr neutralen Ton und ich wurde dann von meinem Freund so ein bisschen irritiert angeguckt.

315
00:33:04,000 --> 00:33:07,000
Aber er hat dann sein Handy rausgeholt und wollte eben Timer stellen.

316
00:33:07,000 --> 00:33:23,000
Aber ja, das war definitiv irritierend und von daher kann ich ein Beispiel gut nachvollziehen, weil eben meine Ansprache gegenüber dieser Technik Siri offenbar eine ganz andere war, die ich gegenüber einem guten Freund so nie an den Tag legen würde.

317
00:33:23,000 --> 00:33:29,000
So würde ich nie einen Freund darum bitten, doch mal einen Kurzzeitbäcker zu stellen.

318
00:33:29,000 --> 00:33:42,000
Naja, und die Frage danach, ob das jetzt aber am Ende irgendwie richtig oder falsch ist, da macht Lo die Frage auf, naja, aber wie verhalten wir uns zum Beispiel gegenüber Kunstwerken oder auch Romanfiguren?

319
00:33:42,000 --> 00:33:45,000
Ist es wirklich egal, wie wir ihnen begegnen?

320
00:33:45,000 --> 00:34:02,000
Da würden wir vermutlich nicht zu dem Schluss kommen zu sagen, naja, wie ich auf ein Kunstwerk reagiere, ob ich da in einem Museum völlig, keine Ahnung, gefühlskalt und so ohne jeden Respekt vor der Kunst dem gegenüber trete.

321
00:34:02,000 --> 00:34:07,000
Naja, da würde niemand sagen, dass das so ganz egal ist, wie wir uns da eigentlich verhalten.

322
00:34:07,000 --> 00:34:08,000
Ja, stimmt.

323
00:34:09,000 --> 00:34:13,000
Und die Frage ist, warum sollte es dann bei Robotern eben egal sein, ne?

324
00:34:13,000 --> 00:34:15,000
Spannend, spannend.

325
00:34:15,000 --> 00:34:17,000
Das fand ich auch ganz spannend, naja.

326
00:34:17,000 --> 00:34:27,000
Dann gibt es zu dem Ganzen sowas, was sie inklusiver Ansätze nimmt, also die probieren so ein bisschen weiterzudenken, das macht sie dann für den zweiten Teil des Buches auch nochmal.

327
00:34:27,000 --> 00:34:38,000
Und diese inklusiven Ansätze bemühen sich eben um so eine Loslösung von dem, was sie Anthropozentrismus nennt, also von diesem Ausgang von, wir gucken nur vom Menschen aus, wie das eigentlich alles beurteilt werden kann.

328
00:34:38,000 --> 00:34:48,000
Den Menschen soll dabei an sich nichts abgesprochen werden, also es geht dann nicht darum, den Menschen quasi kleiner zu machen, als er es traditioneller Philosophie gemacht wird.

329
00:34:48,000 --> 00:34:54,000
Aber ja, Janina Loh macht auch ganz viel zum Thema Posthumanismus und Transhumanismus.

330
00:34:54,000 --> 00:35:07,000
Und da fasst sich so zusammen, dass der kritische Posthumanismus die tradierten und zumeist humanistischen Dichotomien, wie etwa Frau-Mann, Natur-Kultur oder Subjekt-Objekt hinterfragt.

331
00:35:07,000 --> 00:35:09,000
Also darum geht es so ein bisschen.

332
00:35:09,000 --> 00:35:21,000
Eine Autorin, auf die sie viel referenziert ist, Donna Haraway, die, glaube ich, Biologin und Verhaltensforscherin und Philosophin ist und die ganz lange schon zu Cyborgs und Tieren schreibt.

333
00:35:21,000 --> 00:35:31,000
Und genau, sie hat seit neuerem wohl mehr über Tiere und Tierethik und die auch einen eigenen Hund hat, den aber nicht als Haustier bezeichnet, sondern als Forschungsbegleiterin.

334
00:35:31,000 --> 00:35:34,000
Aber früher hat sie ja halt über Cyborgs offenbar geschrieben.

335
00:35:34,000 --> 00:35:44,000
Und die weiß daraufhin, dass Cyborgs natürlich irgendwie sind so kybernetische Organismen und ja, hybride aus Maschine und Organismus.

336
00:35:44,000 --> 00:35:49,000
Und die gibt es sowohl in der gesellschaftlichen Wirklichkeit als auch in der Fiktion.

337
00:35:49,000 --> 00:35:58,000
Also ja, und die These, die Haraway offenbar aufmacht und die Loh, glaube ich, nicht unsympathisch findet, ist, dass wir alle schon Cyborgs sind.

338
00:35:58,000 --> 00:36:12,000
Also wir benutzen irgendwie schon unsere Handys. Im Prinzip benutzen wir auch alle schon künstliche Dinge wie Pullover und Hosen und fahren eigentlich auch mit Straßenbahnen durch die Gegend und so.

339
00:36:12,000 --> 00:36:17,000
Also wenn man das ein bisschen weiter denkt, dann ist die Maschine schon Teil von uns.

340
00:36:17,000 --> 00:36:25,000
Nur um mal aufzuzeigen, man kann über diese Differenz von Maschine und Mensch anders denken, als es klassischerweise unterm Alltag getan wird.

341
00:36:25,000 --> 00:36:32,000
Ja, dann geht Loh noch so ein paar Beispiele durch. Und das will ich auch ein bisschen zumindest machen.

342
00:36:32,000 --> 00:36:38,000
Sie zitiert eine Person, die heißt Suchmann. Ich weiß nicht genau, wie man sie ausspricht.

343
00:36:38,000 --> 00:36:50,000
Die sagt zum Beispiel, na ja, man kann auch über Materie zum Beispiel anders denken und als man es gemeinhin tut, also die Differenz zwischen belebt und unbelebt ein bisschen auflösen.

344
00:36:50,000 --> 00:36:56,000
Das kam mir erst ein bisschen merkwürdig vor, weil Suchmann offenbar sagt, dass Materie diskursiv ist.

345
00:36:56,000 --> 00:37:04,000
Aber sie bringt als Beispiel eine kalifornische Rosine, die, wenn ich sie esse, mehr ist als eine kalifornische Rosine.

346
00:37:04,000 --> 00:37:16,000
Sondern wenn ich so eine Rosine esse, dann beschäftige ich mich oder habe Einfluss oder nehme in mich auch so Konzepte wie Kapitalismus einerseits,

347
00:37:16,000 --> 00:37:23,000
weil die Rosine wurde ja irgendwo erwirtschaftet und ich habe sie gekauft und so weiter, als auch Kolonialismus, also gerade in den USA.

348
00:37:23,000 --> 00:37:28,000
Und vielleicht auch Rassismus, denn wer hat diese Rosine zum Beispiel gepflückt?

349
00:37:28,000 --> 00:37:40,000
Also die Idee ist einfach das, was da aufgezeigt werden soll, ist, man kann anders über das Verhältnis von Materie und Mensch und unbelebt und belebt denken.

350
00:37:40,000 --> 00:37:45,000
Wobei da natürlich mal schnell bei dieser Ebene sind einfach kulturelle Überformungen sozusagen von Materie zu fassen.

351
00:37:45,000 --> 00:37:48,000
Das ist jetzt ja gar nicht so neu, darüber nachzudenken.

352
00:37:48,000 --> 00:37:50,000
Absolut, naja.

353
00:37:50,000 --> 00:37:55,000
Naja, und daher gibt es dann jetzt eben noch ein paar Mehrbeispiele, die gehe ich jetzt nicht noch alle durch.

354
00:37:55,000 --> 00:38:07,000
Genau, wer das Buch liest, sie zitiert häufig eine Person namens Körkelberth, denke ich, also C-O-E-C-K-E-L-B-E-R-G-H geschrieben.

355
00:38:07,000 --> 00:38:11,000
Der Person steht sie immer ziemlich positiv gegenüber und diesen Ansätzen.

356
00:38:11,000 --> 00:38:18,000
Wenn ich es richtig sehe, ist das ihr Vorgesetzter an der Uni Wien, also quasi ihr Chef.

357
00:38:18,000 --> 00:38:22,000
Das sollte man im Hinterkopf haben, wenn man das alles probiert, durchzubewerten.

358
00:38:22,000 --> 00:38:29,000
Also ich vermute, dass sie es auch wirklich gut findet, aber nur um zu wissen, dass sie einfach aus der gleichen Kerke herbekommt.

359
00:38:30,000 --> 00:38:37,000
Ihre Zwischenbilanz zu dem Ganzen ist dann jetzt erstmal, dass Roboter als moralische Akteure, hatte ich ja schon gesagt,

360
00:38:37,000 --> 00:38:43,000
sowas wie eigentlich Autonomie und sowas wie Lernfähigkeit brauchen, eigentlich auch Intentionalität,

361
00:38:43,000 --> 00:38:51,000
irgendeine Form von Urteilskraft und Verantwortung und das eben auffällig ist, dass immer von Menschen ausgedacht wird.

362
00:38:51,000 --> 00:38:59,000
Als Handlungsobjekte gelten sie im Normalfall eben nur dann, wenn sie auch Handlungsubjekte mal irgendwann sein sollten.

363
00:38:59,000 --> 00:39:07,000
Aber man könnte auch sagen, da wo eine emotionale Bindung entsteht zu etwas, zu jemandem, entsteht auch Moralität.

364
00:39:07,000 --> 00:39:09,000
Also die These macht sie auch auf.

365
00:39:09,000 --> 00:39:18,000
Also es gibt so relationale Moraltheorien, die eben sagen, da wo etwas zwischen zwei Wesen entsteht, sowas wie eine Bindung oder so,

366
00:39:18,000 --> 00:39:26,000
da ist eben auch Moral am Werk und das finde ich, kann man zumindest sehr gut an Tieren erkennen, da ist es ja auch nicht egal.

367
00:39:26,000 --> 00:39:31,000
Also wenn wir uns an die Binden, empfinden wir sowas wie eine moralische Verantwortung ziemlich schnell

368
00:39:31,000 --> 00:39:42,000
und warum sollte das bei Robotern eigentlich anders sein, weil ob die Kapazitäten, die gedanklich in sich so groß unterscheiden, sei mal dahingestellt.

369
00:39:42,000 --> 00:39:47,000
Das ist was, was du gerade auch bei diesem Beispiel mit dem Kunstwerk meinst, was mir gerade so ein bisschen reinkommt,

370
00:39:47,000 --> 00:39:56,000
dass dieser moralische Bezug gegenüber Objekten meist in erster Linie daran hängt, wie andere Menschen emotional an diese Objekte gebunden sind.

371
00:39:56,000 --> 00:39:59,000
Ja, das stimmt.

372
00:39:59,000 --> 00:40:06,000
Dass der Roboter, der in der Fabrik irgendwelche Autos zusammenbaut, da ist kein Mensch emotional daran gebunden.

373
00:40:06,000 --> 00:40:09,000
Deswegen fällt es uns da leicht zu sagen, ja komm schmeiß den halt auf den Müll.

374
00:40:09,000 --> 00:40:21,000
Wenn das jetzt aber der Objekt Liebe oder Objekt Sexuell sozusagen, wenn das irgendwie das, was auch immer das für ein Gegenstand ist,

375
00:40:21,000 --> 00:40:27,000
zu dem jemand irgendwie eine sehr enge emotionale Bindung aufgebaut hat, dann ist mit diesem Gegenstand umzugehen,

376
00:40:27,000 --> 00:40:31,000
ja einen mit diesen Menschen umgehen und dadurch dann sehr stark moralisch geprägt.

377
00:40:31,000 --> 00:40:36,000
Das finde ich gerade spannend, das ist nicht nur die Beziehung, wie gehe ich mit einem Gegenstand um,

378
00:40:36,000 --> 00:40:44,000
sondern auch wie sind andere Menschen moralisch an diesen Gegenstand oder emotional in diesen Gegenstand gebunden, die dann da mit ins Spiel laufen.

379
00:40:44,000 --> 00:40:50,000
Ich finde dabei durchaus auch zu beachten, gerade wenn es irgendwie um Kunstwerke und das Museumsbeispiel, was sich da gebracht hatte,

380
00:40:50,000 --> 00:40:59,000
oder auch um Romanenfiguren geht, ja also die Kunstwerke und Romanenfiguren referenzieren immer auch sehr direkt auf die Person,

381
00:40:59,000 --> 00:41:05,000
die diese Dinge erschaffen hat, also das steht ja irgendwie im Hintergrund, wenn ich ein Kunstwerk gegenüber trete,

382
00:41:05,000 --> 00:41:11,000
ist das ja auch immer der Person, die es erschaffen hat irgendwie, ja.

383
00:41:11,000 --> 00:41:17,000
Von da aus geht es dann eben über zu Robotern als Verantwortungs-Subjekten und ich muss sagen,

384
00:41:17,000 --> 00:41:24,000
mit meiner nicht großen philosophischen Vorbildung ist es mir da manchmal ein bisschen schwer gefallen irgendwie nachzuvollziehen,

385
00:41:24,000 --> 00:41:31,000
warum diese Trennung so stark gemacht wurde zwischen Moral-Subjekt und Objekt und Verantwortungs-Subjekt und Objekt,

386
00:41:31,000 --> 00:41:39,000
aber gut, nehmen wir es erstmal so hin, aber genau, rein von der Zugänglichkeit her für Leute, die vielleicht philosophisch nicht so bewandert sind,

387
00:41:39,000 --> 00:41:45,000
hätte ich mich da darüber gefreut, wenn das vielleicht einfach in einem Guss gewesen wäre und nicht so stark getrennt,

388
00:41:45,000 --> 00:41:48,000
aber ich probiere mal das irgendwie nachzuziehen.

389
00:41:48,000 --> 00:41:56,000
Na ja, da startet sie damit zu sagen, dass Verantwortung und die Möglichkeit zur Verantwortung an Akteurschaft gebunden ist

390
00:41:56,000 --> 00:42:03,000
und dass man Verantwortung da von Haftbarkeit unterscheiden sollte, also das ist nicht das Gleiche,

391
00:42:03,000 --> 00:42:11,000
also ein bisschen die Unterscheidung zwischen verantwortlichem Handeln einerseits, die irgendwie moralisch total aufgeladen ist

392
00:42:11,000 --> 00:42:20,000
und der rein begrenzten Verursachung, also auch, haben wir jetzt Beispiel, wenn ein Kind irgendwas kaputtmacht,

393
00:42:20,000 --> 00:42:27,000
dann ist es vielleicht haftbar zu machen dafür oder halt die Eltern, aber man würde nicht unbedingt sagen,

394
00:42:27,000 --> 00:42:34,000
dass es in dem Sinne moralisch verantwortlich ist, auch wenn es im Deutschen zumindest von der Sprache her manchmal so ein bisschen verwischt,

395
00:42:34,000 --> 00:42:36,000
also Verantwortung und Verursachung quasi.

396
00:42:36,000 --> 00:42:42,000
Interessant, da hätte ich jetzt, mein intuitives Begriffsverständnis wäre quasi genau andersrum gewesen,

397
00:42:42,000 --> 00:42:47,000
dass gerade derjenige, der verantwortlich ist, dafür haftbar ist, weil die Eltern zum Beispiel nicht genug auf das Kind aufgepasst haben.

398
00:42:47,000 --> 00:42:52,000
Genau, das wäre vermutlich das, aber sie haben es nicht verursacht, sondern das Kind hat verursacht.

399
00:42:52,000 --> 00:42:57,000
Genau, und deswegen Kind als Verursacher, Eltern als Verantwortliche, vielleicht.

400
00:42:59,000 --> 00:43:07,000
Ja, Lo macht ihre eigene Position auch deutlich, also sie sagt, dass manche Roboter die notwendigen Kompetenzen dafür haben,

401
00:43:07,000 --> 00:43:13,000
Verantwortung zu simulieren zumindest, was das dann von, das ist das, worüber wir gerade eben gesprochen haben,

402
00:43:13,000 --> 00:43:19,000
inwiefern man nachgucken kann, ob sie es nun wirklich verantwortlich sich irgendwann mal fühlen können, wissen wir nicht so genau.

403
00:43:19,000 --> 00:43:25,000
Ja, und dass Menschen absehbar erst mal besser für Verantwortungsübernahme qualifiziert sind, überrascht jetzt auch.

404
00:43:25,000 --> 00:43:33,000
Verantwortung ist dabei noch ein relativ modernes Konzept im Prinzip, was irgendwie so mit komplexen Gesellschaften entstanden ist.

405
00:43:33,000 --> 00:43:41,000
Also, weil man irgendwie nicht mehr direkt zuordnen konnte, wer welches Vergehen eigentlich ausgemacht hat,

406
00:43:41,000 --> 00:43:46,000
muss man darauf umstellen, quasi gesellschaftlich zu sagen, verhaltet euch mal verantwortlich,

407
00:43:46,000 --> 00:43:51,000
weil ansonsten haben wir hier irgendwie Dynamiken, die wir nicht mehr so richtig überblicken können.

408
00:43:51,000 --> 00:43:58,000
Aber auch da ist es doch wieder eigentlich extrem ähnlich wie die Haftbarkeit, weil bei der Haftbarkeit geht es ja auch wieder genau darum, wer ist denn jetzt haftbar dafür.

409
00:43:58,000 --> 00:43:59,000
Ja, total.

410
00:43:59,000 --> 00:44:02,000
Also für mich fällt dieses Verantwortung und Haftbarkeit extrem zusammen.

411
00:44:02,000 --> 00:44:05,000
Das hat sich mir noch nicht erschlossen, wie das unterschiedlich ist.

412
00:44:05,000 --> 00:44:14,000
Ja, ich glaube, das ist tatsächlich eine philosophische Unterscheidung, weil mit Verantwortung definitiv mehr einhergeht.

413
00:44:14,000 --> 00:44:22,000
Das wäre glaube ich ein bisschen die These, da hängt halt irgendwie moralisch gut und böse mit dran und an Haftbarkeit eben nicht so sehr.

414
00:44:23,000 --> 00:44:31,000
Aber Lo weist auch darauf hin, na ja, bei Verantwortung geht es eigentlich schon genuin darum, dass man ein konkretes Subjekt der Verantwortungsübernahme immer wieder ausmacht.

415
00:44:31,000 --> 00:44:32,000
Okay.

416
00:44:32,000 --> 00:44:42,000
Ich hatte als Gedanken dazu im Kopf, die Klagen in den USA gegen Volkswagen im Zuge dieses ganzen Abgasskandals und so weiter,

417
00:44:42,000 --> 00:44:48,000
weil da ja dann tatsächlich am Ende auch nicht, also die Firma Volkswagen kann nicht ins Gefängnis gehen.

418
00:44:48,000 --> 00:44:54,000
Die kann irgendwie Geld strafen oder so verantworten, aber das ist vielleicht auch gar nicht das, was manche Leute sehen wollen.

419
00:44:54,000 --> 00:45:00,000
Und das heißt, dass am Ende tatsächlich konkrete Manager und Managerinnen ins Gefängnis mussten.

420
00:45:00,000 --> 00:45:05,000
Also man probiert dann irgendwie auszumachen, die und die Person ist es jetzt gewesen.

421
00:45:05,000 --> 00:45:16,000
Auch wenn glaube ich relativ klar ist, dass das, was Lo für ein Verantwortungsnetzwerk ist, eigentlich das wäre, was viel Treffener beschreibt, wo da die Verantwortlichkeiten liegen.

422
00:45:16,000 --> 00:45:23,000
Das ist ja nicht der eine Manager, der jetzt irgendwie eigentlich verantwortlich ist, aber er wird zumindest haftbar gemacht.

423
00:45:23,000 --> 00:45:24,000
Ja klar.

424
00:45:24,000 --> 00:45:32,000
Ja, genau. Die Idee von Verantwortungsnetzwerken ist dann auch, dass alle beteiligten Parteien in einer Situation Verantwortung tragen.

425
00:45:32,000 --> 00:45:41,000
Das Problem ist, dass sowas entwickelt wurde, um irgendwie solche Situationen, wie ich es hier gerade eben beschrieben habe, zu bewältigen.

426
00:45:41,000 --> 00:45:51,000
Also rein philosophisch jetzt erstmal nicht justiziabel quasi, aber Lo weist darauf hin, dass wenn man probiert, eben Verantwortungsnetzwerke zu begreifen,

427
00:45:51,000 --> 00:45:56,000
gerade im Kontext von autonomen Systemen, also wie der Beispiel Autounfall, wer ist jetzt eigentlich verantwortlich?

428
00:45:56,000 --> 00:46:00,000
Das Auto kann es nicht so richtig sein, der, die Fahrerin vielleicht auch nicht.

429
00:46:00,000 --> 00:46:07,000
Und welchen Anteil hat die Person, der die Unfall zugestoßen ist, eigentlich selbst?

430
00:46:07,000 --> 00:46:13,000
Naja, und was haben die Ingenieure damit zu tun und die Programmierer, die das Programm geschrieben haben und so weiter?

431
00:46:13,000 --> 00:46:20,000
Also so Verantwortungsnetzwerke, wenn man wirklich erfassen will, wer eigentlich schuld an irgendetwas ist, werden riesig groß.

432
00:46:20,000 --> 00:46:27,000
Damit torpediert man quasi die Idee hinter Verantwortung wieder, nämlich dass man eine Person hat, die man haftbar machen kann.

433
00:46:27,000 --> 00:46:29,000
Ja, da fällt es nämlich doch wieder zusammen.

434
00:46:29,000 --> 00:46:38,000
Ja, total, ja. Und da unterscheidet man dann eben vor Gericht quasi zwischen Verantwortung und Haftbarkeit, würde ich sagen.

435
00:46:38,000 --> 00:46:43,000
Gehen wir über zu Verantwortungsobjekten so ein bisschen.

436
00:46:43,000 --> 00:46:50,000
Ja, da ist die Idee dahinter, dass selbst die komplexesten Maschinen nicht mehr als bloße Instrumente menschlichen handeln sind.

437
00:46:50,000 --> 00:46:59,000
Man kennt das von der National Rifle Association aus den USA, also nicht Waffen töten Menschen, sondern Menschen töten Menschen.

438
00:46:59,000 --> 00:47:02,000
Und da sagt Lou, naja, so einfach ist das nicht.

439
00:47:02,000 --> 00:47:10,000
Also eine Person mit einer Pistole ist ein Schütze, eine Schützin und eine Person ohne eine Pistole ist eine Person.

440
00:47:10,000 --> 00:47:15,000
Also sie denkt das schon eher zusammen. Und das Gleiche geht dann eben auch für Roboter.

441
00:47:15,000 --> 00:47:21,000
Sie nennt da einen Philosophen, der sich eigentlich mit Computersystemen beschäftigt und das schon relativ lange.

442
00:47:21,000 --> 00:47:34,000
Und der sieht Probleme im Verantwortungsbereich, in der Philosophie dann, wenn Computersysteme schneller und akkurater als Menschen in der Lage dazu sind, Entscheidungen zu treffen.

443
00:47:34,000 --> 00:47:43,000
Und man kann sich das durchaus vorstellen, gerade in, so nennt sie das, Delikaten Kontexten wie militärischen Frühwarnsystem, wo es dann darum geht,

444
00:47:43,000 --> 00:47:52,000
okay, ein Frühwarnsystem sieht, dass eine Rakete oder so auf einen zufliegt und muss dann darauf reagieren.

445
00:47:52,000 --> 00:47:59,000
Und das kann dann kein Mensch mehr machen, sondern das muss dann eben, ja, muss dann ein Computersystem machen.

446
00:47:59,000 --> 00:48:11,000
Da sieht dieser Hans Lenk eben ein Beleg dafür, dass die Abhängigkeit von Technologien dafür sorgt, dass Computersysteme auch heute schon Verantwortung und Verantwortung übernehmen.

447
00:48:11,000 --> 00:48:20,000
Die Frage ist aber, wenn man so ein Computersystem nicht haftbar machen kann, wer steht dann am Ende Rede und Antwort?

448
00:48:20,000 --> 00:48:21,000
Ja, genau.

449
00:48:21,000 --> 00:48:26,000
Also die Frage nach Haftbarkeit, Verantwortung kommt da dann eben wieder auf.

450
00:48:26,000 --> 00:48:35,000
Zusammenfassend kann man sagen, dass den verantwortlichen Subjekten generell eine moralische Verantwortung für die Produkte ihres Schaffens zugeschrieben wird.

451
00:48:35,000 --> 00:48:44,000
Also auch heute ist es noch so, dass Computersysteme, Roboter und so weiter quasi als verlängerte Arm ihrer ErschafferInnen gelten.

452
00:48:44,000 --> 00:48:45,000
Ja.

453
00:48:45,000 --> 00:48:54,000
Was ich gerade eben schon gesagt hatte, ist, dass es so Ideen gibt von relationalen Konzepten von Moral und die gibt es eben auch im Bereich der Verantwortung.

454
00:48:54,000 --> 00:49:02,000
Das heißt, inklusiven Ansätzen aufgemacht, dass Verantwortung sich ausschließlich in der Interaktion zwischen Wesen abspielt.

455
00:49:02,000 --> 00:49:09,000
Die Idee dahinter ist, dass das Narrativ vom autarken Handlungssubjekt eine gesellschaftliche, rechtliche und politische Illusion ist.

456
00:49:09,000 --> 00:49:11,000
Das ist wieder diese Haraway, die sie da zitiert.

457
00:49:11,000 --> 00:49:19,000
Also dass wir alle so wunderbar autark und autonom sind, ist vielleicht notwendig, dass wir uns das so denken.

458
00:49:19,000 --> 00:49:29,000
Aber faktisch ist das nicht so, weil wir natürlich irgendwie gesellschaftlich eingebunden sind und leben und manchmal gar nicht anders können, als die Situation es irgendwie zulässt.

459
00:49:29,000 --> 00:49:32,000
Ich glaube, wir als Soziologen können das ganz gut nachvollziehen.

460
00:49:32,000 --> 00:49:33,000
Ja.

461
00:49:33,000 --> 00:49:45,000
Ja, dabei muss man irgendwie immer wieder daran erinnert werden, finde ich zumindest, dass die Idee der Individualität dahinter und der entsprechenden Verantwortung und Verantwortungsübernahme noch relativ neu ist.

462
00:49:45,000 --> 00:49:48,000
Also Losak, die ist erst im Mittelalter entstanden.

463
00:49:48,000 --> 00:49:52,000
Das Gleiche gilt übrigens für Begriffe wie Identität.

464
00:49:52,000 --> 00:49:58,000
Also dass jeder Mensch eine eigene Identität hat, ist auch noch eine relativ moderne Erfindung.

465
00:49:58,000 --> 00:50:03,000
Und dass auch solche Individualitätsideen ziemlich kulturraum explizit sind.

466
00:50:03,000 --> 00:50:13,000
Sie meinen zum Beispiel, dass es im asiatischen Kulturraum gar nicht so verbreitet ist, dass man irgendwie die Idee hat, dass man irgendwie ein Individuum ist.

467
00:50:13,000 --> 00:50:19,000
Ein bisschen die Unterscheidung zwischen individualistischen und kollektivistischen Kulturen.

468
00:50:19,000 --> 00:50:22,000
Ich bin da nie in die Materie tief eingestiegen.

469
00:50:22,000 --> 00:50:27,000
Ich weiß nicht, wie gut sich das tatsächlich aufrechterhalten lässt.

470
00:50:27,000 --> 00:50:30,000
Also ich gebe das einfach nur erst mal so wieder.

471
00:50:31,000 --> 00:50:40,000
Aus der Idee von dieser interaktionsbasierten Verantwortung gibt es dann das Konzept der Extended Agency aus.

472
00:50:40,000 --> 00:50:47,000
Dass Interaktionen eben aus einer Menge von menschlichen und nichtmenschlichen Beteiligten bestehen.

473
00:50:47,000 --> 00:50:52,000
Und eine Handlung sich eben nicht mehr nur auf die menschlichen Komponenten bezieht.

474
00:50:53,000 --> 00:50:58,000
Fand ich auch ganz interessant für alle NichtsoziologInnen da draußen.

475
00:50:58,000 --> 00:51:09,000
Die klassische Definition von Handlung ist nämlich nach Max Weber in der Soziologie eigentlich eben, dass sie mit Intention passiert und genuin menschlich ist.

476
00:51:09,000 --> 00:51:13,000
Also Tiere können sich zum Beispiel verhalten, aber nur Menschen können handeln.

477
00:51:13,000 --> 00:51:19,000
Da gibt es eben offenbar Ansätze, die probieren das ein bisschen anders zu sehen.

478
00:51:20,000 --> 00:51:28,000
Wenn da diese Begriffe so das Gegenstände handeln können, das erinnert mich ein bisschen an Latours aktuellen Netzwerktheorie.

479
00:51:28,000 --> 00:51:29,000
Ja ja witzig.

480
00:51:29,000 --> 00:51:34,000
Das Latour taucht auf, da ich Latour nicht kenne, gebe ich dir hier nicht so richtig wieder.

481
00:51:34,000 --> 00:51:39,000
Aber genau darauf wird offenbar viel auch referenziert in der Literatur.

482
00:51:40,000 --> 00:51:49,000
Ja, so wie ich das jetzt gerade eben mal probiert habe, irgendwie knapp zusammenzufassen, gibt es einige Schwierigkeiten, die da auftauchen.

483
00:51:49,000 --> 00:51:52,000
Und ich finde, die liegen auch relativ deutlich klar.

484
00:51:52,000 --> 00:51:59,000
Autonomer werdende Systeme machen es zunehmend schwieriger, Verantwortlichkeiten zuzurechnen.

485
00:51:59,000 --> 00:52:08,000
Also wenn wir mehr Systeme haben, die irgendwie in einem Zwischenraum zwischen ich mache so ein bisschen, was mir einprogrammiert wurde, ein bisschen, was ich irgendwie autonom gelernt habe.

486
00:52:08,000 --> 00:52:16,000
Und aber es sind auch noch Menschen involviert, macht es irgendwie ganz schwierig zu sagen, wer ist denn jetzt eigentlich schuldig, haftbar, verantwortlich für irgendwelche Handlungen.

487
00:52:18,000 --> 00:52:32,000
Und auch in diesen Relationskonstrukten von Verantwortung, wo man sagt, ok, wenn Dinge, Tiere, Roboter mit Menschen agieren oder auch Dinge mit Dingen oder Menschen mit Menschen, nur da entsteht Verantwortung,

488
00:52:32,000 --> 00:52:42,000
ist, dass man da durch diese neuen technischen Systeme auch so Verantwortungslücken in den normativen Kriterien hat, weil man irgendwie für diese technischen Systeme gar nicht so richtig weiß, wie man das alles bewerten soll.

489
00:52:42,000 --> 00:52:43,000
Ja, stimmt.

490
00:52:43,000 --> 00:52:52,000
Ja, ja. Und worauf Lo noch hinweist, das fand ich noch ganz spannend, ist, dass sie sagt, wir erleben auch sowas wie neue Raum-Zeit-Dimensionen im Kontext.

491
00:52:52,000 --> 00:53:02,000
Also sie sagt, die Rückbindung, globale Ereignisse, sie nennt Fluchtbewegungen oder auch den Klimawandel als Beispiele an einzelne Gruppen von Akteurinnen wird schwieriger.

492
00:53:02,000 --> 00:53:09,000
Also man kann nicht mehr so richtig sagen, wer ist denn jetzt beim Klimawandel eigentlich, wer ist daran jetzt schuld, wem ziehen wir dafür zu Rechenschaften.

493
00:53:09,000 --> 00:53:21,000
Oder wenn wir uns komplexe Migrations- und Fluchtbewegungen angucken, wie kann man sie einerseits vielleicht besser koordinieren, wie kann man das steuern, wie kann man dafür sorgen,

494
00:53:21,000 --> 00:53:26,000
dass die Bedingungen deren Wegen geflohen wird, dass die geändert werden.

495
00:53:26,000 --> 00:53:34,000
Auch das ist total schwierig da irgendwie zuzurechnen, wer da zur Verantwortung gezogen werden kann.

496
00:53:34,000 --> 00:53:39,000
Wobei ich da sagen würde, das Genuin hängt jetzt nicht mit Robotern oder auch mit artificiellen Systemen zusammen.

497
00:53:39,000 --> 00:53:41,000
Nur einfach mit unglaublich komplexen Wirkketten.

498
00:53:41,000 --> 00:53:49,000
Ja, genau. Die Frage, die ich mir dabei jetzt am Ende gestellt habe, ist, wenn man jetzt so Verantwortungskonzepte irgendwie erweitert,

499
00:53:49,000 --> 00:53:54,000
probiert in Netzwerke zu integrieren oder auch Roboter zur Verantwortung ziehen möchte.

500
00:53:54,000 --> 00:54:04,000
Ich frage mich ein bisschen, was dadurch gewonnen werden kann, weil ich das Gefühl habe, dass Verantwortungsaufforderungen oder so,

501
00:54:04,000 --> 00:54:11,000
oder die sich eben gerade dadurch da ausdrücken, dass sie nicht immer mit einer rechtlichen Keule um die Ecke kommen,

502
00:54:11,000 --> 00:54:17,000
sondern sagen, verhaltet euch mal verantwortlich, weil es kommen noch Leute nach euch, wir müssen an unsere Kinder denken,

503
00:54:17,000 --> 00:54:23,000
beim Klimawandel oder was auch immer. Und ich frage mich so ein bisschen, gewinnt man eigentlich praktisch irgendwas dadurch,

504
00:54:23,000 --> 00:54:26,000
wenn man das alles so weit zieht quasi?

505
00:54:26,000 --> 00:54:32,000
Ja, das habe ich auch gerade mal nachgedacht, weil ich mir diesen Begriff der Verantwortung überlegt habe,

506
00:54:32,000 --> 00:54:37,000
gerade wenn sie ihn von der Haftbarkeit trennen so stark. Was heißt das überhaupt?

507
00:54:37,000 --> 00:54:43,000
Ich meine, Verantwortung wirkt ja im Grunde als Handlungsprägen, kann das ja nur über mögliche Konsequenzen wirken.

508
00:54:43,000 --> 00:54:49,000
Es können auf der einen Seite sein die Haftbarkeit, die klassische rechtliche, das kann aber auch so etwas sein wie sozialer Status,

509
00:54:49,000 --> 00:54:58,000
wie irgendwie so ein, das bin nicht ich, so ein Selbstachtungsverlust oder Scham oder sowas in der Art.

510
00:54:58,000 --> 00:55:04,000
Aber das kann ja immer nur über diese Wirkmechanismen sozusagen nach außen treten.

511
00:55:04,000 --> 00:55:10,000
Dann ist natürlich die Frage, wie weit die bei irgendwelchen robotischen Systemen sozusagen wirken können.

512
00:55:10,000 --> 00:55:12,000
Kann ein Roboter Scham empfinden?

513
00:55:12,000 --> 00:55:19,000
Ja, das finde ich sind auch so die spannenden Fragestellungen. Das ist mir nicht so richtig klar.

514
00:55:19,000 --> 00:55:25,000
Ich finde, das, was ich gelesen habe, weist eigentlich eher in die Richtung von, es ist zumindest erstmal nicht abzusehen,

515
00:55:25,000 --> 00:55:31,000
dass da sowas wie genuines Bewusstsein wirklich ist. Und dann ist für mich ein bisschen die Frage, warum das alles?

516
00:55:31,000 --> 00:55:39,000
Weil Roboter haben auch absehbar kein eigenes Konto, von dem du irgendwelche Dinge bezahlen kannst.

517
00:55:39,000 --> 00:55:43,000
Das gleiche gilt auch für autonome Fahrsysteme.

518
00:55:43,000 --> 00:55:49,000
Wem ist damit geholfen, wenn du ein Auto für einen Unfall zur Rechenschaft ziehen kannst?

519
00:55:49,000 --> 00:55:53,000
Genau, was heißt das überhaupt ein Auto zur Rechenschaft zu ziehen?

520
00:55:53,000 --> 00:55:56,000
Dann darf das dann drei Monate nicht fahren, weil es ihm ja Spaß macht zu fahren.

521
00:55:56,000 --> 00:55:59,000
Ja, das finde ich auch, das ist irgendwie total schwierig.

522
00:55:59,000 --> 00:56:04,000
Kriegt das irgendwie ein fiktives Gehalt für jeden Kilometer, den es fährt, und muss dann davon wieder was...

523
00:56:04,000 --> 00:56:07,000
Also irgendwie so richtig erschließt sich mir das nicht.

524
00:56:07,000 --> 00:56:11,000
Diese Wirkmechanismen, die fehlen da irgendwie komplett, so Sanktionsmechanismen im Grunde.

525
00:56:11,000 --> 00:56:13,000
Ja, das stimmt.

526
00:56:13,000 --> 00:56:14,000
Auf welcher Ebene auch immer.

527
00:56:14,000 --> 00:56:20,000
Vielleicht muss man das Ganze auch eher als wirklich, ja Gedankenspiel finde ich, ist immer so ein bisschen respektierlich,

528
00:56:20,000 --> 00:56:28,000
aber vielleicht muss man es einfach als wirklich philosophische Überlegung erstmal auch für sich stehen lassen und das als das begreifen.

529
00:56:28,000 --> 00:56:34,000
Also zu sagen, okay, die philosophische Überlegung darüber, was bedeutet Verantwortung, was bedeutet Moral,

530
00:56:34,000 --> 00:56:40,000
in quasi außerhumanen Kontexten ist ein Erkenntnisgewinn für sich.

531
00:56:40,000 --> 00:56:47,000
Ich glaube, wenn man so darauf guckt, wird man dem vielleicht eher gerecht.

532
00:56:47,000 --> 00:56:53,000
Wobei es mir gerade so spontan einfällt, wenn man so Sanktionen irgendwie auch als Lernreiz sozusagen versteht.

533
00:56:53,000 --> 00:56:57,000
Also man sagt, man verheckt eine Sanktion, damit die Person das nicht wieder tut.

534
00:56:57,000 --> 00:57:02,000
Oder im Vorhinein als abschreckend. Das kann man ja wiederum in Computersystemen sehr gut abbilden.

535
00:57:02,000 --> 00:57:07,000
Gerade wenn du so ein selbstlernendes System hast und das bringt irgendwie jemand um, ihm dann ganz böse auf den Finger zu hauen und sagen,

536
00:57:07,000 --> 00:57:12,000
ne, ne, ne, das machst du nie wieder. Das könnte man ja sogar technisch relativ gut abbilden.

537
00:57:12,000 --> 00:57:16,000
Ja, vielleicht ist, ja, das stimmt. Das könnte ganz gut funktionieren.

538
00:57:16,000 --> 00:57:25,000
Gerade wenn die These ist, dass eben Top-Darren-Programmierung offenbar, weil das alles so zwischen den abstrakten Regeln und dem konkreten Verhalten,

539
00:57:25,000 --> 00:57:30,000
dass da so eine große Diskrepanz ist, dass eben Top-Darren nicht dauerhaft gut funktioniert.

540
00:57:30,000 --> 00:57:37,000
Ja, also dann müsste man eben als Regel einprogrammieren, dass sich XY nicht gut anfühlt für das Auto.

541
00:57:37,000 --> 00:57:40,000
Genau, was auch immer nicht gut anfühlen dann wieder heißt.

542
00:57:40,000 --> 00:57:47,000
Genau, aber das kann man ja, das wäre die These dahinter. Und das wissen wir aber eigentlich schon für andere Leute nicht.

543
00:57:47,000 --> 00:57:53,000
Also wir glauben halt nur zu wissen, dass die uns ähnlich, dass die so ähnlich fühlen wie wir. Spannend, spannend.

544
00:57:53,000 --> 00:58:04,000
Naja, kritische Zwischenbilanz zu dem ganzen Verantwortungsthema ist, naja, Akteurschaft ist irgendwie notwendig, um überhaupt Verantwortung zurechnen zu können.

545
00:58:04,000 --> 00:58:10,000
Hinter dem Ganzen steht irgendwie die Befürchtung, dass wir Roboter irgendwann nicht mehr kontrollieren können.

546
00:58:10,000 --> 00:58:18,000
Also wir vertrauen unseren eigenen Geschöpfen quasi nicht mehr so richtig. Und man kann feststellen, dass Menschen zweierlei Verantwortung haben.

547
00:58:19,000 --> 00:58:27,000
Also einmal sehr individuell als Designer innen von autonomen Systemen oder Robotern oder Programmiererinnen und so weiter.

548
00:58:27,000 --> 00:58:33,000
Aber auch kollektiv als Unternehmen, die bestimmte Roboter oder nicht Roboter einsetzen.

549
00:58:33,000 --> 00:58:41,000
Oder auch in Form von Ethikgremien, die irgendwas in Bezug auf diese ganze Roboter-Thematik entscheiden oder nicht entscheiden.

550
00:58:42,000 --> 00:58:54,000
Und ja, was sie sagt, was bei dem ganzen klar wird, ist, Bedürfnis der modernen Gesellschaft, alles immer kontrollieren zu können, wird auch bei Robotern offenbar.

551
00:58:54,000 --> 00:59:02,000
Warum sie das so hervorhebt, weiß ich nicht so ganz genau, weil ich das Gefühl habe, es gibt schon sehr, ich weiß nicht, vielleicht ist es einfach nur eine philosophische Feststellung.

552
00:59:02,000 --> 00:59:12,000
Ich hatte erst mal das Gefühl, na ja, es gibt schon gute Gründe, die die die Resultate des eigenen Schaffens, die Ergebnisse kontrollieren können zu wollen.

553
00:59:15,000 --> 00:59:19,000
Genau, dann schließt sie das Buch mit klassischen abschließenden Bemerkungen.

554
00:59:19,000 --> 00:59:26,000
Das gibt es ja häufiger mal, dass dann eben so ein bisschen nochmal quasi in die Zukunft gedacht werden soll oder ein bisschen praktischer.

555
00:59:26,000 --> 00:59:30,000
Hier in dem Fall ist jetzt so ein bisschen politische Implikation des Ganzen.

556
00:59:30,000 --> 00:59:36,000
Sie wiederholt ihre Aussage, dass Produkte des menschlichen Handelns moralisch nie neutral sind.

557
00:59:36,000 --> 00:59:45,000
Das gibt es eben im Bereich der Technik-Philosophie offenbar diese Position, na ja, es ist halt Technik, die kann man gut oder schlecht verwenden.

558
00:59:45,000 --> 00:59:57,000
Und wenn sie eben schlecht verwendet wird, ist das zwar problematisch, aber die problematische Verwendung liegt eben bei Menschen und nicht in der Technik.

559
00:59:57,000 --> 01:00:04,000
Und sie sagt, na ja, Vorsicht, Dinge sind eben moralisch immer aufgeladen, weil wir eben nach gewissen Normen agieren.

560
01:00:04,000 --> 01:00:16,000
Das gibt es offenbar auch schon länger, das Konzept, also so Gedanken wie zum Beispiel, wenn wir uns einen Tisch vorstellen, der ein schmales Kopfende hat und dann eine lange Tafel hat,

561
01:00:16,000 --> 01:00:22,000
dann drückt das im Prinzip schon eine hierarchische Struktur aus, die Vorsicht, dass jemand am Kopfende sitzt und Chef ist.

562
01:00:22,000 --> 01:00:29,000
Ja, es ist ja auch bei den diplomatischen Verhandlungen, dass dann die kreisrunden Tische eingesetzt werden und solche Sachen, die genau das eben abbilden.

563
01:00:29,000 --> 01:00:35,000
Na ja, Forderungen, die sie am Ende auf jeden Fall stellt, sind mehr Ethik- und Informationsunterricht in Schulen.

564
01:00:35,000 --> 01:00:44,000
Sie sagt, Ingenieurswissenschaftliche Ausbildung sollte mit Ethikpflichtkursen behaftet sein, weil sie, also sie nimmt da das Beispiel der Medizin.

565
01:00:44,000 --> 01:00:56,000
Man stelle sich nur mal vor, bei Dingen wie der Präimplantationsdiagnostik würden Ärzte und Ärztinnen irgendwie beraten, ohne je einen Ethikpflichtkurs im Bereich gemacht zu haben.

566
01:00:56,000 --> 01:01:02,000
Und sie meint, na ja, wenn jetzt irgendwie Roboter entwickelt werden, muss man auch die ethischen Hintergründe kennen.

567
01:01:02,000 --> 01:01:10,000
Und sie sagt, auch Unternehmen sollten Weiterbildungskurse im Bereich der Technik- und Roboterethik anbieten oder verpflichtend machen.

568
01:01:10,000 --> 01:01:19,000
Und sie meint, wir brauchen auch zunehmend öffentliche Ethikgremien, die zum Thema Roboter irgendwie beschlagen und bewandert sind.

569
01:01:19,000 --> 01:01:31,000
Und sie spricht sich als, ich glaube, kann man durchaus so sagen, linke Philosophin für eine Diskursöffnung aus, für die Vermittlung von Sachverstand, Reflektions- und Urteilskraften auf allen Ebenen, für alle Mitglieder einer Gesellschaft.

570
01:01:32,000 --> 01:01:46,000
Was natürlich demokratischen Idealen absolut entspricht, dass man, wenn man sagt, okay, wir müssen darüber reden, wie wir mit Robotern umgehen, was sie dürfen, was sie nicht dürfen, was sie ersetzen sollen oder auch nicht, dann sollten da auch möglichst viele an diesem Diskurs beteiligt sein.

571
01:01:46,000 --> 01:02:01,000
Sie hat auch bei Rahel Yagi promoviert, glaube ich, und das ist auf jeden Fall auch eine ziemlich interessante Person und auch auf jeden Fall aber linke Philosophin, die ursprünglich, glaube ich, so aus der Hausbesetzer-Szene kommt.

572
01:02:01,000 --> 01:02:30,000
Genau. Was sie sagt, was manchmal ein bisschen schwierig ist in dieser ganzen Diskussion, gerade so im Bereich, wenn es um Superintelligenzen und so geht, ist, dass sie sagt, okay, es wird häufig so getan, als sei gesetzt, dass sowas wie eine Superintelligenz irgendwann entsteht, die deutlich cleverer, schneller, ethischer, besser wie auch immer ist als der Mensch oder halt völlig dystopisch.

573
01:02:30,000 --> 01:02:40,000
Die alles kaputt macht und den Menschen überholt und uns braucht es dann gar nicht mehr. Und sie meint, das sind alles keine Naturgesetze, also es muss nicht alles passieren, was möglich ist.

574
01:02:40,000 --> 01:02:57,000
Wir kennen das gerade in Deutschland, dass einige Technologien, die theoretisch möglich sind, wie zum Beispiel das Klonen oder so oder das, was immer wieder unter so Designer-Baby oder so filmiert, sowas wäre ja im Prinzip technisch möglich, zumindest grundlegend.

575
01:02:57,000 --> 01:03:04,000
Das ist mit guten Gründen verboten und das kann man auch im Bereich der Roboter- und Computersystem-Ethik und so weiter auch fortführen.

576
01:03:04,000 --> 01:03:11,000
Also es ist nicht ausgemacht, dass bestimmte Technologien irgendwann real werden und sie bittet darum, dass man sich das mal vor Augen führt.

577
01:03:11,000 --> 01:03:16,000
Und das fand ich einen ganz schönen Hinweis, weil ich das tatsächlich bislang auch, glaube ich, wenig reflektiert habe.

578
01:03:16,000 --> 01:03:30,000
Also mir schien das auch immer so wie ausgemacht, naja, irgendwann gibt es halt so eine, ja, wird es den Punkt geben, wo Computer allumfassend besser und klüger handeln können als wir.

579
01:03:30,000 --> 01:03:36,000
Mal gucken, nur wann der kommt. Und sie meint, naja, das muss nicht so kommen. Wir können uns auch schon dagegen entscheiden, mit guten Gründen.

580
01:03:36,000 --> 01:03:37,000
Stimmt.

581
01:03:37,000 --> 01:03:42,000
Ja, das wäre es von mir zu diesem Buch. Hast du Fragen, Anmerkungen, Kritik?

582
01:03:42,000 --> 01:03:45,000
Ja, ich glaube, wir haben ja schon währenddessen ziemlich viel diskutiert.

583
01:03:45,000 --> 01:03:52,000
Deswegen, da ist, glaube ich, genug Anknüpfungspunkte auch für euch da draußen zum auch mal drüber nachdenken.

584
01:03:52,000 --> 01:03:55,000
Mich hat es auf jeden Fall viel zu denken gebracht. Spannend.

585
01:03:55,000 --> 01:03:56,000
Schön.

586
01:04:00,000 --> 01:04:06,000
Bist du zwischendurch schon auf Bücher, Vorschläge des Weitermachens gestoßen oder soll ich das eben noch machen und du übernehmst da nach?

587
01:04:06,000 --> 01:04:13,000
Ja, ich habe tatsächlich schon Ideen gehabt, auch kurz davor, als ich mir vor der Episode mal kurz zwei Minuten angeguckt hatte, worum es überhaupt geht.

588
01:04:13,000 --> 01:04:23,000
Das ist einmal tatsächlich das Sachbuch, was ich gerade vor dem Zettelkastenprinzip zu Ende gelesen habe, nämlich Life in Code von Ellen Ullman.

589
01:04:23,000 --> 01:04:31,000
Und das ist im Grunde so eine Essay-hafte Autobiografie von einer Frau, die halt irgendwie seit den 80er Jahren in der Softwareentwicklung gearbeitet hat.

590
01:04:31,000 --> 01:04:39,000
Und im Grunde so dann auch als erfahrene Programmiererin so ein bisschen in die Start-up-Kultur reingerutscht ist und die beobachtet hat.

591
01:04:39,000 --> 01:04:50,000
Und sie hat an einer Stelle, gibt es eine Stelle, wo sie eben drüber nachdenkt, wie es die Welt mit Robotern sozusagen aussieht, wenn man die irgendwie noch menschlicher behandelt.

592
01:04:50,000 --> 01:04:55,000
Und da überlegt sie dann, was denn eigentlich für Roboter heißt, dass sie genießen.

593
01:04:56,000 --> 01:05:03,000
Dass sie ein Essen oder irgendetwas genießen, dass es dann irgendwie darum geht, wie das Kühlmittel ihre Rohre durchzieht oder so.

594
01:05:03,000 --> 01:05:05,000
Das fand ich ein sehr, sehr schönes Bild.

595
01:05:05,000 --> 01:05:09,000
Und sie widmet sich eben auch so dieser Perspektive, Technik macht die Welt besser.

596
01:05:09,000 --> 01:05:12,000
Das ist das, was du auch dahin, was du auch so angesprochen hast.

597
01:05:12,000 --> 01:05:17,000
Das ist jetzt kein hardcoreiges Sachbuch mit irgendwelchen durchargumentierten Punkten oder so.

598
01:05:17,000 --> 01:05:23,000
Aber vielen interessanten Gedanken über das, was wir so seit 20 Jahren über Technik denken.

599
01:05:23,000 --> 01:05:27,000
Und wie Technik unsere Gesellschaft geprägt und verändert hat.

600
01:05:27,000 --> 01:05:29,000
Mit vielen persönlichen Anekdoten und so weiter drin.

601
01:05:29,000 --> 01:05:31,000
Aber es ist wirklich ein echt spannendes Buch.

602
01:05:31,000 --> 01:05:33,000
Life in Code von Alan Ullman.

603
01:05:33,000 --> 01:05:34,000
Und was natürlich...

604
01:05:34,000 --> 01:05:35,000
Entschuldigung.

605
01:05:35,000 --> 01:05:37,000
Nee, wirklich.

606
01:05:37,000 --> 01:05:38,000
Das klingt sehr inspirierend, finde ich.

607
01:05:38,000 --> 01:05:39,000
Ja, schön.

608
01:05:39,000 --> 01:05:45,000
Und was natürlich, wenn es um Roboter geht, rührt das an meiner Science-Fiction-Seele.

609
01:05:45,000 --> 01:05:47,000
Also Science-Fiction-Tipps, Weltenflüstern und so.

610
01:05:47,000 --> 01:05:49,000
Mein zweiter Podcast.

611
01:05:49,000 --> 01:05:51,000
Zwei Bücher, die mir da spontan eingefallen sind.

612
01:05:51,000 --> 01:05:53,000
Das ist einmal ein Klassiker.

613
01:05:53,000 --> 01:05:54,000
Der 200-Jährige.

614
01:05:54,000 --> 01:05:57,000
Ursprünglich mal von Isaac Asimov als Kurzgeschichte geschrieben.

615
01:05:57,000 --> 01:06:00,000
Sowieso Isaac Asimov, der große Roboter, Science-Fiction-Autor.

616
01:06:00,000 --> 01:06:03,000
Und dann von Robert Silverberg zum Roman ausgearbeitet.

617
01:06:03,000 --> 01:06:07,000
Und auch mit, äh, nicht doch mit Dustin Hoffman verfilmt.

618
01:06:07,000 --> 01:06:12,000
Da geht es im Grunde um einen Roboter, der als Mensch anerkannt werden will.

619
01:06:12,000 --> 01:06:15,000
Mit relativ klassischem Motiv so in den 70er Jahren.

620
01:06:15,000 --> 01:06:17,000
In der Science-Fiction.

621
01:06:17,000 --> 01:06:20,000
Und dann gibt es einen Roman zwischen zwei Sternen von Becky Chambers.

622
01:06:20,000 --> 01:06:24,000
Sowieso eine ganz tolle Autorin, von der lohnen sich im Grunde alle Romane.

623
01:06:24,000 --> 01:06:27,000
Aber inzwischen zwei Sternen geht es im Grunde um genau das gegenteilige Problem.

624
01:06:27,000 --> 01:06:31,000
Um einen Roboter, der sich als Mensch ausgeben muss, das aber eigentlich nicht will.

625
01:06:31,000 --> 01:06:33,000
Oh, das ist auch gut.

626
01:06:33,000 --> 01:06:36,000
Also es ist ein schöner Twist zu diesem sehr klassischen Motiv.

627
01:06:36,000 --> 01:06:39,000
Also diese beiden Romane, der 200-Jährige und zwischen zwei Sternen,

628
01:06:39,000 --> 01:06:41,000
die kann man glaube ich auch mal direkt hintereinander lesen,

629
01:06:41,000 --> 01:06:43,000
um diesen Unterschied sich ein bisschen deutlich zu machen.

630
01:06:43,000 --> 01:06:47,000
Auch wie sich die Science-Fiction als Genre verändert hat, lustigerweise.

631
01:06:47,000 --> 01:06:48,000
Das ist gut.

632
01:06:48,000 --> 01:06:54,000
Das kommt tatsächlich in dem Buch, also Roboter-Ethik auch, zwischendurch eben mal vor.

633
01:06:54,000 --> 01:06:57,000
Ich glaube bei dieser Haraway eben, wo so ein bisschen aufgemacht wird,

634
01:06:57,000 --> 01:07:01,000
dass so die Grenzen zwischen Science-Fiction und Science irgendwie

635
01:07:01,000 --> 01:07:07,000
und der Wirklichkeit absolut verwischen und gar nicht so klar sind,

636
01:07:07,000 --> 01:07:10,000
wie man das vielleicht sich manchmal vorstellt.

637
01:07:10,000 --> 01:07:13,000
Ich finde, man kann das jetzt auch gerade, merkt man das wieder,

638
01:07:13,000 --> 01:07:26,000
dass so die Vorstellungen von Dystopien auch massiv von Filmen und Büchern geprägt sind.

639
01:07:26,000 --> 01:07:30,000
Schönen Artikel zu, den ich auch gerne den Jones verlinken kann.

640
01:07:30,000 --> 01:07:31,000
Ja, sehr gut.

641
01:07:31,000 --> 01:07:34,000
Was mir noch gerade spontan einfällt, weil ich gerade Isaac Asimov erwähnt habe,

642
01:07:34,000 --> 01:07:39,000
tauchen in dem Buch irgendwo seine drei Robotik-Gesetze auf,

643
01:07:39,000 --> 01:07:41,000
zumindest mal in der Fußnote oder in einem Nebensatz?

644
01:07:41,000 --> 01:07:49,000
Warte, ich gehe mal ganz kurz hinten ins Literaturverzeichnis, aber zitiert ist

645
01:07:49,000 --> 01:07:53,000
The Complete Robot, The Definitive Collection of Robot Stories.

646
01:07:53,000 --> 01:07:55,000
Okay, da wird es definitiv drin sein.

647
01:07:55,000 --> 01:07:59,000
Ja, aber mir ist das jetzt zumindest nicht aufgefallen.

648
01:07:59,000 --> 01:08:06,000
Weil es gibt irgendwie drei Roboter-Gesetze, ich weiß nicht, ob ich sie jetzt gerade auswendig hinkriege.

649
01:08:06,000 --> 01:08:10,000
Das erste Gesetz ist auf jeden Fall, Roboter dürfen Menschen niemals schaden.

650
01:08:10,000 --> 01:08:11,000
Ja.

651
01:08:11,000 --> 01:08:15,000
Das zweite Gesetz ist, glaube ich, Roboter müssen Menschen immer helfen.

652
01:08:15,000 --> 01:08:21,000
Und das dritte Gesetz ist, Roboter müssen immer die Befehle ihres Inhabers oder wie auch immer gehorchen.

653
01:08:21,000 --> 01:08:26,000
Jeweils darf dadurch das vorherige Gesetz aber nicht verletzt werden.

654
01:08:26,000 --> 01:08:31,000
Also du musst zwar Befehle befolgen, darfst dadurch aber niemandem schaden, sozusagen.

655
01:08:31,000 --> 01:08:37,000
Das ist so ein Versuch, damit umzugehen, aber da verlinken wir euch auch die korrekten Sachen in den Show Notes.

656
01:08:37,000 --> 01:08:38,000
Sehr gut.

657
01:08:38,000 --> 01:08:44,000
Ich finde auch da merkt man, wie das so, also Asimov eigentlich als Science Fiction Autor,

658
01:08:44,000 --> 01:08:49,000
wie das dann aber vermutlich jetzt auch in die Wirklichkeit von Roboter-Programmierung zurück spielt.

659
01:08:49,000 --> 01:08:50,000
Natürlich.

660
01:08:50,000 --> 01:08:56,000
Und das halt eben nicht irgendwie der Science Fiction Autor bleibt, sondern halt in die Realität einfach zurückgeht.

661
01:08:56,000 --> 01:09:02,000
Naja, im Vorlauf zu diesem Buch oder während ich es gelesen habe, habe ich mit Jennifer,

662
01:09:02,000 --> 01:09:06,000
mit der ich auch das Soziologische Kaffeekränzchen zusammen mache und die auch einen eigenen Podcast hat,

663
01:09:06,000 --> 01:09:09,000
den Föllefanzcast, auch darüber gesprochen ein bisschen.

664
01:09:09,000 --> 01:09:13,000
Und sie meinte, naja, wenn du hier diese Bücher da empfiehlst und vorschlägst,

665
01:09:13,000 --> 01:09:17,000
schlag doch mal mit vor Maschinen wie ich von Ian McEwan.

666
01:09:17,000 --> 01:09:24,000
Habe ich jetzt selbst nicht gelesen, scheint momentan aber irgendwie ganz schön viel Bass quasi drum zu sein.

667
01:09:24,000 --> 01:09:27,000
Und ich weiß nicht, du weißt da glaube ich ein bisschen mehr.

668
01:09:27,000 --> 01:09:29,000
Ich habe den Roman selber auch nicht gelesen.

669
01:09:29,000 --> 01:09:34,000
Er wird so in Science Fiction Kreisen so ein bisschen kritisch gesehen, weil das jetzt wieder so ein Fall ist,

670
01:09:34,000 --> 01:09:41,000
in Anführungszeichen literarischer Autor macht ein Thema auf, was die Science Fiction seit 30, 40 Jahren diskutiert.

671
01:09:41,000 --> 01:09:45,000
Und bei der Science Fiction hört irgendwie nie jemand zu und jetzt springen auf einmal alle auf diesen Autoren.

672
01:09:45,000 --> 01:09:48,000
Und er ist so der große Experte für das Thema.

673
01:09:48,000 --> 01:09:53,000
Dadurch wird das da so ein bisschen kritisch gesehen. Der Roman an sich soll aber tatsächlich relativ gut sein.

674
01:09:53,000 --> 01:09:56,000
Okay, das ist ja schon mal ganz schön.

675
01:09:56,000 --> 01:10:01,000
Was ich vor einigen Jahren gelesen habe, ist Germany 2064 von Martin Walker,

676
01:10:01,000 --> 01:10:05,000
der seinerseits schotte ist, wenn ich es jetzt vorher richtig nachgeschlagen habe,

677
01:10:05,000 --> 01:10:11,000
finde ich einen ganz schönen Zukunfts Roman oder Thriller oder wie man es nennen möchte.

678
01:10:11,000 --> 01:10:16,000
Das ist im Prinzip ein Kriminalfall. Ich habe aber vergessen, worum es um eigentlichen Krimi geht, also was der eigentliche Kriminalfall ist.

679
01:10:16,000 --> 01:10:25,000
Aber es gibt eben einen Inspector, der 2064 an seiner Seite als engsten Mitarbeiter einen Roboter hat,

680
01:10:26,000 --> 01:10:34,000
der, glaube ich, direkt zu Beginn quasi einen Upgrade erfährt, weil er in einem vorherigen Kriminalfall zu Bruch gegangen ist

681
01:10:34,000 --> 01:10:37,000
und danach deutlich intelligenter ist, als er es vorher ist.

682
01:10:37,000 --> 01:10:41,000
Und die beiden sind auch gut befreundet miteinander, also der Roboter und der Inspector.

683
01:10:41,000 --> 01:10:44,000
Und die Welt, in der das Ganze stattfindet, ist eben Deutschland.

684
01:10:44,000 --> 01:10:50,000
Und Deutschland ist so ein bisschen zweigeteilt, einerseits ein Hochtechnologieland mit allem, was dazugehört,

685
01:10:50,000 --> 01:10:56,000
irgendwie fliegende Autos, selbstfahrende Autos und kluge Maschinen wie eben auch diese Roboter.

686
01:10:56,000 --> 01:11:00,000
Es gibt aber parallel dazu auch Kommunen, die dem Ganzen entsagt haben.

687
01:11:00,000 --> 01:11:04,000
Das muss man sich, glaube ich, vorstellen wie so ein paar Enklaven quasi in Deutschland.

688
01:11:04,000 --> 01:11:10,000
Nicht wie richtig zweigeteilt, es gibt ein Land, das ist so, ein Land, das ist so, sondern es gibt eben so ein paar Kommunen,

689
01:11:10,000 --> 01:11:15,000
die machen mal dem ganzen Kram nicht mit, die sind irgendwie wieder ziemlich zurück auf ganz normale Landwirtschaft,

690
01:11:16,000 --> 01:11:20,000
haben, glaube ich, auch so rudimentäre Technik. Ich glaube, die haben sich irgendeinen Stichtag gesetzt,

691
01:11:20,000 --> 01:11:25,000
irgendwas in den 90er Jahren oder so. Bis dahin benutzen sie alles und danach nicht mehr.

692
01:11:25,000 --> 01:11:30,000
Also es ist aber gar kein großes Gegeneinander, aber es spielt eben in beiden Welten so ein bisschen.

693
01:11:30,000 --> 01:11:35,000
Und ich glaube, der Kommissar muss eben in beiden Bereichen ein bisschen ermitteln.

694
01:11:35,000 --> 01:11:38,000
Das habe ich damals auf jeden Fall gerne gelesen.

695
01:11:38,000 --> 01:11:41,000
Und was ich noch mit empfehlen würde, ist ein Interview.

696
01:11:41,000 --> 01:11:44,000
Ich weiß nicht, ich glaube, es ist ungefähr eine halbe Stunde oder ein bisschen mehr.

697
01:11:44,000 --> 01:11:53,000
Janina Loh war im Zuge des Buches bei Sein und Streit einer empfehlenswerten Sendung ganz generell vom Deutschlandfunk Kultur.

698
01:11:53,000 --> 01:11:55,000
Und das verlinke ich euch auch noch.

699
01:11:55,000 --> 01:12:01,000
Grundsätzlich Deutschlandfunk hat eigentlich so mittlerweile gefühlt die besten Radiosendungen.

700
01:12:01,000 --> 01:12:06,000
Ich höre fast nur noch Deutschlandfunk, das hätte ich mir vor einem halben Jahr noch nicht vorstellen können.

701
01:12:06,000 --> 01:12:11,000
Genau, ja, das war doch mal wieder ein echt spannendes Buch mit viel Diskussionsansatz.

702
01:12:11,000 --> 01:12:16,000
Habt ihr, glaube ich, auch gemerkt, dass wir ein bisschen mehr ins Diskutieren gekommen sind als sonst oft bei den Büchern.

703
01:12:16,000 --> 01:12:18,000
Sehr schön und super ausgesucht.

704
01:12:22,000 --> 01:12:26,000
Genau, jetzt bleibt uns im Grunde nur noch, euch wieder aufzurufen,

705
01:12:26,000 --> 01:12:32,000
dass vielleicht jetzt die kontroversen Diskussionen als Anlass zu nutzen, uns irgendwie einen Kommentarbeitrag,

706
01:12:32,000 --> 01:12:39,000
eine Frage oder irgendwas zuzuschicken, die ihr mal hier im Podcast aufgegriffen haben möchtet zu diesem Buch.

707
01:12:39,000 --> 01:12:41,000
Das würden wir am Anfang der nächsten Episode machen.

708
01:12:41,000 --> 01:12:44,000
Also habt ihr jetzt so zwei Wochen Zeit oder so vielleicht was einzuschicken.

709
01:12:44,000 --> 01:12:48,000
Macht das im Idealfall natürlich einfach als Audiophile.

710
01:12:48,000 --> 01:12:51,000
Es reicht, wenn ihr die kurz mit dem Handy aufnehmt, irgendwo in der ruhigen Ecke.

711
01:12:51,000 --> 01:12:55,000
Dann können wir das direkt in den Podcast einbinden, was natürlich ganz toll ist.

712
01:12:55,000 --> 01:13:00,000
Aber ihr könnt es auch einfach als Kommentar unter dem Beitrag auf der Webseite machen.

713
01:13:00,000 --> 01:13:04,000
Den erreicht ihr unter zwischenzweideckeln.de

714
01:13:08,000 --> 01:13:10,000
Da findet ihr die aktuelle Episode.

715
01:13:10,000 --> 01:13:15,000
Und da findet ihr auch Links zu allen Möglichkeiten, wie ihr zwischen zwei Deckeln abonnieren könnt.

716
01:13:15,000 --> 01:13:21,000
Im Grunde auf der Plattform eurer Wahl, mittlerweile auch bei Spotify, zu hören.

717
01:13:21,000 --> 01:13:24,000
Da findet ihr auch Links zu unseren Social Media Aktivitäten.

718
01:13:24,000 --> 01:13:29,000
Wir sind auf Instagram und Twitter jeweils als atdeckeln zu finden.

719
01:13:29,000 --> 01:13:32,000
Bei Facebook gibt es zwischen zwei Deckeln eine Fanseite.

720
01:13:32,000 --> 01:13:37,000
Da freuen wir uns auch über Likes, Kommentare, Shares und was da alles so geht.

721
01:13:37,000 --> 01:13:42,000
Und wenn ihr jetzt diese Episode und auch vielleicht unsere anderen Episoden toll fandet,

722
01:13:42,000 --> 01:13:49,000
dann freuen wir uns natürlich auch riesig, wenn ihr auf den diversen Plattformen vielleicht eine Rezension hinterlasst oder zumindest ein paar Sterne.

723
01:13:49,000 --> 01:13:54,000
Das ist auch ganz toll, macht uns Motivation, hier noch weiter zu machen.

724
01:13:54,000 --> 01:13:57,000
Aber auch keine Angst, wir haben jetzt nicht vor aufzuhören.

725
01:13:57,000 --> 01:14:02,000
Das motiviert uns nun noch mehr und gibt uns natürlich auch nochmal ein gutes Gefühl, dass wir hier irgendwie was tun,

726
01:14:02,000 --> 01:14:06,000
was andere Leute gerne hören und auch vielleicht irgendwie unterhält oder informiert.

727
01:14:07,000 --> 01:14:10,000
Genau, hast du noch irgendwelche Sachen, die du ansprechen wolltest, Christoph?

728
01:14:10,000 --> 01:14:15,000
Ich glaube, damit hast du vollumfänglich abgedeckt, was wir so am Ende zu sagen haben.

729
01:14:15,000 --> 01:14:17,000
Von daher bleibt mir nur danke zu sagen, dass sie zugehört habt.

730
01:14:17,000 --> 01:14:22,000
Und ja, bis zur nächsten Folge quasi. Und tschüss!

731
01:14:27,000 --> 01:14:29,000
Untertitel im Auftrag des ZDF, 2021

