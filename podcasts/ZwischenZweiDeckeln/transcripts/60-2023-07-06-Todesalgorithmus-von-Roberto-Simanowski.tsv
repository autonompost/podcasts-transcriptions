start	end	text
0	23660	Hallo und herzlich willkommen zu Episode 59 von Zwischen zwei Deckeln, eurem 3-wöchentlichen
23660	31340	Sachbuchpodcast. Mein Name ist Nils und ich habe heute die Amanda dabei. Hallo Amanda. Hallo zusammen.
31340	40180	Ja und bevor wir uns deinem Buch und deinem Themen sozusagen widmen, vielleicht ganz kurz so ein
40180	46300	bisschen was, was uns beschäftigt. Mich beschäftigt gerade ganz stark. Ich lese gerade eine Biografie von
46300	52420	einem Bauhausarchitekten. Ich habe wieder vergessen, ob er Franz oder Fritz ehrlich heißt. Ich kann es
52420	59460	mir nicht merken und das so ganz spannend ist. Es ist also kein ganz berühmter Architekt aus dem
59460	64580	Kontext. Ich hatte den Namen vorher auch noch nie gehört, aber er zeigt sehr schön so diese
64580	68900	diese Entwicklungen und wie sich das Bauhaus mit seiner Technikoffenheit und seinem Blick so
68900	76100	auf Funktionen und Moderne, wie sich das irgendwie ja auch dann auf die Architektur der Nazis und
76100	84260	auf die Architektur der DDR ausgewirkt hat. Und da eben auch mal zu sehen, dass das Bauhaus als
84260	88340	eine Kunst und Design und Gestaltungsrichtung halt nicht nur irgendwie so einen so einen positiv
88340	95140	westlichen Twist hat, sondern auch einen ganz starken Einfluss oder ähnlichen Grundlagen unterlag,
95140	101100	irgendwie wie auch zum Beispiel die Naziarchitektur beispielsweise in den KZs. Das ist wirklich extrem
101100	105740	spannend zu lesen. Das ist eben ein Beispiel dieses Herrn Ehrlich, der eben sowohl ein Bauhaus
105740	110500	lehrte, aber auch Architektur in Buchenwald betrieben hat. Er ist als Häftling und dann als
110500	119060	Angestellter. Ganz, ganz spannend zu lesen. Wie bist du drauf gekommen? Das ist eine gute Frage.
119060	123180	Also Bauhaus ist ein Thema, was mich schon länger beschäftigt. Das ist jetzt tatsächlich für mich
123180	127660	gar nicht neu. Ich weiß nicht, wo ich das Buch zum ersten Mal gesehen habe und dann habe ich
127660	133260	irgendwo in Karlsruhe in der Buchhandlung bei 2001 auf einen halben Preis reduziert liegen sehen.
133260	138260	Und da habe ich es mitgenommen. Das ist gefangen in der Titotalitätsmaschine von Friedrich von
138260	143540	Borjes und Jens-Uwe Fischer. Ich weiß gerade nicht, wie der Herr Fischer mit Vornamen heißt,
143540	150860	aber irgendwas in der Art. Das ist ein Surkampband. Ja, also wenn man sich so für Bauhausarchitektur,
150860	155980	auch gerade moderne Architektur, ersten Hälfte des 20. Jahrhunderts interessiert, ist das wirklich
155980	161220	ein äußerst empfehlenswertes Buch. Klingt spannend. Wahrscheinlich nicht gehaltvoll genug,
161220	167820	um es jetzt hier in den Podcast zu bringen, aber als Lesetipp vielleicht ganz cool. Ja,
167820	175700	was beschäftigt dich gerade so? Ich hatte nicht so viel Zeit zu lesen in letzter Zeit. Ich musste
175700	185740	so ein paar fachliche Dinge zwangslesen, aber ich habe ein schönes kleines Sammelbändchen gefunden.
185740	194180	Das heißt Ungerechte Ungleichheiten und wurde herausgegeben von Steffen Mao. Und ja, es ist ganz
194180	200220	cool, weil das so verschiedene Themen in kurzen Essays von unterschiedlichen AutorInnen behandelt.
200220	206300	Also es geht da um Bildung, es geht um Markt, um den Sozialstaat, Geschlecht, Erben und so weiter.
206300	213980	Und es sind ganz kurze Beiträge, also ein paar Seiten und auch von AutorInnen, die wir hier
214020	220300	schon im Podcast gehört haben. Steffen Mao hatten wir schon. Hartmut Rosa, was haben wir noch?
220300	227260	Stefan Lesse nicht, da habe ich mal was vorgestellt von dem Philosophikum Lech. Also auf jeden Fall
227260	234060	lesenswert und es geht ganz gut so. Ein Beitrag vor dem Schlafen. Wie hieß das Buch Ungleiche
234060	241260	Ungleichheiten? Ungerechte Ungleichheiten und das Un ist so un eingeklammert bei beiden. Auch vom
241260	251340	Sohrkampf. Spannend. Gut, das Thema, was du uns heute mitgebracht hast, ist ein bisschen ein
251340	258180	anderes. Stimmt. Da geht es nicht um Ungleichheiten, zumindest nicht auf den ersten Blick. Du hast ein
258180	264860	Buch mit einem sehr provokanten Titel mitgebracht. Ich bin mal sehr gespannt, was der hält, weil es
264860	268660	auch ein Thema ist, was ja sehr aktuell ist. Und ich weiß jetzt gerade nicht, wie aktuell ist das
268660	275140	Buch. Wann ist das erschienen? Das ist, ich muss schnell drinnen nachschauen. Ich weiß es nicht
275140	281100	aus, wenn nicht. Das ist 2020. Okay, also tatsächlich schon vor dem aktuellen Hype sozusagen. Es geht
281100	286860	nämlich ums Thema künstliche Intelligenz. Und das Buch heißt Todesalgorithmus, das Dilemma der
286860	291980	künstlichen Intelligenz von Roberto Simonowski. Und der Gute ist, wenn ich das richtig gesehen habe,
291980	297300	Kultur- und Medienwissenschaftler. Irgendwie viele im englischsprachigen Ausland unterwegs gewesen,
297300	303780	jetzt aber mittlerweile in Berlin. Ja, ich bin sehr gespannt. Magst du uns das TLDL geben? Ja.
303780	313220	In dem Essay Todesalgorithmus, das Dilemma der künstlichen Intelligenz, geht Roberto Simonowski
313220	317780	den Themen und Fragen auf den Grund, die heute und zukünftig in Zusammenhang mit künstlicher
317780	326420	Intelligenz relevant sind. Er nimmt dazu als roten Faden den Algorithmus in selbstfahrenden Autos.
326420	339860	Das ist ja so das Standardbeispiel der KI-Debatte vor 2022 sozusagen. Ich bin gespannt.
339860	347620	Ja, es ist so, es ist ein Essay. Das Büchlein ist ein bisschen mehr als 100
347620	353660	Seiten lang. Und auch in der Einleitung sagt er, er macht das im Stil von einem
353660	358220	vagabundierenden Denken. Und das ist auch so aufgebaut. Also das ist irgendwie nicht so
358220	362780	eine systematische Abhandlung von dem Thema, sondern er macht viele Beispiele, macht viele
362780	371380	aktuell popkulturelle Referenzen, kann man schon fast sagen. Aber im letzten Teil geht es auch
371380	376620	sehr viel dann um philosophische Themen. Das werde ich wahrscheinlich gar nicht so hier behandeln,
376620	381660	sondern es geht auch einfach ein bisschen darum, ja, um die spannenden Gedanken und Gedankenexperimente,
381660	390180	die halt dieses Thema auch mit sich bringt. Es ist, vielleicht muss man zu Beginn schnell den
390180	394980	Begriff klären von schwacher und starken künstlichen Intelligenz. Ich finde das im
394980	400700	Deutschen nicht so ganz gelungen. Im Englischen heißt das Narrow und General Intelligence.
400700	408460	Und die schwache künstliche Intelligenz, das ist eigentlich das, womit wir auch heute tatsächlich
408460	412780	zu tun haben. Also das ist das, was bei uns in den Smartphones steckt. Das ist aber auch eben
412780	420580	das dieses Autonomofahren. Das ist also diese Art von Technologie, die eine ganz spezifische
420580	427180	Aufgabe lösen kann. Und natürlich eventuell besser oder schneller oder was auch immer als Menschen,
427180	433300	deswegen der Begriff Intelligenz, ist aber ganz was anderes von dieser starken, von dieser General
433300	440700	Intelligence, Artificial Intelligence, wo es dann eher darum geht, dass man halt dieses Problemlösungsverhalten
440700	445980	auch auf andere Kontexte anwenden kann. Und das gibt es im Moment nicht. Also diese starke
445980	452820	künstliche Intelligenz, die existiert zurzeit nicht. Ja, wobei diese aktuelle Debatte um jetzt
452820	458940	LGBT und Co., da fällt das ja immer mal wieder, dass das jetzt so in die Richtung ginge. Genau,
459260	468540	das ist natürlich, aber auch das ist natürlich trotzdem ein sehr begrenztes Feld. Der Output ist
468540	476300	immer noch sehr klar umschrieben. Das ist einfach eigentlich eine probabilistische Ausgabe von
476300	481220	Wörtern. Was könnte jetzt als nächstes kommen für ein bestimmtes Thema? Es ist halt sehr geschickt,
481220	486540	darin so zu tun, als wäre es mehr. Es erweckt halt den Eindruck, als könnte es mehr, als einfach nur
486540	491300	Texte generieren, wie du sagtest, so eine bessere Autovervollständigung. Aber faktisch ist es dann
491300	497020	halt doch nur das und das wird oft nicht so ganz gesehen. Das stimmt, das stimmt. Das ist aber,
497020	503980	das ist auch das Problem. Also die Debatten sind auch immer, also das Spannende oder das,
503980	507820	was halt sexy ist an dieser Diskussion, ist natürlich diese starke künstliche Intelligenz und die
507820	511860	Gefahren, die damit dann hergehen und so weiter. Aber man muss sich schon bewusst daneben. Das gibt
511860	518620	es im Moment nicht und auch der Zeitpunkt, wann es diese Art von Technologie geben wird, ist nicht
518620	523020	klar. Da gibt es Stimmen, die sagen in zehn Jahren oder in fünf Jahren und dann gibt es andere,
523020	529980	die sagen ja das vielleicht gar nie und so weiter. Also das ist noch nicht, steht nicht direkt vor
529980	533980	unserer Tür. Das ist glaube ich auch ein Thema, wo es seit irgendwie 30 Jahren heißt, das gibt es in
533980	540740	zehn Jahren. Ist das so? Ich weiß nicht. Ich habe so ein bisschen das Gefühl, dass das immer zehn
540740	546380	Jahre weg ist, egal wann man fragt. Ja, das kann gut sein. Also in der Science Fiction Literatur
546380	552140	wurde das natürlich schon sehr früh aufgegriffen. Also das ist schon, in den Köpfen von den Menschen
552140	558900	steckt das schon drin. Ja, aber auf jeden Fall, also diese Unterscheidung gliedert so ein bisschen
558900	566020	auch diesen Essay. Also es beginnt am Anfang eben dieser schwachen Intelligenz und diesem Todesalgorithmus
566020	571060	im Auto und erweitert das dann sozusagen immer schrittweise aus. Dann in so einem Mittelteil geht
571060	579540	es dann um die starke Intelligenz und im letzten Teil dann eher so um eine Integration und wie man
579540	583860	das dann mit Hegel verbinden kann und so weiter. Das ist dann sehr hypothetisch und ich habe auch
583860	591860	nicht alles verstanden wahrscheinlich. So oft, wenn es um Hegel geht, ne? Genau. Ja, also beginnen
591860	600140	du das Ganze mit einem Beispiel, mit einem Werbespot. Vielleicht kennst du den. Es ist ein Werbespot,
600140	606500	der von einer Kunstakademie, glaube ich, herausgegeben wurde. Es ist ein Fake-Werbespot und da sieht man
606500	614460	ein Auto, das fährt auf einer Straße in Braunau und das ist ein selbstfahrendes Auto und dann
614460	619820	kommt ein Kind vor das Auto. Also es läuft, es spielt mit einem Ball, läuft vor das Auto und
619860	627420	wird von dem Auto umgefahren. Oha. Und dann zoomt das so raus und man sieht dann so die Gliedmaßen
627420	634460	des Kindes formen dann so ein Hakenkreuz und die Quintessenz ist eigentlich, dass dieses
634460	641300	selbstfahrende Auto die Gefahr erkannt hat, die von diesem Kind ausgeht und dieses Kind sollte
641300	648420	Adolf Hitler darstellen. Ah, okay. Sag dir das was, dass du davon hast, das mal gesehen. Ah, ganz,
648660	652900	ich erinnere mich ganz dunkel an irgendwie sowas, aber ich habe keine Bilder dazu im Kopf. Also ich
652900	657460	habe gerade dieses Hakenkreuzbild müsste ja wahrscheinlich eigentlich relativ einprägsam
657460	662380	sein. Insofern vermute ich mal, dass ich diesen konkreten Spot nicht gesehen habe. Aber so dieses
662380	666620	Motiv, dass irgendwie auch Zeitreisende irgendwie Hitler als Kind umbringen oder so, das ist
666620	675340	natürlich nicht ganz fremd. Genau, ja. Also es ist, ich finde es sehr ins Pferd dieses Video einfach,
675340	681340	weil es macht es schon, also es bringt halt das ziemlich gut auf den Punkt. Also diese Frage nach,
681340	686980	unter welchen Umständen darf man eigentlich töten, um Leben zu retten? Ganz grundsätzlich,
686980	693460	aber auch ganz spezifisch, was bedeutet das, wenn wir Maschinen haben, die all das über uns wissen?
693460	702220	Also man sieht im Spot nun ein Kind, das mit einem Ball spielt. Und im Essay geht es eben zuerst
702420	712940	um diese Frage, was bedeutet das? Darf man töten, um Leben zu retten? Und aktuell gibt es das schon
712940	718060	so ein bisschen auf eine Art und Weise was Ähnliches, was man so beim Predictive Policing
718060	724060	hat. Also diese Vorhersagen der Analyse, was man im Moment auch, basierend auf Daten vor
724060	731540	allen Dingen, versucht vorauszusehen, wann jemand kriminell werden könnte. Oder diese
731580	738300	Täterprofile erstellt. Und also das gibt es schon so ein bisschen. Ich glaube, das wird auch ganz
738300	742700	viel mit Räumlichkeit gemacht. Also ich kenne das irgendwie mehrfach gelesen, dass man so versucht,
742700	746700	gar nicht mal herauszufinden, wer begeht als nächstes ein Verbrechen, sondern wo wird als
746700	751740	nächstes ein Verbrechen begangen. Was natürlich wesentlich unproblematischer ist, weil mal eben
751740	755420	eine Streife irgendwo lang fahren zu lassen, um zu gucken, ob wirklich was passiert. Das ist
755420	760020	natürlich wesentlich weniger invasiv, als mal in Anführungszeichen mal einfach so irgendjemanden
760020	766060	festzunehmen. Ja, und da hast du schon gleich wieder dieses inhärente Problem. Wenn du das dann
766060	772100	machst, dann hast du eine Intervention und die Daten. Also du weißt ja dann nicht, ob das
772100	780180	tatsächlich passiert wäre. Also was ist jetzt, was hat das verhindert? Die Streife oder war die
780180	788780	Vorhersage falsch? Das ist ein anderes Thema. Basierend von dieser Frage, bringt er dann das
788780	793300	Beispiel von dem Buch und vor allen Dingen auch von dem Film, ich glaube ich ist es,
793300	802020	ein Theaterstück von Chirach, das heißt Terror. Und das war von ein paar Jahren, wurde das
802020	809340	aufgeführt, auch im deutschen Fernsehen. Und da geht es darum, dass eigentlich ein Flugzeug mit
809340	816260	160 Menschen an Bord wird von einem Terroristen oder einer Terroristengruppe gekapert und auf die
816260	821140	Allianz Arena in München zugesteuert. Die ist voll, die hat 70.000 Menschen da drin und dann geht
821140	826980	es eigentlich darum, was soll man machen, weil es gibt die Möglichkeit, dieses Flugzeug abzuschießen.
826980	833500	Und man hat dann eigentlich das Ganze so aufgebaut, dass dann das Publikum partizipativ das entscheiden
833500	842060	kann, was man macht. Genau, und das wurde an vielen Orten aufgeführt und gemacht. Das ist
842540	854660	wie so eine empirische Studie schon fast. Also was machen die Menschen? Und interessant ist,
854660	862420	dass wenn man jetzt diesen Outcome hat, dass das Flugzeug abgeschossen wurde, also dass eigentlich
862420	868020	diese 160 Menschen im Flugzeug geopfert wurden, zugunsten dieser Tausenden in der Arena,
868020	876700	dass das eigentlich für die meisten in Deutschland jetzt okay ist. So im Sinne von die Personen,
876700	883500	also der General in diesem Fall wurde freigesprochen. Und das steht aber eigentlich
883500	890540	nicht, also das ist unvereinbar mit dem deutschen Grundrecht. Also Menschenleben dürfen nicht
890540	895980	aufgerechnet werden. Und spannend ist natürlich, dass eigentlich intuitiv dann die Mehrheit von
896060	904900	diesen Personen verfassungswidrig fühlt sozusagen. Und das ist so ein bisschen eines dieser
904900	910940	Grundprobleme. Und er sagt dann auch, und das finde ich interessant, dass Deutschland ganz besonders
910940	920660	damit oder mit diesen Technologien oder mit dieser Frage ein Sonderfall darstellt, weil hier diese
920660	926340	Pflichtenethik, wir kennen das von Kant, also dieses deontologische, sehr eigentlich tiefer
926340	932060	ankert ist. Im Gegensatz jetzt das zu der konsequenzialistischen Ethik, also dem
932060	936740	Utilitarismus klassischerweise, kennt man das, was zum Beispiel im angelsächsischen Raum viel
936740	947300	verbreiteter ist als jetzt bei uns. Und das ist, ja finde ich spannend und ich finde es auch
947300	955620	interessant, dass weil 2006 offenbar das Luftsicherheitsgesetz, so heißt es, hat genau
955620	962300	einen Absatz drin, wo es um dieses Abschießen von Flugzeugen bei Terrorgefahr geht. Und das
962300	968700	wurde aber vom Bundesverfassungsgericht als nicht verfassungswidrig eingestuft. Also wurde
968700	978900	wie abgekanzelt, aber die Menschen scheinen das eigentlich gut zu heißen. Ich habe dann so ein
978900	984300	bisschen geschaut bezüglich der Menschenwürde, das ist auch nämlich ein Konzept, was nicht in
984300	990420	allen Verfassungen steht, also Deutschland hat das drin, ein paar andere EU-Länder hat das auch,
990420	996180	die Schweiz hat das auch drin in der Bundesverfassung und dann noch irgendwie Südafrika und Kenia,
996180	1002220	aber sonst ist das gar nicht oft explizit erwähnt. Das ist diese Menschenwürde, sondern eher dann
1002220	1009620	so Freiheit und was man sonst so darunter subsumieren könnte, aber die Menschenwürde als
1009620	1019900	Prinzip so erwähnt ist, da ist auch Deutschland so ein bisschen ein Sonderfall. Wenn man das
1019900	1028180	philosophisch betrachtet, dann ist dieses Problem von dieser Aufwiegung von Menschenleben das
1028180	1035380	klassische Trolley-Problem, dieses Weichensteller-Problem, das kennen die meisten. Straßenbahn auf der einen
1035380	1041980	Seite tötet sie fünf Menschen, auf der anderen tötet sie eine Person, ist es okay, wenn man da die Weiche
1041980	1048860	umstellt. Dann gibt es so die ziemlich provokante Variante mit dem fetten Mann-Problem, hast du die
1048860	1055180	von schon gehört? Ich kann mir, glaube ich, grob was darunter vorstellen. Ja, geht einfach darum,
1055180	1060820	dass man die Straßenbahn stoppen kann, indem man einen fetten Mann davor wirft und die dann so
1060820	1066380	gestoppt wird. Also ist natürlich ganz provokativ und absichtlich so doof formuliert, aber das ist
1066380	1072580	so die nächste Variante und geht einfach darum, dass man natürlich Menschen nicht so im kantischen
1072580	1080260	Sinne als Zweck dafür verwenden bzw. als Mittel verwenden kann, sondern sie immer als Zweck sehen
1080260	1089740	sollte. Und wenn man jetzt das weiter, also ein anderes Beispiel von dieser empirischen Ethik,
1089740	1096860	eigentlich wie wir das bei diesem partizipativen Theaterstück sehen, gibt es auch sowas vom MIT,
1096860	1103180	das heißt Moral Machine. Das ist auch ganz spannend, das kann man sich noch anschauen,
1103180	1113060	das lief von 2016 bis 2020 und nimmt eigentlich genau dieses Weichenstellerproblem und man kann
1113060	1118140	dann, man konnte dann einfach so sagen, ja es gibt immer zwei Varianten, man konnte sich dann
1118140	1124100	für eine von beiden entscheiden und die wurden so konstruiert, dass da noch andere Dinge drin sind,
1124100	1130820	also man kann da entweder von jungen und alten Menschen, also man unterscheidet zwischen jungen
1130820	1137460	und alten z.B. oder tatsächlich es hat irgendwie übergewichtige Menschen da drin, also das ist
1137460	1144220	auch eine Kategorie oder halt eine Schwangere oder eine Obdachlose und dann gibt es so abstruse
1144220	1152060	Szenarien wie eben zwei Obdachlose fahren in einem selbstfahrenden Auto und sollten die eine
1152060	1156740	schwangere Frau überfahren, die aber über Rot, also als die Ampel auf Rot ist, die Straße überquert.
1156740	1162940	Also es ist irgendwie noch mehrschichtig und es kommt dann eben auch dieser Gesetzesverstoß
1162940	1170460	jetzt hier im Sinne von man geht bei Rot über die Straße, kommt dann auch noch dazu und der
1170460	1177460	Output davon ist auch, das wurde mit mehr als zwei Millionen Menschen haben das ausgefüllt auf der
1177460	1184060	ganzen Welt und nachher gibt es sehr starke regionale Unterschiede, also man hat dann gesehen,
1184060	1192740	dass in Lateinamerika und in Afrika beispielsweise Personen tendenziell eha die Kinder favorisieren,
1192740	1199260	also die Kinder nicht sterben lassen, dann in asiatischen Kulturen oder im asiatischen Raum
1199260	1210300	eher die Älteren bevorzugt werden und so, also dass es da wirklich ja auch regionale Unterschiede
1210300	1216060	gibt, macht irgendwo durch auch Sinn, aber stellt natürlich dann die Frage, ja wenn es jetzt
1216060	1220700	tatsächlich darum geht, das in einem Auto zu verbauen, wie soll das denn gehandhabt werden?
1220700	1226900	Ja, insbesondere auch eben wenn es dann um diese Regeln, also Südostasien und arabische Staaten
1226900	1231100	haben dann eher die verschont, die die Regeln befolgen beispielsweise.
1231100	1235300	Was hätte ich jetzt mir vorgestellt, dass das so im deutschsprachigen Raum oder zumindest in
1235300	1239660	Deutschland auch eine große Rolle gespielt hat, so wer die Regeln befolgt ist erstmal sicherer als
1239660	1244620	jemand der über die Straße geht, aber soweit geht unser deutscher Michel dann vielleicht doch nicht.
1244620	1251580	Weiß ich nicht, also müsste man die Auswertung genau anschauen, das ist jetzt nur so ganz grob
1251580	1255940	abgebrochen, übers Knie gebrochen, was für Tendenzen es gibt, aber man kann das auch,
1255940	1262180	ich glaube es gibt ein Buch dazu, das heißt The Car That Knew Too Much von Jean-Francois Bonnefond,
1262180	1268780	ich glaube das Buch fasst die Ergebnisse von diesem Moral Machine zusammen, also falls man
1268780	1275900	das da sich anschauen möchte. Auf jeden Fall ist also dieses Grundproblem ein bisschen auch an
1275900	1284300	diesen an diesen Beispielen, dass man hat wie oder das ist auch was Siemanowsky kritisiert,
1284300	1290260	man kann sich gar nicht dagegen entscheiden, du kannst gar nicht dieses, hier ist es ja ganz
1290260	1295140	klar utilitaristisch, man muss wie zwischen zwei Dingen abwägen, man kann sich dagegen gar nicht
1295140	1302020	entscheiden, du musst eine Entscheidung treffen. Nur schon wie das dieses Experiment aufgestellt
1302020	1308820	ist und das ist auch so ein bisschen das Problem mit diesem Todesalgorithmus, wie er den nennt im
1308820	1314860	Auto, dass der kann nicht nicht programmiert werden, oder? Man muss den irgendwie da rein
1314860	1320700	packen und jetzt in unserem Verständnis liegt der Gesetzesverstoß eigentlich in dieser
1320700	1327020	Vorentscheidung, also es ist nicht, es ist eben nicht diese Reaktion oder dieses reflexhafte,
1327020	1332460	was zu der Entscheidung führt, sondern dass man das so kaltblütig sag ich mal im Vornherein
1332460	1339660	programmiert. Ja und auch irgendwie formalisiert und allgemeingültig, also es ist ja, wenn jetzt
1339660	1343820	irgendwie eine Person fahren würde, die würde halt, also nehmen wir mal an, sie hätte Zeit zu
1343820	1349220	entscheiden in der Situation, was jetzt irgendwie nicht wirklich wahrscheinlich ist, aber sie hätte
1349220	1353260	Zeit zu entscheiden, dann würde sie halt nach ihren eigenen Biases und ihren eigenen Verzerrungen
1353260	1358380	irgendwie halt entscheiden und dann wäre das halt für diesen einen Fall, aber wenn du es, wenn du es
1358380	1364620	einem automatischen Auto gibst, dann ist es ja wahrscheinlich, also es kommt immer die Frage,
1364620	1368780	wie weit das vorprogrammiert ist oder sich quasi im Lernprozess ergibt, aber dann ist es halt
1368780	1374580	wahrscheinlich so, dass es halt immer dasselbe, die selbe Entscheidung treffen wird. Genau, ja und
1374580	1380900	das ist, damit sprichst du auch so ein bisschen mit einem Kernproblem an, was ich, also ich fand,
1380900	1385580	das bringt ein bisschen spät, aber ich finde halt den Gedanken interessant, wenn er sagt, wenn wir
1385580	1392380	von den Daten sprechen, wenn das jetzt, wenn jetzt alle Daten zur Verfügung stehen, dann ist es ja
1392380	1398740	nicht mehr dieses Partikulare, das fällt dann ja wie weg und es ist auch nicht per se die gute
1398740	1403900	Entscheidung, die dann getroffen wird, sondern einfach das Mittel aus allem, oder? Also siehst du,
1403900	1411020	dieses, dieses Gut und Schlecht, das fällt sowieso ein bisschen weg beim Algorithmus, aber es ist auch
1411020	1415660	nicht mehr, also es ist einfach so ein Mischmasch von der Mehrheit eigentlich, also die Mehrheit
1415660	1421260	gewinnt sozusagen in den Daten, weil das ist halt das, was die Überhand nimmt. Ja, wobei ich glaube,
1421260	1425300	da muss man noch mal gucken, ob es ein Mischmasch ist oder ob es sowas wie, wie du gerade sagtest,
1425300	1428980	die Mehrheit ist, also ein Mischmasch wäre ja irgendwie so, auch wieder vielleicht eine
1428980	1432580	probabilistische Entscheidung, in 60 Prozent der Fälle macht das so, in 40 Prozent der Fälle
1432580	1437740	macht das anders, die andere Variante wäre halt, die 51 Prozent würden es so machen, also macht
1437740	1442580	es das Auto immer so, das ist ja auch noch mal eher so eine Median, also man nimmt die Median als
1442580	1447340	absolut wert sozusagen, vielleicht eine Entscheidung wäre, dass sie da mal zwei Perspektiven. Stimmt,
1447340	1455140	das stimmt, aber das ist natürlich die Frage, ob der Median, das ist was, also ich meine,
1455140	1461540	ob das überhaupt so umgesetzt wird oder ob auch das natürlich das noch das Erstrebenswerte ist,
1461540	1465580	aber du hast recht, ja, also Mittelwert und Median können hier in dem Fall natürlich ganz
1465580	1470860	weit auseinander liegen, je nachdem, wie man es sich anschaut. Auf jeden Fall geht es dann
1470860	1476340	natürlich darum, wenn man diesen Todesalgorithmus, wie er den nennt, programmiert, stellt sich
1476340	1484060	natürlich die Frage, ja, wer hat den Vorrang, also wie kann man sowas entscheiden. Und was
1484060	1495220	ein spannender Gedanke ist, ist natürlich, ja, die AutoherstellerInnen haben natürlich, die würden
1495220	1502540	es am liebsten so gestalten, dass die Insassen immer Vorrang haben, weil klar, also bringt
1502540	1509460	natürlich einen Wettbewerbsvorteil für dein Auto. Du bist geschützt? Genau, du bist geschützt und
1509460	1515780	es ist so, dass offenbar empirische Studien gezeigt haben, er hat die nicht genau zitiert,
1515780	1522300	ich weiß nicht, wie die zustande gekommen sind, aber es ist wohl so, dass viele Personen eher
1522300	1527340	dazu neigen zu sagen, ja, eigentlich würde ich mich lieber opfern, also wenn es um so eine
1527340	1535860	Entscheidung geht, aber ich würde nicht so ein Auto kaufen, das mich opfert. Im abstrakten sind
1535860	1544900	wir gut, im konkreten dann vielleicht nicht so. Und hier eben, also diese, wer hat Vorrang, diese
1544900	1550780	Frage ist natürlich dann, die kann man ganz ad absurdum führen, wenn es eben darum geht,
1550780	1556220	was soll alles berücksichtigt werden. Wir haben jetzt hier ganz, ganz wenige Proxies, die uns so,
1556220	1561100	die naheliegen, das ist irgendwie das Alter, das kann ich mir vorstellen, es hat kulturell vielleicht
1561100	1565740	einen Einfluss eben, wie es schon erwähnt, aber nur schon wenn es dann darum geht, irgendwie Tiere,
1565740	1574060	was machen wir da, oder wenn es dann eben heißt, ja, diese KI, die hat dann plötzlich Zugang zu
1574100	1580060	allen möglichen Daten und sieht dann ja, okay, da hat es eine Fahrradfahrerin, aber die ist auch
1580060	1585220	Vielfliegerin und dann hat es hier eine Schwangere, aber die ist des Todes geweiht und dann hat es
1585220	1589940	noch ein älterer Herr, der hat schon das und das Gutes getan und wenn es dann zu einer Aufwiegung
1589940	1601340	kommt von, also das ist natürlich dann wirklich schwierig. Ja, und eben, also dieses, dieses
1601340	1606140	auswählen, also es ist klar, man kann das nicht einfach den Herstellern und Herstellenden von
1606140	1611380	Autos überlassen, diesen Algorithmus zu programmieren, aber es stellt sich ja auch die Frage,
1611380	1616780	ja, soll ich den selber auswählen dürfen, wenn ich ein Auto kaufe? Also, was mache ich denn so,
1616780	1621500	irgendwie Sonntagnachmittag im Park und dann klicke ich mir hier mein Todesalgorithmus zusammen für
1621500	1627340	mein nächstes Auto? Seltsam, oder? Ja, definitiv. Und wie macht man das? Also, gibt's, sollte man
1627340	1633260	dann verschiedene Möglichkeiten erlauben, soll man irgendwie die Leute dazu zwingen, keine Ahnung,
1633260	1639820	mit einem Ethikbeauftragten zu sprechen, bevor man so ein Auto kauft, gibt's dann irgendwie Autos auf
1639820	1646380	dem Schwarzmarkt, die dann eben einen anderen Algorithmus drin haben, den wir nicht so verwenden.
1646380	1649820	Also, das sind alles Fragen und das fand ich mega spannend, weil ich bin da nicht drauf gekommen und
1649820	1656380	er hat das dann so aufgebracht, ja, gibt natürlich keine Lösung dafür, aber auf jeden Fall muss man
1656380	1669620	darüber nachdenken. Ja, ganz grundsätzlich gibt es als Gegenargument dann so dieses statistische
1669620	1675740	Argument, das auch heißt, man darf, also, es klingt ja alles so ein bisschen, ja, sollte man das gar
1675740	1682820	nicht benutzen oder gar nicht erst anfangen, aber so, das Konterargument ist ja immer, ja, auch wenn
1682900	1686700	wir dann selbstfahrende Autos haben, wir können das natürlich nicht verhindern, dass das zu
1686700	1693740	Unfällen kommt, aber insgesamt retten wir trotzdem sehr viele Menschenleben dadurch, dass man davon
1693740	1699540	ausgeht, dass Unfälle sehr viel weniger werden und wir eigentlich eine Verpflichtung dazu haben,
1699540	1709100	dass diese Technologie auch zustande kommt deswegen. Und was ich auch ein sehr spannender und ein guter
1709100	1716020	Gedanke fand, ist, dass er sagt, das Risiko jetzt im Straßenverkehr, das geht ja nicht von Kindern
1716020	1723340	und Fahrradfahrern aus, sondern das geht vom Autoverkehr aus, der per se, also der Autoverkehr,
1723340	1727780	der sagt dann ja, das ist der, der schwere Gegenstände so schnell durch den Raum bewegt,
1727780	1732540	dass ein Zusammenstoß tödlich sein kann, das ist das Problem. Unfallopfer können schon gesenkt
1732540	1738460	werden mit selbstfahrenden Autos, aber die Opfer an sich sind Folge vom Autoverkehr. Ja, stimmt.
1738460	1749820	Finde ich auch. Kann man natürlich dann auf andere Dankendinge anwenden, das kann man sehr
1749820	1756260	weiter spinnen, aber trotzdem finde ich das ein sehr valides Argument und auch für die Konsequenz dann,
1756260	1761380	dass man sagt, ja, es kann natürlich schon angemessen sein, dass man dann sagt, die Opfer
1761380	1769540	sind die NutzerInnen dieser Mobilität. Ja, ich finde, da wird noch was anderes schön deutlich.
1769540	1774260	Da wird eben deutlich, dass wir diese Abwägung, nur halten ihn nicht ganz so plakativ, aber doch
1774260	1778460	auch täglich treffen. Ich meine, auch jetzt schon haben wir ja die Entscheidung getroffen,
1778460	1783180	dass, ich weiß nicht, wie viele es sind, irgendwie mehrere hundert Verkehrstote jedes Jahr, die
1783180	1788780	Bequemlichkeit und die schnelle Bewegung anderer Menschen wert sind. Sprich, anscheinend rechnen
1788780	1794220	wir Menschenleben sogar gegen etwas so Abstraktes und erstmal, na ja, maximal vielleicht ökonomisch
1794220	1799660	und irgendwie komfortmäßig relevantes, die Bewegungsgeschwindigkeit auf. Also wir tun es
1799660	1806420	da ja sogar schon. Naja, das ist recht. Eben per Gesetz. Und wenn man weiterdenkt, jetzt irgendwie
1806420	1812820	Klimaschutz oder Radverkehr in Berlin ist ja gerade großes Thema, da wird das mit jeder Entscheidung
1812820	1818260	wird das neu getroffen. Wer sozusagen ein Risiko eingeht, wessen Leben in Gefahr gebracht wird und
1818340	1822580	wem dadurch welcher Nutzen entsteht. Und da ist genau diese Abwägung, nur dass sie halt nicht so auf
1822580	1828860	diesen einen Punkt fokussiert ist, der uns allen dann sofort irgendwie in sich widersprüchlich und
1828860	1836580	konfliktär erscheint. Ja, ja, ja, das ist absolut recht. Ist auch irgendwie noch, man kann das dann
1836580	1841860	so ein bisschen ins Positive drehen, indem man dann sagt, ja, jetzt in dem Fall von selbstfahrenden
1841860	1847900	Autos, ja, ich nehme sozusagen dieses kleine Risiko eines Unfalls auf mich für das größere Gut,
1847900	1853020	dass selbstfahrende Autos insgesamt Unfälle vermindern. Ja, also es ist so wie das. Ich
1853020	1859380	opfere mich für das Gesamtwohl. Das würde ja dann dafür sprechen, dass im Grunde der Fahrer die
1859380	1867740	letzte Priorität in der Entscheidung sein dürfte. Genau, ja, je nachdem. Also es kann ja auch
1867740	1874340	prinzipiell. Ja, genau. Also wenn man jetzt das so programmiert, dass die Insassen vom Auto nicht
1874340	1878780	die höchste Priorität haben. Ja, das wäre ja, ich als Fahrer, der ich mich in so ein gefährliches
1878780	1882660	Gefährt setze, weiß das und nehme dann den Großteil des Risikos auf mich. Wobei dann
1882660	1888020	natürlich wieder so Konflikte entstehen. Bringe ich den Fahrer jetzt um oder breche ich den Passagier
1888020	1893020	beide Beine, dass er danach im Rollstuhl sitzt? Da wird ja noch mehr Differenzierung,
1893020	1899300	da kommt ja noch mehr Differenzierung rein. Ja, also ich finde eben, die Differenzierung,
1899300	1904620	die hört ja nie auf. Und ich finde das schon auch das Beängstigende, insbesondere,
1904620	1912820	wenn es eben um diese Datenmacht geht, weil das ist ja schon ein bisschen so der Trend,
1912820	1922100	dass wir uns immer mehr dieser Datenmaschine hingeben. Und das kann ich mir vorstellen,
1922100	1925700	dass es tatsächlich irgendwann mal zu einem Problem führen wird für solche Entscheidungen.
1925700	1930580	Weil ich da auch, ich bin da immer so ein bisschen hin und her gerissen bei den Problemen dieser
1930580	1935260	Entscheidungen. Weil es ist ja nicht so, als würden diese Entscheidungen jetzt nicht getroffen. Also
1935260	1938100	ich habe da auch keine Antwort. Ich will auch nicht sagen, dass die dem jetzt völlig gegen
1938100	1942180	argumentieren. Aber muss ich halt auch vor Augen halten, wie so Entscheidungen jetzt getroffen
1942180	1946980	werden. Wenn es jetzt um was kleineres geht als ein Autounfall umgebracht zu werden, sondern
1946980	1952780	vielleicht um eine Kreditvergabe oder so. Wenn ich da vorher einen Bankberater, vorher vor eine
1952780	1956860	Bankberater gesessen habe, da hat es vielleicht noch mehr eine Rolle gespielt, welche Hautfarbe
1956860	1962020	ich denn habe, als jetzt, wo es halt mein Kreditscore bei der Schufa ist. Gut, der wird
1962020	1964540	wahrscheinlich auch wieder von der Hautfarbe ein bisschen beeinflusst an den ein, zwei Stellen.
1964540	1970220	Aber dessen Einfluss geht wahrscheinlich erstmal ein bisschen runter. Zumindest solange du,
1970220	1974540	naja, vielleicht dann doch irgendwie eine Rassisten oder eine Rassisten vor dir sitzen hast bei der
1974540	1978980	Bank. Wenn du da jemanden hast, der das für sich reflektiert und bewusst versucht irgendwie
1978980	1982300	auszublenden, dann hast du vielleicht an der Stelle mehr Glück. Aber das heißt ja auch nicht,
1982980	1987860	dass die menschliche Entscheidung immer besser sein muss als die der künstlichen Intelligenz oder
1987860	1993180	fairer oder weniger verzerrt. Sie hat halt nur eine wesentlich größere Streuung gefühlt.
1993180	1998540	Ja, einverstanden. Aber ich sehe das Problem ein bisschen weitergehen als dass man halt,
1998540	2003500	also wir haben ja so, wir hinterlassen so eine Datenspur, oder? Und ich höre von ganz vielen
2003500	2008180	Menschen, mit denen ich darüber spreche, ich habe ja nichts zu verbergen. Ja, okay, würde ich auch
2008180	2013820	von mir sagen, ich mache ja nichts Illegales und so weiter. Aber das Problem liegt ja darin,
2013820	2019500	dass diese Daten, die gibt es auch noch, nachdem wir, nachdem es uns nicht mehr gibt. Potenziell.
2019500	2024380	Also die bleiben erhalten, aber die bleiben auch einfach noch in zehn Jahren erhalten und wir
2024380	2033420	wissen nicht, was dann zum Beispiel plötzlich als Norm gilt. Das ist richtig. Sagen wir, wenn es jetzt
2033420	2039980	zusammengeht, ich mache das Beispiel dann meistens mit genetischer Sequenzierung. Klar kann ich das
2039980	2045180	rausgeben, weil ich bin gesund und so, ich habe nichts zu verbergen. Aber vielleicht kann man dann
2045180	2050940	in zehn Jahren aus meinem Genom rauslesen, dass ich so und so eine Störung oder ein Persönlichkeitsprofil
2050940	2054380	habe, das vielleicht dann nicht mehr so ganz zu meiner Versicherung passt oder zu was auch immer.
2054380	2061700	Und dann gibt es ganz neue Diskriminierungspotenziale und das sehe ich halt schon problematisch mit Daten.
2061700	2069340	Und jetzt haben wir halt so ganz klare Proxys oder man nennt das auch diese schützenswerten
2069340	2081260	Elemente, die man festgelegt hat. Genau, genau, richtig. Und gegen die man nicht diskriminieren
2081260	2086100	darf. Achso, das meinst du, ja, okay. Das meine ich ja. Also Gesundheitszahlen, die sind schützenswert,
2086100	2091060	auf jeden Fall. Aber wir sprechen mit Hautfarben, von Religionen, von solchen Dingen. Gegen die darf
2091060	2097460	man per Gesetz nicht diskriminiert werden, wegen diesen Dingen. Aber wohin das dann irgendwann
2097460	2103060	führen wird, das wissen wir nicht. Ja, das stimmt. Das finde ich schon problematisch, weil eben diese
2103060	2107380	KI potenziell Zugang zu diesen Daten haben wird. Ja, wobei das, da braucht es ja nicht mal eine KI,
2107380	2111940	das sieht man ja gerade in den USA ganz schön, also schön in Anführungszeichen, in den Bundesstaaten,
2111940	2117780	wo es jetzt um die zielische Versorgung von Transidenten, Kindern zum Beispiel geht, die
2117780	2121340	halt wollen tatsächlich, ich weiß nicht mehr was es war, aber wirklich rückliegend irgendwie
2121340	2127020	Chatverläufe oder sowas ausgewertet werden oder Kommunikation zwischen Ärzten und PatientInnen und
2127020	2131580	dann daraus irgendwie auch geschlussfolgert werden kann, wo irgendwie eine medizinische
2131580	2139460	Behandlung stattfindet, die nach dem neuen, nach der neuen Doctrin nicht mehr opportun ist.
2139460	2147100	Naja, du hast recht. Und da braucht man doch nicht mal eine KI für. Ja, das stimmt, das gibt es
2147100	2154180	schon jetzt und es gibt auch genügend Beispiele wie, also eben für algorithmischen Ungerechtigkeiten,
2154180	2160700	also das haben wir schon jetzt, da braucht es wirklich keine KI für. Aber eben, also wer diese
2160700	2165620	Merkmale festhält, ich weiß nicht, ich habe schon das Gefühl, manchmal hängen wir da schon ein
2165620	2171580	bisschen so hinterher, wenn es um diese Diskriminierungsmerkmale geht, aber ja,
2171580	2178020	meinen Standpunkt ist halt, wir wissen nicht, was es in Zukunft geben wird, wogegen oder wofür wir
2178020	2183500	diskriminiert werden können und deswegen müssen wir eigentlich jetzt schon damit beginnen, dass
2183500	2188140	wir unsere Daten schützen, aber ja. Wobei ja auch dann die Frage ist, wie dann in der KI kannst du
2188140	2193460	es dann auch viel weniger festlegen, du kannst halt viel weniger sagen, an welchem Prozentsatz oder an
2193460	2199100	welchem Faktor hängt, weil ja auch die internen Parameter und sowas, die in den Entscheidungen
2199100	2203540	einfließen, ja auch gar nicht mehr menschlich irgendwie interpretierbar sind. Das ist ja auch
2203540	2209220	wieder noch so ein Punkt. Stimmt, ja, stimmt, aber es gibt natürlich schon auch hier Möglichkeiten,
2209220	2213700	also zu dieser Interpretability von, Interpretierbarkeit von KI, das ist schon,
2213700	2222700	das ist auch gefordert, also auch diese EU-AI-Legal Act und so weiter, also das ist schon mit bedacht
2222700	2227700	und da gibt es schon auch Tools, aber das ist recht, also ganz, ganz nachvollziehbar wird das
2227700	2233100	nicht mehr sein irgendwann. Also so, dass man, dass ein einziger Mensch das verstehen kann bis,
2233100	2243500	bis in alle Details. Ja, also eben, also dieses, diese ganze, diese ganze Thematik führt ein bisschen
2243500	2248340	auch zu der Frage, ja, können wir überhaupt einen universellen Konsens dafür finden? Das ist so die
2248340	2257180	Frage, die er auch aufwirft. Führt Technik zu einem transkulturellen Einvernehmen? Kann die
2257180	2264780	Technik eigentlich, ja, die Werte, bringt die sie selbst mit, die ihre Operationsweise bestimmt?
2264780	2272340	Das ist ein bisschen die Frage, wenn es auch um die starke KI geht. Und was ich noch spannend finde,
2272340	2276780	ist eben diese Ambivalenz. Ich kann es noch nie so ganz einordnen, aber ich finde es interessant,
2276860	2282340	weil dieser Individualismus, in dem ja eigentlich alle so leben und das auch, was ein bisschen immer
2282340	2289780	zum Thema wird, das wird einfach relativiert. Also die, es kommt dann, es ist so ein starkes
2289780	2297820	utilitaristisches oder so ein starkes Gruppenelement oder Moment da in all diesen Argumentationen,
2297820	2302540	dass das Individuum da wirklich untergeht. Und das finde ich, finde ich irgendwie eine Spannung,
2302540	2309940	so eine Spannung zu dieser Gesellschaftsidee, die wir, wie ich sie wahrnehme und wie das dann
2309940	2315860	vielleicht irgendwann, ja, sich entwickelt. Ja, es ist irgendwie so beides. Es hat einerseits
2315860	2321580	diesen individualistischen Charakter, weil es eben, sagst du, diese Sakrosantheit des Individuums,
2321580	2327980	die setzt es ja schon irgendwie an. Aber gleichzeitig, der ich jetzt auch in keiner Weise widersprechen
2327980	2333940	will, aber gleichzeitig macht es eben so diese, ja, diese verbindliche Festlegung von tatsächlichem
2333940	2337580	Handeln. Also das ist ja im Grunde, es gibt ja auch diesen schönen Satz von Code is Law, also
2337580	2342380	Programmiercode ist, sind Gesetze in gewisser Weise, weil das ist halt nicht nur ein Gesetz,
2342380	2346300	wo irgendwo steht und falls du dagegen verhandelst, falls du dagegen handelst, wirst du vielleicht
2346300	2352140	bestraft, sondern das wird halt immer so passieren, weil Code da einfach sehr deterministisch ist in
2352140	2357700	dem Moment. Und ja, wenn das da einmal so drin steht, dann wird das auch immer so passieren.
2357700	2361220	Dann kann nicht danach in einen Richter sagen, ja, das war vielleicht gerechtfertigt, dass die
2361220	2365020	Person in der Situation anders gehandelt hat. Es wird einfach nie anders gehandelt werden.
2365020	2378980	Ja, ja, hast du recht. Ja, ich finde halt dieses Deterministische ist halt irgendwie,
2378980	2386700	ist halt irgendwie noch, noch, noch schwierig, weil ich, ich würde so spontan dazu trainieren,
2386700	2391340	man könnte das doch einfach lösen, indem man jetzt ganz konkret auf diesen Todesalgorithmus,
2391340	2398780	indem man da die, den Zufall entscheiden lässt. Ja. Oder? Und ich muss sagen, ich finde das sehr
2398780	2403180	unbefriedigend, wie er das abgehackt hat. Das ist da irgendwie so zwei Sätze zu und irgendwie
2403180	2409940	trotzdem, das ändert dann nichts daran, sagt er, auch das degradiert die Person im Straßenverkehr
2409940	2414460	zu einem bloßen Objekt. Und ich hätte mir da ein bisschen mehr erhofft zu diesem Argument, weil
2414500	2421100	für mich scheint das nicht, gar nicht so unplausibel zu sein. Mit dem Zufallgenerator. Ja, oder? Ich
2421100	2424780	weiß nicht. Also so intuitiv finde ich das gar keine schlechte Idee. Ja, die Idee hatte ich
2424780	2430060	tatsächlich auch. Die Frage ist dann erstens, wie gewichtest du den Zufall? Also wirfst du eine
2430060	2435340	Münze oder machst du vorher irgendeine Gewichtung? Dann stellt sich das Problem im Grunde gleich
2435340	2442540	genau wieder. Was gewicht ich jetzt höher, was gewicht ich niedriger, das, ja. Trotzdem ist
2442540	2446500	es irgendwie eine Bewertung von Menschen. Andererseits ist es aber auch genau der Punkt,
2446500	2451820	wir machen das ständig implizit. Und ich glaube, es herrscht da irgendwie das, was ich auch gerade
2451820	2456820	sagte, so ein bisschen so ein Bild vom Menschen als jemanden, der irgendwie automatisch richtig
2456820	2462820	oder neutraler handelt oder dass eine menschliche Entscheidung in irgendeiner Form inherent an sich
2462820	2469940	wertvoller wäre als eine KI-Entscheidung. Also das ist ganz, ganz schwer irgendwie zu fassen,
2470100	2474020	was sagt das eigentlich über unsere Sicht auf menschliche Entscheidungen aus,
2474020	2477580	wie wir über Entscheidungen künstlicher Intelligenz, ich setze das ganz gerne in
2477580	2482180	Anführungszeichen, wie wir darüber denken. Das sagt ja auch was darüber aus, wie wir über
2482180	2490780	Entscheidungen echter Intelligenz denken. Total, aber da kann natürlich das Argument ins Spiel
2490780	2494660	gebracht werden, das sagt ja, also wir sprechen jetzt von Willensfreiheit in den Menschen
2494660	2499220	beispielsweise, die gibt es ja gar nicht. Der Mensch ist eigentlich auch nur Algorithmus,
2499220	2504100	das ist ja auch eine philosophische Standpunkt, die ganz aktuell ist. Aber da kannst du sagen,
2504100	2507940	ja gut, also dann ist es... Meistens von den gleichen Menschen, die sagen,
2507940	2517500	dass KI das doch genauso gut entscheiden kann, logischerweise. Ja, also generell mit dieser
2517500	2522140	Anthropomorphisierung der KI, das ist auch so etwas, das mich persönlich nervt das immer so ein
2522140	2527460	bisschen, ja KI übernimmt dann die Herrschaft, weil und Gier und Triebe und so weiter, wie der
2527460	2533660	Mensch. Ich kann das nicht so ganz nachvollziehen, weshalb man das immer so sieht. Und auch
2533660	2540620	Simonowski macht das eben, er sagt das auch, also es ist nicht klar, dass sich die KI tendenziell
2540620	2548300	grundsätzlich wie ein Mensch verhält. Aber er sagt dann einfach, wenn man davon ausgeht,
2548300	2556460	dass die Intelligenz so darin vorhanden oder so operiert, wie wir die im Menschen kennen,
2556460	2560780	dann kann es sein, dass sie tatsächlich auch so wie diese Triebe entfaltet, die auch zu
2560780	2570020	unserer menschlichen Intelligenz gehören. Und wie kann man das wie so ein bisschen verhindern? Da
2570020	2577340	gibt es so eine Möglichkeit oder eine, ja das heißt so eine KI-Nanny, also dass man wie eigentlich
2577340	2584300	die KI, die Erfindung dieser, sagen wir mal potentiell bösen KI überwachen lässt durch
2584380	2591260	eine andere KI, die wir weltumspannend einsetzen, die dann auch Zugang auf alle Daten hat und dann
2591260	2598500	verhindert, dass es eine KI gibt, die sich gegen uns richtet. Bester Plan ever, not. Ja ich bin
2598500	2606460	auch KI-Nanny, so der Begriff, irgendwie schräg. Aber ja, er hat recht, also es zeigt sich dann
2606460	2616420	auch, also wir haben nur schon Schwierigkeiten, wenn es darum geht, irgendwie eine Atomwaffen zu
2616420	2621420	beschränken, also das schaffen wir schon gar nicht irgendwie weltweit, geschweige denn Klimakrise und
2621420	2632060	so weiter. Und wenn es dann darum geht, eben wir sollten, also die KI soll ja eigentlich das tun,
2632060	2640420	was wir ihnen vorschreiben, dann stellt sich eben die Frage, wer ist dieses Wesen? Haben wir
2640420	2646060	überhaupt eine gemeinsame Vorstellung, was wir wollen? Also man geht dann immer so ein bisschen
2646060	2650860	davon aus, dass unser gemeinsames Interesse darin liegt, dass unsere Gattung überlebt,
2650860	2659180	oder? Das ist so das, was man uns als Menschheit unterstellt. Tut man das? Muss man ja wissen,
2659300	2665780	also das muss man ja wie als Vorbedingungen? Also wenn wir das täten zumindest, also wenn
2665780	2669260	man das empirisch überprüfen würde, machen wir gerade einen verdammt schlechten Job, was das
2669260	2675620	angeht. Sprichst du jetzt in Bezug auf Klima? Ja, genau, ja. Und das ist auch das Argument,
2675620	2680900	das er dann sagt oder verwendet, es geht jetzt ein bisschen um diese Öko-KI, also die ist ja
2680900	2688020	allwissende KI, wo es darum geht, ja, wenn gehen wir gesetzt, das Überleben unserer eigenen Gattung
2688020	2697940	ist unser gemeinsames Interesse, können wir davon profitieren, wenn wir eine KI installieren,
2697940	2706460	die uns eigentlich, also die uns regiert, also dass wir uns selbst entmächtigen durch diese KI.
2706460	2713900	Also eine KI, die unsere Aktionsmacht einschränkt, indem sie oder nachdem sie von uns dazu beauftragt
2713900	2720020	sind. Hast du deinen Hobbs gelesen, Leviathan? Das klingelt gerade bei mir, weil der Leviathan
2720020	2726580	ist ja im Grunde genau dieses Instrument mit einer unbeschränkten Macht, das einmal per Vertrag mit
2726580	2731980	dieser Macht ausgestattet wird und danach unser soziales Leben kontrolliert. Das stuppert doch
2731980	2737940	sehr danach. Du hast recht, ja, daran habe ich nicht gedacht, aber es ist genau das,
2737940	2747020	und Leviathan, das passt dann auch vom Namen zu so einem System, oder? Ja, genau, also es ist
2747020	2753660	halt so die Frage, wieso soll der Mensch seiner eigenen Bevorzugung zustimmen und ich glaube,
2753660	2761060	oder er sagt tendenziell, ist das gar nicht mehr so unvorstellbar, weil wir Menschen sind im Denken
2761060	2766140	besser als im Handeln und wir haben diese Probleme, wir haben diesen Present-Bias,
2766140	2775780	wir bevorzugen die Gegenwart gegenüber der Zukunft, wir haben diesen Regional-Bias, also wir sind
2775780	2783820	eigentlich so eine Zentralinstanz, die ist zwar unpersönlich, aber das wollen wir im Prinzip ja
2783820	2789140	genau, es soll eben nicht diese Partikularinteressen berücksichtigt werden, wenn es jetzt darum geht,
2789460	2799340	Öko-Richtlinien durchzusetzen. Er nennt das, du hast Hobson, er nennt das dann, das wäre dann
2799340	2806620	der Robespierre des 21. Jahrhunderts. Okay, wie Robespierre geendet ist, wissen wir auch. Genau,
2806620	2816300	ja, also es ist, man kann da viele Analogien anführen, aber ja, es ist wie, es läuft so ein
2816300	2820940	bisschen drauf hinaus, dass man auch natürlich in Frage stellen kann, ist die Demokratie überhaupt
2820940	2830460	dafür geeignet, also wenn wir von unseren eigenen Schwächen da, wie wir sie jetzt in dieser Form haben,
2830460	2835420	ausgehen, dann ist die Demokratie möglicherweise tatsächlich die schlechteste Form, um unser
2835420	2842140	Überleben zu sichern. Wobei ich dann jetzt so die Perspektive einer KI-basierten Expertokratie,
2842140	2847660	also einer Kaiokratie sozusagen, ist jetzt auch nicht unbedingt das, was ich unglaublich
2847660	2855340	erstrebenswert finde. Und warum nicht? Ja, genau, aus einerseits dem Grund, dass es halt wirklich
2855340	2859940	im Grunde eine komplette Selbstentmündigung sozusagen wäre, was ja im Grunde, wenn man
2859940	2869940	jetzt mal weiterdenkt, ist das, naja, auch eine Entmenschlichung sozusagen. Und ja, weil du eben
2869940	2874700	auch wieder nicht weißt, ich meine, was dich jetzt irgendwie an den eigenen Interessen entwickelt
2874700	2879380	oder so was, soweit will ich jetzt eigentlich gar nicht spinnen, aber wie vollständig irgendwie
2879380	2882820	so Informationen sind, die man glaubt zu haben, und dann lernt man irgendwann, dass die Welt doch
2882820	2887460	anders funktioniert. Das habe ich jetzt gerade in diesem Bauhausbuch gelesen, wo es halt auch
2887460	2894780	stark darum ging, dass am Anfang in der frühen DDR es durchaus Initiativen gab, also alte Innenstädte,
2894780	2898980	alte Innenstädte wie in Dresden oder so was, komplett abzureißen und die Städte komplett
2898980	2903100	neu zu planen nach einem Vorbild dessen, was wir heute aus den USA kennen mit irgendwie den
2903100	2913620	weitläufigen autozentrierten Städten und so. Und so sehr man das viel in der DDR verachten kann,
2913620	2918980	bin ich zumindest an dem Punkt ganz froh, dass sie es nicht gemacht haben. Dass sie, auch wenn es in dem
2918980	2924260	Moment nationalistisch motiviert war, gesagt haben, nee, wir rekonstruieren die Städte mal eher
2924260	2929300	wieder ein bisschen so, wie wir sie mal hatten. Das hat dann alles so seine Schwierigkeiten gekriegt,
2929300	2933780	aber das war die Grundidee dahinter, zumindest am Anfang. Und da bin ich dann auch wieder ganz
2933780	2937180	ganz froh, weil mittlerweile wissen wir, dass das die bessere Idee ist. Damals wussten wir das
2937180	2942140	vielleicht nicht unbedingt und sowas würde sich halt auch in einer KI in irgendeiner Form verankern
2942140	2948180	und festschlagen, weil alles so, die kann über alle Daten verfügen und völlig neutral, das ist
2948180	2952140	halt auch Fiktion. Das ist halt ein Gedankenexperiment und nicht mehr.
2952140	2959220	Total, das stimmt. Ich finde es spannend, wie du das, ich werde nachher noch gleich die Pünke
2959220	2965260	dazu schlagen, was du jetzt gesagt hast, wenn es um diese Willensfreiheit geht. Aber trotzdem ist
2965260	2969380	natürlich diese Demokratie, also es ist wieder so sehr, ich sage jetzt mal sehr deutsch, ganz doof
2969380	2976020	gesagt, was dieses Empfinden, ich teile das mit dir. Aber es zeigt ja schon auch ein bisschen,
2976020	2985100	wenn man jetzt nach China schaut zum Beispiel, so die Stabilität und Wohlstand, das kann ja schon
2985100	2990740	auch für die Menschen erstrebenswert sein. Also dass der Staat das wie sich erstellt und die
2990740	2998260	Würde wird dann nicht mehr individualistisch gedacht, sondern kollektiv. Dann hat man so
2998260	3004180	diesen pragmatischen Autoritarismus, wie man den eben in China beispielsweise sieht. Also man kann
3004180	3012540	schon auch dafür argumentieren und offenbar ist es auch so, dass immer mehr Menschen sich da gar
3012540	3017980	nicht mehr so abgeneigt sind. Wir haben da schon so ein bisschen eine Sonderposition, wie wir das
3017980	3025620	wahrnehmen. Und ich weiß nicht, ob sich das nicht irgendwann ein bisschen verschieben wird.
3025620	3030820	Ja, also ich finde das spannend, weil klar, wenn du sagst, das Überleben der Menschheit geht über
3030820	3037260	alles, dann wäre wahrscheinlich das schöne Begriff vom wohlwollenden Diktator. Der hätte
3037260	3041780	natürlich einen Vorteil, aber wer stellt bitte sicher, dass der in irgendeiner Form wohlwollend
3041780	3047500	ist? Wer kontrolliert den wohlwollenden Diktator? Und dann läuft man wieder in das Problem rein.
3047500	3052500	Also du sagst jetzt China Stabilität, da kann man schon für argumentieren, da ist natürlich was
3052500	3058900	dran, aber frag mal die Uiguren. Die sehen das Problem dann vielleicht aus einer anderen
3058900	3062620	Perspektive. Und das ist glaube ich wirklich schwierig. Ich meine, unsere demokratisch-kapitalistischen
3062620	3068260	Systeme haben das auch. Du findest ja auch viele Leute, die von dem System komplett vergessen und
3068260	3074380	abgehängt werden und sich dann irgendwie auch aus dem demokratischen Diskurs verabschieden. Damit
3074380	3079340	meine ich jetzt nicht AfD wählen, sondern nicht wählen. Ich will nicht das Narrativ vorantreiben,
3079340	3085140	dass das irgendwie AfD-Wähler die Armen abgehängt werden. Das ist nämlich leider nicht so. Aber
3085460	3090700	ich tue mich damit schwer, aber das Schöne ist, ich muss darauf keine definitive Antwort finden.
3090700	3100340	Ja, genau. Ich verstehe natürlich deine Ambivalenz, Gemi, genauso. Ich finde auch,
3100340	3110420	wie du das vorhin gesagt hast, diese KI-Okratie. Das ist auch so ein bisschen das, wenn es darum
3110420	3117500	geht. Man kann ja sagen, diese Willensfreiheit des Menschen wird so ein bisschen infrage gestellt
3117500	3125300	durch diese KI. Ein Aspekt davon ist eigentlich die Beschränkung der Freiheit, dass man anders
3125300	3133340	kann. Und zwar im Sinne von, wir gehen weg von dieser Disziplinargesellschaft, wie wir sie jetzt
3133340	3140580	kennen. Also wir werden dafür bestraft, wenn wir was falsch machen, hin zu eigentlich einer
3140580	3148980	Kontrollgesellschaft. Wir dürfen gar nicht erst mehr was falsch machen. Es wird so designed,
3148980	3152580	sagen wir mal, unser Leben, dass wir das gar nicht mehr können. Wir können nicht, genau. Das ist
3152580	3157740	noch mal was anderes als, wir dürfen gar nicht mehr. Genau. Und das ist das, was wir uns doch so ein
3157740	3162340	bisschen intuitiv, das ist so ein bisschen Kant und Leib, jetzt in ein gutes Handeln, ist nur dann
3162340	3167780	moralisch, wenn es freiwillig ist, so Kant, aus Pflichtgefühl muss das passieren, dann ist es gut,
3167780	3174300	moralisch gut. Und Leibniz hat mit seiner Theorie C, das ist die beste aller Welten, man kennt
3174300	3178900	so ein bisschen davon, es muss so wie das Üben der Welt geben, damit sich das Gute beweisen kann.
3178900	3188940	Ja. Das ist so, was ich auch fühle, sage ich mal. Und ganz lustig war dann, ich glaube,
3188940	3197660	es war ehemaliger Bundesverfassungsrichter, sah unter anderem ein Problem in dieser Auto-KI,
3197660	3206420	dass die FahrerInnen durch die KI an der Missachtung der Verkehrsregeln gehindert
3206420	3212340	werden können. Ja. Das war wie ein Argument. Die Menschen, die dürfen ja dann gar nicht mehr,
3212340	3217540	die können gar nicht mehr die Verkehrsregeln missachten. Und bei uns gibt es halt wie kein
3217540	3221980	Gesetz, das jetzt zum Beispiel Sicherheit immer vor Freiheit stellt. Und das ist so ein bisschen
3221980	3227260	auch was, was ich mir vorstellen kann, was in deinem Argument vorhin mitspielt. Was ist denn gut oder
3227260	3233300	wer stellt das sicher? Aber klar kann man sagen, wenn jetzt bei uns ökonomisches Wachstum und
3233300	3239740	Wohlstand und so und so als das Gute in dem Sinne definiert wird, dann kann man schon eine KI kriegen,
3239740	3245460	die das sicherstellt. Ja gut, wenn man das vielleicht wieder genau genug definiert
3245460	3250420	kriegt. Du kommst natürlich wieder darauf hin, dass du dein Ziel so konkret wie es irgendwie
3250420	3254540	geht nur definieren musst. Das erlebt man ja auch gerade bei, nehmen wir mal das einfache
3254540	3261060	Beispiel, ChatGPT. Schreib mir jetzt mal einen Text, der genau das tut. So und dann musst du
3261060	3265420	fünffach korrigieren und auf Vorschläge davon reagieren und antworten. Nee, mach das mal ein
3265420	3270660	bisschen größer. Oder nein, diese Zahl ist falsch. Bist du dann irgendwann nach einer Stunde im
3270660	3273700	Gespräch mit ChatGPT einen guten Text? Das in der Zeit hättest du ihn wahrscheinlich auch selber
3273700	3280380	schreiben können. Und das ist glaube ich noch mal so dieser Gedanke, der hinterlässt, die Präzision
3280380	3286380	des Ziels zu kennen. Die Funktion, die man da reinschreibt oder die Bewertungsfunktion, nach
3286380	3291020	der man einen Zustand bewertet, dass das eben auch einfach ein unglaublich schwieriger Aspekt ist,
3291020	3294660	weil du dann natürlich voll in so eine Reduktionismus-Falle reinläufst. Wie sagst du,
3294660	3301860	ja, ich bewerte alles in Geld. Zum Beispiel, wie es die Ökonomie ja gerne mal tut. Naja,
3301940	3309980	guter Punkt, das stimmt. Ja, aber das ist klar, man kann das natürlich dann auch wieder abtun,
3309980	3317580	das ist ein ganz spezifisches Implementierungsproblem. Ja, nee. Ja, nee, natürlich nicht,
3317580	3323940	weil das dann gleichzeitig eine riesige Frage ist, wie man dieses Ziel festhalten möchte.
3323940	3333780	Absolut. Es ist so ein bisschen ein Traum, der Traum der Technokraten, Technokratinnen. Auch so
3333780	3337500	ein bisschen nach Habermas, nicht der Bildungsbildungsprozess, sondern eigentlich
3337500	3342860	nur noch die Regelungsoptimierung ist relevant. Und da ist natürlich die Frage, was optimiert
3342860	3351060	man nämlich? Was will man denn am Schluss überhaupt erreichen? Und das geht dann über
3351100	3357140	in diesen zweiten Punkt, wenn wir davon ausgehen, dass eben diese Willensfreiheit in Frage gestellt
3357140	3363300	wird. Das ist eigentlich, dass der Mensch dadurch verlärmt, durch die KI etwas zu wollen,
3363300	3370460	im Sinne von seinen freien Willen überhaupt zu gebrauchen. Das geht so ein bisschen da in das
3370460	3378580	Argument, wir müssen das auch trainieren. Also was wir als wichtig oder richtig oder nicht richtig
3378700	3385700	empfinden oder machen oder und so weiter, das haben wir vielleicht nicht einfach in uns drin,
3385700	3390500	sondern das muss gelernt und trainiert werden. Das schließt wunderbar, also das, was du sagst,
3390500	3395380	das schließt an die nächste Folge an, die schon aufgezeichnet ist, aber natürlich noch nicht
3395380	3399820	veröffentlicht, wenn ihr diese hört, vermutlich. Da wird es um ein anderes Buch gehen, nämlich
3399820	3406540	Truckpoint Capitalism, wo es unter anderem, dass das Argument auftaucht, warum Dienste wie Spotify
3406540	3410820	ein großes Interesse daran haben, dass wir unsere Musik, die wir hören, nicht mehr selber aussuchen,
3410820	3416740	sondern uns einfach von Spotify das aussuchen lassen im Rahmen von Playlists. Das spielt ein
3416740	3420580	ganz ähnliches Element rein, dass man irgendwann dann verlärmt, sozusagen nochmal zu gucken,
3420580	3425180	was gibt es denn an KünstlerInnen, was gefällt mir denn an Musik eigentlich wirklich und wo sind
3425180	3429940	denn jetzt die feinen Unterschiede und wo bin ich jetzt bereit, eventuell Zeit oder Geld in einen
3429940	3434900	Künstler zu investieren und wo vielleicht auch nicht. Das finde ich einen ganz spannenden Punkt,
3435060	3439780	es gibt auch bei dem bei dem Chat-GPT-Thema so dieses Argument, ja, das übernimmt halt so die
3439780	3446780	langweiligen Arbeiten, also die standardisierten Arbeiten und so. Da sagt man, ja, das mag das mag
3446780	3451860	es sogar in manchen Bereichen können, aber woran übt man denn dann eigentlich, um die schwierigen
3451860	3456220	und die nicht so standardisierten Sachen überhaupt machen zu können? Weil jetzt nutzt man ja die
3456220	3460900	einfachen standardisierten Fälle, um für die schwierigen komplizierten Fälle zu üben. Aber
3460900	3464020	wenn man das nicht mehr macht, wer lernt denn dann noch die schwierigen komplizierten Fälle
3464020	3469380	zu bearbeiten? Also ganz spannende Argumente, die sich daraus ergeben. Ja, das ist auch ein
3469380	3475180	Argument, das ich ganz oft bringe, wenn es um in der Medizin darum geht. Also wenn ich mit meinen
3475180	3480340	Ärztinnen und Kollegen darüber spreche und dann kommt es ja, da wird uns dann das abgenommen,
3480340	3485740	zum Beispiel diese banalen Fälle auf dem Notfall, der Schnupfen, Husten und so, das ist dann weg und
3485740	3491860	das ist für mich ein totales Problem, weil mein Mensch, auch mein statistisches Empfinden einer
3491860	3496820	Krankheit, ist er ganz krass davon geprägt, dass sich neun von zehn Fälle sind banal und dann
3496820	3503580	erkenne ich den einen, der nicht banal ist. Und wenn irgendeine KI oder was auch immer mir diese
3503580	3510340	neuen banalen Fälle abnimmt, dann habe ich gar keine Relation mehr. Das spielt also ein bisschen mit
3510340	3517740	rein und deswegen, ich sehe das auch ganz und gar nicht positiv. Ein gutes Beispiel, um das von
3517740	3524980	Spotify aufzunehmen, das bringt er auch im Buch, das ist dieses Video, das von Google geleakt wurde
3524980	3533540	oder es ist nicht ganz klar, ob es einfach geleakt wurde oder absichtlich, wo es um diesen Selfish
3533540	3539900	Letcher geht. Ich weiß nicht, ob du davon gehört hast, da geht es darum, es ist ein internes Video,
3539900	3548420	soll auch das Thought Provoking sein. Also es geht darum, dass sie das so konstruieren,
3548420	3560300	dass der Mensch eigentlich ein Konto hat, wo die Daten drin sind und das geht dann so weit,
3560300	3566620	dass man das dann weitergeben kann. Dieses Mein-Daten-Konto wird dann irgendwann für die
3566620	3574900	ganze Menschheit zu Verfügung gestellt. Und es ist sehr unangenehm, das Video, wenn man da schaut,
3574900	3582900	weil es ist ein Beispiel, es geht dann so darauf hin, dass du hast eine App, dann kannst du da
3582900	3588860	einstellen, was dein Lebensziel sozusagen sein soll und alles wird dann danach ausgerichtet. Also
3588860	3596020	wenn du jetzt nur öko-bewusst, umweltbewusst leben möchtest, dann, wenn du eine Banane kaufst,
3596020	3601980	dann wird dir angezeigt, ja, aber du kannst hier auch die lokale, gewachsene Banane kaufen. Und das
3601980	3606940	ist ja noch okay, das ist so ein Nudging, so ein bisschen okay, man hat eine Alternative, aber so
3606940	3611580	der zweite Teil vom Video und so weiter geht dann darum, ja, irgendwann kann der Algorithmus auch
3611580	3616740	für dich entscheiden. Du hast dann gar nicht mehr die Wahl und es geht dann auch so weit, dass irgendwann
3616740	3621620	merkt dann diese Daten, ja, ich weiß jetzt gar nicht, wie schwer du bist, das heißt, ich gehe
3621620	3626780	mal auf die Suche nach einer Waage und präsentiere die dann so, als dass du die brauchst, damit ich
3626780	3634460	dann dein Gewicht kenne. Also schon ein bisschen verstörend, aber es ist neun Minuten lang, wer Lust
3634460	3646220	hat, ich kann das dann nicht schauen, es ist schon noch interessant. Ja, das wären so ungefähr die
3646220	3653300	Gedanken, die ich auch hier präsentieren wollte. Wie gesagt, der letzte Teil geht dann so ein bisschen
3653300	3660020	ins Philosophische auch interessant, wer dann so ein bisschen Heidegger und Kittler und eben Hegel,
3660020	3668220	ja, wer diese Argumente darin finden möchte, dem sei das nagelegt auch zu lesen. Ich habe das Buch
3668220	3673620	jetzt schon ein paar Mal durchgelesen, ich finde es wirklich, ja, es gibt viel, es liest sich ringen
3673620	3685260	und wer das Thema spannend findet, dem kann ich das echt empfehlen. Ja, super, danke dir. Ja,
3685260	3689820	viel gedacht, viel diskutiert, haben wir jetzt auch schon in der Vorstellung. Ich habe noch
3689820	3694580	so ein paar Anschlusspunkte, wo man noch mal drüber nachdenken könnte. Also ein Thema,
3694580	3699780	was ich gerade bei diesem Thema KI, was man immer so ein bisschen Blick drauf haben muss,
3699780	3706100	ich weiß nicht, ich sagte den Begriff Critihype, was? Nein. Das ist ein Begriff, den hat Lee Wenzel,
3706100	3711660	der auch schon mal hier im Podcast aufgetaucht ist, geprägt. Da geht es darum, dass gerade so diese
3711660	3716580	Kritik vor dem, was KI mal alles können wird und dieses Warnen davor, was das alles können wird,
3716580	3723700	auch eine Strategie sein kann, den Hype darum zu pushen. Einfach weil man eben, dass es das mal
3723700	3730140	können wird, ist gesetzt. Die Frage ist nur noch, wie gehen wir damit um? Das ist so ein schöner
3730140	3735820	Mechanismus, hat man jetzt bei JetGPT gesehen, dass so jemand wie der CEO von OpenAI, also den
3735820	3740100	Machern von JetGPT, dass der auf einmal sagt, man sollte sechs Monate alle Forschung an solchen
3740100	3746100	Modellen verbieten, weil die könnten ja irgendwann die Weltherrschaft übernehmen. Ja, das pusht
3746100	3752380	vor allen Dingen den Wert seines Produkts. Die Diskussion darum sorgt halt dafür, wie das kann
3752380	3756140	die Weltherrschaft übernehmen. Das muss ja was sein. Und so weiter und so fort. Auf einmal ist das
3756140	3760860	wieder in aller Munde. Also das ist so ein Punkt, den man immer so mit Blick haben muss. Das hatte
3760860	3765340	ich jetzt deiner Darstellung nach bei dem Buch nicht unbedingt, aber es ist ein gutes Korrektiv,
3765340	3769420	wenn man sich den Artikel von Lee Wenzel dazu vielleicht mal durchliest. Den packe ich logischerweise
3769420	3773660	in die Show Notes. Also das ist jetzt als Leseempfehlung etwas aus fühliger gemeint gewesen.
3773660	3782100	Mir sind noch zwei weitere Bücher über den Weg gelaufen, die ich ganz spannend finde. Also
3782100	3785820	Sachbücher. Einmal Choke Point Capitalism. Das werden wir euch in der nächsten Episode
3785820	3793100	vorstellen von Rebecca Giblin und Cory Doctorow. Und dann habe ich noch von Stefan Kühl. Das ist
3793100	3799900	ein deutscher Organisationssoziologe. Da gibt es das Buch Brauchbare Illegalität vom Nutzen des
3799900	3804020	Regelbruchs in Organisationen. Da musst dich dran denken, als du das gerade ansprachst mit dem,
3804020	3809020	man verhindert, dass die Leute Regeln brechen. Aber da muss ich genau an so eine Situation denken,
3809020	3813740	wie Autofahrer, das ist eine rote Ampel, Autofahrer sieht aber, dass da irgendwie,
3813740	3817540	dass er jetzt Leben retten kann, wenn er über die rote Ampel fährt, aber die rote Ampel lässt ihn
3817540	3821700	halt nicht. So, ne? Also manchmal kann es dann einfach Sinn machen, Regeln zu brechen. In
3821700	3825540	Organisationen ist das noch viel relevanter. Jeder, der irgendwie in einem Unternehmen oder in einem
3825540	3829340	größeren Kontext arbeitet, weiß, warum es manchmal auch sinnvoll sein kann, sich nicht ganz an die
3829340	3834540	Regeln zu halten. Aber das im Großen und Ganzen auch im Sinne der Regeln dann ist, wenn man das
3834540	3839260	in dem Fall nicht tut. Also da muss ich dran denken, als du den Aspekt ansprachst. Das waren
3839260	3846260	die zwei Sachbücher und dann noch zwei Fiktionen, fiktive Texte oder auch Filme. Also ein Film ist
3846260	3851020	sicherlich nicht unbekannt, das Minority Report. Das ist ja eine Kurzgeschichte, ich glaube,
3851020	3858060	von Philip K. Dick war sie oder war sie von, Gott, jetzt blamier ich mich gerade. Ja, aber ich hab's
3858060	3862780	auch so irgendwie abgespeichert. Und natürlich als Film, da ist sie halt bekannt und berühmt
3862780	3866900	geworden. Da geht es eben um dieses Predictive Policing, also dieses Leute festnehmen,
3866900	3871220	bevor sie ein Verbrechen vergehen, um das Verbrechen eben zu verhindern. Und dann gibt's
3871220	3877780	von Andreas Eschbach den Roman NSA, Nationales Sicherheitsamt. Ist ein bisschen kontrovers,
3877780	3883860	weil er halt so das Nazi-Judenverfolgungs-Setting als so ein bisschen Hintergrund für eine spannende
3883860	3889940	Roman-Geschichte nimmt, eine fiktive, weil er da eben das Nationale Sicherheitsamt, Abkürzung NSA,
3889940	3896020	was ein Zufall, zeigt, die irgendwie sämtliche Datenbanken und so was haben und daraus so Dinge
3896020	3900700	nutzen, wie die Leute kaufen mehr als für vier Personen notwendig ist, die beherbergen wahrscheinlich
3900700	3905660	irgendwie jemanden, den wir nicht finden sollen oder so. Und erzählt dann eben auf dieser Grundlage
3905660	3912620	eine Geschichte, was mit so vorgeblich unschuldigen Daten, wie zum Beispiel ein Einkaufszettel, was
3912620	3916740	daraus irgendwie geschlussfolgert werden kann und wie das, wenn man jetzt sich in einem autoritären
3916740	3922820	Regime sieht, dann auch genutzt werden kann und auf einen zurück schlagen kann. Ist in der Hinsicht
3922820	3925660	ganz spannend, ich fand ihn sehr spannend zu lesen, aber wie gesagt, das hat so ein bisschen
3925660	3930580	dieses, ja so ein bisschen, ich will nicht sagen, Nazi-Romantik, weil es die auch extrem kritisch
3930580	3937220	sieht, aber da kann man ein bisschen draufgucken. Genau, das waren meine Lesetipps über das Thema
3937220	3944300	hinaus. Cool, ja, vielen Dank. Das klingt nach Sommerferienlektüre, so das Letzte. Das könnte
3944300	3947780	ich mir vorstellen, dass ich das mit... Also es macht nicht unbedingt Spaß, es ist eher so dunkel,
3947780	3952740	es ist nicht gute Laune. Dafür ist das Thema auch zu ernst, aber man kann es gut, Espar kann
3952740	3961780	einfach sehr gut schreiben, sehr fluffig, sehr gefällig. Okay, ja, sehr gut. Ich habe so als
3961780	3966820	weitergehende Lektüre, es gibt ein anderes Buch von Szymanowski, also es gibt mehrere natürlich,
3966820	3972020	aber ein anderes, das heißt Data Love, das ist bei Mathes und Salz erschienen, wo es so eben
3972420	3978580	um die ganze Daten-Thematik geht. Ich finde, er schreibt einfach auch sehr ansprechend,
3978580	3983060	ich finde auch dieses Buch, das hat sehr viele Referenzen und es geht ihm auch Filme und konkrete
3983060	3989940	Beispiele ein und man sieht dann aber eben im letzten Teil, dass es trotzdem auch irgendwie
3989940	3995900	inhaltlich nicht einfach so dahergesagt ist, also wenn er dann diese Beispiele oder diese
3995900	4002340	Philosophen ins Spiel bringt, deswegen noch ein anderes Buch von ihm. Dann eines, das habe ich
4002340	4007100	jetzt angefangen, ich bin noch nicht durch, aber es passt ganz gut zum Thema, das heißt Was das
4007100	4015540	Valley Denken nennt von Adrian Daub, wo es eben darum geht, natürlich auch bei dieser KI, das ist
4015540	4020860	ja eine ganz kleine Gruppe von Personen, die eigentlich an der Entwicklung davon überhaupt
4020860	4029980	beteiligt sind und ja das Silicon Valley ist natürlich ein Brennpunkt für Daphne und Adrian Daub
4029980	4036700	ist Adrenalin, ist er Deutscher sogar? Ich glaube ja, ich glaube er ist tatsächlich, also er lebt und
4036700	4040340	arbeitet in den USA, aber ich habe ihn auch schon in deutschen Interviews gehört, also er ist auf
4040340	4049620	jeden Fall deutsch auf hohem Sprachlevel und ich meine tatsächlich auch, er wäre gebürtig in
4049620	4056100	Deutschland, aber ich kann es dir jetzt nicht garantieren. Ja okay, auf jeden Fall, das würde
4056100	4064500	dazu passen und von einer deutschen Professorin Katrin Misselhorn ist das kleine Büchlein KI und
4064500	4071500	Empathie, das beim Reclam erschienen ist, auch sehr empfehlenswert meiner Meinung nach, geht es auch
4071500	4077660	so ein bisschen darum, eben Empathie ist auch was man trainieren kann, soll, muss, wie wird das in
4077660	4082940	Frage gestellt oder beeinflusst durch Roboter, durch KI und so weiter. Also Adrian Daub ist
4082940	4089420	tatsächlich in Köln geboren, aber wenn ich das richtig sehe, akademisch komplett in den USA oder
4089420	4093700	Großbritannien, in den USA sozialisiert, also in den USA sogar seinen Bachelor schon gemacht.
4093700	4102620	Ach so, okay. Ja, wir haben auch so ein paar passende Folgen natürlich zum Thema, ich habe
4102620	4108500	jetzt einfach zwei rausgepickt, das ist einerseits das metrische Wir von Steffen Mau, das wird auch
4108500	4117100	explizit im Buch erwähnt, das Buch, was auch dazu passt, ist Roboterethik von Janina Loh, wo es
4117100	4124580	generell also ein bisschen Maschinenethik auch geht. Ja, also es gibt natürlich ganz ganz vieles, was
4124580	4130780	passt, aber das könnte man sich dazu anhören und so Filmtipps, auch ein Film, das den eher erwähnt
4130780	4139580	ist, ein neuerer Film ist Ex Machina. Ja. Wieso zögerst du? Also ich kenne Ex Machina,
4139580	4145460	den Film, ist ein sehr sehr guter Film, mir war nur der Bezug zu seinem Thema nicht sofort klar. Ach so,
4145460	4156460	ja, es geht ein bisschen darum, dass, um diesen Sprung davon, dass Technik eigentlich wie dann
4156460	4162820	in die Welt rausgeht und sich selbst, um die Welt zu entdecken, also man hat wie in, ich glaube,
4162820	4167940	Hell nimmt als anderes Beispiel von dieser starken KI, die zwar dann diese Astronaut, das ist Space
4167940	4172660	Odyssey, oder? Ja, genau. Space Odyssey, wo die Astronauten dann nicht mehr zurück ins Raumschiff
4172660	4180580	dürfen, aber weil Hell sozusagen die Menschheit oder seinen Auftrag erfüllen möchte, also es ist
4180580	4188100	wie noch, er macht das noch im Sinne von seinem Auftrag, der von den Menschen vorgegeben ist und
4188100	4192900	bei Ex Machina gibt es dann wie diesen Schritt, dass diese Roboter in diesem Fall, diese starke KI
4192900	4200460	eigentlich sich davon löst. Es ist wie wirklich diese eigene Welt dann und so, das ist wie er das
4200460	4205780	im Buch beschreibt. Aber ich finde es auch generell so eigentlich ein guter Film, der das Thema so,
4205780	4213940	wenn es um Roboter und Maschinen geht, gut darstellt. Ja, das wäre es von meinen Tipps.
4213940	4219100	Sehr schön, ja. Vielen Dank dir, Amanda, für die Buchvorstellung. Vielen Dank euch an den
4219100	4227500	Kopfhörern oder Lautsprechern fürs Zuhören und dann bleibt mir nur zu hoffen, euch gewünscht zu
4227500	4232540	haben, dass ihr viel Spaß hattet, keine Ahnung, ich hab mich verhaspelt, es ist spät, dass ihr was
4232540	4238140	mitgenommen habt, dass ihr viel Spaß hattet beim Hören dieser Episode und wenn ihr uns auf den
4238140	4243500	sozialen Medien folgen wollt, könnt ihr das tun. Wir sind noch Fragezeichen, Stand heute ist es
4243500	4248340	gerade ein bisschen problematisch mit Twitter, bei Twitter als atdeckelen unter dem gleichen Handel
4248340	4257580	auch bei Instagram zu finden. Neben Fidiversum sind wir atzzd, atpodcasts.social und auf Facebook
4257580	4261420	haben wir nämlich auch noch eine Seite, da findet ihr uns als zwischen zwei Deckeln. Ihr könnt aber
4261460	4266780	auch ganz klassisch das alte web10 bemühen und auf zwischenzweideckel.de gehen, da findet ihr
4266780	4273060	alle Episoden genauso wie im Podcatcher eurer Wahl. Ihr könnt uns auf Spotify hören, aber wenn
4273060	4278660	ihr andere Tools nutzt, nutzt doch lieber die anderen, wenn es irgendwie geht. Auch wieder im
4278660	4283100	Anschluss an das Thema heute. Jetzt bleibt mir nur euch, bis zum nächsten Mal, viel Spaß beim
4283100	4287420	Lesen zu wünschen, macht es gut und bis bald. Bis dann, tschüss zusammen.
