Herzlich willkommen bei Future Histories, dem Podcast zur Erweiterung unserer Vorstellung von
Zukunft. Mein Name ist Jan Groß und ich freue mich sehr, dass ihr wieder dabei seid. Heute
spreche ich mit Felix Stahlder über Formen technopolitischer Machtausübung. Die aufmerksame
Hörerin, der aufmerksame Hörer dieses Podcasts, wird sich da fragen, Jan, was ist los? In Folge 0
hast du uns doch erklärt, dass es immer abwechselnd ein Interview und eine Denk-mit-mir-Folge gibt.
Letzte Folge war Amanda Vanessian zu Gast, also eine Interviewfolge. Das heißt, es müsste jetzt
eine Denk-mit-mir-Folge kommen. Da habt ihr vollkommen recht, aber ich breche quasi bei
erster Gelegenheit aus diesem Schema aus, denn ich habe gemerkt, es gibt einfach zu viele Interviews,
die ich gerne führen möchte und auf der anderen Seite reicht es vollkommen, wenn ich in unregelmäßigen
Abständen dann eine größere Denk-mit-mir-Folge mache. Das heißt, es gibt mehr Interviews und
ich werde dann auch einfach vor den jeweiligen Interviewfolgen, Updates, Neuigkeiten, kleinere
Pferden und so weiter kommunizieren, wie zum Beispiel den Input von Seiten der Association
for the Design of History, über die ich auf ein Buch mit dem Titel The People's Republic of
Walmart gestoßen bin oder den neuen Artikel von Evgeny Morozov in der New Left Review,
auf den mich der User Ed Guy Philosopher aufmerksam gemacht hat, mit dem Titel Digital Socialism,
beides very much on topic zur zweiten Folge von Future Histories zum Thema Socialist Calculation
Debate. Ich mache die Links in die Shownotizen, wirklich sehr interessantes Material und ich
werde hier definitiv in Future Histories auch weiterhin die Socialist Calculation Debate in
ihrer zeitgenössischen Ausprägung weiterverfolgen. Ich glaube, dass das ein sehr zentrales Thema ist.
In der heutigen Folge, im Gespräch mit Felix Stahl, da wird das Thema übrigens auch gestreift. Das
Interview wurde aufgezeichnet irgendwann Anfang des Jahres, ich weiß nicht mehr genau, aber da
hatte ich auf jeden Fall den Artikel von Evgeny Morozov noch nicht gelesen. Das Thema wird uns
also auf jeden Fall erhalten bleiben. Viel Spaß beim Interview und wenn ihr mehr über den Podcast
erfahren wollt, dann geht auf www.futurehistories.today, da gibt es einiges an Zusatzmaterial. Lasst mich
wissen, was ihr zu dem Ganzen denkt, auf Twitter unter Hashtag Future Histories oder auf Reddit.
Da gab es zum Beispiel auch schon sehr spannenden Input zum Thema Kybernetik vom User, ich glaube,
er hieß Bjarne. Danke dafür Bjarne. Also auf Reddit, zum Beispiel in unserem Subreddit oder
ganz klassisch per Mail unter future underscore histories at protonmail.com. Und wenn ihr den
Podcast darüber hinaus unterstützen wollt, dann schaut doch auf patreon.com schrägstrich Future
Histories vorbei. Herzlich willkommen bei Future Histories, dem Podcast zur Erweiterung unserer
Vorstellung von Zukunft. Mein Name ist Jan Groß und ich freue mich sehr, heute Felix Stahlder als
Gast begrüßen zu dürfen. Felix ist Kultur- und Medienwissenschaftler und Professor für digitale
Kultur und Theorien der Vernetzung an der Züricher Hochschule der Künste. Herzlich Willkommen Felix.
Hallo Jan. Im heutigen Gespräch geht es um formentechnologische Machtausübung im Allgemeinen
und im Speziellen um die Frage der Machtausübung durch Algorithmen. Vorweg also mal die grundsätzliche
Frage, was ist ein Algorithmus und welche unterschiedlichen Formen von Algorithmen gibt es?
In der allgemeinsten Form ist ein Algorithmus eine Serie von Handlungsanweisungen, um einen Input in
einen Output zu überführen, um ein Problem im allgemeinen Sinne zu lösen. Das Wesentliche am
Algorithmus in dieser ganz allgemeinen Definition ist, dass diese Handlungsschritte eindeutig
beschreibbar sind und dass es eine endliche Anzahl von Handlungsschritten hat. In dem Sinne
könnte man schon fast die Gebrauchsanleitung, die einem Ikea-Möbel beiliegt, als einen Art
Algorithmus bezeichnen. Es ist eine Serie von Schritten, die möglichst eindeutig beschrieben
sind. Deshalb nimmt man eine formale Sprache. Im Falle des Ikea-Beipackzettels sind das
Pictogramme, die ein Input, also die Serie von Brettern, die man kauft, in ein Output,
beispielsweise ein Tisch oder ein Regal verwandeln. Die ausführende Maschine sozusagen sind wir
selber, die diesen Schritten folgen. Damit eine Maschine tatsächlich diese Schritte ausführen
kann, müssen sie eindeutig beschrieben sein. Auch ein Pictogramm ist nie ganz eindeutig,
wir kennen das. Probleme beim Aufbauen gibt es immer, weil es unklar ist, was man jetzt
machen muss. Deshalb sind die meisten Algorithmen in Mathematik geschrieben, damit es keine Frage
der Interpretation der Schritte mehr gibt. Und dann, wenn diese Schritte interpretationsfrei
beschrieben sind, kann eine Maschine sie auch ausführen. In diesem Sinne gibt es Algorithmen
eigentlich schon so lange, wie es Computer gibt. Im Grunde ist es kein Unterschied zwischen
einem Algorithmus und einem Programm in einem Computer, der ja auch einen Input in ein Output
überführt, in einer Serie von definierten Schritten. Was jetzt heute aber neu ist, dass
diese Algorithmen, diese programmierbaren Schritte auf neue Klassen von Problemen angewandt werden
können und damit Dinge automatisiert werden können, also Problemlösungen automatisiert
werden können, die bis vor sehr kurzer Zeit noch nicht automatisiert werden konnten. Und
darum sprechen wir heute von Algorithmen, weil sie jetzt neu auftauchen in einer Serie von
Situationen, wo sie bisher noch nicht aufgetaucht sind und damit die Grenze neu definiert wird,
was lässt sich von einer Maschine erledigen und was ist eine Fähigkeit, die genuin menschlich
ist. Und welche Formen von Algorithmen ergeben sich denn jetzt durch technologischen Fortschritt?
Also ich denke da auch an selbstlernende Systeme und dergleichen? Zunächst sind zwei Voraussetzungen
oder eigentlich drei Voraussetzungen mussten zusammenkommen, um diese Welle von Algorithmen,
die sozusagen in den letzten fünf, zehn Jahren sozusagen in alle Bereiche der Gesellschaft
und des Lebens eingedrungen sind, möglich zu machen. Das Erste ist die Computing Power,
also die Rechenleistung von Computern ist enorm gewachsen und gleichzeitig sind die
Preise sehr billig geworden. Das ist ein klassisches Moore's Law, alle 18 Monate verdoppelt sich
die Rechenleistung bei gleichbreitendem Preis. Das Zweite ist, dass über das Internet enorm
viele Daten jetzt zur Verfügung stehen, Millionen und abermillionen von Bildern, ganz viele
Textdaten, ganz viele Bewegungsdaten, die man analysieren kann, ganz viele Videos und
Sprachdaten und so weiter. Das heißt, wir haben jetzt sehr große Mengen an Daten, mit
denen wir diese Algorithmen entwickeln können. Und das Dritte ist, dass es eine Serie von
konzeptuellen Durchbrüchen gab, also neue Innovationen auf dem Bereich der Algorithmusentwicklung,
oder der Computerwissenschaften, die es jetzt erlauben, ganz andere und komplexere Algorithmen
zu schreiben. Und das hat dazu geführt, dass sozusagen ein neuer Idee entstanden ist,
eine neue Praxis oder eine Idee hat sich durchgesetzt, sagen wir so, wie wir Algorithmen konstruieren.
Quasi die ersten Ideen der künstlichen Intelligenz in den 1960er- und 1970er-Jahren gingen von
sogenannten Experten-Systemen aus. Das heißt, eine Idee, dass man menschliche Wissen oder
zumindest in einer Experten-Domäne, deshalb der Name, so weit formulisieren kann, dass
man es auf eine Maschine übertragen kann. Also beispielsweise man macht Kategorien,
ein Baum ist eine Pflanze, viele Bäume sind ein Wald, und so kann man ein logisches System
von Zusammenhängen generieren. Das hat funktioniert in sehr engen Bereichen, deshalb diese Experten-Systeme,
die gibt es eigentlich seit den 1970er-Jahren, war aber sehr limitiert, weil es eben nicht
möglich war, diese Ontologien, diese Kategorien-Systeme, widerspruchsfrei und interpretationsfrei
zu generieren. Und was jetzt passiert ist, ist, dass ein anderer Ansatz sozusagen sich
durchgesetzt hat, der geht überhaupt nicht mehr um Kategorien-Systeme, sondern hier
geht es rein um statistische Auffälligkeiten. Es geht nicht mehr darum, einen Gegenstand
zu verstehen, sondern nur noch Korrelationen zwischen verschiedenen statistischen Merkmalen
zu bestimmen. Beispielsweise Übersetzungsprogramme, die wurden früher versucht zu schreiben,
indem man versucht, eben so eine Art eine Struktur der Sprache zu finden und die dann
von einer Sprache in die andere zu übersetzen, das ist vollkommen aufgegeben worden. Und
jetzt heute geht es nur noch um Statistik. Also Übersetzungsprogramme werden heute geschrieben,
indem sehr, sehr viel Text, von dem man weiß, dass sozusagen eine adäquate Übersetzung
besteht, beispielsweise aus der gesamten Publikationsbereich der UN, der Vereinten Nationen, die ja sehr,
sehr vielsprachig sind. Auch die Publikationen der europäischen Gemeinschaft sind ganz,
ganz wichtig, als ein Korpus von Mehrsprachigkeit. Und diese werden jetzt einfach statistisch
analysiert. Wenn im Deutschen das Wort x steht, dann ist eine so und so Wahrscheinlichkeit,
dass nachher das Wort y und z folgt. Und im Englischen ist es dann so und so und daraus
wird Übersetzung gemacht. Und das ist nur möglich mit sehr großen Datenmengen und
sehr starken Computerrechenleistungen und eben spezifischen Modellen, wie so etwas dann
gerechnet werden kann. Und das ist der Grund, warum wir heute plötzlich Algorithmen, also
automatische Entscheidungssysteme in Bereichen sehen, wo man bis vor sehr, sehr kurzer Zeit
dachte, so was ist eine Maschine nicht fähig zu tun. Bilderkennung und ähnliche Dinge
sind da vielleicht am bekanntesten. Und wie wird jetzt mit diesen gesteigerten
Fähigkeiten, auch eben durch die Strategieänderungen mitunter, wie wird über Algorithmen Macht
ausgeübt? Es gibt verschiedene Arten, wie Algorithmen
Teil einer Struktur von Macht und Machtausübung sind. Vielleicht ist es wichtig zu unterscheiden
zwischen verschiedenen Begriffen, was ein Algorithmus ist. Ein Algorithmus in den Computerwissenschaften
ist tatsächlich nur der ausführbare Code, also die Serie von Schritten, die ein Computer
ausführen kann. Das hat kein Vorher und kein Nachher. In einer umfassenden vielleicht so
soziologischen oder politischen Analyse eines Algorithmus muss man ihn sehen als ein Teil
einer Entscheidungskette. Der ist nie autonom. So etwas wie autonome Technologie kann es
nicht geben, sondern es ist eingebettet in eine Serie von Entscheidungen, die ein paar
davon werden von Menschen gemacht und ein paar von diesen Entscheidungen werden von
Maschinen gemacht. Aber es ist immer eingebettet. Das heißt, man kann sich zunächst einmal
fragen, auf welche Probleme wird ein Algorithmus ausgerichtet? Was ist ein Problem, das ich
lösen will mit Hilfe der Technologie? Das ist der erste grundsätzlich sozusagen politische
Aspekt, der immer sozusagen institutionell entschieden wird. Das entscheidet nie der
Algorithmus selber, wofür er entwickelt werden soll. Also wenn das Ziel meines Algorithmus
ist, die Grenze oder die Zahl sozusagen der Kredite, die nicht zurückgezahlt werden,
zu senken, dann ist das eine ganz spezifische Ausrichtung. Und dann konstruiere ich den
entsprechend. Das ist das erste. Also was ist das Problem überhaupt? Das zweite ist
dann, wie wird dieses Problem operationalisiert? Weil selten lässt sich ein soziales Problem
oder ein Ziel, das aus einem Businessplan kommt, direkt sozusagen quantifizieren. Sondern
man muss dann anfangen, ein Modell zu konstruieren. Das sagt, für die Frage, die ich beantworten
will, sind diese und diese Punkte relevant und alles andere ist nicht relevant. Und damit
wird ein Modell konstruiert. Oft ist es auch so, dass das, was man eigentlich messen will,
gar nicht direkt messbar ist, weil es zu diffus ist, weil es zu kompliziert ist, weil es zu
verteilt ist. Und dann macht man einen sogenannten Proxy. Also man definiert einen Wert, der
für etwas anderes steht. Beispielsweise, wenn ich messen will, wie gut ein Kind lernt,
das ist ein sehr komplexes Thema. Was ist denn das eigentlich? Was heißt Lernen? Was
ist das relativ zu anderen Dingen? Das kann ich nicht messen. Also füge ich einen Proxy
ein, einen standardisierten Test. Und dann kann ich sagen, okay, das Kind hat neun von
zehn Fragen richtig beantwortet. Dann habe ich eine Zahl. Und somit werden eine Serie
von Entscheidungen getroffen, wie ich ein Problem konstruiere, was ich überhaupt als
ein lösbares oder für mich relevantes Problem entscheide. Und damit sind natürlich immer
gewisse Interessen derjenigen, die diese Probleme oder diese Fragen dann entscheiden, sind verbunden
mit den Agenten und den Plänen und den Ideen, die diese Akteure haben. Und diese wird jetzt
in Technologie eingeschrieben, von der dann gesagt wird, sie sei autonom. Und damit verschwinden
diese Ideen und diese Setzungen, die an ganz vielen Stellen gemacht werden. Es gibt noch
weitere Stellen, wo diese Setzungen gemacht werden. Die verschwinden und am Schluss kommt
eine angeblich objektives Ergebnis raus. Dieses Kind hat neun von zehn Fragen beantwortet,
der andere hat nur sieben von zehn Fragen beantwortet. Und das sagt uns doch was, wer
besser und wer schlechter ist.
Also das ist ja im Grunde dann ein Ansatzpunkt, wo man sagt, okay, es ist schon die Art, wie
auf die Dinge geschaut wird, an der eigentlich in erster Instanz auch Macht ausgeübt wird,
indem Weichenstellungen vorgenommen werden, indem eben Entscheidungen darüber getroffen
werden, was ist das Problem und wie wird es gelöst. Darüber hinaus gibt es ja dann auch,
und das ist sicher dann ein Folgeproblem, sag ich mal in Anführungsstrichen, gibt es
ja bestimmte Architekturen, die zum Beispiel auch algorithmisch bedingt sind, denen wir
tagtäglich ausgeliefert sind. Also die sozialen Medien, in denen uns eine Welt konstituiert
wird durch den Algorithmus oder Google Maps oder sowas, was uns eine bestimmte Fahrtrichtung
oder einen Fahrplan empfiehlt und wir den wahrscheinlich in den meisten Fällen dann
auch wählen werden und so weiter und so fort. Also das heißt, es gibt quasi in einer ganz
pragmatischen Art und Weise eine Formung unserer Umwelt durch Algorithmen. Was mich interessiert
ist auch, sagen, eine Frage der sich verschiebenen politischen Souveränität. Es ist tatsächlich
so, dass ja politische Souveränität von solchen Konstrukten wie dem Nationalstaat
oder sowas sich tendenziell wegverlagert hin zu anderen Formen des Regierens, unter
anderem eben des algorithmischen Regierens. Und wie ist da deine Einschätzung, wer sind
die Hauptakteure, das ist vielleicht noch am einfachsten zu beantworten, aber was ist
vielleicht auch der dahinterliegende Gedanke des politischen Regierens an sich, der diesen
Formen innewohnt?
Also die Macht, die ausgeübt wird, die ist, wie wir gesprochen haben, einerseits auf der
Ebene der Problemlösung oder der Problemdefinition und dann der Modellierung, um es zu lösen.
Es werden hier ganz viele Sätze gemacht. Und auf der anderen Seite sozusagen, auf der Nutzerseite
haben wir dann den Fall, dass wir auch im besten Fall, also es geht nicht darum, ob der Algorithmus
stimmt oder nicht, nehmen wir mal an, der stimmt und der ist auch sozusagen mit einer
höheren Absicht gemacht, haben wir das Problem, dass dann gewisse Weltsichten plötzlich
sehr stark werden. Also der Teil der Welt, der sozusagen algorithmisch berechnet werden
kann, der wird uns zugänglich gemacht. Und andere Arten, andere Bereiche der Welt, die
sozusagen so nicht erfasst werden können, werden dann plötzlich schwieriger sichtbar.
Die werden entwertet, wenn das andere sozusagen ins Zentrum gerückt wird. Und das ist auch
etwas, das eine Weltsicht formt und das sozusagen implizite, nicht quantifizierbare Wissen sozusagen
immer schwieriger oder immer prekärer macht. Also beispielsweise Google Maps, wir haben
eine Vorstellung, wie wir von A nach B fahren wollen, aber Google Maps sagt uns, wir sollen
ihn woanders durchfahren. Dann haben wir natürlich die Frage, wem trauen wir jetzt? Und außer
in Orten, wo wir sehr, sehr gut, sehr, sehr genau wissen und sehr, sehr zeitnah auch wissen,
wie die Straßenverhältnisse sind, werden wir wahrscheinlich Google Maps trauen, weil
es Echtzeitdaten einbauen kann und so weiter und so fort. Und oft sind wir damit gut beraten,
also es ist gar keine Kritik daran. Aber es heißt einfach, dass sozusagen eine Form des
Wissens durch eine andere Form des Wissens sozusagen überschrieben wird. Und damit beginnt
auch sozusagen unsere Sicht auf die Welt. Nicht nur wird die Welt gemacht, sondern wie
erleben wir uns in der Welt, wie agieren wir in der Welt, wird geändert. Und die Akteure
sind eigentlich diejenigen Institutionen, die über die Voraussetzungen verfügen, um
überhaupt im großen Stil Algorithmen in ihre Entscheidungsprozesse einbauen zu können.
Das heißt, sind diejenigen, die über große Datenmengen verfügen, um überhaupt Algorithmen
trainieren zu können und dann anwenden zu können, sind diejenigen, die über große
Rechenkapazitäten verfügen und sind diejenigen, die dann nachher auch über ein Output verfügen,
der es ihnen erlaubt, aufgrund dieser Entscheidungen auch handeln zu können. Also beispielsweise
uns dann bei Google Maps eine Karte anzuzeigen oder Suchergebnisse anzuzeigen. Also diese
drei Voraussetzungen beschreibt die Gruppe derjenigen, die überhaupt fähig sind auf
diese Art und Weise zu handeln. Und damit ist eigentlich relativ klar beschrieben, wer
das ist. Es sind große Institutionen, für einzelne ist das quasi unmöglich auf diesem
Feld zu handeln. Es sind große Institutionen, die über sehr viele Daten verfügen und die
entsprechende Handlungsmöglichkeiten haben, um mit dem Wissen, das sozusagen generiert
wird, dann auch tatsächlich Handeln in der Welt zu verändern.
Und diese Asymmetrie der Ressourcen letztlich ja auch, die ist ja nicht so ohne Weiteres
zu überwinden, oder, würde ich mal denken. Das heißt aber auch, dass alle Handelnden,
die ähnliche, also die positiven Seiten dieser Entwicklung nutzen wollen, aber in einer
Form, die eben, ich weiß nicht, wie man das dann benennt, weniger gängelnd oder weniger
durch einen libertären Paternalismus geprägt sind. Wenn Leute das quasi anders nutzen
wollen, dann sind sie eigentlich per se immer schon im Hintertreffen, oder? Ist das zunächst
mal, stimmt das, weil das wäre ja schon mal fatal eigentlich?
Für die absolute Mehrheit der Nutzer, im Grunde für alle Nutzer, stellen diese Systeme
eine Take-it-or-leave-it-Situation dar. Wenn ich Facebook nutzen will, dann nutze ich es
mit allem, was dort ist. Ich kann nur sagen, ja, ich lasse mich einfach sein, oder nein,
ich lasse mich nicht einfach sein. Das ist aber in ganz vielen Situationen gar keine
richtige Wahl mehr, weil gewisse Dinge, um am heutigen Leben teilnehmen zu können, vorausgesetzt
werden, weil das sozusagen das neue Normal ist. Das machen alle. Es ist ungewöhnlich,
daran nicht teilzunehmen, und wenn man das nicht hat, dann kriegt man Probleme mit der
Aktion. In ganz vielen Beispielen ist das so. Es gibt ganz viele Dinge, die wir brauchen,
um am Leben teilzunehmen. Wir brauchen zum Beispiel eine Adresse. Und wenn man keine
Adresse hat, beispielsweise wenn man wohnungslos ist, dann kann man sich um keine Wohnung bewerben,
dann kann man sich um keine Stelle bewerben, dann kann man sich um ganz viele Dinge nicht
bewerben. Das heißt, wir brauchen all diese Dinge und eben auch Zugang zu diesen Systemen
und Nutzung dieser Systeme, um überhaupt uns in der Gegenwart am Leben teilzunehmen zu
können. Das sind dann aber, wie gesagt, Entscheidungen, die alles oder nichts sind. Entweder nehme
ich Facebook, wie es ist, oder ich bleibe außen vor, was für die meisten keine Option
ist. Und es ist ja auch nicht so, dass diese Dienste und sozusagen diese algorithmische
Unterstützung einfach nur negativ ist. Die sind ja gewachsen und entstanden, weil es
ein Problem gab, zum Beispiel mit der Menge an Informationen, die wir bewältigen müssen
in einer Gesellschaft, die halt immer informationsintensiv und immer komplexer wird. Und da brauchen
wir neue Werkzeuge dazu und diese algorithmischen Filters in dem Fall oder diese Möglichkeiten,
eben sehr große Bildmengen durchzuschauen nach Bildinhalten und nicht nur nach Bildbeschreibungen,
das ist etwas, das genuin nützlich ist. Das heißt, für die meisten Personen ist, für
die meisten Nutzer ist der unmittelbare Mehrwert durch die Nutzung gegeben und der soll auch
nicht abgestritten werden. Aber es wird halt mit dieser Alles-oder-Nichts-Proposition werden
auch Dinge eingehandelt, dass man sich eben in einer Umgebung bewegt, die man kaum durchblicken
kann, wo man überhaupt nicht sieht, was sozusagen hinter den Kulissen stattfindet und die aber
von einer einzelnen Identität, von einem Akteur, sagen wir jetzt in dem Fall Facebook
oder Google oder der Suchmaschine für Flugzeugtickets oder was auch immer vollständig kontrolliert
wird. Das heißt, wenn diese sich entscheiden, jetzt wollen wir lieber X statt Y zeigen,
dann ist das einfach so und als Nutzer hat man keinen Einfluss darauf, oft merkt man
das auch gar nicht. Und da entsteht natürlich ein extremes Machtgefälle zwischen der einen
Seite, die die Parameter kontrollieren kann und der anderen Seite, der Nutzerseite, die
dann sozusagen in dieser Welt sich zurechtfinden muss, im positiven, im negativen Sinn, wo
aber kaum eine Feedback-Möglichkeit, kaum eine Einflussmöglichkeit ist, kaum eine Entscheidungsmöglichkeit
ist der Nutzer, Nutzerinnen, wie sie dann diese Parameter konstruiert hätten. Das liegt
vollkommen in der Hand dieser relativ kleinen Anzahl von Akteure, die hier diese neuen Systeme,
in denen wir dann letztendlich leben, halt konstruiert und immer auch so konstruiert,
dass der Raum, der dadurch entsteht, das Problem sozusagen, das sie lösen wollen,
eines ist, das ihnen nützt. Und wie würden Alternativen aussehen, die versuchen das anders
zu gestalten, wo also die einzelnen User und das sehe ich so ein bisschen als den Kernpunkt,
auf den du dann auch eben ansprichst, wo die einzelnen User eben die Möglichkeit haben,
an Entscheidungsprozessen, höher gelegenen Entscheidungsprozessen zu partizipieren. Also
zum einen, wie müsste man das angehen und ist das überhaupt jetzt noch möglich, wo
schon eine solch große Plattform Macht etabliert worden ist, dass im Grunde die jegliche Konkurrenz
schon immer vor dem Problem steht, dass ein breiter Teil des Kuchens quasi schon vergeben ist?
Ich glaube, es gibt zwei oder mindestens zwei verschiedene Arten der Ansätze, wie man da
agieren könnte. Das sind jeweils kollektive Ansätze. Ich glaube, das individuelles Verhalten,
ich kann jetzt mein Verhalten auf Facebook ein bisschen so oder anders ausrichten. Grundsätzlich
kann ich an der Struktur nichts ändern. Also die individuelle Handlungsebene, die ja immer
so in den Vordergrund gesetzt wird, ist hier sehr, sehr beschränkt. Eine Handlungsebene
ist Transparenz zu schaffen, dass man weiß, was passiert, dass man weiß, wie ausgewählt
wird und da gibt es verschiedene Ansätze dazu. Der eine Ansatz, der beispielsweise die Datenschutzgrundverordnung
verfolgt, ist, dass es jetzt neu in Recht eingeführt wird, eine sozusagen relevante
Auskunft zu bekommen, wie ein Algorithmus funktioniert bzw. wie ein Entscheid zustande
gekommen ist. Das ist jetzt noch kein Algorithmentiff im Sinne einer Audit, eines Codes, sondern
das ist vielleicht eher wie auf einer Lebensmittelpackung, die Zutaten stehen, wo man sehen kann, wie
viel Zucker enthält etwas oder andere Dinge, wie viele Stabilisatoren sind in einer Schokolade
drin und Ähnliches. Das ist noch nicht das Rezept, nachdem dieses Lebensmittel hergestellt
wurde, aber es gibt mir eine gute Vorstellung davon, was es ist, was ich jetzt esse. Es
gibt mir einen Ansatz einer Vorstellung davon und das versucht die Datenschutzgrundverordnung
so etwas einzuführen, dass man Recht hat zu sehen, wie ein Entscheid zustande gekommen
ist. Die andere Art Transparenz zu schaffen, die mir eigentlich fast die Wichtigere erscheint,
ist, dass man Vergleichbarkeit generiert, weil wenn ich jetzt auf Facebook gehe oder auf
eine Google-Suche mache, habe ich keine Möglichkeit zu sehen, was ich nicht sehe. Ich sehe nur
ein Resultat und das Resultat hat eine gewisse Glaubwürdigkeit, hat eine gewisse Nützlichkeit.
Ich gehe auf Google, suche etwas und finde etwas, das ungefähr dem entspricht, was ich
denke finden zu können, weil es natürlich auch meinen Erwartungshorizont mit in die
Auswertung einnimmt. Ich gehe auf Facebook und sehe eine Auswahl von Meldungen von Leuten,
die ich kenne, aber ich kann nicht wissen, was ich nicht sehe. Eine Möglichkeit hier
Transparenz zu schaffen, wäre, verschiedene Algorithmen, verschiedene Filter in dem Fall,
verschiedene Suchstrategien vergleichbar zu machen. Wenn ich so suche, kommt das, wenn
ich so suche, kommt das. Schon nur, wenn ich wählen könnte, will ich die Personalisierung
anstellen oder ausstellen oder vielleicht auch wählen könnte, will ich meine eigene
Personalisierung oder die einer anderen Person. Da würde ich plötzlich sehen, aha, unter
dem kommt ja auch das, unter dem anderen und ich sehe jetzt plötzlich Dinge, die ich sonst
nicht sehen würde. Und das ist, glaube ich, diese Vergleichbarkeit des Outputs ist umso
wichtiger, weil diese Algorithmen ja vollkommen intransparent sind. Sie sind einerseits intransparent
aus Gründen der Geschäftsgeheimnisse, andererseits sind sie intransparent, weil sie sich dauernd
ändern. Die werden teilweise vorhand nachjustiert. Facebook sagt, die wollen jetzt weniger, sozusagen
kommerziell hergestellte News haben, sondern mehr sozusagen von deinen Freunden und dann
ändern sie die Algorithmen, weil sie ihr Geschäftsmodell oder ihre Ausrichtung ein bisschen ändern.
Das ist das eine. Das heißt, wir sind mit Outputs einer Maschine konfrontiert, die sich
dauernd ändert. In einer stabilen Maschine, in einer klassischen Blackbox, kann ich mit
der Zeit erahnen, wie die innen funktioniert, weil ich sehe, wenn ich x reindehne, dann
kommt y und wenn ich z reindehne, dann kommt a. Das ist okay, gut, das macht wahrscheinlich
dieses oder jenes. Aber diese neuen Algorithmen, die sind hochdynamisch. Das heißt, wenn
ich zweimal dasselbe reingebe, kommt nicht dasselbe raus. Das heißt, ich habe noch viel
weniger Möglichkeit zu verstehen, was da passiert. Ich kann das nur nehmen, wie es kommt. Wenn
ich es aber vergleichen kann, dann verstehe ich zwar immer noch nicht, was innen passiert,
aber ich sehe, dass es verschiedene Möglichkeiten gibt, das Problem zu lösen und dass vielleicht
eine besser ist als die andere oder eine nützlicher ist als die andere. Das heißt, die Frage,
wie ich hier Vergleichbarkeit schaffen kann, also Vergleichbarkeit zwischen verschiedenen
algorithmischen Strategien scheint mir sehr, sehr relevant zu sein, um Transparenz zu schaffen,
die eben weitergeht wie nur eine Liste von, ja, wir berücksichtigen Alter, Wohnort und
sozusagen Einkommen im Filter, den wir konstruieren. Transparenz ist so die eine Richtung, das
wird beides nur gehen, die Datenschutzgrundverordnung hat da angefangen, wir beides nur gehen mit
staatlicher Regulierung, die sowas vorschreibt, weil das natürlich nicht im Interesse des
Anbieters ist. Interesse des Anbieters ist sozusagen, ihre eigenen Entscheidungen quasi
unsichtbar zu machen und damit aus jeder Kritik, aus jeder Kollektivitätskontrolle, aus jeder
politischen Verhandlung zu entfernen. Das ist der eine Teil. Und der andere Teil ist sicher,
eine gewisse Regelung, gewisse Grenzen einzuführen, was ein Algorithmus darf, was er kann, können
sein muss und wie die Verantwortlichkeit für Entscheidungen konstruiert ist. Also beispielsweise
wenn ein Algorithmus eine Fehlentscheidung macht, dann kann ich niemanden zur sozusagen
Verantwortung ziehen. Weil wer ist es? Ist es die Stelle, die ihn einsetzt? Ist es die
Stelle, die ihn geschrieben hat? Ist es sozusagen, ist es überhaupt ein Fehlentscheid oder ist
es sozusagen nur ein personalisierter Entscheid und so weiter. Und damit kreieren wir eine
Serie von sehr problematischen Anreizen. Beispielsweise, wenn mein Ziel ist, die Rate derjenigen zu
senken, die, beispielsweise ich bin Versicherer, ich biete Autoversicherungen an und ich will
diejenigen nicht versichern oder die teurer versichern, die mit größerer Wahrscheinlichkeit
einen Unfall machen. Das ist klassisches Versicherungsmathematik. Aber wenn ich jetzt
sage, okay, ich habe jetzt einen neuen Algorithmus, der mir das macht, damit kann ich sozusagen,
aus irgendeiner Population habe ich 100 richtige Identifikationen, also Leute, die tatsächlich
höhere Wahrscheinlichkeit haben. Ich habe aber auch 30 Folgen drin. Leute, die sozusagen
jetzt ungerechtfertigt entweder die Dienstleistung nicht bekommen oder sie zu einem höheren
Preis bekommen, dann macht sich das aus einer betriebswirtschaftlichen Rechnung Sinn. Ich
habe mich immer noch um 70 Fälle verbessert. Aus der Sichtweise des Nutzers, der ist dann
plötzlich mit dem System konfrontiert, dass er aus seiner Perspektive, aus ihrer Perspektive
vollkommen willkürlich entscheidet. Ich will mit dem Fehlentscheid konfrontiert und ich
kann nichts dagegen machen, weil sozusagen für den Anbieter es ist egal. Und ich weiß
ja nicht mal genau, woher der kommt. Ich kann ja nicht mal sicher sagen, dass das ein Fehlentscheid
ist. Und hier denke ich, ist es wichtig, dass auch die Frage der Verantwortung und das heißt
in einem ökonomischen Sinn auch der Schadenersatz so geregelt wird, dass diese Anreize anders
funktionieren. Dass es eben ein schlechter Algorithmus ist, der 30 Leute fälschlicherweise
aussortiert, aber 100 richtig identifiziert, dass die Rechnung einfach nicht mehr funktioniert.
Das wäre sozusagen der zweite oder der dritte Punkt. Das erste ist Transparenz, der zweite
ist sozusagen eine Begrenzung dessen, was erlaubt ist. Und das dritte ist Frage nach
den Konsequenzen, die einklagbar sein müssen und die mit Schadenersatz verbunden sein müssen,
weil sonst sind sie irrelevant.
Da gibt es jetzt ein paar Anschlussfragen, die sich mir stellen. Zum einen ganz konkret
in Bezug auf die ja eigentlich künstliche Limitierung der Algorithmen auf Algorithmen,
die eigentlich letztlich vollständig nachvollziehbar bleiben. Ist es nicht so, dass diese Grenze
jetzt schon überschritten worden ist? Also gibt es nicht schon längst Algorithmen, die
eigentlich von Menschen selbst im Grunde nicht mehr nachvollziehbar sind, weil eigentlich
Algorithmen Algorithmen trainieren und dann sagen am Ende einer für den Menschen im Grunde
allein schon aus kognitiven Fähigkeiten, also aus der Begrenzung der kognitiven Fähigkeiten
heraus, dass es dann eigentlich nicht mehr nachvollziehbar ist? Und ist es dann, scheint
es realistisch für dich?
Das hat wiederum zu tun mit der Frage, was ein Algorithmus ist.
Wenn ich sage, ein Algorithmus ist eine Serie von kodierten Schritten und das fängt sozusagen
mit der ersten Zeile des Codes an und hört auf mit der letzten Zeile des Codes, dann
ist es tatsächlich so, dass wir mit ganz vielen Algorithmen zu tun haben, die wir nicht mehr
lesen können. Und wenn wir sie endlich gelesen haben und vielleicht auch verstanden hätten,
falls das noch geht, dann stimmt das, was wir verstanden haben, schon nicht mehr, weil
der Algorithmus sich selber immer weiterentwickelt. Das heißt, wir haben auf der Ebene des Codes
tatsächlich die Situation, dass wir vieles nicht mehr verstehen. Aber wenn wir etwas
erweiterten Begriff des Algorithmus nehmen, dann ist ein Algorithmus eben immer eingebettet
in ein institutionelles Gefüge, der einerseits das Problem definiert und andererseits die
Lösung definiert. Also was ist eine gute Lösung? Was der Algorithmus dann macht, ist
einen Weg zu finden von Problem zur Lösung. Aber wir können einen Algorithmus nicht trainieren,
wenn wir nicht eine Vorstellung haben von der Lösung. Also wir wollen einen Gesichtserkennungsalgorithmus
definieren, der eine Person erkennt, die auf einem Video ist und das gegen eine Datenbank
von gespeicherten Passfotos abgleicht. Dann müssen wir zuerst eine Serie von Videos haben,
wo wir wissen, wer drauf ist und sagen, schau, das ist so, das ist so, das ist so. Und dann
können wir anfangen, den Algorithmus zu trainieren. Das heißt, auch wenn wir vielleicht nicht
mehr wissen, den Code nicht mehr lesen können, wissen wir immer noch, wie ist das Problem
definiert und was ist die Lösung. Das heißt, wir können dann auf der Ebene des Schrittes
vorher und nachher immer noch eine Analyse dieses Algorithmus machen, jetzt weniger als
ein technisches Artefakt, sondern als ein institutionelles Artefakt. Was für eine Entscheidungskette
wird hier gemacht und ist die kompatibel mit Dingen, die wir in der Gesellschaft hochhalten,
Antidiskriminierung, sozusagen Preistransparenz, was es danach immer ist und um was es konkret
geht. Aber wir können diese rein technische Intransparenz, kann man sozusagen begegnen,
indem man eine institutionelle Analyse macht, die diesen Algorithmus ja erst hervorbringt,
diesen Algorithmus sozusagen laufen lässt, die auch laufend korrigiert, ob er immer noch
aufs Ziel hingeht und wenn nicht, Adaptionen macht, damit er genau auf das Ziel ist. Das
heißt, solange wir das Problem und das Ziel, also das Problem und die Lösung kennen, müssen
wir gar nicht so genau wissen, ob die Schritte, die gemacht werden, um von A nach B zu kommen,
wie die aussehen.
Auch nicht für das, was du eben angesprochen hattest, also für die Frage zum Beispiel
einer rechtlichen Verantwortlichkeit für getroffene Entscheidungen. Das wäre, glaube
ich, der erste Aspekt, der sich mir stellt. Und die zweite Frage wäre, vor, ich glaube,
einem Jahr oder anderthalb gab es ja dieses relativ Publicity-trechtige, die so eine Presse
Sendung, wo, glaube ich, Deep Mind darauf trainiert worden ist, so Old-School-Arcade-Games
zu erlernen ohne jegliche Zieldefinierung. Weil sie noch nicht mal, glaube ich, soweit
ich mich erinnern kann, als ich das gelesen hatte, noch nicht mal gesagt worden ist, es
handelt sich um ein Spiel, noch gesagt worden ist, erlerne dieses Spiel, sondern, so habe
ich es zumindest verstanden, dass es einfach nur zueinander gesetzt worden ist und dann
auf Basis dessen dann die günstliche Intelligenz sowohl das Spiel erlernt hat und eine Meisterschaft
darin erlangt hat, die jedes menschliche Tun bei weitem übertrifft.
Also Spiele haben ja in sich eine ganz klare Lösung drin. Also mach Punkte, next level,
sei schneller. Und Spiele sind eigentlich sehr gut, um sie zu lernen, weil es man eben
dann sagen kann, okay, der eine algorithmische Strategie, die hat 7350 Punkte geschafft und
die andere algorithmische Strategie hat 7480 Punkte geschafft. Also ist klar, zwei ist
besser als eins, wir fahren dort weiter. Von dem her ist auch da natürlich eine Lösung
ist drinnen. Aber Sie haben noch nicht mal dem Algorithmus
gesagt, dass es sich um ein Spiel handelt. Ich glaube, das war so ein bisschen der Punkt,
worauf Sie hinweisen wollen, weil das sozusagen ein Spiel an sich gut erlernt werden kann,
weil es eben ja mit quantifizierbaren Parametern arbeitet. Das hätte jetzt nicht so verwundert.
Ich glaube, was betont worden war, war wirklich der Aspekt, dass vorher, also dass diese Zielmaßgabe
eben nicht gesetzt wurde, glaube ich. Da müsste man sich eben im Detail anschauen.
Also ein Training kann nicht funktionieren ohne Ziel, weil man immer, gerade diese Selbstlernen
Algorithmen sind ja iterativ. Sie sind in Feedback schlafen, immer wieder zehntausende,
hunderttausende, millionenfach. Und was passiert ist, also wie das funktioniert, ist, dass man
beginnt mit sehr primitiven Lösungsstrategien und schaut dann, wie nah sind Sie am Ziel.
Und diejenige, die näher ist, die nimmt man, macht ganz viele Variationen, schaut, welches näher,
dann nimmt man die, die am nächsten ist, macht ganz viele Variationen davon und so weiter.
Das heißt, ganz ohne eine Zielstrategie kann so ein Lernprozess nicht funktionieren.
Von dem her ist die Frage, auf welcher Abstraktionsebene macht man das Spiel. Also macht man diese
Lernprozesse. Vielleicht heißt es jetzt in dem Fall noch, lerne selber die Regeln des Spiels und
dann wird gut darin. Die Zielvorgabe ist abstrakt. Aber jetzt sozusagen, wenn der Algorithmus zuerst
entscheiden muss, will ich überhaupt das Spiel spielen, dann wird das nicht gehen, weil er dann
nicht weiß, was er lernen soll. Das heißt, Algorithmen sind eigentlich immer, auch nach wie vor,
sehr eng im Sinne dieser Lösungsfindung auf und vor definiertes Problem. Generelle künstliche
Intelligenz, die sozusagen diesen Range von Problemen lösen kann, wie das menschliche
Intelligenz ist, ist nach allem, was ich weiß, noch sehr, sehr weit weg. Wir können eben sehr,
sehr gut was trainieren, um Gesicht zu erkennen in Bildern. Aber das ist was fundamental anderes,
wie ein Algorithmus, der weiß, wie ich im Roboter eine weiche Traumate aufgreife,
ohne sie zu zerdrücken. Das sind fundamental andere Algorithmen, die sich auch nicht einfach
verbinden lassen. Von dem her glaube ich, dieses Zielgerichtete ist ganz, ganz entscheidend. Und
ohne das gibt es meines Wissens nach so gut wie nichts oder gar nichts, dass man tatsächlich als
Lernend betrachten könnte. Der erste Teil war, ob die Frage der rechtlichen Verantwortung auch
damit beantwortet ist, dass man sagt, wir wissen noch um einen Anfang und um ein Ziel. Und über
die Setzung dieser Parameter können wir quasi auch eine von mir aus Einklagbarkeit erstellen,
wenn jetzt zum Beispiel, wie du das bei der Versicherung gesagt hast, die 30 Leute dann zu
Unrecht einen höheren Beitrag zahlen oder so. Die Frage ist, was müssen die dann ja auch liefern
als Beweisführung oder liegt die Pflicht zur Beweisführung dann bei der Versicherung? Das
wäre ja zum Beispiel logisch. Das wäre die Frage. Also beispielsweise, wenn man ein
Antidiskriminierungsverbot nimmt und dann gibt es einen, jemand wird angehalten, von der Polizei
eingekontrolliert und die reist nach einer Diskriminierungsklage ein und sagt, ich bin
hier nur kontrolliert worden, weil ich eine dunkle Hautfarbe habe. Dann würde jetzt der
Polizist ja gefragt, nachher um das zu entscheiden, ob das jetzt tatsächlich ein diskriminierender
Handler war oder nicht. Er sagt, warum hast du diese Person kontrolliert? Und dann sagt er vielleicht,
ja ich hatte hier den Auftrag, jede zehnte Person, die an dieser Ecke vorbeigeht, zu kontrollieren.
Das war mein Auftrag, den habe ich ausgeführt, das war die zehnte Person, fertig, oder? Und dann
wäre sozusagen die Frage der inneren Motivation des Polizisten, wäre eigentlich auch relativ egal.
Das wäre sozusagen der Code, den wir auch nicht ganz verstehen, warum hat er das jetzt gemacht.
Wäre relativ egal, weil er in einem ganz klaren regelgebundenen Ablaufsystem drin war. Und dann
könnte man aber sozusagen die nächste Frage stellen, warum wurde diese Art von Kontrolle an dieser Ecke
durchgeführt? Ist es nicht klar, dass an dieser Ecke nur sozusagen, weil das eine Gegend ist,
die und die Gruppe von Personen vorbeikommt. Dann wäre aber die Verantwortung sozusagen,
ob das jetzt ein Racial Profiling ist, nicht mehr beim einzelnen Polizisten, sondern sozusagen
eine Ebene höher beim Einsatzleiter gesagt, geh dorthin. Und insofern kann man das schon
auf dieser institutionellen Ebene, wer definiert das Problem und wer macht sowohl das Problem und
die Lösung definieren. Und dann die Frage, ob diese Problemdefinition und diese Lösungsstrategie
diskriminierend waren, muss dann halt im Einzelfall entschieden werden. Das können wir aber heute
sozusagen in anderen Bereichen ja auch machen. Und die Frage ist da nur nicht, oh Gott, wir haben da
eine Million Fälle, die wir dann plötzlich jeden Tag entscheiden müssen, sondern nur, was ist der
Preis dafür, wenn Diskriminierung festgestellt wird und lohnt sich jetzt für den Anbieter,
das Risiko, diesen Preis zahlen zu müssen, wenn Diskriminierung festgestellt wird. Das ist eine
betriebswirtschaftliche Überlegung. Und da muss diese Ausgestaltung der Verantwortung nach der
möglichen Schadenersatzforderungen entsprechend gestaltet sein. Das wäre sagen ein Lösungsvorschlag,
wie du es jetzt gerade sagst, dass wir halt quasi ein Regelwerk erschaffen für Fälle der
Diskriminierung und wie dann vorgegangen wird. Aber das kann dann immer noch heißen, dass der
Versicherer oder die Versichererin selbst, also die Firma jetzt, selbst das intern gar nicht
nachvollziehen kann eventuell, oder? Weil zum Beispiel der Algorithmus sich selbst trainiert hat und
innerhalb des Trainings darauf gekommen ist, dass ja bestimmte ZIP-Codes, bestimmte Postleitzahlen
irgendwie zu günstigeren Ergebnissen führen für die Versicherung. Aber ohne dass der Algorithmus
das jetzt wusste, sind das halt dann Postleitzahlen, in denen mehrheitlich Leute mit zum Beispiel
Migrationshintergrund leben und so weiter und so fort. Also das könnte dann immer noch passieren,
oder? Das könnte immer noch passieren, aber dann ist es halt klassisches Compliance einer Firma,
halt darauf ein Auge zu werfen. Im Grunde ist ein Algorithmus nichts anderes, wie automatisiert
die Arbeit von potenziell sehr vielen Leuten. Mit Mitteln, die Menschen nicht zur Verfügung
stünden in dem Fall dann auch, oder? Ja, es ist eine Automatisierung halt. Und wenn man es jetzt
aber sozusagen zurück übersetzt in sehr viele Leute, dann kann man ja auch nicht sagen, der CEO
einer Firma kann ja unmöglich wissen, was alle 100.000 Angestellten machen. Wie kann der verantwortlich
sein für das, was die Firma macht? Ist er aber. Ja, das wollte ich auch gar nicht. Also ich wollte
nicht sagen, dass die Firma deswegen nicht mehr... Nein, nein, ich sage nur, nur weil das in einer
anderen Materialität, in einer anderen Sprache ist, eine Sprache, die tatsächlich oft schwierig zu
verstehen ist und teilweise sozusagen in Echtzeit nicht verständlich ist für niemanden, heißt ja
nicht, dass diese klassischen Themen der Verantwortung, der sozusagen Rechtssicherheit und
weiß ich nicht was, nicht mehr gelten. Es ist einfach ein Versuch, sich dieser Verantwortung zu
entziehen, indem man sagt, ja, das sind Maschinen. Google sagt, das ist ja immer nur ein Algorithmus,
wir sind nicht verantwortlich für die Suchresultate. Tatsächlich sind sie natürlich verantwortlich
für die Suchresultate, weil sie verantwortlich für den Algorithmus sind und tatsächlich die
Suchresultate so gestalten, dass es innerhalb ihres Businessmodells richtig ist. Vielleicht gehen wir
mal zur Ebene der gesellschaftspolitischen Entwürfe, weil da ist es ja schon so, dass die
ungeheure Effizienz, die ja durchaus entsteht durch die Verfügbarkeit dieser großen Datenmengen und
die entsprechende Rechenleistung, die eben nicht jedem Akteur zur Verfügung steht, sondern nur
wenigen, dass die eine Convenience, eine Bequemlichkeit in der alltäglichen Interaktion
mit den jeweiligen Produkten erzeugt, die unglaublich verführerisch ist. Also eben Google Maps ist
wirklich also in den allermeisten Fällen unglaublich praktisch, unglaublich gut, auch
einfach und dem zu widerstehen gelingt den wenigsten. Wenn man das jetzt umlegt auf eine
größere gesellschaftspolitische Vision von Zukunft, jetzt mal ganz grob gesprochen, ja,
was siehst du für Entwürfe von Zukunft, die es schaffen, diesem Bequemlichkeitsangebot,
das da ja im Raume steht, zu widerstehen? Ich würde das nicht als ein Bequemlichkeitsangebot
bezeichnen und deshalb sozusagen die Frage, dem Zuckerl zu widerstehen, glaube ich, ist eine
falsche Art, darüber zu denken. Ich glaube, das sind Angebote, die uns erlauben, eine sehr,
sehr viel komplexere Welt zu navigieren. Nehmen wir Google Maps. Wir reisen alle viel mehr. Wir
sind alle viel mehr an Orten, an denen wir uns nicht gut auskennen. Und jetzt ist natürlich
Google Maps sehr, sehr gut, um sich an den Orten, wo ich mich nicht gut auskenne, von A nach B zu
finden. Und dass wir es oft in diesen Situationen sind, ist eine Funktion eines veränderten Lebens.
Ich will wissen, was für ein Angebot gibt es, um von A nach B zu fliegen. Da ist die Komplexität
sozusagen des Marktes so gross, dass ich überhaupt nicht fähig bin, mich zu orientieren, ohne die
Hilfe von Plattformen, die diese Angebote sozusagen aggregieren und vergleichbar machen. Für sehr,
sehr viele Probleme, wie optimiere ich den Verkehr in der Stadt? Brauche ich sehr große Datenmengen,
um einen anderen Blick darauf zu bekommen? Wie können wir vielleicht komplex auch über
Energiekreisläufe da nachdenken und entsprechend unser Wirtschaftssystem reorganisieren? Brauchen
wir sehr, sehr komplexe Modelle, die auf sehr, sehr großen Datenmengen generieren? Insofern sind diese
sozusagen algorithmischen Dienstleistungen, die uns erlauben, die Welt anders zu sehen,
notwendig, weil wir brauchen ein anderes Bild der Welt. Auf ganz, ganz vielen Gründen. Nichts
zuletzt eben aus Klimaschutzgründen. Aber wir leben halt auch in einer Welt, die dynamisch ist,
die sehr groß ist, weil wir uns sehr weit bewegen und entsprechend brauchen wir Hilfsmittel, um uns
zu orientieren. Von dem her würde ich das nicht als sozusagen etwas sehen, dass das jetzt nur
Bequemlichkeit ist. Es hat das schon auch, wie wir schon darüber gesprochen haben, diese eine Art,
sich zu orientieren, jetzt bei Google Maps beispielsweise, die ersetzt zunehmend auch
andere Arten, sich zu orientieren. Das heißt, man wird danach sozusagen von diesem neuen Normal bis
zu einem gewissen Grad abhängig. Aber das heißt einfach, wir werden abhängig als Gesellschaft von
den Werkzeugen, mit denen die Gesellschaft gebaut ist. Das war eigentlich schon immer so. Das Problem
kommt jetzt daher, dass wir diese Hilfsmittel, die uns erlauben sozusagen die Welt zu sehen in
einer Art und Weise, wie sie unserem Verhalten in der Welt entspricht, die bekommen wir im Rahmen
eines Paketes. Und dieses Paket sagt, wir geben dir das Tool, aber du hast überhaupt keine Möglichkeit
zu sehen, wie wir dich über dieses Tool beeinflussen können. Wie wir dich steuern können,
was du siehst und wie du dich bewegst. Wir wissen nicht, wie Google Maps, das ist noch
ein relativ nachvollziehbares Beispiel, weil es tatsächlich in der Stadt ist und ich kann mich
dadurch bewegen und sehen, ah, ich hätte auch anders gehen können. Aber wir wissen nicht,
wie es an der Route zustande kommt. Und man kann sich einfach vorstellen, wie beispielsweise ein
Staat, Google sagt, wir wollen aber nicht, dass zu viel Verkehr an dieser Ecke vorbeigeht,
weil die ist sensitiv. Und dann werden wir nie die Route an dieser Ecke bekommen und wir wissen
nicht, warum. Und da liegt das Problem. Das Problem, dass diese Tools, wir brauchen die,
die sind positiv, die erlauben uns sozusagen in der Gegenwart zu leben. Und sehr, sehr komplexe
Probleme, die wir haben, die ja nicht einfach sozusagen von diesen Tools gemacht wurden,
sondern die bestehen ja. Vielleicht besser anzugehen wie ohne diese Tools. Aber sie sind
gefasst in einer sozusagen politisch-ökonomischen Institution, die keine Regeln akzeptieren will,
also demokratische Fassungen und sozusagen die, die diese Macht, die damit zusammenhängt,
vollkommen intransparent verwaltet. Und da keinerlei Möglichkeiten sind, von den demokratischen,
oder schwachen Möglichkeiten bisher nur, von den demokratisch verfassten Willensbekundungen,
da gewisse Regeln einzuführen, gewisse Beschränkungen einzuführen, gewisse Entwicklungslinien
zu präferieren und andere sozusagen abzuklemmen. Da habe ich mich vielleicht dann eben auch zu
ungenau ausgedrückt, weil ich meinte nämlich auch gar nicht unbedingt, dass mit dem Zucker,
dass man, oder eigentlich gar nicht mit dem Zucker, dass man eben auf diese positiven Effekte der
Zugänglichmachung von Welt und so weiter, die ja Algorithmen und Technologie im Allgemeinen auch
leisten kann, dass man davon absehen solle, sondern ich frage mich, wie kann auch eine
Vorstellung von Zukunft aussehen, die eben diesem letztlich ja auch politisch-ökonomischen Geflecht,
was sich da herausschält und was auch eine gewisse Dominanz erlangt eben auf einer
politisch-gesellschaftlichen Ebene? Wie kann man Visionen von Zukunft, oder was für Visionen von
Zukunft gibt es, die eine andere, nicht offenere oder demokratisch verfasstere Struktur verfolgen?
Also gibt es Alternativen und wie schauen die aus? Also was im Moment relativ breit diskutiert wird,
als es beginnt zu diskutieren zu werden, ist der Begriff der technologischen Souveränität. Und was
das bedeutet, ist, dass man die Technologien, und das beinhaltet auch die Daten, die diese
Technologien brauchen, in den Bereich bringt, in dem die demokratische Willensbildung derjenigen,
die von diesen Technologien betroffen sind, stattfindet. Das Problem heute, das sogar auf
der Ebene der EU besteht, ist, dass die großen Technologieanbieter schlecht greifbar sind. Die
sagen, ja, wir sind amerikanisches Recht, oder wenn wir in Europa sind, sind sie irgendwie in
Irland. Und das macht es beispielsweise jetzt einer Stadt wie Wien. Es ist schwierig, von Airbnb
Daten zu bekommen, um hier die Gesetze durchzusetzen, was Abgaben auf Mietwohnungen und so weiter
besteht. Und hier ist der Versuch zu sagen, wir wollen eigentlich, dass diejenigen, die jetzt
unsere Stadt sozusagen mit Daten erfassen und mit Technologie verändern, innerhalb der Stadt und
ihrer politischen Institutionen angesiedelt sind. Das heißt, es wäre jetzt ein Versuch,
eine Politik, die sagt, wir müssen wegkommen von diesen zentralisierten Lösungen, die irgendwo im
Ausland schwer zu erreichen, sozusagen durch die politischen Institutionen, die demokratisch
verankert sind, angesiedelt sind. Sondern wir wollen die eigentlich schauen, dass die in unserem
sozusagen politischen Gehäuse agieren. Und da sind aktuell die Städte am weitesten. Amsterdam,
Berlin, Barcelona, New York, also auch in den USA, wo das sehr stark diskutiert wird. Die Frage,
wie können wir den Einfluss der demokratisch legitimierten Institution und der demokratisch
legitimierten Macht wieder stärken auf diesen Bereich? Und da haben wir im Moment halt das
Problem, dass diese beiden sozusagen Geografien vollkommen auseinandergehen. Und die Idee der
technologischen Souveränität wäre, diese Geografien wieder aufeinanderzubringen. Aber nicht,
indem man sagt, gut, wir brauchen jetzt die globale Internetregierung, das war sozusagen die Idee der
90er- und 2000er-Jahre. Sondern, dass man sagt, nein, wir müssen das wieder zurückbinden an einen
tatsächlichen Ort, in dem quasi demokratische Politik stattfindet. Und das war jetzt in dem
konkreten Fall dann jeweils die Stadt. Das würde heißen, Airbnb kann sich eben nicht darauf
ausreden, na, wir haben eigentlich mit Wien gar nichts zu tun. Sondern wir sitzen in Irland, das ist
eigentlich eine amerikanische Firma. Sondern, man müsste sagen, gut, wenn die in Wien agieren
wollen, dann müssen sie halt tatsächlich gewisse Regeln hier akzeptieren und sich sozusagen dieser
demokratischen Willensbildung, die hier stattfindet, beugen beispielsweise, als man sagt, dürfen jetzt
Privatwohnungen vermietet werden, was sind die Steuern, die da drauf kommen, etc. Das ist aber
sozusagen eine politische Auseinandersetzung. Auf der einen Seite gewisse demokratische Institutionen,
auf der anderen Seite sehr mächtige Akteure, ja nicht nur Internetakteure, sondern auch andere,
die eigentlich sich immer stärker versuchen, aus dieser demokratischen Kontrolle zu verabschieden.
Und das ist eine klassische politische Auseinandersetzung. Und die Akteure sind aber
tatsächlich dann, also die Hauptakteure, sage ich mal, sind tatsächlich wieder quasi die zwei. Also
die Alternative ist, oder sagen, die Kontrollinstanz oder die Hilfsinstanz, nach der dann gerufen wird,
in Anführungsstrichen, ist dann eine staatliche. Tendenziell, ist das so? Letztlich ist es immer
eine staatliche, weil die staatliche setzt den gesetzlichen Rahmen. Der gesetzliche Rahmen kann
durchaus so gemacht werden, dass er auch andere Akteure stärkt. Aber der Rahmen sozusagen,
solange es auf Gesetzen beruht, ist immer der Staat. Beispielsweise als Teil dieser Idee der
technologischen Souveränität gibt es auch die Idee, dass man die Daten, die zum Beispiel in einem
intelligenten Verkehrssystem anfallen oder die in einer Stadt anfallen, ganz allgemein nicht mehr
als Privateigentum versteht, sei das meine eigenen persönlichen Daten oder der Firma,
sondern eigentlich sagt, das ist ein gemeinschaftliches. Das gehört sozusagen
allen in der Stadt. Also Daten sind eine neue Form von Cummins. Und dann könnte man Regeln
erstellen, wie diese Daten genutzt werden können. Ich könnte beispielsweise sagen,
alle in der Stadt ansässigen Akteure, Firmen und NGOs oder auch Bürgerinitiativen haben freien
Zugang zu diesen Daten, also immer im datenschutzrechtlichen Rahmen, aber haben
freien Zugang und können darauf sozusagen in Services neue Angebote entwickeln. Und Akteure
aus einem nationalen Rahmen, die haben unter gewissen Auflagen Zugang, beispielsweise müssen
dafür zahlen. Und globale Akteure, die haben nochmal unter anderen Bedingungen Zugang,
müssen noch mehr dafür zahlen. Da hätte man so ein abgestuftes System, wo man sagen kann,
das Ziel dieser Regelung ist es, lokale Akteure zu stärken. Und so ein Rahmen muss zumindest auf
der staatlichen Ebene gemacht werden, dass eine Institution ermächtigt wird, sozusagen damit
beauftragt wird, so einen Rahmen, eine Datenschutzbehörde oder eine Datenkommensbehörde
oder ein Datenkommensnetzwerk oder was denn das immer für eine institutionelle Form ist. Aber
die muss damit beauftragt werden, so einen Rahmen auszuarbeiten und durchzusetzen.
Und gibt es weiterreichende Vorstellungen von Zukunft, wo die Lösungen nicht so
particular sind? Also man sagt, okay, es gibt quasi für ein bestehendes Problem eine Insellösung,
die nach Prinzipien funktioniert, die wir unterstützenswerter finden, sondern kennst
du Entwürfe, also auch Entwürfe von gesellschaftspolitischen Ordnungssystemen,
die weder auf Staat als Ordnungsinstanz noch auf Markt als Ordnungsinstanz basieren,
wobei man ja eigentlich die beiden auch fix zusammen denken müsste. Kennst du da Entwürfe?
So in der Größe nicht, aber es ist natürlich ein jetzt schon lang und ein sich verstärkender
Auseinandersetzung zwischen drei Gruppen von Akteuren, staatlichen Akteuren,
marktorientierten Akteuren und zivilgesellschaftlichen Akteuren, also NGOs jetzt im breitesten Sinne,
die ja immer mehr auch öffentliche Aufgaben mit übernehmen können und das teilweise auch tun.
Und dieser Sektor ist sicher einer der durch neue technologische Möglichkeiten der Organisation
und der Kommunikation und des Abbaus auch von diesen teilweise extrem in Institutionen gefangenen
Wissensmonopolen sich besser organisieren kann. Und da ist eine große Auseinandersetzung,
welcher Bereich soll wie geregelt werden und da gibt es natürlich auch einzelne Beispiele dafür,
wo man sagt, beispielsweise der Energieversorger, der lokale Energieversorger, der soll nicht mehr
quasi privatwirtschaftlich ausgerichtet werden, sondern eine Art gemeinwirtschaftlich ausgerichtet
werden. Und dann so diese ganze Bewegung der Rekommunalisierung der Energienetze,
die ja beispielsweise in Hamburg durch als erfolgreich war, in Berlin ist sie gescheitert,
aber war auch sehr populär, aber ist dann am Quorum der Abstimmung gescheitert. Also da gibt
schon Ideen, wie diese drei Bereiche neu miteinander verknüpft werden können und wie dieses wachsende
sozusagen Organisationspotenzial der Zivilgesellschaft besser auch eingebunden werden kann,
indem eben auch die demokratischen Entscheidungsmöglichkeiten, Mitbestimmungsmöglichkeiten
ausgebaut werden. Aber ich glaube, es ist nicht ein entweder-oder. Es ist nicht Markt oder Staat.
Also die libertäre Utopie, die wird das immer bleiben, eine Utopie. Es ist klar sozusagen,
die vollkommen staatliche Planung ist auch nicht sozusagen die Vision, die man möchte,
aber das Ausverhandeln im besten Sinne, wie diese drei Sachen zueinanderstehen können,
ich glaube, da ist sehr, sehr viel Bewegung drin. Jetzt basiert ja oder ist die dominante
Erzählung der politischen Ökonomie heutzutage immer noch basierend auf der Effizienz der Märkte,
der angeblichen, die in letzter Instanz ja auch sozusagen sagt über die Preisbildung und könne
quasi eine höchstmögliche Allokation von Mitteln erlangt werden, weil sonst kein anderes System in
der Lage wäre, ein so vollständiges Wissen über den ja so komplexen Markt zu erlangen,
wie es eben Märkte können, weil die dezentral organisiert sind, jeder einzelne Käufer und
Verkäufer da quasi seinen Informationsinput reingibt. Wo kommt man hin, wenn man sich dann
fragt, ob diese Erzählung, die ja auf einer Effizienz basiert, ob die in Zukunft noch
haltbar sein wird? Und da denke ich zum einen an Algorithmic Pricing, was ja schon quasi auch
andere Faktoren als nur den Preis alleine mit einbezieht, um zu einer Basis des Warentauschs
zu kommen letztlich, zum einen in die Richtung gedacht und eben auch, ob es sowas gibt, wie eine
kybernetische Planwirtschaft in Zeiten ihrer technischen Machbarkeit, um das ein bisschen
polemisch zuzuspitzen. Ich glaube, was wir sehen, ist, dass die erhöhte Fähigkeit der nicht
marktorientierten Akteure, sich zu organisieren und Outputs zu generieren, dass die gewissen
Tätigkeiten, die vorher nur im Markt organisiert waren und entsprechend sozusagen auf Preis
orientiert waren, anders zu organisieren und damit ein anderer Kalkulus mit ins Spiel kommt.
Also klar, beispielsweise ist es denkbar und wird auch experimentiert, verschiedene dezentrale
Energieversorgungssysteme miteinander so zu koppeln, dass Produzenten direkt miteinander
Energie tauschen können in einer Nachbarschaft oder in einer Region. Ganz ohne Preise involviert
dann letztlich? Da sind schon auch Preise involviert, aber der Preis ist nicht mehr nur die einzige
sozusagen Kindzahl, die relevant ist, sondern kommen eben auch andere Qualitäten wie Selbstversorgung,
wie Nachhaltigkeit und so weiter und so fort mit rein. Und der Preis ist eigentlich nur eine Art
Rechengröße, um Austauschverhältnisse sozusagen verifizieren zu können. Ich habe jetzt für so
viel Strom von dir bezogen und du hast so viel Strom von mir bezogen. Das kommt darauf raus,
dass am Ende des Monats ich noch so viel Geld von dir kriege, aber das lassen wir auf dem Konto,
weil im nächsten Monat ist es vielleicht umgekehrt und so tauschen wir vielleicht langfristig Dinge
miteinander aus, wo kaum je sozusagen der Geld tatsächlich transferiert wird. Aber es
ist natürlich eine Accounting-Ebene, ist nach wie vor aha, du hast jetzt so viel Strom von mir
bezogen, ich habe so viel Strom von dir bezogen. Also da ist schon denkbar, etwas zu machen,
das eben nach anderen Kriterien passiert, das ist eher zivilgesellschaftlich passiert,
die eine komplexere, mehrdimensionale Idee haben von dem, was sie machen und der Preis,
der ist da mit drin, aber er ist nicht mehr das einzige dominierende Element, was am Schluss nur
noch darum geht, was steht in der Quartalsrechnung unten beim Total. Und von einer quasi Neubelebung
planwirtschaftlicher, staatlicher Strukturen im Zuge neu zur Verfügung stehender technologischer
Mittel, da gibt es keine Entwürfe, von denen du wüsstest? Nein, ich habe gesagt, es ist ja schwierig,
weil der Staat die ganze Zeit schon plant und gewisse Dinge sozusagen alloziert. Jetzt wollen
wir hier eine Autobahn bauen und nicht dort und so und so viel. Also man kann es gut vorstellen,
dass das im Rahmen eines Klimaregimes irgendwie entschieden werden muss, wer kriegt welche
sozusagen Verschmutzungsrechte oder wie allozieren, wie die Verschmutzung, die wir bieten können,
die wir uns leisten können, so ökologisch in der Gesellschaft. Und das einfach über einen Markt
zu machen für CO2-Ausstoß, das ist ja bis jetzt grandios gescheitert. Aber dass der Staat,
irgendwo man sagt, oder eine öffentliche Stelle, irgendwo man sagt, mit unseren Klimazielen ist
nur noch so und so viel Stahl-Output oder Aluminium-Output oder Verkehr denkbar. Und
jetzt müssen wir das anders organisieren. Das ist schon denkbar. Aber das ist auch so ein
Selbstverständnis dessen, was der Staat macht, der sich immer stärker nur noch als eine Entität
sieht, die Rahmenbedingungen für Märkte herstellt. Da sind wir einfach extrem weit davon entfernt.
Von dem her habe ich das Gefühl, dass diese komplexeren Planungsideen eigentlich eher über
die Zivilgesellschaft kommen, wie über den Staat. Der Staat, seine Aufgabe wäre es dann,
den Rahmen herzustellen, damit auch sozusagen nicht marktorientierte Akteure da agieren können. Und
auch da sind wir noch sehr, sehr weit weg davon. Vielleicht noch als ein Schlusswort. Ich ende
gerne auf eine positive Note. Deshalb die Frage, wenn du dir Zukunft vorstellst, was stimmt dich
freudig? Ja, ich glaube, die Fähigkeit der Gesellschaft, mit Komplexität umzugehen und
damit mit Vielfalt, die nimmt zu. Auch wenn viele Akteure damit Probleme haben und quasi
dagegen arbeiten, leben wir in einer vielfältigeren Welt. Und wir brauchen Mittel, und das sind oft
auch technologische Mittel, um mit dieser Vielfalt umgehen zu können. Sei das eine kulturelle
Vielfalt, aber auch eine Vielfalt von Akteuren, dass es Menschen, Tiere, Pflanzen, Wettersystemen
und so weiter sind. Und das finde ich grundsätzlich eine positive Entwicklung. Wunderbar. Felix,
vielen Dank für das Gespräch. Gerne. Das war Future Histories für heute. Vielen Dank fürs
Zuhören. Show-Notizen und vieles mehr findet ihr auf www.futurehistories.today. Diskutiert mit
auf Twitter unter dem Hashtag Future Histories oder auf Reddit. Lasst mich wissen, was ihr zu
dem Ganzen denkt und wie euch diese Folge hier gefallen hat. Unbedingt gut bewerten auf allen
Podcast-Plattformen, die ihr nutzt. Für unsere Patreon-Unterstützerinnen und Unterstützer gibt
es auf www.patreon.com schräg Strich Future Histories vieles an Zusatzmaterial. Ich lese
zum Beispiel jeden Monat einen Text ein, der zum jeweiligen Thema passt. Da könnt ihr also
auch vorbeischauen. Bis zum nächsten Mal. Ich freue mich.
