WEBVTT

00:00.000 --> 00:06.000
Herzlich willkommen bei Future Histories, dem Podcast zur Erweiterung unserer Vorstellung von Zukunft.

00:06.000 --> 00:12.000
Mein Name ist Jan Groß und ihr hört heute die zweite Future Histories Live-Episode,

00:12.000 --> 00:15.000
die ich kürzlich in Hamburg aufgezeichnet habe.

00:15.000 --> 00:19.000
In diesem Fall, da war ich beim Mind the Progress-Kongress eingeladen

00:19.000 --> 00:23.000
und hatte die Freude, mit Friederike Kaltheuner als Gast zu sprechen.

00:23.000 --> 00:31.000
Friederike, die forscht, schreibt und denkt zu Fragen der Datafizierung und ist Expertin im Bereich Tech Policy.

00:31.000 --> 00:37.000
Sie hat unter anderem das Corporate Exploitation Program bei Privacy International geleitet,

00:37.000 --> 00:40.000
war Tech Policy Fellow bei der Mozilla Foundation.

00:40.000 --> 00:46.000
Gemeinsam mit Nele Obermüller hat sie auch ein Buch geschrieben mit dem Titel Datengerechtigkeit.

00:46.000 --> 00:50.000
Vielen Dank an dieser Stelle hier an die OrganisatorInnen des Kongresses für die Einladung

00:50.000 --> 00:55.000
und vor allem auch an Sophia für diese wirklich ausgezeichnete Organisation und Betreuung.

00:55.000 --> 01:00.000
1.000 Dank. Bevor wir jetzt aber zum Gespräch mit Friederike kommen,

01:00.000 --> 01:03.000
hatte ich ja in der vergangenen Episode angekündigt,

01:03.000 --> 01:08.000
noch ein paar Worte zur gerade begonnenen zweiten Staffel von Future Histories zu sagen,

01:08.000 --> 01:12.000
ob sich da was geändert hat im Vergleich zur Staffel 1 und wenn ja, was.

01:12.000 --> 01:18.000
Falls euch diese Reflektionen über den Verlauf des Podcasts Future Histories nicht so sehr interessieren

01:18.000 --> 01:23.000
und ihr eigentlich nur Friederike hören wollt, dann nehme ich euch das nicht im geringsten Übel,

01:23.000 --> 01:27.000
sondern habe genau dafür einen Kapitelmarker gesetzt.

01:27.000 --> 01:32.000
Wenn eure App das unterstützt, dann könnt ihr da also jetzt vorspringen.

01:32.000 --> 01:37.000
Für alle, die es sehr wohl interessiert, willkommen bei diesem kurzen Einschub.

01:37.000 --> 01:40.000
Also vielleicht zunächst mal für euch zur Einordnung.

01:40.000 --> 01:45.000
Ich hatte damals, als ich Future Histories vor jetzt über zwei Jahren gestartet habe,

01:45.000 --> 01:49.000
ja eher so eine vage Idee davon, wie ich das ungefähr machen will.

01:49.000 --> 01:53.000
Und der Aufbau in Staffeln, der schien mir damals einfach sinnvoll,

01:53.000 --> 01:58.000
um zum einen eben die Möglichkeit einzubauen, dass man auch mal eine Pause machen kann.

01:58.000 --> 02:03.000
Und zum anderen wollte ich auch nicht, dass die Themen, mit denen ich damals gestartet bin,

02:03.000 --> 02:05.000
auf ewig in Stein gemeißelt sind.

02:05.000 --> 02:11.000
Eine neue Staffel, die bietet also auch die Möglichkeit, thematisch da Neuausrichtungen vorzunehmen.

02:11.000 --> 02:15.000
Und das ist im Fall von Future Histories nicht als harter Bruch zu verstehen,

02:15.000 --> 02:22.000
sondern eher als eine stetige Entwicklung der forschenden Suchbewegung dieses Podcasts hier.

02:22.000 --> 02:25.000
Es wird also auch in der zweiten Staffel drei Überthemen geben,

02:25.000 --> 02:30.000
aber zum einen werden sie auch weiterhin eher als grobe Orientierung dienen

02:30.000 --> 02:34.000
und eine Offenheit zulassen, die ich als essentiell empfinde.

02:34.000 --> 02:39.000
Und zum anderen leiten sie sich auch unmittelbar aus den unterschiedlichen Auseinandersetzungen,

02:39.000 --> 02:42.000
den Pferden und den Entwicklungen der ersten Staffel ab.

02:42.000 --> 02:46.000
Aus dem Thema Homo economicus, da hat sich zum Beispiel,

02:46.000 --> 02:48.000
ich hatte das an anderer Stelle schon erwähnt,

02:48.000 --> 02:53.000
über den Verlauf der ersten Staffel hinweg die Frage entwickelt,

02:53.000 --> 02:57.000
wie alternative Systeme politischer Ökonomie denn aussehen könnten.

02:57.000 --> 03:02.000
Politische Ökonomien der Zukunft wird also ein Überthema der zweiten Staffel sein.

03:02.000 --> 03:07.000
Ich bin ja absolut der Meinung, dass es nicht reicht, in der Kritik zu verharren,

03:07.000 --> 03:12.000
sondern dass es zwingend notwendig ist, konstruktive und produktive Vorschläge zu machen,

03:12.000 --> 03:19.000
wie man es denn anders machen könnte, gerade auch auf einer Makroebene wie der der politischen Ökonomie.

03:19.000 --> 03:25.000
Denn diese Makroebene, die es absolut unterversorgt, wenn es um Alternativen geht, finde ich,

03:25.000 --> 03:32.000
das gilt es zu ändern und ich möchte dabei zum einen bestimmte Stränge der ersten Staffel weiterverfolgen,

03:32.000 --> 03:35.000
die sich als fruchtbare Pferden herausgestellt haben.

03:35.000 --> 03:39.000
Konkret ist, dass die Auseinandersetzung mit dem Themenkomplex,

03:39.000 --> 03:45.000
den ich mittlerweile unter dem spielerisch provokanten Begriffspaar Freie Planwirtschaft zusammenfasse,

03:45.000 --> 03:50.000
da ist noch vieles offen und ich habe sehr, sehr viele Fragen an diesen Themenkomplex,

03:50.000 --> 03:53.000
die definitiv noch umgeklärt sind.

03:53.000 --> 04:00.000
Und daran anschließend gibt es auch eine andere Fragestellung oder einen thematischen Ansatz

04:00.000 --> 04:06.000
zu diesem Komplex zukünftiger politischer Ökonomien, einen anderen Ansatz,

04:06.000 --> 04:09.000
den ich ebenfalls in Staffel 1 wiederholt auch aufgegriffen habe,

04:09.000 --> 04:14.000
der aber auch absolut überhaupt nicht abschließend geklärt ist in irgendeiner Form.

04:14.000 --> 04:20.000
Und das ist die Frage nach alternativen Regierungskünsten, nach alternativer Governamentalität,

04:20.000 --> 04:24.000
wie es Frieda Vogelmann in Episode 11 nennt.

04:24.000 --> 04:28.000
Das sind also so zwei Aspekte, die ich aus der ersten Staffel mitnehmen möchte,

04:28.000 --> 04:35.000
aber ich will natürlich auch neue Pfade eröffnen, die für politische Ökonomien der Zukunft von Bedeutung sind.

04:35.000 --> 04:39.000
Das Ganze wird also explorativ sein, wie man so schön sagt.

04:39.000 --> 04:41.000
Und ich freue mich auch immer sehr über Hinweise.

04:41.000 --> 04:50.000
Also solltet ihr in dieser Richtung irgendwie gute Ansätze kennen, Zugänge, Bücher, TheoretikerInnen und so weiter,

04:50.000 --> 04:55.000
gerne immer mir schreiben unter jan-at-future-histories.today.

04:55.000 --> 05:00.000
Ein wichtiger Bereich, den es für diesen Themenblock in jedem Fall zu erarbeiten gilt,

05:00.000 --> 05:09.000
ist das Blicken über den Tellerrand, um nicht in so einer leider ja immer wieder sehr eurozentristischen Debatte eigentlich stecken zu bleiben.

05:09.000 --> 05:15.000
Ein wichtiger Aspekt wird also sein zu fragen, wie wird denn die Frage nach alternativen politischen Ökonomien

05:15.000 --> 05:18.000
in anderen Teilen der Welt gestellt und verhandelt.

05:19.000 --> 05:26.000
Der zweite Themenblock, der ergibt sich im Grunde unmittelbar aus dem ersten und widmet sich der Frage der Transformation.

05:26.000 --> 05:30.000
Ich finde es unglaublich wichtig, diese Ebenen miteinander zu verbinden,

05:30.000 --> 05:36.000
dass man sich also auf der einen Seite wieder traut, auch über Alternativen auf der Makroebene nachzudenken,

05:36.000 --> 05:40.000
dass aber gleichzeitig auch eine plausible Idee davon entwickelt wird,

05:40.000 --> 05:44.000
wie diese alternativen politischen Ökonomien denn in die Welt gebracht werden könnten,

05:44.000 --> 05:48.000
wie eine Transformation also ganz konkret aussehen könnte.

05:48.000 --> 05:55.000
Nicht das Blaupause, also bitte nicht falsch verstehen, nicht als ein Prozess mit einem finalen Ziel oder sowas,

05:55.000 --> 06:01.000
sondern als ein wechselseitiger Prozess zwischen gelebter alternativer Praxis im Hier und Jetzt

06:01.000 --> 06:08.000
und auch Projekten, die quasi an die Substanz dessen gehen, was es neu zu verhandeln gilt

06:08.000 --> 06:12.000
und im Entwickeln alternativer Zukunft auch jenseits der Nische.

06:12.000 --> 06:19.000
Ich glaube, dass das sich überhaupt nicht irgendwie widerspricht, sondern im Gegenteil einfach ganz fantastisch gegenseitig befruchten kann.

06:19.000 --> 06:25.000
Die Frage der Transformation ist ein sehr weites Feld und auch eine harte Nuss, finde ich.

06:25.000 --> 06:33.000
Da wird es also einiges zu besprechen geben und ich freue mich schon auf viele, viele spannende Episoden zu diesem Thema.

06:33.000 --> 06:37.000
Das dritte Themenfeld, das uns in der zweiten Staffel begleiten wird,

06:37.000 --> 06:45.000
das greift den technopolitischen Strang von Future Histories auf und verdichtet ihn entlang eines Forschungsprojektes,

06:45.000 --> 06:48.000
das ich gemeinsam mit Robert Seifert verfolge.

06:48.000 --> 06:53.000
Das Thema lautet das Regieren der Algorithmen und das freut mich ganz besonders,

06:53.000 --> 06:58.000
nicht nur, weil ich Robert Seifert sehr schätze und das Thema auch unglaublich gut in Future Histories passt,

06:58.000 --> 07:04.000
sondern auch, weil es bedeutet, dass hier zwei Sphären noch enger zueinander finden,

07:04.000 --> 07:09.000
die für mich persönlich einfach von großer Bedeutung sind, nämlich Future Histories als Projekt

07:09.000 --> 07:17.000
und die Forschung im Rahmen meiner Dissertation zu sozio-technischen Imaginationen algorithmischer Regierungskunst.

07:17.000 --> 07:22.000
Diese Dissertation, die ist nämlich Teil des Forschungsprojektes, das Regieren der Algorithmen.

07:22.000 --> 07:30.000
Und weil ich ebenfalls daran arbeite, Podcasten als Teil der erweiterten Forschungspraxis zu etablieren und zu plausibilisieren,

07:30.000 --> 07:36.000
passt das alles natürlich ganz großartig zusammen und freut mich wirklich sehr.

07:36.000 --> 07:41.000
Worum wird es also im Themenstrang das Regieren der Algorithmen gehen?

07:41.000 --> 07:46.000
Das Forschungsprojekt, das trägt den Untertitel eine Soziologie algorithmischer Regierungskunst

07:46.000 --> 07:55.000
und steht somit in ausgezeichneter Korrespondenz zur in Themenblock 1 bereits erwähnten Frage nach alternativen Regierungskünsten.

07:55.000 --> 08:04.000
Der Begriff des Regierens, der ist dabei angelehnt an Foucault weit gefasst und geht über eine rein politische Definition hinaus.

08:04.000 --> 08:10.000
Regieren bezieht sich hier ganz allgemein auf soziale Felder, Technologien und individuelle Handlungsformen,

08:10.000 --> 08:15.000
die der Selbst- und Fremdführung von Menschen dienen und Regierungskünste,

08:15.000 --> 08:20.000
die stellen dabei dann das reflektierte Nachdenken über die beste Form des Regierens dar.

08:20.000 --> 08:29.000
Algorithmische Regierungskünste tun dies wiederum unter Einbezug algorithmischer Verfahren als maßgebliche Elemente des Regierens.

08:29.000 --> 08:35.000
Und wir führen im Namen des Forschungsprojektes das Regieren der Algorithmen ja eine Kecke Zweideutigkeit mit,

08:35.000 --> 08:43.000
denn der Name das Regieren der Algorithmen, das kann sich ja beziehen sowohl auf das Regieren von Algorithmen,

08:43.000 --> 08:52.000
also im Sinne des Regulierens von Algorithmen, als auch auf soziotechnische Imagination des Regierens mit algorithmischen Technologien.

08:52.000 --> 09:00.000
Siehe mein Promotionsprojekt. Das heißt, wir kommen da aus verschiedensten Richtungen, was ich als sehr produktiv empfinde.

09:00.000 --> 09:07.000
Mehr dazu gibt es dann in einer kommenden Episode mit Robert Seifert als Gast. Ich freue mich schon sehr.

09:08.000 --> 09:13.000
Was mich auch sehr freut und das wollte ich auch schon längst ganz fröhlich verkünden,

09:13.000 --> 09:18.000
ist, dass Future Histories eine Förderung der Wiener Medieninitiative erhalten hat,

09:18.000 --> 09:25.000
um das Projekt etwas auszubauen und auch auf solidere Beine stellen zu können, sage ich mal.

09:25.000 --> 09:30.000
Daher stammt zum Beispiel auch der Umstand, dass jetzt zumindest mal für die nächsten 20 Episoden

09:30.000 --> 09:35.000
jede zweite Woche ein Future Histories Kurzvideo veröffentlicht wird auf YouTube.

09:35.000 --> 09:43.000
Gerne auch übrigens den Kanal abonnieren. Das ist definitiv auch eine Hilfe im Ausbau dieses Medienstranges, sage ich mal.

09:43.000 --> 09:48.000
Es gibt jetzt auch ein kleines Studio in einem Gemeindebau direkt beim Wiener Prater

09:48.000 --> 09:54.000
und die Förderung ermöglicht auch, dass ich endlich nicht mehr gezwungen bin, alles alleine zu machen

09:54.000 --> 09:57.000
und mich an den Rand der Überlastung zu treiben.

09:57.000 --> 10:01.000
Denn ich werde jetzt hier bei Future Histories von Clara unterstützt.

10:01.000 --> 10:08.000
Clara, vielen Dank für alles. Ich freue mich total, dass du mitmachst und dass du mich in all diesen Sachen unterstützt.

10:08.000 --> 10:10.000
Ich freue mich auch. Hallo.

10:10.000 --> 10:15.000
Die Förderung der Wiener Medieninitiative bewegt sich im Bereich dessen, was man gemeinhin

10:15.000 --> 10:19.000
wahrscheinlich Kreativwirtschaft nennen würde und so ist es integraler Bestandteil,

10:19.000 --> 10:24.000
dass ich mich im Zuge dessen mit einer Neugründung selbstständig gemacht habe.

10:24.000 --> 10:28.000
Die Firma, die daraus entsprungen ist, die trägt den Namen MetaLapsis

10:28.000 --> 10:34.000
und ihr findet sie unter www.metalapsis.net mit einer sehr gelungenen Homepage vertreten,

10:34.000 --> 10:39.000
für deren Gestaltung und Umsetzung ich Thomas, Daniel und Leon danken möchte.

10:39.000 --> 10:45.000
Und wo wir schon bei Webseiten sind, es gibt jetzt auch Infos zu meiner Person gebündelt unter

10:45.000 --> 10:52.000
jan-groß.de, geschrieben Gustav Richard Otto Otto Siegfried, also mit zwei O und einem normalen S.

10:52.000 --> 10:58.000
Auf eine Art fand ich es zwar auch ganz schick, muss ich sagen, dass es bisher nur so verstreute

10:58.000 --> 11:05.000
Informationen über mich gab im Netz, aber nun hat dann doch der pragmatische Zugang gewonnen.

11:05.000 --> 11:12.000
So, jetzt genug Eigenwerbung. Ich freue mich riesig auf die zweite Staffel, möchte ich noch mal sagen.

11:12.000 --> 11:18.000
Und mir verbleibt noch Marianne, Andrea und Nausi K.A. ganz herzlich in der Gemeinschaft

11:18.000 --> 11:24.000
der Patreon-UnterstützerInnen zu begrüßen. Und ich danke Fabian, Carmen, Rudolf und Wilfried

11:24.000 --> 11:30.000
für ihre Spenden und wünsche euch jetzt viel Freude mit einer neuen Episode Future Histories Live,

11:30.000 --> 11:33.000
diesmal mit Friederike Kalt-Heuner.

11:41.000 --> 11:46.000
Ja, herzlich willkommen auch von meiner Seite. Ich freue mich sehr, dass wir heute hier sein dürfen.

11:46.000 --> 11:48.000
Herzlich willkommen, Friederike.

11:48.000 --> 11:50.000
Danke für die Einladung.

11:50.000 --> 11:56.000
Ich freue mich sehr. Friederike, ich mag gerne mit einer ganz basic Frage einsteigen, die vielleicht

11:56.000 --> 12:02.000
gar nicht so einfach ist, wie sie im ersten Moment scheint. Denn gerade im Zusammenhang mit Daten,

12:02.000 --> 12:07.000
da scheint mir allzu oft zu einer gewissen Naturalisierung am Werke. Man sieht es an Framings,

12:07.000 --> 12:14.000
wie Daten sind, das neue Öl oder auch der Begriff Raw Data, Rotdaten. Also zunächst mal vorab.

12:14.000 --> 12:17.000
Was sind Daten und wie entstehen sie?

12:19.000 --> 12:24.000
Vielleicht ist es am einfachsten, mit einer ganz einfachen Aussage anzufangen.

12:24.000 --> 12:32.000
Wir leben in einer Welt, in der es noch nie so einfach war, so viel über jeden von uns zu wissen.

12:32.000 --> 12:38.000
Und das hat natürlich auch was mit Daten zu tun. Also der Begriff Datafizierung heißt,

12:38.000 --> 12:48.000
die Umwandlung von Räumen, Verhalten, Informationen in Daten. Wenn ich einen Vortrag zu dem Thema halte,

12:48.000 --> 12:53.000
habe ich eine Lieblingsfolie, die ich eigentlich fast immer verwende. Und auf dem Bild sieht man eine,

12:53.000 --> 13:01.000
das ist ein Foto von einer Stasi-Akte. Und die zeigt so ein bisschen, wie auf der einen Seite,

13:01.000 --> 13:07.000
wie arbeitsaufwendig es früher war, Informationen herauszufinden und wie trivial die Informationen

13:07.000 --> 13:13.000
zum Teil auch waren. Also da steht dann in der Akte, welches Buch liest jemand, wer sind so die Freunde,

13:13.000 --> 13:20.000
wo verbringen die so ihren Nachmittag. Und was Datafizierung jetzt ganz grob runtergebrochen heißt,

13:20.000 --> 13:24.000
ist, dass wir jetzt in einer Welt leben, in all diese Dinge, die früher sehr aufwendig

13:24.000 --> 13:30.000
und kostenspielig waren, herauszufinden. Diese Dinge werden automatisch über uns aufgezeichnet

13:30.000 --> 13:37.000
in Form von Daten. Und wenn wir über Daten sprechen, zum öffentlichen Diskurs, geht es eigentlich meistens

13:37.000 --> 13:41.000
um die Daten, die wir mehr oder weniger bewusst teilen. Also die Bilder, die wir teilen,

13:41.000 --> 13:47.000
was wir ins Internet stellen. Aber das ist eigentlich nur die Spitze des Eisbergs. Der viel größere Teil,

13:47.000 --> 13:51.000
der den meisten Menschen, glaube ich, gar nicht so bewusst ist, sind die Daten, die automatisch

13:51.000 --> 13:58.000
über uns aufgezeichnet werden. Unser Bewegungsmuster, nicht nur was wir kaufen, sondern wie sich unsere Maus bewegt,

13:58.000 --> 14:03.000
wo wir vielleicht zögern, was uns vielleicht vorher noch interessiert hat. Und dann die dritte Kategorie,

14:03.000 --> 14:08.000
die mich, ich glaube, das ist vielleicht eigentlich auch der Einstieg für mich persönlich zu dem Thema.

14:08.000 --> 14:15.000
Die dritte Kategorie sind all die Rückschlüsse, die man aus diesen Mustern ableiten kann.

14:15.000 --> 14:21.000
Also aus meinem Bewegungsmuster, aus meinem Netflix-Verhalten kann man sehr viel ablesen,

14:21.000 --> 14:27.000
zum Beispiel, ob jemand depressiv ist, ob jemand jetzt das Haus nicht verlassen hat.

14:27.000 --> 14:32.000
Man kann anhand meiner Telefondaten sehen, bin ich eher jemand, die sofort zurückruft?

14:32.000 --> 14:39.000
Oder muss ich angerufen werden? Und wen rufe ich an? Und das ist eigentlich das, was mich interessiert.

14:39.000 --> 14:46.000
Weil auf der einen Seite ist es so, wenn diese Vorhersagen stimmen, sind sie unheimlich genau

14:46.000 --> 14:52.000
und können Dinge aussagen über uns, die wir vielleicht selber gar nicht wissen oder nicht wahrhaben wollen.

14:52.000 --> 14:57.000
Gleichzeitig ist es aber auch so, dass diese Muster und Vorhersagen oft völliger Quatsch sind.

14:57.000 --> 15:01.000
Und man sieht das ganz gut anhand zum Beispiel der Online-Werbung, die ich sehe.

15:01.000 --> 15:07.000
Da gibt es manchmal Dinge, die sind unheimlich fast hellseherisch genau.

15:07.000 --> 15:11.000
Und dann wiederum gibt es einfach Quatsch.

15:11.000 --> 15:16.000
Also Facebook hat jetzt über 12, 13 Jahre Daten von mir und denkt immer noch, ich wäre ein Mann.

15:16.000 --> 15:19.000
Und dieses Spannungsfeld ist das, was mich interessiert.

15:19.000 --> 15:22.000
Also das Spannungsfeld zwischen Orwell und Kafka.

15:22.000 --> 15:25.000
Auf der einen Seite sind wir sehr lesbar geworden.

15:25.000 --> 15:30.000
Auf der anderen Seite ist das, was gelesen wird, auch oft völlig falsch.

15:30.000 --> 15:35.000
Und manchmal ist es einfach nur irritierend, dass es falsch ist.

15:35.000 --> 15:38.000
Manchmal geht es aber auch um etwas ganz anderes.

15:38.000 --> 15:40.000
Manchmal werden wir in Kategorien gepackt.

15:40.000 --> 15:44.000
Manchmal werden wir falsch einsortiert auf Arten und Weisen, die diskriminierend sind,

15:44.000 --> 15:48.000
die bestehende diskriminierende Strukturen reproduzieren.

15:48.000 --> 15:56.000
Und ich glaube, neben Klimawandel, Ungerechtigkeit, wachsender Ungleichheit,

15:56.000 --> 16:00.000
ist eben einer der wichtigsten und dringendsten Themen unserer Zeit,

16:00.000 --> 16:03.000
wie wir die Spielregeln dieser neuen Welt gestalten,

16:03.000 --> 16:08.000
die in ihren vollen Konsequenzen immer noch nicht viele Leute wirklich verstehen.

16:08.000 --> 16:14.000
Also wir leben einer radikal anderen Welt, als wir 2001 gelebt haben.

16:14.000 --> 16:16.000
Und es liegt an uns, diese Welt zu gestalten.

16:16.000 --> 16:18.000
Und deswegen finde ich das Thema Zukunft so spannend,

16:18.000 --> 16:21.000
weil im Deutschen das Wort Digitalisierung suggeriert manchmal,

16:21.000 --> 16:25.000
als wäre das ein Prozess, der uns passiert, der über uns kommt.

16:25.000 --> 16:28.000
Dabei ist es eigentlich andersrum. Es liegt an uns, ihn zu gestalten.

16:28.000 --> 16:31.000
Alles, was jetzt ist, hätte auch anders sein können.

16:31.000 --> 16:36.000
Es ist viel freier, viel offener.

16:36.000 --> 16:41.000
Und was mich motiviert oder meine persönliche Motivation ist,

16:41.000 --> 16:46.000
ich möchte eben mehr Menschen dazu bewegen und auch selber Teil dieser Gestaltung sein

16:46.000 --> 16:49.000
und das nicht einfach nur mich überkommen lassen.

16:49.000 --> 16:53.000
Da hast du jetzt verschiedene Sachen angesprochen,

16:53.000 --> 16:58.000
unter anderem auch dieses Spannungsfeld zwischen Genauigkeit und danebenliegen.

16:58.000 --> 17:01.000
Ich finde das auch unter einem anderen Aspekt interessant,

17:01.000 --> 17:06.000
weil ja auch auf Seiten der Kritikerinnen und Kritiker dieser Technologien

17:06.000 --> 17:10.000
das mitunter dann fast schon unbeabsichtigt scheint mir irgendwie passiert,

17:10.000 --> 17:15.000
dass die an den Mythen dieser Technologien eigentlich mitarbeiten,

17:15.000 --> 17:21.000
indem sie, ich glaube fast aus Versehen sozusagen,

17:21.000 --> 17:25.000
diese Erzählungen, diese Narrative für bare Münze nehmen

17:25.000 --> 17:30.000
und dann quasi im Aufschrei dagegen, wie genau man uns denn jetzt sehen könne,

17:30.000 --> 17:33.000
wie genau man jetzt Zukunft voraussehen könne und so weiter,

17:33.000 --> 17:37.000
eigentlich auf eine Art eben diese Mythologien auch mit verfestigen.

17:37.000 --> 17:42.000
Wie versuchst du das zu umschiffen? Wie gehst du mit dieser Problematik um?

17:44.000 --> 17:49.000
Kritik ist ja erstmal grundsätzlich positiv, aber es gibt natürlich Kritik und Kritik.

17:49.000 --> 17:55.000
Man sieht das immer anhand dieses Beispiels oder zwei Beispiele finde ich hier besonders gut.

17:55.000 --> 17:59.000
Das eine ist künstliche Intelligenz. Natürlich muss man das kritisch sehen,

17:59.000 --> 18:03.000
aber Kritik heißt nicht, dass wir jetzt Angst vor Terminator haben müssen

18:03.000 --> 18:08.000
oder dass KI jetzt bald alles, wie sagt man das auf Deutsch,

18:08.000 --> 18:12.000
also dass KI jetzt plötzlich unser ganzes Leben entscheiden wird oder bestimmen wird.

18:12.000 --> 18:16.000
Das ist die falsche Sorge, sondern die eigentliche Sorge ist,

18:16.000 --> 18:22.000
dass KI oft nicht funktioniert, oft richtig schlampig gebaut wird,

18:22.000 --> 18:29.000
Vorurteile, Diskriminierung reproduziert, also die Gefahr ist oft viel subtiler als man das denkt.

18:30.000 --> 18:33.000
Und dann die eigentliche Gefahr oder wir müssen gar nicht über Gefahren reden,

18:33.000 --> 18:38.000
aber ich glaube der Bereich, der am meisten kritisiert werden muss,

18:38.000 --> 18:43.000
ist die Tatsache, dass es ganz viele Fragen gibt, die wir nicht beantworten können,

18:43.000 --> 18:49.000
die nur eine ganz kleine Anzahl an Firmen sind überhaupt in der Lage diese Fragen zu beantworten.

18:49.000 --> 18:54.000
Ein Beispiel ist sowas Radikalisierung im Netz von Empfehlungsalgorithmen.

18:54.000 --> 18:57.000
Die Studien dazu sind extrem schwierig durchzuführen,

18:57.000 --> 19:01.000
weil personalisierte Algorithmen eben für jeden anders aussehen.

19:01.000 --> 19:07.000
Alleine schon ist es sehr schwierig, auch kollektiv zum Beispiel über Online-Werbung zu sprechen.

19:07.000 --> 19:11.000
Meine Online-Werbung sieht anders aus als deine, sieht anders aus als ihre.

19:11.000 --> 19:17.000
Und da gibt es oft Kritik in der Öffentlichkeit, sowas wie das Internet macht uns dumm,

19:17.000 --> 19:22.000
oder das Internet macht, oder soziale Medien machen X.

19:22.000 --> 19:26.000
Und ich finde diese Art von Kritik schwierig, weil die Frage ist ja,

19:26.000 --> 19:29.000
was ist denn überhaupt Social Media und was sollte es sein?

19:29.000 --> 19:36.000
Indem wir so monokausale Kritik üben, sehen wir die eigentliche Technik Social Media

19:36.000 --> 19:38.000
als etwas viel starreres als es eigentlich ist.

19:38.000 --> 19:43.000
Wir meinen damit eine Bandbreite an Produkten, die sich permanent verändern,

19:43.000 --> 19:46.000
die selbst vor fünf Jahren anders aussahen als jetzt.

19:46.000 --> 19:49.000
Das heißt, diese kausalen Aussagen sind schwierig.

19:50.000 --> 19:53.000
Also im Kern geht es für mich um zwei Sachen.

19:53.000 --> 19:59.000
Die eine Sache ist, es ist nicht nur, dass wir in einer Welt leben, in der man mehr über uns weiß,

19:59.000 --> 20:03.000
sondern der öffentliche Raum, und das ist ja nicht nur Raum, in dem wir sprechen,

20:03.000 --> 20:07.000
das ist auch der Raum, in dem sich soziale Bewegungen formen,

20:07.000 --> 20:10.000
in dem wir über die Welt lernen, jetzt diese Woche Afghanistan.

20:10.000 --> 20:15.000
Das liest man auf Twitter mit und das lesen auch Journalisten auf Twitter mit.

20:15.000 --> 20:22.000
Und es ist schon sehr erstaunlich, dass diese Räume von einer Handvoll an Firmen kontrolliert und gesteuert werden.

20:22.000 --> 20:25.000
Das ist historisch schon extrem einmalig.

20:25.000 --> 20:29.000
Das ist eine Art von Macht, die diese Firmen haben, die wir, ich glaube,

20:29.000 --> 20:32.000
die wir im vollen Ausmaß noch nicht richtig verstanden haben.

20:32.000 --> 20:39.000
Und die andere Sache, die ich so interessant finde, ist die Informationsungleichheit.

20:39.000 --> 20:42.000
Auf der einen Seite kann man sehr viel über uns wissen,

20:42.000 --> 20:48.000
auf der anderen Seite ist es sehr schwer überhaupt zu verstehen, wie zum Beispiel ein Handy funktioniert.

20:48.000 --> 20:50.000
Was passiert eigentlich auf dem Handy?

20:50.000 --> 20:55.000
Oder sehr wenig Menschen wissen eigentlich, hört mein Handy mir zu, wie wird eigentlich Werbung personalisiert?

20:55.000 --> 21:00.000
Und das kreiert eine Informationsasymmetrie, die natürlich auch ein Machtgefälle ist.

21:00.000 --> 21:04.000
Also das sind für mich diese Fragen um Macht und Gerechtigkeit.

21:04.000 --> 21:08.000
Das sind eigentlich die Kernfragen oder das ist da, wo die Kritik ansetzen muss.

21:09.000 --> 21:15.000
Es gibt so eine Demarkationslinie, die ich noch nie so recht habe begreifen können.

21:15.000 --> 21:19.000
Vielleicht kannst du mir da ein bisschen weiterhelfen und das ist die Wahlwerbung.

21:19.000 --> 21:25.000
Also die Mechanismen, die Tools, die eingesetzt werden in dieser Wahlwerbung,

21:25.000 --> 21:31.000
das sind Tools, die sind im normalen Online-Werbegeschäft total frei verfügbar für jeden.

21:31.000 --> 21:36.000
Also du kannst quasi diese Lookalikes oder sowas, das ist ganz normal,

21:36.000 --> 21:40.000
wenn du eine Facebook-Werbung schaltest, dass du das dann auswählen kannst.

21:40.000 --> 21:45.000
Wenn das aber wiederum dann auf den politischen Bereich übertragen wird,

21:45.000 --> 21:48.000
dann gibt es immer plötzlich so einen Aufschrei.

21:48.000 --> 21:53.000
Dadurch können jetzt irgendwie in einer Art und Weise die öffentliche Meinung manipuliert werden,

21:53.000 --> 21:56.000
wie das vorher noch nicht dagewesen sei.

21:56.000 --> 22:01.000
Es gäbe jetzt Microtargeting und das würde quasi diesen demokratischen Willensbildungsprozess

22:01.000 --> 22:05.000
in einer Art und Weise beeinflussen, die jetzt aber wirklich nicht mehr zulässig wäre.

22:05.000 --> 22:09.000
Und ich frage mich immer, wenn ich das höre, ohne jetzt sagen zu wollen,

22:09.000 --> 22:12.000
dass das eine komplett unberechtigte Kritik wäre oder sowas,

22:12.000 --> 22:15.000
aber ich habe noch nie begriffen, wie man diese Linie zieht.

22:15.000 --> 22:20.000
Also was ist quasi, was ist irgendwie okay?

22:20.000 --> 22:23.000
Früher gab es auch immer schon Medienkonglomerate,

22:23.000 --> 22:28.000
die hatten halt dann irgendwie eine Mehrheit der Tageszeitungen in Besitz

22:28.000 --> 22:34.000
und hatten dadurch eine gewisse Medienherrschaft, wenn man so will.

22:34.000 --> 22:43.000
Warum wird diese Demarkationslinie so gezogen in Bezug auf zum Beispiel Online-Targeting bei politischen Wahlkämpfen?

22:43.000 --> 22:46.000
Erstmal danke für die Frage. Das Thema ist mir extrem wichtig.

22:46.000 --> 22:52.000
Ich arbeite gerade für die EU-Kommission zu der Regulierung von politischer Online-Werbung.

22:52.000 --> 22:55.000
Man muss unterschiedliche Sachen unterscheiden.

22:55.000 --> 23:00.000
Es wurde ganz viel über Online-Werbung diskutiert im Kontext des Brexit-Referendums.

23:00.000 --> 23:02.000
Und ich habe jetzt jahrelang in England gelebt.

23:02.000 --> 23:09.000
Und es ist natürlich nicht so, dass Anti-EU, Anti-Einwanderer-Rechte, Populismus nur online stattfindet.

23:09.000 --> 23:11.000
Man muss nur in einen Kiosk gehen.

23:11.000 --> 23:15.000
Und die Tabloids sind voll von Missinformationen.

23:15.000 --> 23:20.000
Missinformationen und Lügen. Also das ist kein inhärentes Online-Problem.

23:20.000 --> 23:24.000
Was aber spannend ist oder was auch eine wirkliche Gefahr ist,

23:24.000 --> 23:29.000
und das ist so ein Phänomen, was man oft auch in anderen Bereichen sieht, zum Beispiel in der Polizeiarbeit.

23:29.000 --> 23:33.000
Die Regeln, die offline im Wahlkampf gelten, gelten nicht online.

23:33.000 --> 23:37.000
Beispielsweise, wenn ich jetzt hier ein Wahlplakat aufhänge,

23:37.000 --> 23:41.000
muss ich deklarieren, von wem das kommt und wer dafür bezahlt hat.

23:41.000 --> 23:46.000
Wenn ich, ich kann nicht einfach als Milliardär, der ich nicht bin,

23:46.000 --> 23:51.000
im deutschen Fernsehen jetzt einfach alle Werbeplätze kaufen

23:51.000 --> 23:54.000
und nur noch Werbung für Friederike als Kanzlerin machen.

23:54.000 --> 23:56.000
Das ist einfach nicht erlaubt.

23:56.000 --> 24:00.000
Weil es da Kontingente gibt, es gibt Plätze, das ist alles sehr streng reguliert.

24:00.000 --> 24:04.000
Oder man merkt ja auch jetzt im Bundeswahlkampf in Deutschland,

24:04.000 --> 24:06.000
es hängen eigentlich erst jetzt Plakate.

24:06.000 --> 24:08.000
Es waren nicht schon seit Monaten Plakate.

24:08.000 --> 24:13.000
Der Unterschied ist, dass online die Regeln einfach nicht greifen.

24:13.000 --> 24:15.000
Aus ganz unterschiedlichen Gründen.

24:15.000 --> 24:21.000
Was dann dazu führt, dass eben Parteien, und das ist in den USA passiert,

24:21.000 --> 24:27.000
dass zum Beispiel die Trump-Kampagne Millionen verschiedene Versionen

24:27.000 --> 24:29.000
von Werbung ausgestrahlt hat.

24:29.000 --> 24:31.000
Das ist noch ein weiterer Unterschied.

24:31.000 --> 24:35.000
Also offline, was auch immer ich auf Wahlplakaten behaupte,

24:35.000 --> 24:38.000
wen ich beleidige, wie radikal auch immer das ist,

24:38.000 --> 24:40.000
es hängt in der Öffentlichkeit.

24:40.000 --> 24:44.000
Und es kann damit gesehen und im öffentlichen Diskurs diskutiert werden.

24:44.000 --> 24:50.000
Der Unterschied zur Online-Werbung ist, weil die Online-Werbung eben personalisiert ist,

24:50.000 --> 24:54.000
ich kann eine Million Versionen derselben Werbung schalten

24:54.000 --> 24:57.000
und im Endeffekt jedem was anderes erzählen.

24:57.000 --> 25:01.000
Ich kann den einen Wählern das versprechen, den anderen Wählern das versprechen.

25:01.000 --> 25:05.000
Ich kann Gruppen gegeneinander aufhetzen,

25:05.000 --> 25:09.000
ohne dass man überhaupt etwas in der Öffentlichkeit davon mitkriegt,

25:09.000 --> 25:13.000
weil es eben so vergänglich ist und so schnelllebig ist.

25:13.000 --> 25:16.000
Die Werbung wird einen Tag geschaltet und dann wird sie nicht mehr geschaltet.

25:16.000 --> 25:19.000
Und vielleicht noch der letzte Grund ist,

25:19.000 --> 25:22.000
man kann durch, und das wurde auch in den USA gemacht,

25:22.000 --> 25:25.000
man kann durch Online-Werbung ganz gezielt auch testen,

25:25.000 --> 25:29.000
was so die Stimmung ist, worauf die Leute anspringen.

25:29.000 --> 25:33.000
Und Werbung wird ja nicht nur geschaltet, sondern das wird dann optimiert

25:33.000 --> 25:37.000
und das wird dann sehr genau gezielt an bestimmte Gruppen ausgeschaltet.

25:37.000 --> 25:41.000
Was ich damit sagen will, in der öffentlichen Debatte

25:41.000 --> 25:45.000
wurde das oft verkürzt auf politische Werbung ist eine Gefahr,

25:45.000 --> 25:49.000
weil die Menschen manipuliert, die dann anders wählen als sie sonst wählen würden.

25:49.000 --> 25:53.000
Das ist natürlich zu kurz gegriffen, das Problem ist viel komplexer

25:53.000 --> 25:56.000
und das zeigt so ein bisschen auch, wenn man über Polizeiarbeit

25:56.000 --> 25:59.000
oder vorausschauende Polizeiarbeit spricht, das Problem ist häufig,

25:59.000 --> 26:03.000
dass dieselben Regeln, die offline gelten, nicht online gelten.

26:03.000 --> 26:07.000
Und das quote-unquote online, das ist ja auch verschmolzen,

26:07.000 --> 26:10.000
dass plötzlich Dinge möglich sind, die so nicht möglich sind.

26:10.000 --> 26:14.000
Es ist offline nicht möglich, jedem einen anderen Wahlspruch zu zeigen.

26:14.000 --> 26:20.000
Das heißt, es bedarf irgendwie Regeln, es bedarf Intervention

26:20.000 --> 26:24.000
und so wie es momentan aussieht, machen das die Plattformen eben freiwillig

26:24.000 --> 26:28.000
und sie machen das auch nicht konsequent und auch nicht immer sehr zuverlässig.

26:28.000 --> 26:31.000
Und deswegen muss es halt Regeln geben, die das klar definieren.

26:31.000 --> 26:34.000
Auch wenn zum Beispiel dann bei Facebook mal ein anderer Chef ist,

26:34.000 --> 26:37.000
dass sich die Regeln dann nicht ändern.

26:37.000 --> 26:41.000
Aber ich mag die Frage, um nochmal darauf zurückzugehen, was ich an der Frage mag,

26:41.000 --> 26:46.000
ist, es stimmt, es gibt viel Digitalisierungskritik.

26:46.000 --> 26:50.000
Allein die Frage, so Digitalisierung gut oder schlecht, ist die völlig falsche Frage,

26:50.000 --> 26:54.000
sondern die Frage ist, wie, wie Digitalisierung, wie Technologien,

26:54.000 --> 26:58.000
was wollen wir eigentlich kollektiv als Gesellschaft

26:58.000 --> 27:03.000
und nicht das Empfinden als so eine unaufweichbare Bewegung.

27:04.000 --> 27:09.000
Und bei dieser Online-Werbung, also zumindest, das würde mich vielleicht auch noch interessieren,

27:09.000 --> 27:16.000
ich wüsste jetzt auch gar nicht, wie effektiv oder ob die, vielleicht eher so,

27:16.000 --> 27:21.000
ob die Erzählung dessen, wie effektiv das angeblich gewesen sei,

27:21.000 --> 27:25.000
wie sehr das mit der Realität übereinstimmt.

27:25.000 --> 27:29.000
Und tendenziell neige ich dazu, dass ich immer so ein bisschen die Intuition habe,

27:29.000 --> 27:34.000
dass es mir so scheint, als ob andere Aspekte, die eher sozioökonomische,

27:34.000 --> 27:39.000
sozio-politische Aspekte sind, die zu verschiedenen Entwicklungen führen,

27:39.000 --> 27:42.000
dass die unterschlagen werden, wenn man das so darstellt als,

27:42.000 --> 27:46.000
eben als sei quasi Trump, also jetzt ich bin absichtlich polemisch,

27:46.000 --> 27:52.000
Trump wäre gewählt worden, weil er über Social Media quasi die Leute gegeneinander aufgehetzt hat.

27:52.000 --> 27:56.000
Also dafür muss es ja ein Fundament geben, das in dem Fall, glaube ich,

27:56.000 --> 27:59.000
auch mit einer realen Not zum Beispiel zu tun hat,

27:59.000 --> 28:04.000
weil man kann Leute in Not besser gegeneinander aufhetzen, zum Beispiel.

28:04.000 --> 28:08.000
Die, sorry, jetzt habe ich meinen Faden verloren.

28:08.000 --> 28:14.000
Ich habe immer noch was hinterhergeschoben, immer noch was hinterhergeschoben.

28:14.000 --> 28:20.000
Es muss ja noch nicht passiert sein, das heißt aber nicht, dass es nicht doch auch noch passieren könnte.

28:20.000 --> 28:24.000
Also wir wissen, was technisch möglich ist und das Beispiel, was ich immer gerne bringe, ist,

28:24.000 --> 28:28.000
es ist belegt, es gab in den USA Abtreibungsgegner,

28:28.000 --> 28:35.000
die abtreibungswillige Frauen im Klinikbesuch mit Antiabtreibungswerbung bombardiert haben.

28:35.000 --> 28:38.000
Werden die deshalb jetzt nicht abtreiben?

28:38.000 --> 28:44.000
Wahrscheinlich nicht, es ist aber eine ganz perfide Form von Belästigung.

28:44.000 --> 28:47.000
Wahlen werden ja auch oft, es gibt immer mehr Wahlen,

28:47.000 --> 28:52.000
die werden in bestimmten Wahlkreisen durch wenige tausend Stimmen getroffen.

28:52.000 --> 28:55.000
Die Wahlerwerbung muss ja noch nicht mal sagen, wähl Trump,

28:55.000 --> 28:57.000
sondern die Wahlerwerbung kann auch einfach sagen,

28:57.000 --> 29:00.000
der ganze Politikbetrieb ist kaputt, geh doch gar nicht wählen.

29:00.000 --> 29:03.000
Und das macht das halt alles so kompliziert.

29:03.000 --> 29:06.000
Und ich glaube, das ist ein gutes Beispiel, weil das zeigt, wir wissen,

29:06.000 --> 29:12.000
technisch ist es eben möglich, sehr spezifische Gruppen sehr spezifisch anzusprechen.

29:12.000 --> 29:15.000
Keine Ahnung, wie effektiv das ist, aber das Problem ist,

29:15.000 --> 29:18.000
niemand kann es messen, außer die Plattformen selber.

29:18.000 --> 29:21.000
Aber hier geht es ja nicht um irgendwas, hier geht es um Demokratie.

29:21.000 --> 29:26.000
Es ist eigentlich extrem beunruhigend, dass wir gar nicht mal richtig wissen,

29:26.000 --> 29:29.000
welchen Einfluss Plattformen auf Wahlen hatten,

29:29.000 --> 29:32.000
weil nur die Plattformen selber überhaupt die Daten dazu haben.

29:32.000 --> 29:34.000
Das alleine für mich reicht schon aus.

29:34.000 --> 29:38.000
Es muss nicht ausgenutzt werden, wir wissen, dass es ausgenutzt werden könnte.

29:38.000 --> 29:40.000
Und das sollte nicht passieren.

29:40.000 --> 29:43.000
Total, also da würde ich natürlich sofort zustimmen.

29:43.000 --> 29:47.000
Wir sollten dafür sorgen, dass die Dinge eben in einer Art und Weise nachvollziehbar werden,

29:47.000 --> 29:49.000
wie sie es bisher leider nicht sind.

29:49.000 --> 29:52.000
Und das nährt ja am Ende dann eben auch diese Mythologien,

29:52.000 --> 29:56.000
weil die Plattformen haben natürlich ein Eigeninteresse daran zu tun,

29:56.000 --> 30:00.000
als wären ihre Werbung unglaublich effektiv und könnten extrem genau,

30:00.000 --> 30:03.000
also detailliert targetten und so weiter und so fort.

30:03.000 --> 30:08.000
Ich würde aber gerne jetzt dann noch zu einer anderen Demystifisierung übergehen,

30:08.000 --> 30:12.000
nämlich zu etwas, was du in einem Block-Eintrag AI Snake Oil genannt hast.

30:12.000 --> 30:17.000
Das ist ein Ausdruck, der mir sehr gut gefallen hat und sofort ins Auge gesprungen ist.

30:17.000 --> 30:25.000
Und du sprichst da auch vom Jahr 2020 als dem Jahr der AI-Desillusionierung.

30:25.000 --> 30:28.000
Vielleicht kannst du zum einen ein bisschen darauf eingehen, warum du meinst,

30:28.000 --> 30:33.000
dass 2020 dieses Jahr der AI-Desillusionierung war

30:33.000 --> 30:40.000
und vielleicht auch ein konkretes Beispiel geben für AI Snake Oil.

30:40.000 --> 30:46.000
Ich glaube, 2020 war das Jahr, also alleine empirisch das Schöne ist,

30:46.000 --> 30:48.000
wenn man schon so lange an diesem Themenkomplex arbeitet,

30:48.000 --> 30:51.000
dass man sieht, dass die Dinge immer in Wellen kommen.

30:51.000 --> 30:54.000
Also 2014 sprachen wir über Big Data.

30:54.000 --> 30:58.000
Niemand redet mehr von Big Data plötzlich, aber die Phänomene sind ja nicht weg.

30:58.000 --> 31:06.000
Dann kam eine Zeit, wo alles war AI, ohne auch kritische Äußerungen.

31:06.000 --> 31:10.000
Und dann gab es die Welle, wo man gesagt hat, na ja, aber AI kann diskriminieren,

31:10.000 --> 31:13.000
AI hat ethische Probleme etc.

31:13.000 --> 31:17.000
Also es gibt immer diese Wellen. Und für mich war 2020 schon ein interessantes Jahr,

31:17.000 --> 31:23.000
weil es wurde jahrelang behauptet, dass AI eigentlich jedes Problem lösen kann.

31:23.000 --> 31:29.000
Und dann 2020 stand und steht die Welt vor einem wirklichen Problem.

31:29.000 --> 31:33.000
Wir haben ja genug Probleme, aber so einer akuten Krise,

31:33.000 --> 31:38.000
wie die Pandemie. Und der Witz war, dass AI relativ wenig dazu beigetragen hat,

31:38.000 --> 31:43.000
die Pandemie zu lösen, weil nämlich Probleme hochkomplex sind.

31:43.000 --> 31:47.000
Und die Pandemie, es liegt ja nicht daran, AI kann die Pandemie nicht lösen,

31:47.000 --> 31:52.000
sondern es ist eine Kombination aus politischem Willen, auch Desinformation online.

31:52.000 --> 31:55.000
Es sind hochkomplexe gesellschaftliche Probleme.

31:55.000 --> 31:57.000
Es ist kein informationelles Problem.

31:57.000 --> 31:59.000
Wir kriegen die Pandemie nicht in den Griff,

31:59.000 --> 32:03.000
weil wir zu wenig wissen, sondern aus ganz vielen komplexen anderen Gründen.

32:03.000 --> 32:07.000
Und das hat, glaube ich, schon ein bisschen dazu zu der allgemeinen Stimmung beigetragen.

32:07.000 --> 32:13.000
Der Grund, warum mich das Thema interessiert, ich habe eben letztes Jahr ein Fellowship gemacht.

32:13.000 --> 32:19.000
Und das Fellowship-Projekt ist ein Buch, was ich herausgebe zum Thema KI-Pseudo-Wissenschaften.

32:19.000 --> 32:24.000
Und in der Einleitung, das kommt auf Englisch raus, und in der Einleitung des Buches steht,

32:24.000 --> 32:30.000
Also dieses Buch ist eine Intervention, weil es ist ein bisschen mehr abgeflaucht jetzt,

32:30.000 --> 32:34.000
aber es vergeht eigentlich kein Tag oder keine Woche,

32:34.000 --> 32:37.000
in der ich nicht irgendeinen Nachrichtenartikel auf Twitter lese,

32:37.000 --> 32:42.000
wo wieder behauptet, dass KI irgendetwas kann, was es auf keinen Fall kann.

32:42.000 --> 32:47.000
Von KI kann erkennen, ob wir unsere sexuelle Orientierung erkennen,

32:47.000 --> 32:50.000
KI kann erkennen, ob wir unsere sexuelle Orientierung erkennen,

32:50.000 --> 32:53.000
KI kann erkennen, ob wir unsere sexuelle Orientierung erkennen,

32:53.000 --> 33:00.000
KI kann etnie, da kommt man ganz schnell in sehr hochproblematische, rassistische Ecken.

33:00.000 --> 33:06.000
Oder auch ganz am Anfang der Pandemie waren auch so KI Videokameras,

33:06.000 --> 33:09.000
können entdecken, ob jemand Covid hat oder nicht.

33:09.000 --> 33:11.000
Das ist ja völliger Quatsch.

33:11.000 --> 33:16.000
Aber die Frage, die dem Buch oder die das Buch nachgeht,

33:16.000 --> 33:21.000
und ich habe auch ganz bewusst, durch die Pandemie konnte ich auch dieses Fellowship nicht so verbringen,

33:21.000 --> 33:25.000
wie ich es verbringen wollte und dachte, ich möchte dieses Projektgeld dann zumindest verteilen

33:25.000 --> 33:29.000
und dann auch mit anderen Leuten teilen.

33:29.000 --> 33:33.000
Ich habe bewusst unterschiedliche Autoren eingeladen, die eben dieser Frage nachgehen,

33:33.000 --> 33:36.000
warum gibt es einfach so viel KI-Pseudowissenschaften?

33:36.000 --> 33:38.000
Warum ist das so?

33:38.000 --> 33:44.000
Und die Autorinnen haben jeweils eine ganz unterschiedliche Erklärung.

33:44.000 --> 33:48.000
Und die erste Frage, mit der wir uns so ein bisschen auseinandersetzen in dem Buch ist,

33:48.000 --> 33:54.000
was ist eigentlich der Unterschied zwischen Pseudowissenschaften, Schlangenöl und Hype?

33:54.000 --> 33:58.000
Das sind ja ganz unterschiedliche Dinge, die unterschiedliche Kerne des Problems beschreiben.

33:58.000 --> 34:03.000
Also Schlangenöl, das ist im Deutschen weniger gängig, als es so im Englischen ist.

34:03.000 --> 34:10.000
Snake Oil, das beschreibt eigentlich unfaires Marketing, also Produkte.

34:10.000 --> 34:14.000
Schlangenöl, Haut, sehr viel Kosmetik, Frauenkosmetik ist Schlangenöl.

34:14.000 --> 34:19.000
Da steht, hier ist eine Wundercreme, die irgendetwas kann und die Creme kann eigentlich gar nichts.

34:19.000 --> 34:21.000
Das ist Snake Oil runtergebrochen.

34:21.000 --> 34:24.000
Hype ist natürlich nochmal was anderes.

34:24.000 --> 34:27.000
Hype ist gar nicht immer schlecht.

34:27.000 --> 34:30.000
Hype ist einfach, naja, auch Aufregung.

34:30.000 --> 34:35.000
Wir leben ja in einer Welt, in der Aufmerksamkeit das teuerste Gut ist.

34:35.000 --> 34:38.000
Irgendwie muss man ja auch gehört werden.

34:38.000 --> 34:41.000
Und dann die dritte Kategorie ist einfach Pseudowissenschaften.

34:41.000 --> 34:46.000
Also einfach Wissenschaft, die nach Wissenschaft aussieht, aber völliger Quatsch ist.

34:46.000 --> 34:49.000
Und diese drei Kategorien schauen sich eben das Buch an.

34:49.000 --> 34:54.000
Wir haben zum Beispiel einen Beitrag von einem Journalisten, der über KI schreibt für The Verge

34:54.000 --> 35:00.000
und so ein bisschen erzählt, wie es ist, wenn er Produktpitches bekommt.

35:00.000 --> 35:03.000
Zum Beispiel hier ist eine KI-gesteuerte Zahnbürste.

35:03.000 --> 35:07.000
Und in dem Artikel reflektiert er auch darüber, wie kann ich darüber kritisch berichten,

35:07.000 --> 35:10.000
ohne dem gleichzeitig noch mehr Aufmerksamkeit zu geben.

35:10.000 --> 35:12.000
Das ist irgendwie ein Aspekt.

35:12.000 --> 35:22.000
Da ist auch eine Autorin, die spricht, eine Ingenieurin, die sagt, ganz viel ist wirklich einfach verantwortungslos.

35:22.000 --> 35:26.000
Manchmal werden einfach verantwortungslose Produkte gebaut, schlampig gebaut.

35:26.000 --> 35:30.000
Und das fand ich ganz spannend, weil das ist natürlich was anderes, wenn ich das sage,

35:30.000 --> 35:33.000
als wenn das eine Ingenieurin sagt, die in diesen Teams gearbeitet hat.

35:33.000 --> 35:35.000
Also wir schauen uns unterschiedliche Aspekte an.

35:35.000 --> 35:43.000
Aber der Begriff KI AI Snake Oil oder KI Schlangenöl stammt von einem Professor aus Princeton,

35:43.000 --> 35:45.000
den wir auch in dem Buch interviewen.

35:45.000 --> 35:52.000
Der hat diesen Begriff geprägt in einem Vortrag, in dem er irgendwie heißt How to recognize AI Snake Oil.

35:52.000 --> 35:56.000
Und das Tolle an dem Vortrag ist, er nimmt so ein bisschen auseinander.

35:56.000 --> 35:58.000
Was ist eigentlich KI?

35:58.000 --> 36:02.000
Das Problem ist ja, dass das Wort hat eine ganz klare technische Definition,

36:02.000 --> 36:07.000
wird im allgemeinen Sprachgebrauch aber benutzt, um alles Mögliche zu bezeichnen.

36:07.000 --> 36:13.000
Es gab mal eine Studie, die hat gesagt, dass 60 Prozent, die ist glaube ich jetzt zwei Jahre alt,

36:13.000 --> 36:19.000
60 Prozent aller europäischen KI Startups machen eigentlich gar nichts mit KI.

36:19.000 --> 36:22.000
Es gibt halt einfach viel Geld, wenn man KI draufschreibt.

36:22.000 --> 36:29.000
Und was er in diesem Vortrag gemacht hat, er unterscheidet, auf der einen Seite gab es wirklich in den letzten Jahren

36:29.000 --> 36:38.000
in sehr klar und eng definierten Problembeschreibungen hat der Einsatz von KI immense Fortschritte gemacht.

36:38.000 --> 36:41.000
Sowas wie ein Objekt zu erkennen.

36:41.000 --> 36:45.000
Oder wenn man sich anschaut, wie gut sich zum Beispiel Google Translate verändert hat.

36:45.000 --> 36:51.000
Ich war vor einem Jahr in China und man kann einfach in ein Gerät auf Englisch reinsprechen

36:51.000 --> 36:54.000
und das Gerät übersetzt automatisch auf Chinesisch.

36:54.000 --> 36:58.000
Das hätten wir noch vor zehn Jahren für unglaublich gehalten.

36:58.000 --> 37:04.000
Sein Punkt ist aber, das sind sehr klar definierte, eng abgegrenzte Aufgaben.

37:04.000 --> 37:09.000
Das sind keine offenen Probleme wie die Pandemie lösen.

37:09.000 --> 37:14.000
Und er beschreibt so ein bisschen, er unterteilt die Aufgaben in unterschiedliche Kategorien

37:14.000 --> 37:19.000
und sagt eben, es gibt eine Kategorie und das habe ich immer noch nicht raus, wie man das gut auf Deutsch übersetzt,

37:19.000 --> 37:22.000
wo er sagt so Predicting Social Outcomes.

37:22.000 --> 37:32.000
Also KI ist einfach grundsätzlich immer dubios, wenn es dazu angewendet wird, um die Zukunft vorauszusagen.

37:32.000 --> 37:35.000
Sowohl die kollektive als auch die individuelle.

37:35.000 --> 37:38.000
Weil das einfach nicht geht.

37:38.000 --> 37:43.000
Und das heißt sowas wie vorausschauende Polizeiarbeit oder so sagen, wer du bist,

37:43.000 --> 37:46.000
was deine sexuelle Orientierung ist, das ist einfach Quatsch.

37:47.000 --> 37:53.000
Und das hat nichts und nur weil wir Fortschritt darin gemacht haben, Objekte zu erkennen, heißt das nicht,

37:53.000 --> 37:58.000
dass wir automatisch auch in diesen anderen Fragestellen, in diesen inherent normativen

37:58.000 --> 38:01.000
und inherent sozialen Fragestellungen denselben Fortschritt machen werden.

38:01.000 --> 38:07.000
Das heißt, das Buch ist so ein bisschen eine Aufforderung, differenzierter zu betrachten,

38:07.000 --> 38:13.000
was ist eigentlich KI, was ist eigentlich der Fortschritt und was ist auch einfach völliger Unsinn.

38:14.000 --> 38:22.000
Und du hast recht, ich mag deine Frage, weil oft trägt die Kritik dann auch dazu bei, zum Hype, zur Überhöhung der Technik.

38:22.000 --> 38:27.000
Weil man sagt, wir müssen jetzt Angst vor Algorithmen haben, wir müssen Angst vor KI haben, das ist ja Quatsch.

38:27.000 --> 38:30.000
Es kommt nämlich einfach darauf an, worum es geht.

38:30.000 --> 38:35.000
Ja und auch in der Darstellung, was sie eben können, liegt die Kritik manchmal halt dann falsch,

38:35.000 --> 38:39.000
weil sie eben zu sehr quasi diesen Marketing-Sprech eigentlich kauft.

38:39.000 --> 38:46.000
Und zur Frage der Zukunft ist natürlich dann noch das perfide, was da vielleicht noch hinzuzusagen wäre,

38:46.000 --> 38:52.000
dass auf einer anderen Ebene es ja sehr wohl wieder funktioniert, nämlich dadurch, dass so getan wird,

38:52.000 --> 38:59.000
als könne man quasi Zukunft bis zu einem gewissen Grad vorausschauen, dann ja auch Politiken betrieben werden,

38:59.000 --> 39:06.000
die auf Basis dieser Pseudo-Vorhersagen handeln und dadurch dann wiederum de facto

39:06.000 --> 39:10.000
möglichkeitskorridore eingeschränkt werden. Und das ist natürlich schrecklich.

39:10.000 --> 39:19.000
Also es gibt dem der schönste Text, das heißt Cheap AI von einer Wissenschaftlerin, die heißt Abhiba Birhane,

39:19.000 --> 39:25.000
die schreibt, man redet manchmal so in der Linguistik von Cheap Talk.

39:25.000 --> 39:31.000
Also es ist sehr einfach, diese Produkte zu konzipieren und zu bauen, aber für diejenigen, die betroffen sind,

39:31.000 --> 39:39.000
haben sie eben wirkliche Konsequenzen von, weiß ich nicht, trans Menschen, die gar nicht erkannt werden,

39:39.000 --> 39:45.000
die nicht existieren in der Kategorisierung des Systems, bis hin zu wirklich übelst rassistischen Systemen.

39:45.000 --> 39:50.000
Und diese, nur wenn man KI draufschreibt, das ist mehr als nur ein Marketing-Trick.

39:50.000 --> 39:56.000
Das gibt dem den Anschein von Zukunft und von Wissenschaftlichkeit und von Neuheit.

39:57.000 --> 40:03.000
Dabei gibt es bestimmte Anwendungen, wo wirklich einfach nur überholte Bilder repliziert werden.

40:05.000 --> 40:11.000
Auf eine Art leitet das auch über, nochmal zur Frage der Daten.

40:13.000 --> 40:22.000
Eine ein bisschen gewagte Hypothese, die ich mal in den Raum stellen will, ist nochmal zu fragen, ob wir es denn tatsächlich mit Big Data zu tun haben

40:22.000 --> 40:29.000
oder ob es nicht vielleicht angebrachter wäre zu sagen, was wir vor uns sehen ist eher Small Data insofern,

40:29.000 --> 40:37.000
als dass die Player, die in der Lage sind, diese Mengen an Daten zu produzieren, das ist ja nichts, was irgendwie frei da draußen rumliegt

40:37.000 --> 40:42.000
und müsste nur eingesammelt werden, sondern es wird produziert, es wird erschaffen und dann ausgewertet.

40:42.000 --> 40:49.000
Und die Player, die überhaupt die Ressourcen haben, um das effektiv zu tun, sind entweder Staaten oder eben Großkonzerne.

40:49.000 --> 40:56.000
Und die haben ja ein spezifisch vorgefertigtes Eigeninteresse, das im Grunde eigentlich dann ja ein Gebiet erzeugt,

40:56.000 --> 41:03.000
was relativ eng gefasst ist, also deswegen eigentlich Small Data und was dann auf dieses spezifische Gebiet hin

41:03.000 --> 41:10.000
vielleicht immer detaillierter werden kann, das mag ich sehr wohl glauben, aber das eigentlich ein riesengroßes Feld,

41:10.000 --> 41:18.000
das jenseits dieser spezifisch formierten Interessen liegt, dass das damit eigentlich unmöglich erfasst werden kann.

41:18.000 --> 41:24.000
Und daraus schließt sich dann eigentlich die Frage an, brauchen wir andere Daten?

41:24.000 --> 41:30.000
Und dann eigentlich natürlich auch bräuchten wir andere Player, die in der Lage wären, diese Daten zu produzieren.

41:30.000 --> 41:36.000
Also ich würde sagen Big but shallow. Also es gibt wirklich, es gab noch nie so viele Daten.

41:36.000 --> 41:43.000
Dieser Begriff Big Data kommt auch daher, das erfordert ganz andere Verarbeitungsmechanismen und Techniken.

41:43.000 --> 41:50.000
Ich hatte mal ein ganz spannendes Gespräch mit einer Entwicklerin, die für die BBC arbeitet

41:50.000 --> 41:57.000
und für die BBC Empfehlungsalgorithmen erarbeitet. Und die Fragestellung, ich weiß nicht, ob das nie jemals verwirklicht wurde,

41:57.000 --> 42:06.000
aber die Fragestellung, die sie sich gestellt hat, was sind Personalisierungs- oder Empfehlungssysteme, die im öffentlichen Interesse sind?

42:07.000 --> 42:13.000
Und dann die Frage, die aufkam, also weiß ich nicht, ich gehe auf BBC News und dann werden wir auch, wie bei Netflix oder bei Amazon,

42:13.000 --> 42:22.000
empfohlen, was ich als nächstes sehen soll. Der Unterschied ist nur, dass die BBC anders als Netflix eine ganz andere Daseinsberechtigung und auch Mission haben.

42:22.000 --> 42:28.000
Und die Frage, die dann aufkam, war, naja, all diese Firmen, die sammeln ja so viel Daten von uns,

42:28.000 --> 42:33.000
wäre es nicht möglich, dass man seine Netflix-Daten an die BBC spenden könnte?

42:34.000 --> 42:43.000
Und die Antwort darauf ist, es ist Quatsch, weil Netflix eben, weil es auf ein ganz anderes Ziel optimiert ist, ganz andere Arten von Daten sammelt,

42:43.000 --> 42:52.000
die für ein Empfehlungssystem im öffentlichen Interesse oder im Gemeinwohlinteresse gar nicht richtig verwertbar sind.

42:52.000 --> 43:01.000
Ich habe mal nach meinem Studium kurz gedacht, ich möchte Data Scientist werden und habe bei Zeit Online ein Praktikum gemacht,

43:01.000 --> 43:09.000
wo es auch darum ging zu messen, ja, wie kann man Qualitätsjournalismus messen?

43:09.000 --> 43:18.000
Also man will ja nicht nur auf Klicks optimieren, die Zeit ist nicht irgendein Blog, so ein Clickbait-Blog,

43:18.000 --> 43:23.000
sondern eine Zeitung, die auch schwierige Themen behandeln möchte.

43:23.000 --> 43:32.000
Was ich so interessant fand aus dieser Erfahrung heraus war, dass die ganzen Tools, die es damals vor zehn Jahren intern gab,

43:32.000 --> 43:37.000
um überhaupt das Leseverhalten zu messen, alle aus dem E-Commerce-Marketing kamen.

43:37.000 --> 43:43.000
Das heißt, die Zahlen waren dann optimiert für ganz andere Szenarien, also Szenarien,

43:43.000 --> 43:48.000
wo man möchte, dass Leute so viel Zeit wie möglich auf der Seite verbringen, dass sie so oft wie möglich klicken.

43:48.000 --> 43:53.000
Dabei ist das eigentlich keine Erfolgsmetrik für ein Medium wie Zeit Online.

43:53.000 --> 44:00.000
Es ist bestimmt jetzt ganz anders. Aber jedenfalls, genau, es gibt sehr viele Daten, aber es gibt auch sehr viele Daten überhaupt nicht.

44:00.000 --> 44:08.000
Also das ist halt dieses Paradox. Also was ich zum Beispiel spannend fand in der 2015 oder auch jetzt in der Pandemie,

44:08.000 --> 44:13.000
ganz viele Gesundheitsämter haben keine kompatiblen Datensysteme und können die Daten nicht austeilen.

44:14.000 --> 44:17.000
Also genau, zu wenig und zu viele gleichzeitig.

44:18.000 --> 44:25.000
Das bringt mich dann jetzt zu einer anderen Frage, die eher auf diese systemische Ebene abzielt eigentlich,

44:25.000 --> 44:33.000
weil sich ja auch die Frage stellt, wie kommen wir dahin, dass diese anderen Daten auch produziert werden,

44:33.000 --> 44:42.000
dass aus einer anderen Logik heraus Daten produziert werden und dadurch dann eben auch andere technologische Infrastrukturen erzeugt werden können,

44:42.000 --> 44:49.000
die andere Zielmaßgaben haben. Und da möchte ich einen kurzen Satz erwähnen, den Felix Stahlder,

44:49.000 --> 44:55.000
ein Gast aus Future Histories, auch in seinem Buch Kultur der Digitalität geschrieben hat.

44:55.000 --> 45:01.000
Der sagte nämlich, nicht der Algorithmus ist pervers, sondern die Situation, in der er lebt.

45:01.000 --> 45:07.000
Und das lässt sich jetzt wiederum ein bisschen quer schließen zum Thema dieses Kongresses hier,

45:07.000 --> 45:11.000
denn hier geht es ja auch um die Frage konsequent, inkonsequent.

45:11.000 --> 45:20.000
Und was mich immer so ein bisschen gestört hat an diesem Framing ist, dass es für meinen Geschmack zu sehr in diese Richtung tendiert,

45:20.000 --> 45:28.000
dem Individuum diese Frage zu beantworten, ob jetzt das richtig gemacht wird oder nicht.

45:28.000 --> 45:35.000
Aber meiner Meinung nach sind diese Infrastrukturen derzeit ja absichtlich und extra einfach so aufgebaut,

45:35.000 --> 45:44.000
dass wir möglichst inkonsequent im Sinne des zum Beispiel Datenschutzes usw. handeln, wenn wir uns in diesen digitalen Welten bewegen.

45:44.000 --> 45:52.000
Das heißt, um quasi den letzten Schritt dann noch zu machen, sind wir nicht inkonsequent oder konsequent inkonsequent

45:52.000 --> 45:58.000
dann insofern, als dass wir diese systemische Ebene in der Fragestellung eigentlich allzu oft ausblenden

45:59.000 --> 46:06.000
und sollten wir nicht eigentlich auf dieser Ebene auch ansetzen, weil die jetzigen Logiken, die incentivieren das ja.

46:06.000 --> 46:12.000
Die viel wichtigere Frage finde ich, wer ist wir? Und das wir ist nämlich kompliziert.

46:12.000 --> 46:17.000
Also Risiken und sowohl Risiken als auch Chancen sind extrem ungleich verteilt.

46:17.000 --> 46:23.000
Und das ist die Hauptlektion aus fast zehn Jahren Arbeit an dem Thema, ist genau das.

46:23.000 --> 46:36.000
Beispiel, wenn man sagt, Gesichtserkennung für die allermeisten, für weiße Männer ist Gesichtserkennung sehr akkurat.

46:36.000 --> 46:44.000
Für nicht weiße Frauen sieht die Genauigkeit ganz anders aus.

46:44.000 --> 46:49.000
Das ist aber nicht nur da der Fall, das ist wirklich so ein Muster, was sich durch alles zieht.

46:49.000 --> 46:56.000
Beispiel, wir werden ja auch, da geht es oft darum so, es gibt dann diese Daten von mir und dann können die gegen mich verwendet werden.

46:56.000 --> 46:58.000
Das hängt natürlich auch von meiner Lebenssituation ab.

46:58.000 --> 47:07.000
Wenn ich beispielsweise finanziell unabhängig bin, dann kann mir das sehr herzlich egal sein, ob ich damit bewerte, ob meine Kreditwürdigkeit dadurch bewertet wird.

47:07.000 --> 47:16.000
Und besonders risikobehaftete Anwendungen werden auch in der Regel tendenziell immer auf bestimmte Bevölkerungsgruppen angewendet.

47:16.000 --> 47:20.000
Deswegen dieses Wir ist kompliziert.

47:20.000 --> 47:26.000
Und das, was für die einen vielleicht sehr angenehm ist, ist vielleicht auch für jemand ganz anderen lebensgefährlich.

47:26.000 --> 47:28.000
Also das Wir ist kompliziert.

47:28.000 --> 47:31.000
Und das andere ist, ich hasse dieses auf die Individualebene bringen.

47:31.000 --> 47:39.000
Und versuche auch aus dem Grund nicht mehr so viel über Daten zu sprechen, sondern viel mehr über Macht, über Infrastruktur.

47:40.000 --> 47:46.000
Weil sobald man über Daten redet, geht es dann schnell auf das Thema, ja, was soll ich denn mit meinen Daten machen?

47:46.000 --> 47:52.000
Weil jeder fühlt sich schuldig, alle haben das Gefühl, dass sie zu viele Daten irgendwie, das ist so wie recyclen.

47:52.000 --> 47:56.000
Ich sollte mehr recyceln, ich gebe mir sehr viel Mühe, aber manchmal mache ich es doch nicht.

47:56.000 --> 47:58.000
Dabei ist das eigentlich der völlig falsche Ansatz.

47:58.000 --> 48:03.000
Niemand hat Zeit, sich jeden Tag damit auseinanderzusetzen, was mit den eigenen Daten passiert.

48:03.000 --> 48:07.000
Das ist meine Arbeit und ich habe keine Zeit und kein Interesse daran.

48:07.000 --> 48:12.000
Und wir unterschätzen auch oft, dass die, genau, das sind systemische Fragen.

48:12.000 --> 48:19.000
Es ist oft, sogar wird es einem unmöglich gemacht, überhaupt eine Entscheidung zu fällen.

48:21.000 --> 48:28.000
Man sagt so, der Satz ist immer so in dem Security-Bereich, die NutzerInnen sind eigentlich das schwächste Glied in dem ganzen System.

48:28.000 --> 48:31.000
Und wir können nicht die Verantwortung darauf abwälzen.

48:32.000 --> 48:44.000
Und wenn man da jetzt in diese Richtung weitergeht, dann, finde ich, liegt da eigentlich auch noch ein Potential für ein eigentlich uneingelöstes Versprechen, wenn man so will.

48:44.000 --> 48:54.000
Weil also diese Frage in Bezug auf was diese Technologien und Technologiebündel, was die leisten können oder nicht,

48:54.000 --> 48:59.000
die ist natürlich auf der einen Seite wichtig zu verhandeln im Sinne des Demystifizierens.

48:59.000 --> 49:08.000
Auf der anderen Seite ist es ja auch so, dass da gerade unglaublich viele Ressourcen, Kreativität, Gestaltungskraft und so weiter,

49:08.000 --> 49:17.000
jetzt mal ein bisschen polemisch zugespitzt da hineinfließen, dass man Leute dazu bringt, auf irgendwelche idiotischen Online-Ads zu klicken.

49:17.000 --> 49:29.000
Also im Grunde gäbe es da ja auch noch eine Potentialität innerhalb alternativer Technologien und alternativer technologischer Infrastrukturen,

49:29.000 --> 49:39.000
die bis zu einem gewissen Grad innerhalb der jetzigen, auch politökonomischen Formatierung eigentlich nicht eingelöst werden kann.

49:41.000 --> 49:45.000
Wenn du in diese Richtung denkst, was zeigt sich dir dann?

49:47.000 --> 49:55.000
Das Wichtigste ist mir, dass ich nicht deterministisch bin, also auch auf Plattformen, die nach fürchterlichen Logiken funktionieren,

49:55.000 --> 50:01.000
auf denen es Hass gibt, die radikalisieren, gibt es trotzdem wunderbare, interessante, spannende Inhalte.

50:01.000 --> 50:07.000
Also das ist alles viel beweglicher, als es einem so scheint.

50:07.000 --> 50:19.000
Und ich arbeite ja im Thema Regulierung, aber weil ich schon glaube, dass, oder meine Erfahrung ist so ein bisschen das Einzige, was so funktioniert,

50:19.000 --> 50:30.000
wir haben es ja mit wirklich marktdominanten Firmen zu tun. Das ist ja keine normale Situation, in dem es hier gibt ein paar Optionen

50:30.000 --> 50:35.000
und wir können uns dafür entscheiden, welche wir am besten finden, sondern wir haben eine Realität geschaffen,

50:35.000 --> 50:41.000
in der, und das war wirklich diese Woche, ich habe so viel über Afghanistan nachgedacht, man muss sich wirklich fragen,

50:41.000 --> 50:46.000
was ist denn die Außenpolitik der Plattform, die die Taliban jetzt als Regierung anerkennen?

50:46.000 --> 50:53.000
Das zeigt nochmal, wie viel Macht diese Plattformen haben. Das ist ja absurd, dass wir überhaupt diese Frage stellen müssen,

50:53.000 --> 51:01.000
weil das das Medium ist, über das die ganze Welt Dinge erfährt. Und selbst wenn man eine Nachrichtenagentur ist,

51:01.000 --> 51:07.000
aber wenn man eine Nachrichtenseite hat, eine Zeitung, man ist abhängig vom Plattform zu den Lesern zu finden.

51:07.000 --> 51:14.000
Das ist schon nur unglaublich, also das ist völlig absurd. Also wir befinden uns nicht in so einem Markt, wo man sagen kann,

51:14.000 --> 51:20.000
das ist so, was könnten die Alternativen sein, sondern für mich ist schon der erste Punkt, es kann nicht sein,

51:20.000 --> 51:29.000
dass so viel Macht in der Hand ungewählter Firmen liegt, auch wenn die gar nicht nur schlecht sind und auch oft,

51:29.000 --> 51:38.000
das wird oft vergessen in Europa, dass es natürlich auch viele Länder gibt, in denen Social Media Firmen lange Zeit

51:38.000 --> 51:45.000
und auch immer noch Alternativen zu staatlicher Zensur sind. Also das ist komplexer, das sind nicht nur die Bösen,

51:45.000 --> 51:52.000
weil man will ja auch nicht, dass autoritäre Regime die Öffentlichkeit kontrollieren. Also es ist hochkomplex,

51:52.000 --> 51:58.000
aber so ganz grundsätzlich, wenn man nur, wenn ich jetzt einen Schritt zurückgehe und mir darüber nachdenken würde,

51:58.000 --> 52:04.000
in welcher Welt möchte ich leben? Ich möchte nicht in einer Welt leben, in der ich mich fragen muss, was Facebooks Außenpolitik ist

52:04.000 --> 52:14.000
und wie Facebook zur Taliban steht. Das ist einfach absurd. Und um zumindest dieser Macht etwas entgegenzusetzen,

52:14.000 --> 52:26.000
das Einzige, was funktioniert, sind börsennotierte Unternehmen, Regeln, Klagen, Regeln, Klagen und natürlich auch so ein bisschen

52:26.000 --> 52:36.000
veränderte Stimmung in der Gesellschaft, verändertes Nutzungsverhalten, aber ich glaube schon an Regulierung zu einem gewissen Grad.

52:36.000 --> 52:43.000
Das sind aber nicht, das sind nur die Spielregeln. Also Regulierung sind die Spielregeln der Digitalisierung.

52:43.000 --> 52:50.000
Und deswegen ist es unglaublich wichtig. Aber das ist nicht das Endziel, das ist nur so der erste Schritt, um überhaupt irgendwie

52:50.000 --> 52:56.000
die Möglichkeit für Alternativen zu schaffen. Weil wenn jedes Start-up davon träumt, von Google gekauft zu werden,

52:57.000 --> 53:00.000
dann werden wir auch keine Alternativen kriegen.

53:03.000 --> 53:04.000
Und von da aus?

53:04.000 --> 53:05.000
Wie bitte?

53:05.000 --> 53:16.000
Und von da aus? Also ich gehe mit, sozusagen kurzfristig, Regulierung sehr gerne und viel. Aber ich finde trotzdem noch die darüber hinausgehende Frage

53:16.000 --> 53:27.000
auch interessant. Also was für digitale Infrastrukturen bräuchten wir, jenseits der Frage, ob man jetzt irgendwie eine Marktkonkurrenz

53:27.000 --> 53:35.000
wieder herstellen kann, in Anführungsstrichen. Weil meine Vermutung wäre, dass auch das Herstellen einer mutmaßlichen Marktkonkurrenz

53:35.000 --> 53:44.000
trotzdem natürlich ja dann wieder Player zurücklassen würde, die am Ende über monetäre Incentivierung funktionieren

53:44.000 --> 53:56.000
und letztlich eigentlich auf einen Profitstreben hinaus ausgerichtet sind. Was ja wieder die Logik eigentlich gewaltsam sehr stark beschränkt oder begrenzt.

53:56.000 --> 54:08.000
Wir leben halt im Kapitalismus, ne? Wir leben halt im Kapitalismus. Also, for better or worse, da geht es dann um ganz Grundsätzliche.

54:08.000 --> 54:16.000
Grundsätzliche, klar. Sollte der Raum, und Raum ist ja schon schwierig. Was ist denn eigentlich, ich war so fasziniert von dem Vortrag, der eben war,

54:16.000 --> 54:25.000
wo es darum ging, da hatte eine Zuhörerin gefragt, ist denn WhatsApp Social Media? Das fand ich großartig, weil oft einfach diese Begriffe verendet werden.

54:25.000 --> 54:35.000
Was ist denn das überhaupt? Also was ist denn die Alternative, ich gehe so ein bisschen deiner Frage aus, aber über die Alternative von was sprechen wir denn?

54:35.000 --> 54:47.000
Also so Amazon, Facebook, das sind ja nicht Social Media Firmen. Das sind Technologiekonzerne, die Monopolien oder marktdominante Stellung in allen möglichen Bereichen haben.

54:47.000 --> 54:54.000
Und die öffentliche Diskussion, da geht es eigentlich immer fast nur um Social Media. Aber es ist ja völliger Quatsch, die ganze Infrastruktur dahinter,

54:54.000 --> 55:04.000
die Daten, die überall herkommen, die Datencenter, die Server, etc. Du merkst, ich bin so ein bisschen in Verlegenheit, weiß ich nicht, soll ich,

55:04.000 --> 55:09.000
es ist ja auch nicht mein, das ist was Kollektives. Wer bin ich, das ich jetzt sagen soll, wie das aussehen soll?

55:09.000 --> 55:16.000
Das Einzige, was ich sagen kann, ist, es gibt so ein paar Grundsätze und Prinzipien, die ich fände sinnvoll wären.

55:17.000 --> 55:27.000
Und ein Prinzip ist, es gibt eine Reihe von Fragen, die einfach, solange sie menschenrechtskonform sind, in unterschiedlichen Teilen der Welt eben anders beantwortet werden.

55:27.000 --> 55:40.000
Was finden wir akzeptabel, was empfinden wir als Hass, was sollte man nicht sagen dürfen, etc. Und das muss natürlich einer demokratischen Kontrolle unterliegen.

55:41.000 --> 55:54.000
Gut, ich lasse dich heraus. Aber am Ende, Frederike, stelle ich einer jeden und einem jeden immer noch die Frage, wenn du dir Zukunft vorstellst, was stimmt dich freudig?

55:54.000 --> 56:07.000
Ich muss wirklich sagen, diese Woche war ich, die ganzen 18 Monate waren wirklich bitter. Also die Pandemie war ganz schön hart, dann irgendwie auch noch politisch.

56:07.000 --> 56:18.000
Also wir leben schon in ziemlich harten Zeiten. Aber auf der anderen Seite merkt man dann halt auch, worum es geht und was wirklich wichtig ist.

56:18.000 --> 56:28.000
Ich bin einfach, das ist halt einfach nur meine Persönlichkeit, ich bin immer optimistisch, ich sehe immer die Möglichkeiten und was man noch ändern kann und was man machen kann.

56:28.000 --> 56:34.000
Sonst würde ich aber auch nicht zu dem Thema arbeiten, weil es wirklich zum Teil diffamierend ist.

56:34.000 --> 56:41.000
Also ich habe für eine Organisation gearbeitet, die Klagen eingereicht hat gegen Geheimdienste nach den Snowden-Enthüllungen.

56:41.000 --> 56:48.000
Und dann Jahre später haben sie Recht bekommen, aber währenddessen haben die Regierung einfach das Gesetz verändert.

56:48.000 --> 56:54.000
Also ja, es war illegal, aber jetzt ist es legal. Es ist schon so ein, ja, was gibt mir Hoffnung?

56:55.000 --> 57:02.000
Ich finde ganz aufregend die Fridays for Future Klimabewegung.

57:02.000 --> 57:07.000
Auch wenn es hier in unserem Gespräch geht es ja nicht um Klima, sondern um Technologien.

57:07.000 --> 57:15.000
Aber irgendwie habe ich das Gefühl, da kommt eine neue Generation, die viel fordernder ist, als ich das in dem Alter war.

57:15.000 --> 57:18.000
Und das gibt mir sehr viel Hoffnung.

57:19.000 --> 57:24.000
Und dann denke ich schon, dass sich in den letzten Jahren, es hat sich schon auch viel verbessert.

57:24.000 --> 57:29.000
Also es gibt viel mehr, wenn ich mit Politikern spreche, das Verständnis ist viel größer.

57:29.000 --> 57:32.000
Vor ein paar Jahren war das noch desaströs.

57:32.000 --> 57:36.000
Es gibt viel mehr Menschen in Entscheidungspositionen, die verstehen, worum es geht.

57:36.000 --> 57:43.000
Und es gibt auch ein größeres Verständnis dafür, dass der Status Quo so nicht richtig nachhaltig ist.

57:43.000 --> 57:47.000
Und das sind kleine Fortschritte und die geben mir Hoffnung.

57:47.000 --> 57:51.000
Wunderbar, dann sollten wir alle viel mehr fordern.

57:51.000 --> 57:54.000
Und ich danke dir, Friederike, für dieses Gespräch.

58:06.000 --> 58:09.000
Euch auch danke fürs Zuhören und Dasein.

58:09.000 --> 58:14.000
Das war Future Histories für heute.

58:14.000 --> 58:21.000
Vielen Dank fürs Zuhören, Show-Notizen und vieles mehr findet ihr auf www.futurehistories.today.

58:21.000 --> 58:27.000
Diskutiert mit auf Twitter unter dem Hashtag Future Histories oder im eigenen Subreddit.

58:27.000 --> 58:33.000
Ihr könnt Future Histories nicht nur auf allen großen Podcast-Plattformen hören und abonnieren,

58:33.000 --> 58:41.000
sondern auch auf YouTube, wo ihr neben den Episoden dann auch Kurzvideos zu Kernbegriffen einzelner Episoden findet.

58:41.000 --> 58:45.000
Schreibt mir gerne unter jan at futurehistories.today.

58:45.000 --> 58:50.000
Ich freue mich immer sehr über interessante Rückmeldungen und Hinweise.

58:50.000 --> 58:57.000
Wenn ihr Future Histories unterstützen wollt, dann könnt ihr das auf patreon.com-futurehistories

58:57.000 --> 59:00.000
oder auch via Spende auf unserer Homepage.

59:00.000 --> 59:06.000
Future Histories ist eine Produktion von MetaLapses, zu finden auf meta-lapses.net.

59:06.000 --> 59:09.000
Bis zum nächsten Mal. Ich freue mich.

