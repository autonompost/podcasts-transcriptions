1
00:00:00,000 --> 00:00:06,000
Herzlich willkommen bei Future Histories, dem Podcast zur Erweiterung unserer Vorstellung von Zukunft.

2
00:00:06,000 --> 00:00:12,000
Mein Name ist Jan Groß und ich freue mich sehr, heute Max Franz-Johann Schnettker begrüßen zu dürfen.

3
00:00:12,000 --> 00:00:20,000
Max ist Autor des Buches Transhumanistische Mythologien, in dem er sich kritisch mit bestimmten Strömungen des Transhumanismus auseinandersetzt.

4
00:00:20,000 --> 00:00:26,000
Vielen, vielen Dank an Fabian für deine Unterstützung und vielen Dank auch für den Hinweis zu Maxes Buch.

5
00:00:26,000 --> 00:00:29,000
Das war wirklich jetzt ein interessantes Interview und Gespräch.

6
00:00:29,000 --> 00:00:34,000
Denn auch für mich waren einige der Bezüge, die Max da herstellt, durchaus neu.

7
00:00:34,000 --> 00:00:45,000
Und als Max dann am Ende warnt, dass sich diese Formen des Denkens auch gerade anschicken, ganz konkret Nähe zu klassischen politischen Machtzentren zu suchen, da war ich erstmal ein bisschen erstaunt.

8
00:00:46,000 --> 00:01:07,000
Aber ein paar Tage nach unserem Interview hat mir Max dann einen Artikel geschickt über einen Skandal um einen Berater von Boris Johnson, der öffentlich für Eugenik eingetreten ist und zum Beispiel die universelle Einführung von Langzeitverhütungsmitteln zu Beginn der Pubertät vorgeschlagen hat, um eine, Zitat, dauerhafte Unterschicht zu verhindern.

9
00:01:07,000 --> 00:01:20,000
Das ist jetzt natürlich ein Extrembeispiel, muss man sagen, aber ganz im Sinne einer kritischen Analyse, auch von zukünftigen Herrschaftsverhältnissen, gilt es unbedingt, den transhumanistischen Diskurs kritisch zu befragen.

10
00:01:20,000 --> 00:01:23,000
Genau das machen wir im heutigen Gespräch.

11
00:01:23,000 --> 00:01:33,000
Wenn euch Future Histories gefällt, dann erzählt es doch bitte einem Freund oder einer Freundin, von der ihr glaubt, dass ihm oder ihr Future Histories vielleicht auch gefallen könnte.

12
00:01:33,000 --> 00:01:40,000
Vielen Dank und ich wünsche euch viel Spaß bei dieser Episode mit meinem Gast Max Franz-Johann Schnettger.

13
00:01:46,000 --> 00:01:48,000
Herzlich willkommen, Max.

14
00:01:48,000 --> 00:01:49,000
Hallo.

15
00:01:49,000 --> 00:01:59,000
Es gab schon einmal eine Folge Future Histories, in der der Transhumanismus zur Sprache kam, nämlich Folge 13 mit Julia Grillmeier zu Transhumanismus, Posthumanismus und Kompost.

16
00:01:59,000 --> 00:02:06,000
Nicht alle werden aber diese Folge, die übrigens sehr empfehlenswert ist, möchte ich da noch anmerken, gehört haben.

17
00:02:06,000 --> 00:02:09,000
Und so mag ich auch dich um eine Begriffsdefinition bitten.

18
00:02:09,000 --> 00:02:12,000
Was ist das? Was ist Transhumanismus?

19
00:02:12,000 --> 00:02:26,000
Ja, das auch ein ganz guter Einstieg, den ich vielleicht auch nutzen sollte, denn ich habe mir erlaubt, dadurch, dass ich mich kritisch mit dem Thema beschäftige, auch eine Begriffsdefinition vorzunehmen,

20
00:02:26,000 --> 00:02:30,000
die sich von der Eigenbezeichnung von Transhumanisten ein bisschen unterscheidet.

21
00:02:30,000 --> 00:02:45,000
Das ist ja vordergründig, sagt man, oder wenn man mit Transhumanisten redet, hört man oft, es ginge eigentlich nur um eine Verbesserung des Menschen durch Technologie und es könnte jede Form von Verbesserung käme da in Frage.

22
00:02:45,000 --> 00:02:58,000
Ich bin aber der Meinung, dass sich da durchaus ein sich in einer Institutionalisierung befindendes ideologisches Narrativ ausmachen lässt.

23
00:02:58,000 --> 00:03:12,000
Und dass es eben nicht nur um eine reine technologische Verbesserung des Menschen geht oder jede technologische Verbesserung des Menschen Transhumanismus wäre, sondern dass sich das recht genau umreißen lässt.

24
00:03:13,000 --> 00:03:33,000
Um ein Beispiel zu geben, was es nicht ist, ich hatte bei einem kürzlichen Vortrag, da war jemand, der leidet an Parkinson und er hat noch gehts, aber er hat als langfristige Therapieform in Aussicht, dass er einen Hirnschrittmacher kriegt.

25
00:03:33,000 --> 00:03:53,000
Da ist es dann ein kleiner Draht, der ins Gehirn implantiert wird, der an bestimmten Nervenzellen einen schwachen Strom abgibt, diese dadurch polarisiert und irgendwie, man weiß es nicht so genau, dafür sorgt, dass Parkinson Symptome unterdrückt werden.

26
00:03:53,000 --> 00:04:01,000
Und der sah sich quasi schon, fing dann halt, hatte für sich die Fragestellung, ja damit bin ich doch Transhuman, das ist es doch.

27
00:04:01,000 --> 00:04:17,000
Draht im Kopf und Implantat und das reguliert irgendwie, wo ich dann sagen würde, genau das ist tatsächlich nicht Transhuman, das ist zutiefst menschlich, weil er ausgeliefert bleibt an seinen Körper die Verhältnisse.

28
00:04:17,000 --> 00:04:34,000
Und wenn man sich ein bisschen mit transhumanistischer Literatur beschäftigt, läuft es immer darauf hinaus Kontrolle zu erlangen, über die Eventualitäten des Lebens, Kontrolle über den Körper, Kontrolle über Krankheit und am Ende auch über den Tod.

29
00:04:34,000 --> 00:04:46,000
Also es wäre zum Beispiel jemand, der auf medizinischen Wege eine neue Anwendung zur Bekämpfung einer Krankheit kriegt, den würde ich zum Beispiel nicht Transhuman nennen.

30
00:04:46,000 --> 00:04:55,000
Das heißt, wir haben festgestellt, was es nicht ist. Was wäre dann aus deiner Perspektivierung das, was es schon ist?

31
00:04:55,000 --> 00:05:22,000
Das, was es schon ist, das ist eine recht spezifische Ideologie, die also, der hat diverse Ursprünge, aber sie hat sich in den letzten 20 Jahren, würde ich sagen, gibt es einen Mainstream, der sehr viel zu tun hat mit dem Silicon Valley bzw. den dort neu entstandenen Geld- und Machteliten, die die Digitalisierung hervorgebracht hat.

32
00:05:22,000 --> 00:05:49,000
Die immer wieder in ein Projekt des Fortschritts, der da ja irgendwie so das Leitbild gibt, dann halt auch die Verbesserung des Menschen mit einbeziehen und schlussendlich auf eben die Überwindung von Leid und so weiter abzielen, aber in einer Art und Weise, die, wenn Sie Mensch sagen, eigentlich so eine ganz spezifisch bürgerliche Subjektvorstellung zugrunde legt.

33
00:05:49,000 --> 00:05:57,000
Und die quasi auf technologischen Säge sichern und verstetigen möchte.

34
00:05:57,000 --> 00:06:15,000
Und da das Ganze nicht so sehr eingebettet ist in geisteswissenschaftliche Diskussionen, sondern zumeist irgendwie aus dem Techsektor kommt, gruppiert sich der Transhumanismus dann auch gar nicht so sehr nach irgendwelchen spezifischen Anthropologien,

35
00:06:15,000 --> 00:06:23,000
sondern es geht dann, zum Beispiel gibt es da den Vorschlag einer Einteilung in Silizium-basierten, Kohlenstoff-basierten Transhumanismus.

36
00:06:23,000 --> 00:06:39,000
Der Silizium-basierte würde darum kreisen, Menschen elektronische Implantate einzubauen und schlussendlich ihren Geist in Computer hochzuladen und auf diesem Wege mit Computern zu verschmelzen.

37
00:06:39,000 --> 00:06:51,000
Der Kohlenstoff-basierte Transhumanismus kreist dann um Vorstellungen von genetischem Enhancement, genetischen Verbesserungen und der Überwindung des körperlichen Alterns.

38
00:06:51,000 --> 00:06:56,000
Und da gibt es schon so unterschiedliche Strömungen und Institutionen tatsächlich auch.

39
00:06:56,000 --> 00:07:11,000
Also in dem Kohlenstoff-basierten haben wir dann eher die Leute, die diese Bewegungen rund um MaxMor, die Wert darauf legen, sich einfrieren zu lassen, bis eben neue medizinische Technologien da sind,

40
00:07:11,000 --> 00:07:21,000
während wir in dem Silizium-basierten eben die Erlösungsvorstellungen rund um KI und so weiter haben, mit denen ich mich dann auch spezifisch beschäftigt habe.

41
00:07:22,000 --> 00:07:31,000
Das ist interessant, weil das war nämlich was, was in den Texten, die ich von dir jetzt gelesen habe, was ich mich da gefragt habe.

42
00:07:31,000 --> 00:07:41,000
Es schien mir da phasenweise so, als ob du den Transhumanismus fast schon gleichsetzen würdest mit einer bestimmten Richtung der KI-Forschung, der künstlichen Intelligenzforschung,

43
00:07:41,000 --> 00:07:49,000
die sich nämlich speziell mit den potentiellen existenziellen Risiken zukünftiger künstlicher Intelligenzen auseinandersetzt.

44
00:07:49,000 --> 00:08:03,000
Aber so, wie du das jetzt gerade aufgemacht hast, ist das quasi ja dann nur eine Richtung unter dem Überschirm des Transhumanismus und dann wiederum auch nur eine Unterrichtung unter eigentlich dem Sektor,

45
00:08:03,000 --> 00:08:07,000
den du jetzt Silikon-basiert genannt hast. Oder sehe ich das richtig?

46
00:08:07,000 --> 00:08:22,000
Genau. Das liegt einfach im Fokus meiner Arbeit. Ich habe nicht den Anspruch, mit meinem Buch eine Gesamtkritik jeder transhumanistischen Strömung vorgelegt zu haben,

47
00:08:22,000 --> 00:08:36,000
sondern ich habe mich spezifisch auf diese eingeschossen, allerdings auch unter der Prämisse, dass ich sie für dominierend halte oder zumindest das Potenzial dominierend für transhumanistische Diskurse,

48
00:08:36,000 --> 00:08:44,000
weil sie so stark verflochten ist mit gesellschaftlichen Geld- und Machtstrukturen.

49
00:08:44,000 --> 00:08:56,000
Also sprich, gerade bei einem Top-Hoster, der immer auftaucht, oder was ich vielleicht noch anmerken sollte, es gibt Autoren, die diese Strömung aus dem Transhumanismus ausklammern.

50
00:08:57,000 --> 00:09:14,000
Dem würde ich widersprechen. Durchaus auch diese Idee der Superintelligenz, respektive des parallelen Menschen in Computer hochladen, sehe ich in einer expliziten transhumanistischen Dynamik oder Teleologie verortet.

51
00:09:14,000 --> 00:09:37,000
Als es darum ging, woran ich arbeite, diese Form von Transhumanismus, die eine ist, also nicht die einzige, aber die, über die ich ein Buch geschrieben habe, hat Vertreter an Institutionen wie der Universität Oxford.

52
00:09:37,000 --> 00:09:51,000
Oxford hat Fans unter Milliardären wie Elon Musk und Peter Seale, die einschlägige Forschungsinstitute zum Thema mit Millionenbeträgen zum Teil fördern.

53
00:09:51,000 --> 00:09:59,000
Es gibt diesen Start-up-Incubator, der sich Singularity University nennt, wo die Singularität dann auch wieder im Namen steckt usw.

54
00:09:59,000 --> 00:10:17,000
Für meinen Unterfangen jetzt eine Kritik des Transhumanismus zu unternehmen, die auch jetzt nicht nur rein akademischer Selbstzweck sein soll, sondern sollte, sondern sich mit neu aufkommenden Herrschaftsideologien beschäftigt.

55
00:10:17,000 --> 00:10:31,000
Und das ist meine Stoßrichtung, dass ich diese Form von Transhumanismus zumindest potentiell für eine neu aufkommende Herrschaftsideologie halte, dadurch dann ja Fokussierung auf diese Form von Transhumanismus.

56
00:10:31,000 --> 00:10:36,000
Zeichnen wir das doch mal für die Zuhörerinnen und Zuhörer ein bisschen nach.

57
00:10:36,000 --> 00:10:45,000
Also du hattest schon angesprochen, einer der ganz prominenten Vertreter dieser spezifischen Richtung sitzt in Oxford.

58
00:10:45,000 --> 00:10:56,000
Ganz konkret ist das Nick Bostrom, der das dortige Future of Humanity-Institut gegründet hat und dort eben Forschung zu existenziellen Risiken betreibt.

59
00:10:56,000 --> 00:11:07,000
Er ist aber auch gleichzeitig, ich glaube mal Vorsitzender der World Transhumanist Association gewesen oder so, hat auf jeden Fall auch publiziert zum Thema Transhumanismus.

60
00:11:07,000 --> 00:11:17,000
Also in ihm als Person vereinen sich diese beiden Stoßrichtungen, die du da beschreibst, auf jeden Fall mal ganz konkret.

61
00:11:17,000 --> 00:11:22,000
Was sind denn seine grundlegenden Thesen?

62
00:11:22,000 --> 00:11:30,000
Seine grundlegenden Thesen, er ist sehr produktiv, das auf jeden Fall, das kann man auf gar keinen Fall in Abrede stellen,

63
00:11:30,000 --> 00:11:37,000
dass die Phase, in der er sich als Transhumanist bezeichnet hat, das war eher so vor 15 Jahren.

64
00:11:37,000 --> 00:11:50,000
Ich würde sagen, aber sein Denken hat mit diesem Transhumanismus nie gebrochen, sondern er macht ihn dann zur weiteren Grundlage seines Nachdenks über andere Themen.

65
00:11:50,000 --> 00:11:57,000
Das aktuelle Thema, zu dem er arbeitet, ist eben der Begriff des existenziellen Risikos.

66
00:11:57,000 --> 00:12:10,000
Das sollen menschheitsvernichtende Risiken sein und er nähert sich der Thematik dann an mit den Werkzeugen des Utilitarismus und der analytischen Philosophie

67
00:12:10,000 --> 00:12:22,000
und versucht da eine Philosophie zu entwickeln, die fast mathematischerweise, aber mit einer gewissen Genauigkeit, Risiken für die Menschheit abschätzt.

68
00:12:22,000 --> 00:12:30,000
Mit auch durchaus einem politischen Impetus, der dafür sorgen soll, dass wir jetzt mal etwas unternehmen müssen an bestimmten Stellen usw.

69
00:12:30,000 --> 00:12:38,000
Jeder Hörer wird jetzt natürlich denken, ja natürlich der Klimawandel, der ist es genau nicht tatsächlich,

70
00:12:38,000 --> 00:12:47,000
sondern das Risiko, das ihn am meisten umtreibt und das sein anderer großer Forschungsgegenstand ist, ist die Superintelligenz.

71
00:12:47,000 --> 00:13:01,000
Das soll salopp formuliert eine die Welt beherrschende künstliche Intelligenz sein und man nimmt so eine gewisse eindeutige Kontinuität von Fortschritt an,

72
00:13:01,000 --> 00:13:08,000
die das zu einer Zwangsläufigkeit macht, oder fast zu einer Zwangsläufigkeit, dass so etwas entsteht.

73
00:13:08,000 --> 00:13:20,000
Darum kreisen dann auch die meisten Abwägungen um das Thema existenzielle Risiken für die Menschheit.

74
00:13:20,000 --> 00:13:33,000
Und dann hat das Ganze noch einen ganz spannenden Dreh, der auch dann zum Thema des Podcasts passt, dass es nicht nur um die jetzige Menschheit geht,

75
00:13:33,000 --> 00:13:43,000
sondern auch immer um die potenziell in der Zukunft vorhandene Menschheit, der im Zweifelsfall auch, weil man annimmt,

76
00:13:43,000 --> 00:13:55,000
dass wenn man es richtig macht, die Bevölkerungszahlen explodieren und man ja auch Utilitarist ist, man also dann in der Zukunft eine größere Zahl von Menschen,

77
00:13:55,000 --> 00:14:06,000
die ein Wohl haben, die relevant sind für utilitaristische Erwägung und deren Glück und Wohl bestimmt werden kann, dass das sogar das größere Gewicht hat

78
00:14:06,000 --> 00:14:17,000
und dass wir unsere politischen Aktivitäten, unser Handeln hier und jetzt quasi an der Maximierung dieses zukünftigen Menschheitsglücks ausrichten müssen.

79
00:14:17,000 --> 00:14:25,000
Das fand ich interessant, dass das quasi gleichgestellt wird. Also um das nochmal vielleicht kurz zusammenzufassen, das heißt nämlich tatsächlich,

80
00:14:25,000 --> 00:14:33,000
dass sie quasi in dem Glauben, also zum einen es gibt ein Glauben, man könne das quasi berechnen, also das Glück und Wohl der Menschen,

81
00:14:33,000 --> 00:14:49,000
das ist ja eine Annahme, die dem Utilitarismus auch zugrunde liegt und dass dann aber in dieser Berechnung zukünftige Generationen Menschen mit hineingenommen werden,

82
00:14:49,000 --> 00:14:59,000
aber eben nicht in einer Logik, in der man jetzt zum Beispiel aus der Ökologieströmung kommend irgendwie sagen würde, okay, wir müssen jetzt, was weiß ich,

83
00:14:59,000 --> 00:15:10,000
ressourcenschonend vorgehen mit der Welt, weil wir müssen ja schauen, dass wir das Bestehende nicht für zukünftige Generationen zerstören,

84
00:15:10,000 --> 00:15:23,000
sondern das Kalkül ist quasi ein anderes, das ist tatsächlich so ein im Grunde sich wertfrei gebendes quantitatives Kalkulieren, oder?

85
00:15:23,000 --> 00:15:33,000
Ja, ziemlich genau, da hat er auch drüber promoviert quasi, dass es seine Doktorarbeit entworft, also so eine Aktualisierung,

86
00:15:33,000 --> 00:15:41,000
und die literistische Moralphilosophie soll das sein, er übernimmt eigentlich aus der Physik, unter anderem, also in verschiedenen Wissenschaften,

87
00:15:41,000 --> 00:15:54,000
nicht Physik, in der Kosmologie, gibt es das sogenannte anthropische Prinzip, das sich mit Fragestellungen greift, bei Fragestellungen der Eingerichtetheit des Universums,

88
00:15:54,000 --> 00:16:11,000
also warum sind die Grundgrößen des Universums, die wir beobachten so und nicht anders, und dann gibt es da diesen Punkt, dass es sehr viele physikalische Konstanten,

89
00:16:11,000 --> 00:16:20,000
wenn sie nur ein bisschen anders wären, aus verschiedenen Ebenen die Existenz menschlichen Lebens unmöglich machen würden,

90
00:16:21,000 --> 00:16:28,000
Temperatur, Bindungsenergie zwischen Teilchen, was weiß ich was, dann können sie keine komplexen Strukturen mehr formen, nur noch kein Leben oder so,

91
00:16:28,000 --> 00:16:35,000
oder Gravitation, plötzlich wären Himmelskörperdynamiken anders, ganz viel ihrer Kram, der vordergründig erst mal darauf drängt,

92
00:16:35,000 --> 00:16:46,000
dass man sich denkt, okay, oder wo so Argumente denkbar wären im Sinne von okay, es scheint so, als wäre das Universum feingetunt auf menschliches Leben,

93
00:16:46,000 --> 00:16:58,000
wo man dann mit einem anthropischen Prinzip antwortet, nee, ist es nicht, das ist ein statistisches Artefakt quasi, ein Universum, das menschliches Leben verunmöglicht,

94
00:16:58,000 --> 00:17:10,000
könnten wir ja gar nicht beobachten, wir wissen also, wir können davon da aus nicht schließen, ob das jetzt wahrscheinlich ist, dass Universen so aussehen oder nicht,

95
00:17:10,000 --> 00:17:20,000
sondern wir können ja, also unser Sample ist eingeschränkt auf die, die wir wahrnehmen können, ein ähnliches Denken überträgt dann eine utilitaristische Moralfilosophie und sagt,

96
00:17:20,000 --> 00:17:36,000
man solle in der utilitaristischen Philosophie also erst mal mitdenken, die eigene Situiertheit, und dann solle man, wenn man moralische Urteile fällt in diesem System,

97
00:17:36,000 --> 00:17:46,000
systemanalytische Philosophie bzw. sich dabei üblicherweise zum Utilitarismus bedienen, dann solle man so schließen, nicht als sei man man selber,

98
00:17:46,000 --> 00:17:56,000
sondern Teil einer zufälligen Referenzklasse von Beobachtern, und da bezieht er dann Zeitlichkeit mit ein.

99
00:17:56,000 --> 00:18:07,000
Wenn wir also ein moralisches Urteil fällen wollen, sollten wir nicht voraussetzen, dass Moral sich irgendwie am Jetzt an unserer gegebenen Situation zu orientieren hat,

100
00:18:07,000 --> 00:18:18,000
sondern wir sollten, wenn wir beurteilen, was richtig ist, dann sollten wir das so schließen, als seien wir zufällig über die Zeit gestreut,

101
00:18:18,000 --> 00:18:22,000
als seien wir zufällig über die sich irgendwo in der Zeit befindliche Beobachter.

102
00:18:22,000 --> 00:18:29,000
Da steckt natürlich dann die Vorstellung drin, dass man Vergangenheit und Zukunft auch irgendwie genau mathematisierbar erfassen kann,

103
00:18:29,000 --> 00:18:37,000
um dann solche Aussagen zu machen, was denn bzw. dann Schlussfolgerungen darüber zu treffen, was moralisch ist und was nicht.

104
00:18:37,000 --> 00:18:43,000
Und so kommt es dann auch, dass es eben nicht darum geht, irgendwie die Umwelt zu schützen oder einen Planeten zu erhalten oder so,

105
00:18:43,000 --> 00:18:47,000
denn es soll ja quantifizierbar sein, es sind verschiedene Zukunften möglich.

106
00:18:47,000 --> 00:19:02,000
Und ironischerweise ist in dem Aufsatz, indem man diese Idee des existenziellen Risikos entwickelt, die globale Dominanz einer ökologischen Bewegung ein existenzielles Risiko.

107
00:19:02,000 --> 00:19:09,000
Also das ist die Katastrophe, die wir verhindern müssen, weil das einzig moralische ist, auf gar keinen Fall auf die Bremse zu treten,

108
00:19:09,000 --> 00:19:20,000
den technischen Fortschritt so richtig durchzuballern, mehr Kapitalismus und so weiter, weil er davon ausgeht, dass sich dann zwangsläufig recht schnell dieser Zustand einstellen wird,

109
00:19:20,000 --> 00:19:29,000
dass erstens eine Superintelligenz entsteht, die eh alles regelt und dann haben wir auch keine ökologischen Probleme mehr und Menschen ihre Körper verlassen.

110
00:19:29,000 --> 00:19:38,000
Und das ist der Punkt, auf den wir raus müssen, die technologische Singularität, indem wir uns in Computer hochladen können

111
00:19:38,000 --> 00:19:47,000
und dann verändern sich ja plötzlich die Ressourcen. Also wenn Menschen Computerprogramme sind, dann kann man viel mehr Menschen haben und man kann auch viel besser regulieren, wie es denen geht.

112
00:19:47,000 --> 00:20:01,000
Und er kommt dann mit dieser auf den ersten Blick erstmal so, also wirkt jetzt nicht, erstmal zumindest schlüssigen Idee, dass man gucken sollte,

113
00:20:01,000 --> 00:20:10,000
dass man die Zukunft mitbedenkt in seinen moralischen Handlungen, dann eben immer auf dieses sehr spezifisch transhumanistische Ergebnis.

114
00:20:10,000 --> 00:20:24,000
Und das würde ich sagen, ist etwas, das sich durchzieht durch seine aktuelle Philosophie, dass er verschiedene, also durchaus erstmal zu griffigen Themen sehr einleuchtende Überlegungen erst mal anstellt,

115
00:20:24,000 --> 00:20:30,000
die aber immer so konstruiert sind, dass man am Ende beim Transhumanismus landet.

116
00:20:30,000 --> 00:20:43,000
Und ich glaube, was vielleicht wichtig ist herauszustellen ist, das funktioniert ja nur, wenn man eine bestimmte zu maximierende Zielgröße annimmt.

117
00:20:43,000 --> 00:20:59,000
Also es muss ja, es wird dafür implizit etwas vorausgesetzt, was es zu maximieren gilt. Also das, was quasi als positiv angenommen wird, ist da schon eingeschrieben.

118
00:20:59,000 --> 00:21:18,000
Wie würdest du das definieren? Also ist das, woran ist das orientiert? Ist das immer so ein zirkrationales, vielleicht auch an so Anthropologien wie dem Homo economicus orientiertes eigentlich Optimierungs- und Profitstreben?

119
00:21:18,000 --> 00:21:26,000
Also was ist das, was es quasi zu mehrern gilt und woran wird es bemessen?

120
00:21:26,000 --> 00:21:41,000
Ja, da ist ganz spannend, dass das gar nicht so wirklich diskutiert wird, sondern bei Bostrom, aber auch bei anderen Autoren, die sich in diesem Superintelligenztranshumanismus bewegen,

121
00:21:41,000 --> 00:21:50,000
ist immer schon vorausgesetzt, dass so ein spezifischer Utilitarismus halt einfach die richtige Moralfilosophie ist.

122
00:21:50,000 --> 00:22:01,000
So, also da ist das geklärt. Sie nennen es Glück oder sie nennen es Nützlichkeit. Utility ist dann immer der Begriff.

123
00:22:01,000 --> 00:22:12,000
Das aber in der Geschichte des Utilitarismus immer so, eigentlich ist das eine Debatte. Also was soll es denn jetzt überhaupt sein? Die kann ich auch nicht beantworten, weil ich kein Utilitarist bin.

124
00:22:12,000 --> 00:22:23,000
Wenn man zurückgeht bis Jeremy Bentham, bei dem gibt es Stellen, wo der dann irgendwann sagen muss, ja, also eigentlich ist es Geld. Also Glück kann man ja wohl in Geld bemessen.

125
00:22:23,000 --> 00:22:33,000
Und da haben wir, weil sehr viele Leute dann mit der KI-Forschung auch zu tun haben, die irgendwie sich als Teil dieser Bewegung sehen.

126
00:22:33,000 --> 00:22:42,000
Und da haben wir ja tatsächlich auch zur Steuerung von KI-Systemen diese Idee der Utility Function.

127
00:22:42,000 --> 00:22:52,000
Also, dass man das definiert in gewisses Maße, numerisch bestimmbare Nützlichkeit, damit das System sich ausrichten kann.

128
00:22:52,000 --> 00:22:59,000
Und dann können wir damit durchaus auch Systeme bauen, die sich irgendwie in der Welt orientieren können.

129
00:22:59,000 --> 00:23:04,000
Wir haben dann aber das Ding, das setzt eigentlich immer am Ende gelabelte Daten quasi voraus.

130
00:23:04,000 --> 00:23:14,000
Die philosophische Frage, was Utility ist, habe ich zumindest in dem, was ich bis jetzt vom Bostrom gelesen habe, nicht wirklich detailliert beantwortet gesehen.

131
00:23:14,000 --> 00:23:23,000
Und auch bei anderen Transhumanisten nicht, die sich aber dieses Weltbild, das eigentlich darauf rekurriert, alle einkaufen.

132
00:23:23,000 --> 00:23:29,000
Und das macht das Ganze auch so ein bisschen fragil, sage ich jetzt mal.

133
00:23:29,000 --> 00:23:35,000
Also sobald man, weswegen ich dann auch darauf kam, mich damit so kritisch zu beschäftigen.

134
00:23:35,000 --> 00:23:44,000
Es wird da sehr viel als selbstverständlich vorausgesetzt, dass sobald man nicht in so einer spezifischen,

135
00:23:44,000 --> 00:23:54,000
Babel aus analytischer Philosophie und angelehnten KI-Sachen sich bewegt, eigentlich gar nicht mehr so einleuchtend wird.

136
00:23:54,000 --> 00:24:03,000
Wir kommen sicher noch mal zu artverwandten Fragestellungen zurück, wie das, worüber du gerade gesprochen hast, nämlich,

137
00:24:03,000 --> 00:24:06,000
auch dann in Bezug auf die Frage der Ideengeschichte.

138
00:24:06,000 --> 00:24:11,000
Aber da lassen sich ja vielleicht dann manche Sachen noch anders vororten.

139
00:24:11,000 --> 00:24:20,000
Aber bevor wir das machen, vielleicht noch mal kurz zur Orientierung dieser Ablauf, der da auch gedacht wird.

140
00:24:20,000 --> 00:24:23,000
Nur, dass man sich das auch vorstellen kann als Hörer und als Hörer.

141
00:24:23,000 --> 00:24:28,000
Der Gedanke ist, es gibt eine exponentielle technische Entwicklung.

142
00:24:28,000 --> 00:24:35,000
Das heißt, es gibt einfach einen wirklich rapiden Anstieg an dem technologisch möglichen.

143
00:24:35,000 --> 00:24:42,000
Und annehmend, dass sich das weiter so entwickeln wird, geht man davon aus,

144
00:24:42,000 --> 00:24:48,000
dass irgendjemand, der sich so einen technologischen Anstieg anstellt,

145
00:24:48,000 --> 00:24:59,000
dass sich das weiter so entwickeln wird, geht man davon aus, dass irgendwann die Rechenleistung so groß sein wird,

146
00:24:59,000 --> 00:25:08,000
dass es zu einem Punkt gibt, wo eine künstliche Intelligenz eigentlich die eigene Leistung auf sich selbst anwenden kann.

147
00:25:08,000 --> 00:25:15,000
Und sobald dieser Punkt erreicht ist, kommt es dann zu einer Art Intelligenz-Explosion, oder?

148
00:25:15,000 --> 00:25:21,000
Vielleicht kannst du das noch mal kurz, diesen Gedanken gann, wie es zu dieser Singularität ja dann auch kommt.

149
00:25:21,000 --> 00:25:23,000
Vielleicht kannst du das mal kurz noch mal beschreiben.

150
00:25:23,000 --> 00:25:29,000
Was ist Singularität und wie wird das konzeptionalisiert, dass die entstehen kann?

151
00:25:29,000 --> 00:25:33,000
Genau, also es sind zwei Topoi, die sich sehr eng aufeinander beziehen.

152
00:25:33,000 --> 00:25:36,000
Das eine ist die Singularität.

153
00:25:36,000 --> 00:25:50,000
Die technologische Singularität kommt eigentlich ursprünglich mal bestimmt in einem Aufsatz von Werner Winsch aus dem Jahr 1993.

154
00:25:50,000 --> 00:25:56,000
Wurde dann in ähnlicher Weise aufgegriffen von Ray Kurzweil, der heute Chefentwickler bei Google ist

155
00:25:56,000 --> 00:26:02,000
und einige Bücher geschrieben hat wie The Age of Spiritual Machines,

156
00:26:02,000 --> 00:26:16,000
wo er quasi utopisch, quasi eschatologisch vorhersagt, was für eine Wunderwelt wir eintreten werden,

157
00:26:16,000 --> 00:26:18,000
wenn erst die technologische Singularität da ist.

158
00:26:18,000 --> 00:26:23,000
Die wird immer von Autor zu Autor ein bisschen unterschiedlich beschrieben.

159
00:26:23,000 --> 00:26:28,000
Meist als eben ein Punkt, an dem der technische Fortschritt so groß ist, dass man es gar nicht mehr vorhersagen kann.

160
00:26:28,000 --> 00:26:34,000
Und dass es selbstverstärkende Systeme sind, dass es immer weiter beschleunigt wird.

161
00:26:34,000 --> 00:26:45,000
Und dass wir dann einen Punkt kriegen, wo die materielle Welt durch die Macht der Technik fundamental transformiert wird und wir auch.

162
00:26:45,000 --> 00:26:55,000
Und zwar, da geht es dann um so Ideen wie Nanotechnologien, dass man da hinkommt, Materie auf Atomara-Ebene beliebig gestalten zu können.

163
00:26:55,000 --> 00:27:05,000
Dass man diverse Energieprobleme einfach löst und natürlich, dass man den menschlichen Geist auch vollständig versteht,

164
00:27:05,000 --> 00:27:10,000
wie er vom Gehirn instanziert wird und auf dem Computer reproduzieren kann.

165
00:27:10,000 --> 00:27:18,000
Und praktisch ist es das technologische Himmelreich, deswegen wird es ironisch auch manchmal Rapture of the Nerds genannt.

166
00:27:18,000 --> 00:27:26,000
Also dass wir an den Punkt kommen, einer technologischen Allmacht, die uns, wenn sie gut verläuft, in ein technologisches Himmelreich überführt.

167
00:27:26,000 --> 00:27:39,000
Damit zusammenhängt dann bei denjenigen, die so eine Singularität am ehesten realisiert durch eine Superintelligenz vermuten, die Idee der Intelligenz-Explosion.

168
00:27:40,000 --> 00:27:54,000
Das ist eine Voraussage über die Zukunft der Entwicklung künstlicher Intelligenz, die davon ausgeht, dass wir da einen plötzlichen Bruch erleben werden.

169
00:27:54,000 --> 00:28:02,000
Nämlich in dem Moment, wenn es uns erst einmal gelingt, eine künstliche Intelligenz zu entwickeln.

170
00:28:02,000 --> 00:28:12,000
Also man teilt da ein in schwache oder starke künstliche Intelligenz bzw. in spezialisierte und generelle künstliche Intelligenz.

171
00:28:12,000 --> 00:28:21,000
Die Schwache bzw. Spezielle ist das, was wir so aus dem Alltag kennen, das den ganzen System zugrunde liegt, die wir auch alltäglich benutzen.

172
00:28:21,000 --> 00:28:34,000
Das sind im Endeffekt statistische Verfahren, die auf spezifische, also z.B. dieses Machine Learning, dass man auf spezifische Sachverhalte trainiert.

173
00:28:34,000 --> 00:28:39,000
Und das aber eben nur darauf anwendbar ist und nicht universalisierbar ist.

174
00:28:39,000 --> 00:28:47,000
Während die generelle Intelligenz dann die menschengleiche sein soll, also die Kontext unabhängig agieren kann.

175
00:28:47,000 --> 00:28:55,000
Da gibt es meines Wissens nach eigentlich keinen realistischen Ansatz, der irgendwie beschreibt, wie das realisierbar sein soll im Moment.

176
00:28:55,000 --> 00:29:01,000
Das hält aber nicht davon ab, schon mal zu spekulieren, wie das sein wird, wenn wir das haben.

177
00:29:01,000 --> 00:29:08,000
Und da ist die Idee, dass die generelle künstliche Intelligenz für sich selbst fungibel bleiben wird.

178
00:29:08,000 --> 00:29:16,000
Also wir als Menschen haben ja irgendwie das Problem, dass wir uns nicht nur auf sehr aufwendigen Wegen irgendwie selber verbessern können.

179
00:29:16,000 --> 00:29:21,000
Also wir müssen irgendwie jahrzehntelang in die Schule gehen oder irgendwie Sport treiben.

180
00:29:21,000 --> 00:29:27,000
Also können wir uns schon in einer bestimmten Weise entwickeln und das auch mit Vorsatz.

181
00:29:27,000 --> 00:29:33,000
Aber wir sind ja am Ende, unsere Körper und Grundlagen und das seien zu uns selber nicht fungibel.

182
00:29:34,000 --> 00:29:44,000
Und diese künstliche generelle Intelligenz, die soll einerseits die Fähigkeiten eines Menschen haben, aber sie sei ja irgendwie programmiert, sie sei ja Software.

183
00:29:44,000 --> 00:29:53,000
Und wenn wir jetzt eine künstliche Intelligenz hätten, die etwas klüger ist als ein Mensch, dann wäre sie ja auch besser darin, künstliche Intelligenzen zu programmieren als ein Mensch.

184
00:29:53,000 --> 00:29:59,000
Und wenn man dann annimmt, dass sie vollständig für sich selber fungibel ist, dann kann sie sich ein bisschen besser programmieren.

185
00:29:59,000 --> 00:30:03,000
Oder meinetwegen auch ein anderes paralleles System.

186
00:30:03,000 --> 00:30:08,000
Und diese ganz kleine Verbesserung wäre dann ja auch wieder der etwas bessere Programmierer.

187
00:30:08,000 --> 00:30:12,000
Also können sie sich diesen schritten und dann kriegt man plötzlich so einen Ex.

188
00:30:12,000 --> 00:30:23,000
Soll man an dem Punkt, wo diese Grundlage erreicht ist, dass es etwas klüger ist als ein Mensch, etwas besser KI programmieren kann als ein Mensch, soll der Take-off erreicht sein.

189
00:30:23,000 --> 00:30:40,000
Und ab dann kam es zu einem rapiden Umschwung, in dem recht plötzlich ein System dann um ein Vielfaches intelligenter wird als die versammelte Menschheit und uns quasi überwältigen kann.

190
00:30:40,000 --> 00:30:53,000
Und das ist dann auch eines der zentralen existenziellen Risiken, um die es in der existential risk Philosophie und die angeschlossenen politischen Bewegungen geht.

191
00:30:53,000 --> 00:30:58,000
Eben diese Idee von, wir müssen jetzt schon mal aufpassen, wir müssen uns jetzt mit der Frage befassen.

192
00:30:58,000 --> 00:31:03,000
Also es kommt auf jeden Fall, es sei denn die Ökos gewinnen oder es gibt einen Atomkrieg, aber sonst kommt es auf jeden Fall.

193
00:31:03,000 --> 00:31:23,000
Und wir müssen jetzt schon mal dafür sorgen, dass diese Wesenheit, die da entstehen wird, uns irgendwie wohlgesunden ist und Rücksicht auf uns nimmt und es nicht irgendwie behandelt, wie wir das mit einem Ameisenhaufen machen, wenn wir irgendwo einen Parkplatz bauen wollen.

194
00:31:23,000 --> 00:31:42,000
Genau. Und da haben wir dann auch wieder so einen Punkt, dass in diesem Utilitarismus dann plötzlich wieder so eine, diese Sache ist das maximale Risiko, aber auch das maximale Potenzial, was wenn man dann utilitaristisch rangeht, rankommt, ja gut, dann machen wir uns unsere Berechnungen kommen.

195
00:31:42,000 --> 00:31:51,000
Die Sache, mit der wir uns jetzt hauptsächlich beschäftigen sollten, ist die Perspektive auf diese künstliche Intelligenz und das mit dem Klimawandel oder globaler Armut oder so, das ist ja jetzt alles nicht mehr so dringend.

196
00:31:51,000 --> 00:32:05,000
Und was sind dann die Methoden, die vorgeschlagen werden, um sich diesem diagnostizierten Problem zu widmen? Also was, was werden da für Vorkehrungen getroffen?

197
00:32:05,000 --> 00:32:29,000
Da gibt es einerseits, gibt es das Machine Intelligence Research Institute in Berkeley, das in Berkeley ist, aber nichts mit der Uni zu tun hat, das versucht, ja am Ende eigentlich Handlungstheorien zu mathematisieren.

198
00:32:29,000 --> 00:32:50,000
Eine andere Idee wäre, dass man das nicht von vornherein festlegt, weil wir ja auch das Problem haben, dass wir ja vielleicht gar nicht in unserem, also dass eine der Grundtorpoi, die da wieder vorausgesetzt werden, ist, dass wir ja irgendwie defizitär sind.

199
00:32:50,000 --> 00:33:01,000
Wir sind zu dumm, zu gemein, zu asozial und das ist ja gerade eine der Sachen, die Superintelligenz für uns regelt, wir sind ja eigentlich unmündig.

200
00:33:01,000 --> 00:33:20,000
Und dann haben wir die Frage, könnten wir denn jetzt schon vielleicht das vorherbestimmen? Und dann gibt es die Idee der Extrapolated Volition, die dann darin besteht, dass man sagt, dass wir der Superintelligenz Möglichkeiten an die Hand geben sollten,

201
00:33:20,000 --> 00:33:33,000
die Sachen für uns so zu entscheiden, wie wir es wollen und wünschen würden, wenn wir eben weiterentwickelt weiser und sozialer wären.

202
00:33:33,000 --> 00:33:47,000
Die konkreten Überlegungen dazu, also es ist jetzt nicht so, dass Technologien in der Schublade liegen, denn dann kommen wir dann auf die ganz, ganz basale oder die sehr, sehr profane Ebene des Ganzen.

203
00:33:47,000 --> 00:33:55,000
Das, was man jetzt als erstes mal tun kann, ist dem Machine Intelligence Institute Geld spenden.

204
00:33:55,000 --> 00:34:06,000
Da kommen wir dann auf die Verknüpfung mit der Bewegung für effektiven Altruismus, in der das alles eine Rolle spielt, die man vielleicht da höre oder auch kennt, weil die ja global an Universitäten präsent ist.

205
00:34:06,000 --> 00:34:14,000
Die besteht jetzt nicht nur aus Superintelligenz, Transhumanisten ist aber ursprünglich aus diesem Milieu entstanden.

206
00:34:14,000 --> 00:34:21,000
Und die spielen da schon eine Rolle. Und dann haben wir dann diese Debatte, was ist denn jetzt der effektivste Altruismus?

207
00:34:21,000 --> 00:34:29,000
Und dann dieses Argument von, das Maximum an altruistischem Handeln erreichen wir, wenn wir die positive Superintelligenz verwirklichen.

208
00:34:30,000 --> 00:34:37,000
Wenn wir jetzt also vor der Frage stehen, ich habe hier irgendwie keine Ahnung, 500 Euro rumliegen, was mache ich damit?

209
00:34:37,000 --> 00:34:44,000
Ich will was Gutes tun, ich kann die Oxfam geben oder irgendwer, der Brunnen in Afrika baut oder ich gebe dir den Miri.

210
00:34:44,000 --> 00:34:54,000
Und das Miri sorgt dann dafür, dass die Superintelligenz positiv ausfällt auf noch nicht geklärten Wegen, weil das ist ja auch ein schwieriges Problem so.

211
00:34:55,000 --> 00:35:05,000
Sprache, Begriffe und Ähnliches in mathematisierter Form darzustellen, da würde ich auch sagen, das ist so ein Grundproblem, das geht halt einfach nicht.

212
00:35:05,000 --> 00:35:13,000
Das ist die ganze Zeit die Schwierigkeit. Deswegen ist das, was man jetzt erstmal tun könnte, wäre, diesen Organisationen zu spenden.

213
00:35:13,000 --> 00:35:19,000
Da kommen wir dann auf sehr, sehr frappante Ähnlichkeiten zu religiösen Organisationen.

214
00:35:19,000 --> 00:35:38,000
Das ist dann überhaupt was, was du ja auch behandelt, nämlich die Frage, inwiefern eben es sich da, es steckt im Titel deines Buches schon drin, um Formen der Mythologie handelt.

215
00:35:38,000 --> 00:35:46,000
In Anlehnung an einen Journalisten, Mark O'Connell heißt er, fällt da dann auch der Ausdruck, materialistischer Mystizismus.

216
00:35:46,000 --> 00:35:54,000
Wie macht sich das bemerkbar? Wo sind da die Anknüpfungspunkte zu mystischen Narrativen?

217
00:35:54,000 --> 00:36:06,000
Ich habe mich ein bisschen von Walter Benjamin inspirieren lassen, eben der Idee, dass in der Säkularisierung da durchaus theologische Potenziale noch aufgehoben sind.

218
00:36:06,000 --> 00:36:18,000
Und da würde ich sagen, dass dieser Transhumanismus mit dieser Kontrollidee, dieser Idee, das materielle Universum am Ende vollständig umzugestalten und fungibel zu machen,

219
00:36:18,000 --> 00:36:25,000
weil darauf läuft es am Ende raus, dass Energie und Materie fungibel werden wie Computerprogramme.

220
00:36:25,000 --> 00:36:30,000
Erkläre das nochmal kurz, weil ich glaube, das ist den Hörerinnen und Hörern wahrscheinlich nicht so geläufig.

221
00:36:30,000 --> 00:36:40,000
Mit fungibel meinst du ja, dass es quasi eine Austauschbarkeit gibt oder eine Kompatibilität auch unter den Dingen letztlich.

222
00:36:40,000 --> 00:36:42,000
Dass das eine in das andere transformierbar sein.

223
00:36:42,000 --> 00:36:52,000
Dass das eine in das andere transformierbar sein, dass es sich beliebigen Zugriffen eben, wenn ich das eine beherrsche, kann ich beliebig auf das andere zugreifen.

224
00:36:52,000 --> 00:36:58,000
Also wenn ich irgendwie einen Computer programmieren kann, kann ich quasi das Universum programmieren und vor allem eine vollständige Zugreifbarkeit.

225
00:36:58,000 --> 00:37:07,000
Wo war denn der Frage dazu?

226
00:37:07,000 --> 00:37:11,000
Inwiefern das quasi mit einem Mystizismus zusammenhängt.

227
00:37:11,000 --> 00:37:13,000
Entschuldige, dass ich dich da unterbrochen habe.

228
00:37:13,000 --> 00:37:16,000
Inwiefern das mit einem Mystizismus zusammenhängt.

229
00:37:16,000 --> 00:37:24,000
Die Idee mit dem materialistischen Mystizismus von Marco Connell, das bezieht sich, der hat Interviews geführt mit verschiedenen Leuten,

230
00:37:24,000 --> 00:37:28,000
die in verschiedenen Ecken der transhumanistischen Bewegung unterwegs sind.

231
00:37:28,000 --> 00:37:34,000
Also auch nicht nur mit den KI-Superintelligenz Leuten, mit denen ich mich da beschäftigt habe.

232
00:37:34,000 --> 00:37:45,000
Der hat benutzt das als Beschreibung für diese Idee des Geistes in Computer-Hochladens.

233
00:37:45,000 --> 00:37:56,000
Der vordergründig radikal-materialistisch daherkommt, also etwas wie Seele oder es gibt keine Essenz oder irgendwie das Menschen, die über natürlich ist.

234
00:37:56,000 --> 00:38:04,000
Dann aber auch annimmt, dass sie dann ja auch vollständig manipulierbar und in Daten erfassbar ist.

235
00:38:04,000 --> 00:38:12,000
Und dass man quasi, da wird eigentlich eine ganz saloppe Analogie, liegt dem zu Grunde.

236
00:38:12,000 --> 00:38:15,000
Also das Gehirn ist ein Computer und Geist ist ein Programm.

237
00:38:15,000 --> 00:38:25,000
Und so wie ich, keine Ahnung, mein altes Super-Nintendo-Spiel auf meinem PC jetzt irgendwie emulieren kann,

238
00:38:25,000 --> 00:38:36,000
indem ich quasi auf dem besseren Gerät eine Umgebung und Software vortäusche, die der Hardware des älteren Geräts entspricht,

239
00:38:36,000 --> 00:38:38,000
kann ich das ja dann auch mit Geist machen.

240
00:38:38,000 --> 00:38:46,000
Wenn wir erstmal richtig leistungsfähige Computer haben, bauen wir einfach Gehirne auf neuronaler Ebene in Code nach.

241
00:38:46,000 --> 00:38:49,000
Und dann können wir Menschen in Computer hochladen.

242
00:38:49,000 --> 00:39:00,000
Und das nennt er materialistischen Mystizismus, weil es eigentlich unglaublich eng an Vorstellungen von Seelen, Wanderungen und Ähnlichem liegt,

243
00:39:00,000 --> 00:39:05,000
nur zumindest vordergründig radikal materialistisch daherkommt.

244
00:39:05,000 --> 00:39:16,000
Zu den grundsätzlicheren religiösen Bezügen, die ich da sehe, da ist so ein grundsätzlicher Punkt eben der Annahme,

245
00:39:16,000 --> 00:39:26,000
dass die moderne Naturwissenschaft geistesgeschichtlich christliches Denken zur Voraussetzung hat.

246
00:39:26,000 --> 00:39:38,000
Zum Beispiel Klaus-Peter Ortlieb ist ein ehemals Leiter des Instituts für Mathematische Modellierung an der Uni Hamburg,

247
00:39:38,000 --> 00:39:42,000
hat das dann in einigen Aufsätzen sehr schön nachgezeichnet, aber auch andere Autoren,

248
00:39:42,000 --> 00:39:49,000
haben eben diese Art des Denkens über die Welt, die die Naturwissenschaft zur Voraussetzung hat,

249
00:39:49,000 --> 00:39:59,000
die eben nicht nur nach dem unmittelbar Gegebenen fragt, sondern nach dem dahinterliegenden Prinzip, das mathematisierbar ist,

250
00:39:59,000 --> 00:40:09,000
hat zur Voraussetzung eigentlich ein Denken, das eben Konstrukteur annimmt, das so eine Gemachtheit der Welt annimmt,

251
00:40:09,000 --> 00:40:14,000
die nach bestimmten Prinzipien funktioniert, die erkennbar sind.

252
00:40:14,000 --> 00:40:22,000
Das ist jetzt keine Fundamentalkritik an den Naturwissenschaften oder sonst irgendwas, die finde ich sehr gut,

253
00:40:22,000 --> 00:40:32,000
nur es gibt diese Voraussetzung und das, was einem da begegnet, glaube ich, in diesen Sachen,

254
00:40:32,000 --> 00:40:36,000
die rund um Superintelligenz und so weiter kreisen, ist eigentlich eine Aktualisierung dessen,

255
00:40:36,000 --> 00:40:46,000
dass wir dieses Denken haben, dass sich um Konstruiertheiten und so weiter, dass er implizit irgendwie doch so eine Gottesfigur drin hat

256
00:40:46,000 --> 00:40:52,000
und die dann aber daraus wieder konstruiert wird und dann landen wir bei diesen Superintelligenz-Sachen.

257
00:40:52,000 --> 00:41:00,000
Noch mal sehr viel konkreter dann, wenn wir dann noch mal hinkommen, dass das alles ja auch noch utilitaristisch ist.

258
00:41:00,000 --> 00:41:13,000
Im Utilitarismus, der kommt aus einer spezifisch christlich-kalvinistischen Tradition und hat aus dieser auch durchaus seine Sinnbestimmung,

259
00:41:13,000 --> 00:41:20,000
was ein Grund ist, warum ich annehmen würde, dass sich die ganze Sache auch außerhalb des englischsprachigen Raums nie so durchsetzen wird,

260
00:41:21,000 --> 00:41:32,000
weil da so gewisse, ich würde es jetzt mal kulturell nicht ganz nennen, aber gewisse geistesgeschichtliche Voraussetzungen drin sind,

261
00:41:32,000 --> 00:41:36,000
die man eher mitgekriegt hat, wenn man in England oder in den USA aufgewachsen ist.

262
00:41:36,000 --> 00:41:42,000
Und das Ganze, um auf Marco Connell zurückzukommen, findet man dann auch tatsächlich noch mal in der Person,

263
00:41:43,000 --> 00:41:56,000
wenn er dann Menschen beschreibt, die in evangelikalen christlichen Haushalten aufgewachsen sind, da dann irgendwie eine Glaubenskrise hatten

264
00:41:56,000 --> 00:42:07,000
und jetzt eben zutiefst davon überzeugt sind, dass sie Atheisten sind, weil sie nicht glauben, dass die Erde 5000 Jahre alt ist

265
00:42:07,000 --> 00:42:21,000
und das Noah eine Arche gebaut hat, die aber immer noch völlig in diesem Denken einer am Ende regulierten Welt drin stecken.

266
00:42:21,000 --> 00:42:31,000
Und die füllen dann quasi diese Lücke mit und die halt dann auch Sterblichkeit nicht konfrontieren, sich damit arrangieren,

267
00:42:31,000 --> 00:42:37,000
was ja auch irgendwie ein Prozess ist, den man als Atheist vielleicht auch irgendwie durchmacht, mit Sterblichkeit überhaupt nicht zurechtkommen,

268
00:42:37,000 --> 00:42:44,000
allerdings eben dieses Tragegerüst des festen Glaubens verlieren und dann eben daran kommen, okay, da muss ich das jetzt aber technologisch lösen

269
00:42:44,000 --> 00:42:56,000
und dann auch sehr viel mehr bereit sind, an Narrative zu glauben, die technologisch daherkommen, aber andererseits auch diese Ängste bedienen.

270
00:42:56,000 --> 00:43:02,000
Also das ist ja ein ganz, ganz zentrales Ding beim Transformatismus, dass man nicht mehr sterben muss, darum geht es auch immer.

271
00:43:02,000 --> 00:43:08,000
Da geht es auch bei den anderen Transformatismen rum und also bei dem, wo es sich einfrieren lässt beispielsweise

272
00:43:08,000 --> 00:43:15,000
und in diesem Superintelligenz-KI-Transformatismus dann eben diese Idee von, okay, wenn ich erstmal meinen Körper los bin, der zerfällt,

273
00:43:15,000 --> 00:43:25,000
der ist schmuddelig, der kriegt Krankheiten, wenn ich erstmal ein Computerprogramm bin, dann habe ich Sterblichkeit zumindest vordergründig überwunden.

274
00:43:25,000 --> 00:43:31,000
Und das sind für mich mythologische und zum Teil mystizistische Elemente.

275
00:43:31,000 --> 00:43:43,000
Ja, also das eine hattest du jetzt gerade auch dann schon konkret eben mit dem Mind Uploading, der Whole Brain Emulation, wie es ja dann auch heißt, benannt.

276
00:43:43,000 --> 00:43:54,000
Da wittert man quasi die Unsterblichkeit. Die andere Sache, die Aufgehobenheit, die du angesprochen hast, vielleicht der Vollständigkeit halber noch erwähnt,

277
00:43:54,000 --> 00:44:00,000
findet sich dann zum Beispiel in sowas wie einer Nanny-AI eigentlich, oder?

278
00:44:00,000 --> 00:44:16,000
Also im Grunde einer allumfassenden künstlichen Intelligenz, die für uns alle quasi die profane täglichen Belange regelt in einer scheinbar besseren Art, als wir es denn jemals könnten, oder?

279
00:44:16,000 --> 00:44:26,000
Das ist dann das Prinzip des Singleton, wie du es benannt hast oder wie bzw. die Protagonistin dieser Denkschule das dann auch benennen.

280
00:44:26,000 --> 00:44:30,000
Vielleicht kannst du das nochmal kurz definieren, was das ist, ein Singleton?

281
00:44:30,000 --> 00:44:40,000
Ein Singleton ist tatsächlich ein Konzept von Nick Bostrom. Das ist eben eher vordergründig, ist es eine Weltregierung, die halt so die totale Kontrolle hat.

282
00:44:40,000 --> 00:44:45,000
Und er nennt das Koordinierungsprobleme, also die Koordinierungsprobleme löst.

283
00:44:45,000 --> 00:45:00,000
Koordinierungsprobleme sind für ihn sowas wie Politik, ganz allgemein. Also alles, was nicht die klarste technische Lösung ist, das wird gelöst, dadurch, dass wir eine möglichst rationale Weltregierung haben.

284
00:45:00,000 --> 00:45:21,000
Und dieser Singleton, er hat diesen Begriff, der ist so ein bisschen Kuddelmuddel, weil da auf der einen Seite ist er so definiert, so ein Singleton könnte jetzt auch ein totalitärer Weltstaat sein oder eine Alienherrschaft oder was weiß ich, was das Einzige wäre, dass es nicht mehr in Frage gestellt wird.

285
00:45:21,000 --> 00:45:29,000
Also es geht, am Ende geht es um totale Herrschaft. Und dann gibt es da noch dieses Ding, das da aber eigentlich immer mit die Superintelligenz gemeint ist.

286
00:45:29,000 --> 00:45:45,000
Also eine Superintelligenz, die diesen Take-off der Intelligenz-Explosion hinter sich lässt, dann ganz schnell dahin kommt, dass sie eben durch menschliche Manöver gar nicht mehr in Frage gestellt werden kann.

287
00:45:45,000 --> 00:46:01,000
Und diese totale Herrschaft ist für ihn auch ein ganz wichtiger Punkt. Daher wird er oft missverstanden. Man liest seine Bücher, hört seine Vorträge und dann kommt da diese totale Weltherrschaft vor und man denkt, das muss ja irgendwie was Negatives sein.

288
00:46:01,000 --> 00:46:18,000
Vordergründig warnt er auch davor. Also es ist schon ein existenzielles Risiko, wenn dieser Singleton dann eben unsere Menschen nicht für wertvoll erachtet und eben planiert wie Ameisen.

289
00:46:18,000 --> 00:46:32,000
Aber eigentlich, wenn wir raus wollen auf dieses, was uns ja moralisch verpflichtet, das Realisieren der bestmöglichen Zukunft, dann müssen wir ganz ganz schnell hinkommen zu diesem Singleton, und zwar einem positiven.

290
00:46:32,000 --> 00:46:53,000
In dem Moment, das ist dann auch eine der Sachen, die ich an der ganzen Nummer so schwierig finde, wo es dann eben auch die Dimension von, ja gut, das ist jetzt alles ein bisschen schrullig, aber auch vielleicht interessante Gedankenspiele verlässt.

291
00:46:53,000 --> 00:47:06,000
Die haben diesen Herrschaftsanspruch, ein legitimatorisches Narrativ und es ist total hip bei den gerade neu aufkommenden Eliten. Also das finde ich sehr schwierig.

292
00:47:06,000 --> 00:47:25,000
Eigentlich artverwandt mit diesem Thema der Frage der Positionierung des Menschen gegenüber der technischen Entwicklung und ob die als, ob man selbst, also der Mensch an sich quasi jetzt dann als defizitär empfunden wird oder nicht.

293
00:47:25,000 --> 00:47:40,000
Daran angehängt ist vielleicht auch die Frage nach dem Bild von Intelligenz, das da unterstellt wird im Thema der Superintelligenz und du hattest das dann an anderer Stelle auch so dargestellt,

294
00:47:40,000 --> 00:47:54,000
als ob dieses Denken in Superintelligenzen dann auch eine Art von, das heißt die Superintelligenz eine Art von verkörpert perfekter Rationalität ist, die dem Menschen eben abgesprochen wird.

295
00:47:54,000 --> 00:48:10,000
Also da wird dann quasi wieder dieser Vergleich aufgemacht. Der Mensch, das zeige ja zum Beispiel die Verhaltensökonomie. Der Mensch, der sei eben nicht perfekt, der Handel immer wieder durch seine Emotionen bedingt ja eigentlich fehlerhaft und nicht rational.

296
00:48:10,000 --> 00:48:29,000
Und man müsse jetzt quasi, um diesem defizitären Status zu entsteigen, eben quasi sich anderer Mittel bedienen, zum Beispiel der Superintelligenz. Was ist denn das Verständnis von Rationalität, was dem zugrunde liegt? Was ist, was gilt da als Rationalität?

297
00:48:29,000 --> 00:48:48,000
Da würde ich es jetzt spezifisch eingrenzen auf die Rationalisten bzw. richtigerweise Neorationalisten. Das ist dann auch ganz spannend, weil es ein Verbreitungsvektor transhumanistischen Denkens, der auch eng verknüpft ist.

298
00:48:48,000 --> 00:49:14,000
Also hat denselben Gründe, ist dasselbe Milieu wie dieses Machine Intelligence Research Institute, ist die neorationalistische Bewegung im Internet, die erstmal ansetzt bei diesen, bei den Erkenntnissen aus Verhaltensökonomik und Ähnlichem, dass wir eben halt nicht perfekt rationale Agenten sind.

299
00:49:14,000 --> 00:49:31,000
Das ist ja durchaus auch in den Wirtschaftswissenschaften irgendwie Gegenstand von Debatten, also ich meine sogar nach wie vor, ob nicht dann die Modelle überarbeitet werden müssen, wenn Menschen am Markt gar nicht perfekt rationale Agenten sind.

300
00:49:31,000 --> 00:49:48,000
Da wird halt die umgekehrte Schlussfolgerung gezogen. Wie können wir das werden? Und das hat erstmal so eine Selbsthilfedimension, dass man also versucht, das eigene Denken zu schulen, um rationaler zu sein.

301
00:49:48,000 --> 00:50:01,000
Auch jetzt erstmal keine schlechte Idee, es ist wie bei, also ich finde auch so eine Sache, die sich da immer irgendwie durchzieht, es ist irgendwie dann immer die Urausführung, die so ein bisschen fragwürdig ist.

302
00:50:01,000 --> 00:50:14,000
Man will das da nämlich erreichen, das ist ja jetzt auch nicht so, dass es nicht Systeme in der Gesellschaft gibt, die es einem ermöglichen, rationaler zu denken.

303
00:50:14,000 --> 00:50:20,000
Man kann in die Schule gehen, man kann studieren, man kann sich mit der Geschichte der Philosophie beschäftigen usw.

304
00:50:20,000 --> 00:50:37,000
Man kann sich vor allem klar machen, dass man eben fehlbar ist in seinen Entscheidungen und die hinterfragen das alles nicht, sondern man soll eine neue Form von Rationalität erlernen, die orientiert sich an der welchen Statistik.

305
00:50:37,000 --> 00:50:50,000
Die Grundannahme ist, geist und menschliches Denken ist eigentlich eine ganz einfache Sache, das ist irgendwie so ein sequenziell belschte Fragestellung, lösen der Rechen-OP-Mechanismus.

306
00:50:50,000 --> 00:50:56,000
Wenn man sich das bewusst macht und Verhedderungen vermeidet, dann wird man rationaler.

307
00:50:56,000 --> 00:51:19,000
Dann gibt es diese völlig skurrilen Sachen rund um das Center for Applied Rationality, also für angewandte Rationalität, das einem beibringen soll, so zu denken, was einem ein Denken beibringen soll, was an der Funktionsweise von KI-Systemen orientiert ist.

308
00:51:19,000 --> 00:51:38,000
Und was sich durch das alles schon ganz erkennbar durchzieht, ist ein sehr, sehr enger Begriff von Rationalität, der sich eben stark an ökonomischem Erfolg ausrichtet und auch das Ganze nicht mehr hinterfragt.

309
00:51:38,000 --> 00:51:54,000
Man ist weit davon entfernt, so etwas aufzumachen, wie man es jetzt beispielsweise aus einer kritischen Theorie kennt, die Einteilung in eine subjektive und objektive Vernunft.

310
00:51:54,000 --> 00:52:06,000
Und das eine ist die Vernunft des Zweck-Rational-Instrumentelle und das andere ist eine Vernunft, die die Frage stellt, sind die Ziele, die wir uns setzen, ist die Gesellschaft, wie wir sie einrichten, ist das überhaupt vernünftig?

311
00:52:06,000 --> 00:52:12,000
Die zweite Art geht gar nicht, das macht die Superintelligenz dann irgendwann.

312
00:52:12,000 --> 00:52:30,000
Und da kommen wir dann auch in den Bereich, wo ich Nähe zu faschistischem Denken sehe, bei dem Intelligenzbegriff, weil die Intelligenz ist das, was der EQ-Test misst.

313
00:52:30,000 --> 00:52:36,000
In der Gedankenwelt, aus der sich der Transhumanismus entwickelt.

314
00:52:36,000 --> 00:52:45,000
Und das ist ja auch eine numerische Größe, die kann größer oder kleiner sein, dann ist auch klar, wenn wir eine Maschine bauen können, die das auch irgendwie mehr hat, dann ist sie auch besser.

315
00:52:45,000 --> 00:53:01,000
Und dadurch ziehen sich dann organische Ideen, die um Intelligenzoptimierung kreisen, weil wir hätten ja auch ein großes existenzielles Risiko in dem Moment, wo sich zu viele dumme Leute vermehren.

316
00:53:01,000 --> 00:53:03,000
Das wäre etwas, wo man aufpassen müsste.

317
00:53:03,000 --> 00:53:13,000
Das ist auch so ein Punkt, wo ich mich dann sehr wundere, warum das ganze Zeug so wenig skandalisiert wird, weil es ist ja jetzt eigentlich keine salonfähige Position.

318
00:53:13,000 --> 00:53:23,000
Und die steht zwar nicht im Vordergrund der Überlegung, aber zum Beispiel durch das Werk von Nick Bostrom zieht sich das an mehreren Stellen.

319
00:53:23,000 --> 00:53:28,000
Also Intelligenz ist eng begriffen, Rationalität ist eng begriffen.

320
00:53:28,000 --> 00:53:34,000
Eine Diskussion, eine philosophische, wird eigentlich vermieden, was das sein soll.

321
00:53:34,000 --> 00:53:50,000
An den Stellen, wo was dazu gesagt wird, ist es irgendwie ein rationaler Agent, hat was mit welcher Statistik zu tun und im Zweifelsfall ist das halt einfach das, was der EQ-Test misst.

322
00:53:51,000 --> 00:54:05,000
Und das ist eben also jetzt nicht nur quasi irgendeine Splitter-Position innerhalb irgendeines Subforums von, wie heißt nochmal dieses Rationality-Wiki?

323
00:54:05,000 --> 00:54:06,000
Das gibt es auch.

324
00:54:06,000 --> 00:54:08,000
Less wrong heißen die.

325
00:54:08,000 --> 00:54:20,000
Less wrong, das ist quasi nicht nur da eben so eine kuriose Sonderposition, sondern das ist was, was du durchaus auch eben bei Nick Bostrom im geschriebenen Wort und gesagtem Wort vor Ort ist.

326
00:54:20,000 --> 00:54:28,000
Bei Nick Bostroms Buch über Superintelligenz ist es zum Beispiel, fasst er das schon auf den ersten paar Seiten als optimaler belgischer Agent.

327
00:54:28,000 --> 00:54:31,000
Da können wir Intelligenz äußerlich bestimmen.

328
00:54:31,000 --> 00:54:33,000
Da brauchen wir nicht.

329
00:54:33,000 --> 00:54:45,000
Und auch diese Schlussfolgerung, dass quasi daraus eigentlich eugenische Handlungen zu setzen sind in irgendeiner Form, wird implizit da dann dann mitgegeben?

330
00:54:45,000 --> 00:54:52,000
Also ich weiß nicht, ob man das in Superintelligenz, also es gibt einerseits diesen Aufsatz rund um existenzielle Risiken.

331
00:54:52,000 --> 00:54:55,000
Da kann man das eigentlich nicht implizit nennen.

332
00:54:55,000 --> 00:55:13,000
Das folgt dann nicht aus diesem aus der spezifischen intelligenten Definition, aber da taucht als eines der eines der Risiken, die wir auf die wir achten müssen, die da verhindern können, dass wir dieses Himmelreich erreichen.

333
00:55:13,000 --> 00:55:35,000
Das Erreichen des Technologischen ist neben Meteoriteneinschlägen, böser Superintelligenz, der globalen Dominanz einer ökologischen Bewegung, auch die dysgenischer Druck, also die Generierung quasi,

334
00:55:35,000 --> 00:55:54,000
durch die, weil Menschen dazu neigen, zu viel Nachkommenschaft zu produzieren, gerade auch die falschen Leute, und man halt dann zu einem Homophiloprogenitus, also das sich fortpflanzenliebend degenerieren würde.

335
00:55:54,000 --> 00:56:02,000
Was dann auch, weil wir dann nicht mehr genug intelligente Leute haben, den Weg in das technologische Paradies verstellt.

336
00:56:02,000 --> 00:56:16,000
Im Superintelligenz Buch werden eugenische Maßnahmen als einer der Wege überhaupt dahin zu kommen, dass wir solche superintelligenten Maschinen haben verhandelt.

337
00:56:16,000 --> 00:56:38,000
Eine Sache, die ich jetzt beschrieben habe, war der Take-off. Es gibt aber auch ein einziges Unterkapitel zu biologischer Kognition, da geht es dann um IQ, und dann, wie wir mit ganz einfachen Maßnahmen dafür sorgen, dass sich besonders kluge Leute und besonders klugen Leute fortpflanzen,

338
00:56:38,000 --> 00:57:05,000
da so eine Dimension haben, weil wir das am besten in vitro machen, eine Dimension von Selektion drin haben, und dann eben in sehr wenigen Generationen dazu kommen, dass wir durch genetische Selektion erstmal eine Elite an Forschern aufbauen, die in der Lage sind, dann uns dahin zu bringen, dass wir die nötigen technologischen Schritte machen können.

339
00:57:05,000 --> 00:57:14,000
Also, das steht nicht im Vordergrund, aber ist jetzt auch nicht, dass da was implizit ist.

340
00:57:14,000 --> 00:57:31,000
Wow, wild. Okay, da muss ich dann jetzt nochmal bei dem, du hattest das zwar eh schon angesprochen, aber da muss ich jetzt trotzdem nochmal nachhaken, wie einflussreich ist denn transhumanistisches Denken und was bedeutet das dann quasi für uns.

341
00:57:31,000 --> 00:57:43,000
Ich meine, in dem, was du gerade gesagt hast, schimmert es ja schon durch, aber also Ray Kurzweil, du hattest ihn angesprochen, ist bei Google, ich kann mich erinnern, der leitet da glaube ich jetzt auch irgendeine Art von internem Think Tank oder sowas.

342
00:57:43,000 --> 00:58:04,000
Peter Thiel, vielleicht für die Hörerinnen und Hörer, der hat mit Elon Musk zusammen PayPal gegründet und Palantir auch, nicht mit Elon Musk zusammen, aber Palantir ist eine Daten-Harvesting-Maschine, die auch mit Geheimdiensten zusammenarbeitet, mit dem Pentagon und so weiter.

343
00:58:04,000 --> 00:58:09,000
Also der ist auf jeden Fall auch eine schillernde Figur, der, versteht er sich auch als transhumanist?

344
00:58:09,000 --> 00:58:18,000
Ich bin mir nicht ganz sicher, ob es irgendwie ein Zitat von ihm gibt, dass er sich explizit selber als transhumanist bezeichnet, aber er fördert Projekte in dieser Richtung.

345
00:58:18,000 --> 00:58:41,000
Er fördert vieles, aber er ist, wenn man sich dann in die entsprechenden Communities und so weiter begibt, ist auf jeden Fall immer, ist ein Ziel, das diskutiert wird, kann ich fördern, ist so eine Möglichkeit, wenn man so ein Projekt hat, das irgendwie so in einem transmonistischen Umfeld, ob man nicht von Peter Thiel auch irgendwie Förderung kriegen könnte.

346
00:58:41,000 --> 00:58:46,000
Stimmt, das kann ich mich erinnern, da habe ich auch mal bei einem Podcast gehört, dass das diskutiert wurde.

347
00:58:46,000 --> 00:58:58,000
Gut, also die Frage wäre, für wie einflussreich hältst du das, sagen diese Formen des Denkens und was bedeutet das für uns?

348
00:58:58,000 --> 00:59:20,000
Ja, da stellt sich dann die Frage, deswegen sprach ich ja schon am Anfang von einem Mainstream, es gibt offensichtlich sehr einflussreiche Leute, die diesem Gedanken gut mindestens nahe stehen oder die große Möglichkeiten haben, auch Einfluss auf die Gesellschaft zu nehmen, weil sie eben über sehr viel Geld verfügen.

349
00:59:20,000 --> 00:59:49,000
Und es hat jetzt in letzter Zeit, also es ist jetzt gerade passiert, es ist eine spannende Sache, was daraus wird, es ist ja jetzt Brexit und Boris Johnson ist Premierminister von Großbritannien und er hat ja diesen auch etwas schillernden Berater Dominic Cummins, der in der transhumanistischen Szene vernetzt ist und der in seiner Zeit schon mal im Bildungsministerium einen recht langen Aufsatz gemacht hat.

350
00:59:50,000 --> 01:00:05,000
Er hat uns geschrieben, einer der Kernaussagen war, dass es bei dem Erfolg von Schülern wesentlich mehr auf ihr genetisches Potenzial ankäme als auf die Umstände ihrer Bildung.

351
01:00:05,000 --> 01:00:33,000
Genau, der ist im Beraterstab des Premierminister von Großbritannien und hat kürzlich auf seiner Webseite Stellengesuche veröffentlicht, bei denen es darum geht, die Verwaltung der englischen Regierung zu optimieren, indem man sich eben Data-Scientists, Leute aus der KI-Programmierung, Unternehmer und was weiß ich was ins Boot holt, eben all diese Personen, die in der Transhumanistischen Szene vernetzt sind.

352
01:00:35,000 --> 01:01:00,000
Dass die in der Transhumanismus so ein bisschen fundierenden, weltsichtentscheidenden Personen sind und da ist die Frage, wo ich jetzt mit etwas Spannung gucke, ob es tatsächlich passiert, dass sich in England transhumanistische Kreise in der Regierungsverwaltung etablieren.

353
01:01:00,000 --> 01:01:21,000
Also das ist nicht ausgemacht, das ist eine völlig chaotische Situation, war aber auf jeden Fall auch so eine Sache, wo ich mir gedacht habe, hui, steht man da vor Stellengesuchen für die Downing Street Number Ten, die so einen transhumanistischen Vibe haben.

354
01:01:21,000 --> 01:01:39,000
Und das ist aber nicht nur jetzt, weil Data-Scientists gibt es ja überall, also das ist ja jetzt noch nicht transhumanistisch irgendwie, da gab es da noch andere Hinweise, die jetzt quasi konkret in diese Richtung deuten, weil jetzt Big-Data-Nutzung gilt ja jetzt quasi schon als, muss man quasi drauf schreiben.

355
01:01:39,000 --> 01:01:59,000
Ich habe jetzt hier die Stellenausschreibung. Das erste Zitat der Stellenausschreibung, also an erster Stelle wird Elisabeth J. Kauski zitiert als KI-Experte, das ist der Leiter des Machine Intelligence Research Institutes.

356
01:01:59,000 --> 01:02:15,000
Das heißt, das hat eine spezifische Richtung aus, wer da abgefragt wird. Und das würde ich schon als einen, also es würde nicht schaden auf diese Stellenausschreibung zu antworten und man hätte schon seine Credentials als Transhumanist, sagen wir es so.

357
01:02:16,000 --> 01:02:22,000
Max, als letztes stelle ich immer noch die Frage, wenn du dir Zukunft vorstellst, was stimmt dich freudig?

358
01:02:22,000 --> 01:02:48,000
Was stimmt mich freudig? Ich sehe gerade, also tatsächlich interessiert mich gerade diese globale Welle, die es ja jetzt irgendwie im letzten Jahr gab von basisdemokratischen Protestbewegungen gegen ökonomische Ungleichheit,

359
01:02:48,000 --> 01:03:00,000
die sich auch in Ländern abgespielt haben, wo man tatsächlich erwartet würde, das landet alles in irgendeinem Islamismus oder was weiß ich was oder in irgendwelchen anderen autokritären Ideologien.

360
01:03:00,000 --> 01:03:27,000
Und dass es zumindest schon mal so aussah, als würde das nicht passieren und das lässt mich mit einer gewissen Erwartung darauf blicken, ob sich nicht die ja doch sehr solide wirkenden Vergesellschaftungsformen der Globalisierung auf so einer wirtschaftlichen Ebene

361
01:03:27,000 --> 01:03:43,000
radikal in Frage stellen lassen. Es gibt ja von Mark Fischer diese Formulierung über capitalist realism, von David Graeber diese Formulierung, dass wir eigentlich für Zukunft wieder kämpfen müssen, Zukunft zu eröffnen, sowas wie Zukunft zu haben.

362
01:03:44,000 --> 01:03:55,000
Und ich glaube, dass da in nächster Zeit sehr viel passieren wird, was den Blick auf Zukunft wieder öffnet, weil sich Möglichkeitenräume plötzlich eröffnen in kollektivem Handeln.

363
01:03:56,000 --> 01:04:01,000
Wunderbar. Das ist doch ein schöner Ausblick. Max, vielen Dank für das Gespräch.

364
01:04:02,000 --> 01:04:03,000
Ich danke dir, dass ich bei dir sein durfte.

365
01:04:25,000 --> 01:04:32,000
Was ihr zu dem Ganzen denkt und wie euch diese Folge hier gefallen hat, unbedingt gut bewerten auf allen Podcast Plattformen, die ihr nutzt.

366
01:04:33,000 --> 01:04:40,000
Für unsere Patreon Unterstützerinnen und Unterstützer gibt es auf www.patreon.com schrägstrich future histories vieles an Zusatzmaterial.

367
01:04:41,000 --> 01:04:44,000
Da könnt ihr also auch vorbeischauen. Bis zum nächsten Mal. Ich freue mich.

