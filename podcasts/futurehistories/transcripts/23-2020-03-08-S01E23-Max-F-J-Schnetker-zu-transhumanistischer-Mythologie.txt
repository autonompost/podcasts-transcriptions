Herzlich willkommen bei Future Histories, dem Podcast zur Erweiterung unserer Vorstellung von Zukunft.
Mein Name ist Jan Groß und ich freue mich sehr, heute Max Franz-Johann Schnettker begrüßen zu dürfen.
Max ist Autor des Buches Transhumanistische Mythologien, in dem er sich kritisch mit bestimmten Strömungen des Transhumanismus auseinandersetzt.
Vielen, vielen Dank an Fabian für deine Unterstützung und vielen Dank auch für den Hinweis zu Maxes Buch.
Das war wirklich jetzt ein interessantes Interview und Gespräch.
Denn auch für mich waren einige der Bezüge, die Max da herstellt, durchaus neu.
Und als Max dann am Ende warnt, dass sich diese Formen des Denkens auch gerade anschicken, ganz konkret Nähe zu klassischen politischen Machtzentren zu suchen, da war ich erstmal ein bisschen erstaunt.
Aber ein paar Tage nach unserem Interview hat mir Max dann einen Artikel geschickt über einen Skandal um einen Berater von Boris Johnson, der öffentlich für Eugenik eingetreten ist und zum Beispiel die universelle Einführung von Langzeitverhütungsmitteln zu Beginn der Pubertät vorgeschlagen hat, um eine, Zitat, dauerhafte Unterschicht zu verhindern.
Das ist jetzt natürlich ein Extrembeispiel, muss man sagen, aber ganz im Sinne einer kritischen Analyse, auch von zukünftigen Herrschaftsverhältnissen, gilt es unbedingt, den transhumanistischen Diskurs kritisch zu befragen.
Genau das machen wir im heutigen Gespräch.
Wenn euch Future Histories gefällt, dann erzählt es doch bitte einem Freund oder einer Freundin, von der ihr glaubt, dass ihm oder ihr Future Histories vielleicht auch gefallen könnte.
Vielen Dank und ich wünsche euch viel Spaß bei dieser Episode mit meinem Gast Max Franz-Johann Schnettger.
Herzlich willkommen, Max.
Hallo.
Es gab schon einmal eine Folge Future Histories, in der der Transhumanismus zur Sprache kam, nämlich Folge 13 mit Julia Grillmeier zu Transhumanismus, Posthumanismus und Kompost.
Nicht alle werden aber diese Folge, die übrigens sehr empfehlenswert ist, möchte ich da noch anmerken, gehört haben.
Und so mag ich auch dich um eine Begriffsdefinition bitten.
Was ist das? Was ist Transhumanismus?
Ja, das auch ein ganz guter Einstieg, den ich vielleicht auch nutzen sollte, denn ich habe mir erlaubt, dadurch, dass ich mich kritisch mit dem Thema beschäftige, auch eine Begriffsdefinition vorzunehmen,
die sich von der Eigenbezeichnung von Transhumanisten ein bisschen unterscheidet.
Das ist ja vordergründig, sagt man, oder wenn man mit Transhumanisten redet, hört man oft, es ginge eigentlich nur um eine Verbesserung des Menschen durch Technologie und es könnte jede Form von Verbesserung käme da in Frage.
Ich bin aber der Meinung, dass sich da durchaus ein sich in einer Institutionalisierung befindendes ideologisches Narrativ ausmachen lässt.
Und dass es eben nicht nur um eine reine technologische Verbesserung des Menschen geht oder jede technologische Verbesserung des Menschen Transhumanismus wäre, sondern dass sich das recht genau umreißen lässt.
Um ein Beispiel zu geben, was es nicht ist, ich hatte bei einem kürzlichen Vortrag, da war jemand, der leidet an Parkinson und er hat noch gehts, aber er hat als langfristige Therapieform in Aussicht, dass er einen Hirnschrittmacher kriegt.
Da ist es dann ein kleiner Draht, der ins Gehirn implantiert wird, der an bestimmten Nervenzellen einen schwachen Strom abgibt, diese dadurch polarisiert und irgendwie, man weiß es nicht so genau, dafür sorgt, dass Parkinson Symptome unterdrückt werden.
Und der sah sich quasi schon, fing dann halt, hatte für sich die Fragestellung, ja damit bin ich doch Transhuman, das ist es doch.
Draht im Kopf und Implantat und das reguliert irgendwie, wo ich dann sagen würde, genau das ist tatsächlich nicht Transhuman, das ist zutiefst menschlich, weil er ausgeliefert bleibt an seinen Körper die Verhältnisse.
Und wenn man sich ein bisschen mit transhumanistischer Literatur beschäftigt, läuft es immer darauf hinaus Kontrolle zu erlangen, über die Eventualitäten des Lebens, Kontrolle über den Körper, Kontrolle über Krankheit und am Ende auch über den Tod.
Also es wäre zum Beispiel jemand, der auf medizinischen Wege eine neue Anwendung zur Bekämpfung einer Krankheit kriegt, den würde ich zum Beispiel nicht Transhuman nennen.
Das heißt, wir haben festgestellt, was es nicht ist. Was wäre dann aus deiner Perspektivierung das, was es schon ist?
Das, was es schon ist, das ist eine recht spezifische Ideologie, die also, der hat diverse Ursprünge, aber sie hat sich in den letzten 20 Jahren, würde ich sagen, gibt es einen Mainstream, der sehr viel zu tun hat mit dem Silicon Valley bzw. den dort neu entstandenen Geld- und Machteliten, die die Digitalisierung hervorgebracht hat.
Die immer wieder in ein Projekt des Fortschritts, der da ja irgendwie so das Leitbild gibt, dann halt auch die Verbesserung des Menschen mit einbeziehen und schlussendlich auf eben die Überwindung von Leid und so weiter abzielen, aber in einer Art und Weise, die, wenn Sie Mensch sagen, eigentlich so eine ganz spezifisch bürgerliche Subjektvorstellung zugrunde legt.
Und die quasi auf technologischen Säge sichern und verstetigen möchte.
Und da das Ganze nicht so sehr eingebettet ist in geisteswissenschaftliche Diskussionen, sondern zumeist irgendwie aus dem Techsektor kommt, gruppiert sich der Transhumanismus dann auch gar nicht so sehr nach irgendwelchen spezifischen Anthropologien,
sondern es geht dann, zum Beispiel gibt es da den Vorschlag einer Einteilung in Silizium-basierten, Kohlenstoff-basierten Transhumanismus.
Der Silizium-basierte würde darum kreisen, Menschen elektronische Implantate einzubauen und schlussendlich ihren Geist in Computer hochzuladen und auf diesem Wege mit Computern zu verschmelzen.
Der Kohlenstoff-basierte Transhumanismus kreist dann um Vorstellungen von genetischem Enhancement, genetischen Verbesserungen und der Überwindung des körperlichen Alterns.
Und da gibt es schon so unterschiedliche Strömungen und Institutionen tatsächlich auch.
Also in dem Kohlenstoff-basierten haben wir dann eher die Leute, die diese Bewegungen rund um MaxMor, die Wert darauf legen, sich einfrieren zu lassen, bis eben neue medizinische Technologien da sind,
während wir in dem Silizium-basierten eben die Erlösungsvorstellungen rund um KI und so weiter haben, mit denen ich mich dann auch spezifisch beschäftigt habe.
Das ist interessant, weil das war nämlich was, was in den Texten, die ich von dir jetzt gelesen habe, was ich mich da gefragt habe.
Es schien mir da phasenweise so, als ob du den Transhumanismus fast schon gleichsetzen würdest mit einer bestimmten Richtung der KI-Forschung, der künstlichen Intelligenzforschung,
die sich nämlich speziell mit den potentiellen existenziellen Risiken zukünftiger künstlicher Intelligenzen auseinandersetzt.
Aber so, wie du das jetzt gerade aufgemacht hast, ist das quasi ja dann nur eine Richtung unter dem Überschirm des Transhumanismus und dann wiederum auch nur eine Unterrichtung unter eigentlich dem Sektor,
den du jetzt Silikon-basiert genannt hast. Oder sehe ich das richtig?
Genau. Das liegt einfach im Fokus meiner Arbeit. Ich habe nicht den Anspruch, mit meinem Buch eine Gesamtkritik jeder transhumanistischen Strömung vorgelegt zu haben,
sondern ich habe mich spezifisch auf diese eingeschossen, allerdings auch unter der Prämisse, dass ich sie für dominierend halte oder zumindest das Potenzial dominierend für transhumanistische Diskurse,
weil sie so stark verflochten ist mit gesellschaftlichen Geld- und Machtstrukturen.
Also sprich, gerade bei einem Top-Hoster, der immer auftaucht, oder was ich vielleicht noch anmerken sollte, es gibt Autoren, die diese Strömung aus dem Transhumanismus ausklammern.
Dem würde ich widersprechen. Durchaus auch diese Idee der Superintelligenz, respektive des parallelen Menschen in Computer hochladen, sehe ich in einer expliziten transhumanistischen Dynamik oder Teleologie verortet.
Als es darum ging, woran ich arbeite, diese Form von Transhumanismus, die eine ist, also nicht die einzige, aber die, über die ich ein Buch geschrieben habe, hat Vertreter an Institutionen wie der Universität Oxford.
Oxford hat Fans unter Milliardären wie Elon Musk und Peter Seale, die einschlägige Forschungsinstitute zum Thema mit Millionenbeträgen zum Teil fördern.
Es gibt diesen Start-up-Incubator, der sich Singularity University nennt, wo die Singularität dann auch wieder im Namen steckt usw.
Für meinen Unterfangen jetzt eine Kritik des Transhumanismus zu unternehmen, die auch jetzt nicht nur rein akademischer Selbstzweck sein soll, sondern sollte, sondern sich mit neu aufkommenden Herrschaftsideologien beschäftigt.
Und das ist meine Stoßrichtung, dass ich diese Form von Transhumanismus zumindest potentiell für eine neu aufkommende Herrschaftsideologie halte, dadurch dann ja Fokussierung auf diese Form von Transhumanismus.
Zeichnen wir das doch mal für die Zuhörerinnen und Zuhörer ein bisschen nach.
Also du hattest schon angesprochen, einer der ganz prominenten Vertreter dieser spezifischen Richtung sitzt in Oxford.
Ganz konkret ist das Nick Bostrom, der das dortige Future of Humanity-Institut gegründet hat und dort eben Forschung zu existenziellen Risiken betreibt.
Er ist aber auch gleichzeitig, ich glaube mal Vorsitzender der World Transhumanist Association gewesen oder so, hat auf jeden Fall auch publiziert zum Thema Transhumanismus.
Also in ihm als Person vereinen sich diese beiden Stoßrichtungen, die du da beschreibst, auf jeden Fall mal ganz konkret.
Was sind denn seine grundlegenden Thesen?
Seine grundlegenden Thesen, er ist sehr produktiv, das auf jeden Fall, das kann man auf gar keinen Fall in Abrede stellen,
dass die Phase, in der er sich als Transhumanist bezeichnet hat, das war eher so vor 15 Jahren.
Ich würde sagen, aber sein Denken hat mit diesem Transhumanismus nie gebrochen, sondern er macht ihn dann zur weiteren Grundlage seines Nachdenks über andere Themen.
Das aktuelle Thema, zu dem er arbeitet, ist eben der Begriff des existenziellen Risikos.
Das sollen menschheitsvernichtende Risiken sein und er nähert sich der Thematik dann an mit den Werkzeugen des Utilitarismus und der analytischen Philosophie
und versucht da eine Philosophie zu entwickeln, die fast mathematischerweise, aber mit einer gewissen Genauigkeit, Risiken für die Menschheit abschätzt.
Mit auch durchaus einem politischen Impetus, der dafür sorgen soll, dass wir jetzt mal etwas unternehmen müssen an bestimmten Stellen usw.
Jeder Hörer wird jetzt natürlich denken, ja natürlich der Klimawandel, der ist es genau nicht tatsächlich,
sondern das Risiko, das ihn am meisten umtreibt und das sein anderer großer Forschungsgegenstand ist, ist die Superintelligenz.
Das soll salopp formuliert eine die Welt beherrschende künstliche Intelligenz sein und man nimmt so eine gewisse eindeutige Kontinuität von Fortschritt an,
die das zu einer Zwangsläufigkeit macht, oder fast zu einer Zwangsläufigkeit, dass so etwas entsteht.
Darum kreisen dann auch die meisten Abwägungen um das Thema existenzielle Risiken für die Menschheit.
Und dann hat das Ganze noch einen ganz spannenden Dreh, der auch dann zum Thema des Podcasts passt, dass es nicht nur um die jetzige Menschheit geht,
sondern auch immer um die potenziell in der Zukunft vorhandene Menschheit, der im Zweifelsfall auch, weil man annimmt,
dass wenn man es richtig macht, die Bevölkerungszahlen explodieren und man ja auch Utilitarist ist, man also dann in der Zukunft eine größere Zahl von Menschen,
die ein Wohl haben, die relevant sind für utilitaristische Erwägung und deren Glück und Wohl bestimmt werden kann, dass das sogar das größere Gewicht hat
und dass wir unsere politischen Aktivitäten, unser Handeln hier und jetzt quasi an der Maximierung dieses zukünftigen Menschheitsglücks ausrichten müssen.
Das fand ich interessant, dass das quasi gleichgestellt wird. Also um das nochmal vielleicht kurz zusammenzufassen, das heißt nämlich tatsächlich,
dass sie quasi in dem Glauben, also zum einen es gibt ein Glauben, man könne das quasi berechnen, also das Glück und Wohl der Menschen,
das ist ja eine Annahme, die dem Utilitarismus auch zugrunde liegt und dass dann aber in dieser Berechnung zukünftige Generationen Menschen mit hineingenommen werden,
aber eben nicht in einer Logik, in der man jetzt zum Beispiel aus der Ökologieströmung kommend irgendwie sagen würde, okay, wir müssen jetzt, was weiß ich,
ressourcenschonend vorgehen mit der Welt, weil wir müssen ja schauen, dass wir das Bestehende nicht für zukünftige Generationen zerstören,
sondern das Kalkül ist quasi ein anderes, das ist tatsächlich so ein im Grunde sich wertfrei gebendes quantitatives Kalkulieren, oder?
Ja, ziemlich genau, da hat er auch drüber promoviert quasi, dass es seine Doktorarbeit entworft, also so eine Aktualisierung,
und die literistische Moralphilosophie soll das sein, er übernimmt eigentlich aus der Physik, unter anderem, also in verschiedenen Wissenschaften,
nicht Physik, in der Kosmologie, gibt es das sogenannte anthropische Prinzip, das sich mit Fragestellungen greift, bei Fragestellungen der Eingerichtetheit des Universums,
also warum sind die Grundgrößen des Universums, die wir beobachten so und nicht anders, und dann gibt es da diesen Punkt, dass es sehr viele physikalische Konstanten,
wenn sie nur ein bisschen anders wären, aus verschiedenen Ebenen die Existenz menschlichen Lebens unmöglich machen würden,
Temperatur, Bindungsenergie zwischen Teilchen, was weiß ich was, dann können sie keine komplexen Strukturen mehr formen, nur noch kein Leben oder so,
oder Gravitation, plötzlich wären Himmelskörperdynamiken anders, ganz viel ihrer Kram, der vordergründig erst mal darauf drängt,
dass man sich denkt, okay, oder wo so Argumente denkbar wären im Sinne von okay, es scheint so, als wäre das Universum feingetunt auf menschliches Leben,
wo man dann mit einem anthropischen Prinzip antwortet, nee, ist es nicht, das ist ein statistisches Artefakt quasi, ein Universum, das menschliches Leben verunmöglicht,
könnten wir ja gar nicht beobachten, wir wissen also, wir können davon da aus nicht schließen, ob das jetzt wahrscheinlich ist, dass Universen so aussehen oder nicht,
sondern wir können ja, also unser Sample ist eingeschränkt auf die, die wir wahrnehmen können, ein ähnliches Denken überträgt dann eine utilitaristische Moralfilosophie und sagt,
man solle in der utilitaristischen Philosophie also erst mal mitdenken, die eigene Situiertheit, und dann solle man, wenn man moralische Urteile fällt in diesem System,
systemanalytische Philosophie bzw. sich dabei üblicherweise zum Utilitarismus bedienen, dann solle man so schließen, nicht als sei man man selber,
sondern Teil einer zufälligen Referenzklasse von Beobachtern, und da bezieht er dann Zeitlichkeit mit ein.
Wenn wir also ein moralisches Urteil fällen wollen, sollten wir nicht voraussetzen, dass Moral sich irgendwie am Jetzt an unserer gegebenen Situation zu orientieren hat,
sondern wir sollten, wenn wir beurteilen, was richtig ist, dann sollten wir das so schließen, als seien wir zufällig über die Zeit gestreut,
als seien wir zufällig über die sich irgendwo in der Zeit befindliche Beobachter.
Da steckt natürlich dann die Vorstellung drin, dass man Vergangenheit und Zukunft auch irgendwie genau mathematisierbar erfassen kann,
um dann solche Aussagen zu machen, was denn bzw. dann Schlussfolgerungen darüber zu treffen, was moralisch ist und was nicht.
Und so kommt es dann auch, dass es eben nicht darum geht, irgendwie die Umwelt zu schützen oder einen Planeten zu erhalten oder so,
denn es soll ja quantifizierbar sein, es sind verschiedene Zukunften möglich.
Und ironischerweise ist in dem Aufsatz, indem man diese Idee des existenziellen Risikos entwickelt, die globale Dominanz einer ökologischen Bewegung ein existenzielles Risiko.
Also das ist die Katastrophe, die wir verhindern müssen, weil das einzig moralische ist, auf gar keinen Fall auf die Bremse zu treten,
den technischen Fortschritt so richtig durchzuballern, mehr Kapitalismus und so weiter, weil er davon ausgeht, dass sich dann zwangsläufig recht schnell dieser Zustand einstellen wird,
dass erstens eine Superintelligenz entsteht, die eh alles regelt und dann haben wir auch keine ökologischen Probleme mehr und Menschen ihre Körper verlassen.
Und das ist der Punkt, auf den wir raus müssen, die technologische Singularität, indem wir uns in Computer hochladen können
und dann verändern sich ja plötzlich die Ressourcen. Also wenn Menschen Computerprogramme sind, dann kann man viel mehr Menschen haben und man kann auch viel besser regulieren, wie es denen geht.
Und er kommt dann mit dieser auf den ersten Blick erstmal so, also wirkt jetzt nicht, erstmal zumindest schlüssigen Idee, dass man gucken sollte,
dass man die Zukunft mitbedenkt in seinen moralischen Handlungen, dann eben immer auf dieses sehr spezifisch transhumanistische Ergebnis.
Und das würde ich sagen, ist etwas, das sich durchzieht durch seine aktuelle Philosophie, dass er verschiedene, also durchaus erstmal zu griffigen Themen sehr einleuchtende Überlegungen erst mal anstellt,
die aber immer so konstruiert sind, dass man am Ende beim Transhumanismus landet.
Und ich glaube, was vielleicht wichtig ist herauszustellen ist, das funktioniert ja nur, wenn man eine bestimmte zu maximierende Zielgröße annimmt.
Also es muss ja, es wird dafür implizit etwas vorausgesetzt, was es zu maximieren gilt. Also das, was quasi als positiv angenommen wird, ist da schon eingeschrieben.
Wie würdest du das definieren? Also ist das, woran ist das orientiert? Ist das immer so ein zirkrationales, vielleicht auch an so Anthropologien wie dem Homo economicus orientiertes eigentlich Optimierungs- und Profitstreben?
Also was ist das, was es quasi zu mehrern gilt und woran wird es bemessen?
Ja, da ist ganz spannend, dass das gar nicht so wirklich diskutiert wird, sondern bei Bostrom, aber auch bei anderen Autoren, die sich in diesem Superintelligenztranshumanismus bewegen,
ist immer schon vorausgesetzt, dass so ein spezifischer Utilitarismus halt einfach die richtige Moralfilosophie ist.
So, also da ist das geklärt. Sie nennen es Glück oder sie nennen es Nützlichkeit. Utility ist dann immer der Begriff.
Das aber in der Geschichte des Utilitarismus immer so, eigentlich ist das eine Debatte. Also was soll es denn jetzt überhaupt sein? Die kann ich auch nicht beantworten, weil ich kein Utilitarist bin.
Wenn man zurückgeht bis Jeremy Bentham, bei dem gibt es Stellen, wo der dann irgendwann sagen muss, ja, also eigentlich ist es Geld. Also Glück kann man ja wohl in Geld bemessen.
Und da haben wir, weil sehr viele Leute dann mit der KI-Forschung auch zu tun haben, die irgendwie sich als Teil dieser Bewegung sehen.
Und da haben wir ja tatsächlich auch zur Steuerung von KI-Systemen diese Idee der Utility Function.
Also, dass man das definiert in gewisses Maße, numerisch bestimmbare Nützlichkeit, damit das System sich ausrichten kann.
Und dann können wir damit durchaus auch Systeme bauen, die sich irgendwie in der Welt orientieren können.
Wir haben dann aber das Ding, das setzt eigentlich immer am Ende gelabelte Daten quasi voraus.
Die philosophische Frage, was Utility ist, habe ich zumindest in dem, was ich bis jetzt vom Bostrom gelesen habe, nicht wirklich detailliert beantwortet gesehen.
Und auch bei anderen Transhumanisten nicht, die sich aber dieses Weltbild, das eigentlich darauf rekurriert, alle einkaufen.
Und das macht das Ganze auch so ein bisschen fragil, sage ich jetzt mal.
Also sobald man, weswegen ich dann auch darauf kam, mich damit so kritisch zu beschäftigen.
Es wird da sehr viel als selbstverständlich vorausgesetzt, dass sobald man nicht in so einer spezifischen,
Babel aus analytischer Philosophie und angelehnten KI-Sachen sich bewegt, eigentlich gar nicht mehr so einleuchtend wird.
Wir kommen sicher noch mal zu artverwandten Fragestellungen zurück, wie das, worüber du gerade gesprochen hast, nämlich,
auch dann in Bezug auf die Frage der Ideengeschichte.
Aber da lassen sich ja vielleicht dann manche Sachen noch anders vororten.
Aber bevor wir das machen, vielleicht noch mal kurz zur Orientierung dieser Ablauf, der da auch gedacht wird.
Nur, dass man sich das auch vorstellen kann als Hörer und als Hörer.
Der Gedanke ist, es gibt eine exponentielle technische Entwicklung.
Das heißt, es gibt einfach einen wirklich rapiden Anstieg an dem technologisch möglichen.
Und annehmend, dass sich das weiter so entwickeln wird, geht man davon aus,
dass irgendjemand, der sich so einen technologischen Anstieg anstellt,
dass sich das weiter so entwickeln wird, geht man davon aus, dass irgendwann die Rechenleistung so groß sein wird,
dass es zu einem Punkt gibt, wo eine künstliche Intelligenz eigentlich die eigene Leistung auf sich selbst anwenden kann.
Und sobald dieser Punkt erreicht ist, kommt es dann zu einer Art Intelligenz-Explosion, oder?
Vielleicht kannst du das noch mal kurz, diesen Gedanken gann, wie es zu dieser Singularität ja dann auch kommt.
Vielleicht kannst du das mal kurz noch mal beschreiben.
Was ist Singularität und wie wird das konzeptionalisiert, dass die entstehen kann?
Genau, also es sind zwei Topoi, die sich sehr eng aufeinander beziehen.
Das eine ist die Singularität.
Die technologische Singularität kommt eigentlich ursprünglich mal bestimmt in einem Aufsatz von Werner Winsch aus dem Jahr 1993.
Wurde dann in ähnlicher Weise aufgegriffen von Ray Kurzweil, der heute Chefentwickler bei Google ist
und einige Bücher geschrieben hat wie The Age of Spiritual Machines,
wo er quasi utopisch, quasi eschatologisch vorhersagt, was für eine Wunderwelt wir eintreten werden,
wenn erst die technologische Singularität da ist.
Die wird immer von Autor zu Autor ein bisschen unterschiedlich beschrieben.
Meist als eben ein Punkt, an dem der technische Fortschritt so groß ist, dass man es gar nicht mehr vorhersagen kann.
Und dass es selbstverstärkende Systeme sind, dass es immer weiter beschleunigt wird.
Und dass wir dann einen Punkt kriegen, wo die materielle Welt durch die Macht der Technik fundamental transformiert wird und wir auch.
Und zwar, da geht es dann um so Ideen wie Nanotechnologien, dass man da hinkommt, Materie auf Atomara-Ebene beliebig gestalten zu können.
Dass man diverse Energieprobleme einfach löst und natürlich, dass man den menschlichen Geist auch vollständig versteht,
wie er vom Gehirn instanziert wird und auf dem Computer reproduzieren kann.
Und praktisch ist es das technologische Himmelreich, deswegen wird es ironisch auch manchmal Rapture of the Nerds genannt.
Also dass wir an den Punkt kommen, einer technologischen Allmacht, die uns, wenn sie gut verläuft, in ein technologisches Himmelreich überführt.
Damit zusammenhängt dann bei denjenigen, die so eine Singularität am ehesten realisiert durch eine Superintelligenz vermuten, die Idee der Intelligenz-Explosion.
Das ist eine Voraussage über die Zukunft der Entwicklung künstlicher Intelligenz, die davon ausgeht, dass wir da einen plötzlichen Bruch erleben werden.
Nämlich in dem Moment, wenn es uns erst einmal gelingt, eine künstliche Intelligenz zu entwickeln.
Also man teilt da ein in schwache oder starke künstliche Intelligenz bzw. in spezialisierte und generelle künstliche Intelligenz.
Die Schwache bzw. Spezielle ist das, was wir so aus dem Alltag kennen, das den ganzen System zugrunde liegt, die wir auch alltäglich benutzen.
Das sind im Endeffekt statistische Verfahren, die auf spezifische, also z.B. dieses Machine Learning, dass man auf spezifische Sachverhalte trainiert.
Und das aber eben nur darauf anwendbar ist und nicht universalisierbar ist.
Während die generelle Intelligenz dann die menschengleiche sein soll, also die Kontext unabhängig agieren kann.
Da gibt es meines Wissens nach eigentlich keinen realistischen Ansatz, der irgendwie beschreibt, wie das realisierbar sein soll im Moment.
Das hält aber nicht davon ab, schon mal zu spekulieren, wie das sein wird, wenn wir das haben.
Und da ist die Idee, dass die generelle künstliche Intelligenz für sich selbst fungibel bleiben wird.
Also wir als Menschen haben ja irgendwie das Problem, dass wir uns nicht nur auf sehr aufwendigen Wegen irgendwie selber verbessern können.
Also wir müssen irgendwie jahrzehntelang in die Schule gehen oder irgendwie Sport treiben.
Also können wir uns schon in einer bestimmten Weise entwickeln und das auch mit Vorsatz.
Aber wir sind ja am Ende, unsere Körper und Grundlagen und das seien zu uns selber nicht fungibel.
Und diese künstliche generelle Intelligenz, die soll einerseits die Fähigkeiten eines Menschen haben, aber sie sei ja irgendwie programmiert, sie sei ja Software.
Und wenn wir jetzt eine künstliche Intelligenz hätten, die etwas klüger ist als ein Mensch, dann wäre sie ja auch besser darin, künstliche Intelligenzen zu programmieren als ein Mensch.
Und wenn man dann annimmt, dass sie vollständig für sich selber fungibel ist, dann kann sie sich ein bisschen besser programmieren.
Oder meinetwegen auch ein anderes paralleles System.
Und diese ganz kleine Verbesserung wäre dann ja auch wieder der etwas bessere Programmierer.
Also können sie sich diesen schritten und dann kriegt man plötzlich so einen Ex.
Soll man an dem Punkt, wo diese Grundlage erreicht ist, dass es etwas klüger ist als ein Mensch, etwas besser KI programmieren kann als ein Mensch, soll der Take-off erreicht sein.
Und ab dann kam es zu einem rapiden Umschwung, in dem recht plötzlich ein System dann um ein Vielfaches intelligenter wird als die versammelte Menschheit und uns quasi überwältigen kann.
Und das ist dann auch eines der zentralen existenziellen Risiken, um die es in der existential risk Philosophie und die angeschlossenen politischen Bewegungen geht.
Eben diese Idee von, wir müssen jetzt schon mal aufpassen, wir müssen uns jetzt mit der Frage befassen.
Also es kommt auf jeden Fall, es sei denn die Ökos gewinnen oder es gibt einen Atomkrieg, aber sonst kommt es auf jeden Fall.
Und wir müssen jetzt schon mal dafür sorgen, dass diese Wesenheit, die da entstehen wird, uns irgendwie wohlgesunden ist und Rücksicht auf uns nimmt und es nicht irgendwie behandelt, wie wir das mit einem Ameisenhaufen machen, wenn wir irgendwo einen Parkplatz bauen wollen.
Genau. Und da haben wir dann auch wieder so einen Punkt, dass in diesem Utilitarismus dann plötzlich wieder so eine, diese Sache ist das maximale Risiko, aber auch das maximale Potenzial, was wenn man dann utilitaristisch rangeht, rankommt, ja gut, dann machen wir uns unsere Berechnungen kommen.
Die Sache, mit der wir uns jetzt hauptsächlich beschäftigen sollten, ist die Perspektive auf diese künstliche Intelligenz und das mit dem Klimawandel oder globaler Armut oder so, das ist ja jetzt alles nicht mehr so dringend.
Und was sind dann die Methoden, die vorgeschlagen werden, um sich diesem diagnostizierten Problem zu widmen? Also was, was werden da für Vorkehrungen getroffen?
Da gibt es einerseits, gibt es das Machine Intelligence Research Institute in Berkeley, das in Berkeley ist, aber nichts mit der Uni zu tun hat, das versucht, ja am Ende eigentlich Handlungstheorien zu mathematisieren.
Eine andere Idee wäre, dass man das nicht von vornherein festlegt, weil wir ja auch das Problem haben, dass wir ja vielleicht gar nicht in unserem, also dass eine der Grundtorpoi, die da wieder vorausgesetzt werden, ist, dass wir ja irgendwie defizitär sind.
Wir sind zu dumm, zu gemein, zu asozial und das ist ja gerade eine der Sachen, die Superintelligenz für uns regelt, wir sind ja eigentlich unmündig.
Und dann haben wir die Frage, könnten wir denn jetzt schon vielleicht das vorherbestimmen? Und dann gibt es die Idee der Extrapolated Volition, die dann darin besteht, dass man sagt, dass wir der Superintelligenz Möglichkeiten an die Hand geben sollten,
die Sachen für uns so zu entscheiden, wie wir es wollen und wünschen würden, wenn wir eben weiterentwickelt weiser und sozialer wären.
Die konkreten Überlegungen dazu, also es ist jetzt nicht so, dass Technologien in der Schublade liegen, denn dann kommen wir dann auf die ganz, ganz basale oder die sehr, sehr profane Ebene des Ganzen.
Das, was man jetzt als erstes mal tun kann, ist dem Machine Intelligence Institute Geld spenden.
Da kommen wir dann auf die Verknüpfung mit der Bewegung für effektiven Altruismus, in der das alles eine Rolle spielt, die man vielleicht da höre oder auch kennt, weil die ja global an Universitäten präsent ist.
Die besteht jetzt nicht nur aus Superintelligenz, Transhumanisten ist aber ursprünglich aus diesem Milieu entstanden.
Und die spielen da schon eine Rolle. Und dann haben wir dann diese Debatte, was ist denn jetzt der effektivste Altruismus?
Und dann dieses Argument von, das Maximum an altruistischem Handeln erreichen wir, wenn wir die positive Superintelligenz verwirklichen.
Wenn wir jetzt also vor der Frage stehen, ich habe hier irgendwie keine Ahnung, 500 Euro rumliegen, was mache ich damit?
Ich will was Gutes tun, ich kann die Oxfam geben oder irgendwer, der Brunnen in Afrika baut oder ich gebe dir den Miri.
Und das Miri sorgt dann dafür, dass die Superintelligenz positiv ausfällt auf noch nicht geklärten Wegen, weil das ist ja auch ein schwieriges Problem so.
Sprache, Begriffe und Ähnliches in mathematisierter Form darzustellen, da würde ich auch sagen, das ist so ein Grundproblem, das geht halt einfach nicht.
Das ist die ganze Zeit die Schwierigkeit. Deswegen ist das, was man jetzt erstmal tun könnte, wäre, diesen Organisationen zu spenden.
Da kommen wir dann auf sehr, sehr frappante Ähnlichkeiten zu religiösen Organisationen.
Das ist dann überhaupt was, was du ja auch behandelt, nämlich die Frage, inwiefern eben es sich da, es steckt im Titel deines Buches schon drin, um Formen der Mythologie handelt.
In Anlehnung an einen Journalisten, Mark O'Connell heißt er, fällt da dann auch der Ausdruck, materialistischer Mystizismus.
Wie macht sich das bemerkbar? Wo sind da die Anknüpfungspunkte zu mystischen Narrativen?
Ich habe mich ein bisschen von Walter Benjamin inspirieren lassen, eben der Idee, dass in der Säkularisierung da durchaus theologische Potenziale noch aufgehoben sind.
Und da würde ich sagen, dass dieser Transhumanismus mit dieser Kontrollidee, dieser Idee, das materielle Universum am Ende vollständig umzugestalten und fungibel zu machen,
weil darauf läuft es am Ende raus, dass Energie und Materie fungibel werden wie Computerprogramme.
Erkläre das nochmal kurz, weil ich glaube, das ist den Hörerinnen und Hörern wahrscheinlich nicht so geläufig.
Mit fungibel meinst du ja, dass es quasi eine Austauschbarkeit gibt oder eine Kompatibilität auch unter den Dingen letztlich.
Dass das eine in das andere transformierbar sein.
Dass das eine in das andere transformierbar sein, dass es sich beliebigen Zugriffen eben, wenn ich das eine beherrsche, kann ich beliebig auf das andere zugreifen.
Also wenn ich irgendwie einen Computer programmieren kann, kann ich quasi das Universum programmieren und vor allem eine vollständige Zugreifbarkeit.
Wo war denn der Frage dazu?
Inwiefern das quasi mit einem Mystizismus zusammenhängt.
Entschuldige, dass ich dich da unterbrochen habe.
Inwiefern das mit einem Mystizismus zusammenhängt.
Die Idee mit dem materialistischen Mystizismus von Marco Connell, das bezieht sich, der hat Interviews geführt mit verschiedenen Leuten,
die in verschiedenen Ecken der transhumanistischen Bewegung unterwegs sind.
Also auch nicht nur mit den KI-Superintelligenz Leuten, mit denen ich mich da beschäftigt habe.
Der hat benutzt das als Beschreibung für diese Idee des Geistes in Computer-Hochladens.
Der vordergründig radikal-materialistisch daherkommt, also etwas wie Seele oder es gibt keine Essenz oder irgendwie das Menschen, die über natürlich ist.
Dann aber auch annimmt, dass sie dann ja auch vollständig manipulierbar und in Daten erfassbar ist.
Und dass man quasi, da wird eigentlich eine ganz saloppe Analogie, liegt dem zu Grunde.
Also das Gehirn ist ein Computer und Geist ist ein Programm.
Und so wie ich, keine Ahnung, mein altes Super-Nintendo-Spiel auf meinem PC jetzt irgendwie emulieren kann,
indem ich quasi auf dem besseren Gerät eine Umgebung und Software vortäusche, die der Hardware des älteren Geräts entspricht,
kann ich das ja dann auch mit Geist machen.
Wenn wir erstmal richtig leistungsfähige Computer haben, bauen wir einfach Gehirne auf neuronaler Ebene in Code nach.
Und dann können wir Menschen in Computer hochladen.
Und das nennt er materialistischen Mystizismus, weil es eigentlich unglaublich eng an Vorstellungen von Seelen, Wanderungen und Ähnlichem liegt,
nur zumindest vordergründig radikal materialistisch daherkommt.
Zu den grundsätzlicheren religiösen Bezügen, die ich da sehe, da ist so ein grundsätzlicher Punkt eben der Annahme,
dass die moderne Naturwissenschaft geistesgeschichtlich christliches Denken zur Voraussetzung hat.
Zum Beispiel Klaus-Peter Ortlieb ist ein ehemals Leiter des Instituts für Mathematische Modellierung an der Uni Hamburg,
hat das dann in einigen Aufsätzen sehr schön nachgezeichnet, aber auch andere Autoren,
haben eben diese Art des Denkens über die Welt, die die Naturwissenschaft zur Voraussetzung hat,
die eben nicht nur nach dem unmittelbar Gegebenen fragt, sondern nach dem dahinterliegenden Prinzip, das mathematisierbar ist,
hat zur Voraussetzung eigentlich ein Denken, das eben Konstrukteur annimmt, das so eine Gemachtheit der Welt annimmt,
die nach bestimmten Prinzipien funktioniert, die erkennbar sind.
Das ist jetzt keine Fundamentalkritik an den Naturwissenschaften oder sonst irgendwas, die finde ich sehr gut,
nur es gibt diese Voraussetzung und das, was einem da begegnet, glaube ich, in diesen Sachen,
die rund um Superintelligenz und so weiter kreisen, ist eigentlich eine Aktualisierung dessen,
dass wir dieses Denken haben, dass sich um Konstruiertheiten und so weiter, dass er implizit irgendwie doch so eine Gottesfigur drin hat
und die dann aber daraus wieder konstruiert wird und dann landen wir bei diesen Superintelligenz-Sachen.
Noch mal sehr viel konkreter dann, wenn wir dann noch mal hinkommen, dass das alles ja auch noch utilitaristisch ist.
Im Utilitarismus, der kommt aus einer spezifisch christlich-kalvinistischen Tradition und hat aus dieser auch durchaus seine Sinnbestimmung,
was ein Grund ist, warum ich annehmen würde, dass sich die ganze Sache auch außerhalb des englischsprachigen Raums nie so durchsetzen wird,
weil da so gewisse, ich würde es jetzt mal kulturell nicht ganz nennen, aber gewisse geistesgeschichtliche Voraussetzungen drin sind,
die man eher mitgekriegt hat, wenn man in England oder in den USA aufgewachsen ist.
Und das Ganze, um auf Marco Connell zurückzukommen, findet man dann auch tatsächlich noch mal in der Person,
wenn er dann Menschen beschreibt, die in evangelikalen christlichen Haushalten aufgewachsen sind, da dann irgendwie eine Glaubenskrise hatten
und jetzt eben zutiefst davon überzeugt sind, dass sie Atheisten sind, weil sie nicht glauben, dass die Erde 5000 Jahre alt ist
und das Noah eine Arche gebaut hat, die aber immer noch völlig in diesem Denken einer am Ende regulierten Welt drin stecken.
Und die füllen dann quasi diese Lücke mit und die halt dann auch Sterblichkeit nicht konfrontieren, sich damit arrangieren,
was ja auch irgendwie ein Prozess ist, den man als Atheist vielleicht auch irgendwie durchmacht, mit Sterblichkeit überhaupt nicht zurechtkommen,
allerdings eben dieses Tragegerüst des festen Glaubens verlieren und dann eben daran kommen, okay, da muss ich das jetzt aber technologisch lösen
und dann auch sehr viel mehr bereit sind, an Narrative zu glauben, die technologisch daherkommen, aber andererseits auch diese Ängste bedienen.
Also das ist ja ein ganz, ganz zentrales Ding beim Transformatismus, dass man nicht mehr sterben muss, darum geht es auch immer.
Da geht es auch bei den anderen Transformatismen rum und also bei dem, wo es sich einfrieren lässt beispielsweise
und in diesem Superintelligenz-KI-Transformatismus dann eben diese Idee von, okay, wenn ich erstmal meinen Körper los bin, der zerfällt,
der ist schmuddelig, der kriegt Krankheiten, wenn ich erstmal ein Computerprogramm bin, dann habe ich Sterblichkeit zumindest vordergründig überwunden.
Und das sind für mich mythologische und zum Teil mystizistische Elemente.
Ja, also das eine hattest du jetzt gerade auch dann schon konkret eben mit dem Mind Uploading, der Whole Brain Emulation, wie es ja dann auch heißt, benannt.
Da wittert man quasi die Unsterblichkeit. Die andere Sache, die Aufgehobenheit, die du angesprochen hast, vielleicht der Vollständigkeit halber noch erwähnt,
findet sich dann zum Beispiel in sowas wie einer Nanny-AI eigentlich, oder?
Also im Grunde einer allumfassenden künstlichen Intelligenz, die für uns alle quasi die profane täglichen Belange regelt in einer scheinbar besseren Art, als wir es denn jemals könnten, oder?
Das ist dann das Prinzip des Singleton, wie du es benannt hast oder wie bzw. die Protagonistin dieser Denkschule das dann auch benennen.
Vielleicht kannst du das nochmal kurz definieren, was das ist, ein Singleton?
Ein Singleton ist tatsächlich ein Konzept von Nick Bostrom. Das ist eben eher vordergründig, ist es eine Weltregierung, die halt so die totale Kontrolle hat.
Und er nennt das Koordinierungsprobleme, also die Koordinierungsprobleme löst.
Koordinierungsprobleme sind für ihn sowas wie Politik, ganz allgemein. Also alles, was nicht die klarste technische Lösung ist, das wird gelöst, dadurch, dass wir eine möglichst rationale Weltregierung haben.
Und dieser Singleton, er hat diesen Begriff, der ist so ein bisschen Kuddelmuddel, weil da auf der einen Seite ist er so definiert, so ein Singleton könnte jetzt auch ein totalitärer Weltstaat sein oder eine Alienherrschaft oder was weiß ich, was das Einzige wäre, dass es nicht mehr in Frage gestellt wird.
Also es geht, am Ende geht es um totale Herrschaft. Und dann gibt es da noch dieses Ding, das da aber eigentlich immer mit die Superintelligenz gemeint ist.
Also eine Superintelligenz, die diesen Take-off der Intelligenz-Explosion hinter sich lässt, dann ganz schnell dahin kommt, dass sie eben durch menschliche Manöver gar nicht mehr in Frage gestellt werden kann.
Und diese totale Herrschaft ist für ihn auch ein ganz wichtiger Punkt. Daher wird er oft missverstanden. Man liest seine Bücher, hört seine Vorträge und dann kommt da diese totale Weltherrschaft vor und man denkt, das muss ja irgendwie was Negatives sein.
Vordergründig warnt er auch davor. Also es ist schon ein existenzielles Risiko, wenn dieser Singleton dann eben unsere Menschen nicht für wertvoll erachtet und eben planiert wie Ameisen.
Aber eigentlich, wenn wir raus wollen auf dieses, was uns ja moralisch verpflichtet, das Realisieren der bestmöglichen Zukunft, dann müssen wir ganz ganz schnell hinkommen zu diesem Singleton, und zwar einem positiven.
In dem Moment, das ist dann auch eine der Sachen, die ich an der ganzen Nummer so schwierig finde, wo es dann eben auch die Dimension von, ja gut, das ist jetzt alles ein bisschen schrullig, aber auch vielleicht interessante Gedankenspiele verlässt.
Die haben diesen Herrschaftsanspruch, ein legitimatorisches Narrativ und es ist total hip bei den gerade neu aufkommenden Eliten. Also das finde ich sehr schwierig.
Eigentlich artverwandt mit diesem Thema der Frage der Positionierung des Menschen gegenüber der technischen Entwicklung und ob die als, ob man selbst, also der Mensch an sich quasi jetzt dann als defizitär empfunden wird oder nicht.
Daran angehängt ist vielleicht auch die Frage nach dem Bild von Intelligenz, das da unterstellt wird im Thema der Superintelligenz und du hattest das dann an anderer Stelle auch so dargestellt,
als ob dieses Denken in Superintelligenzen dann auch eine Art von, das heißt die Superintelligenz eine Art von verkörpert perfekter Rationalität ist, die dem Menschen eben abgesprochen wird.
Also da wird dann quasi wieder dieser Vergleich aufgemacht. Der Mensch, das zeige ja zum Beispiel die Verhaltensökonomie. Der Mensch, der sei eben nicht perfekt, der Handel immer wieder durch seine Emotionen bedingt ja eigentlich fehlerhaft und nicht rational.
Und man müsse jetzt quasi, um diesem defizitären Status zu entsteigen, eben quasi sich anderer Mittel bedienen, zum Beispiel der Superintelligenz. Was ist denn das Verständnis von Rationalität, was dem zugrunde liegt? Was ist, was gilt da als Rationalität?
Da würde ich es jetzt spezifisch eingrenzen auf die Rationalisten bzw. richtigerweise Neorationalisten. Das ist dann auch ganz spannend, weil es ein Verbreitungsvektor transhumanistischen Denkens, der auch eng verknüpft ist.
Also hat denselben Gründe, ist dasselbe Milieu wie dieses Machine Intelligence Research Institute, ist die neorationalistische Bewegung im Internet, die erstmal ansetzt bei diesen, bei den Erkenntnissen aus Verhaltensökonomik und Ähnlichem, dass wir eben halt nicht perfekt rationale Agenten sind.
Das ist ja durchaus auch in den Wirtschaftswissenschaften irgendwie Gegenstand von Debatten, also ich meine sogar nach wie vor, ob nicht dann die Modelle überarbeitet werden müssen, wenn Menschen am Markt gar nicht perfekt rationale Agenten sind.
Da wird halt die umgekehrte Schlussfolgerung gezogen. Wie können wir das werden? Und das hat erstmal so eine Selbsthilfedimension, dass man also versucht, das eigene Denken zu schulen, um rationaler zu sein.
Auch jetzt erstmal keine schlechte Idee, es ist wie bei, also ich finde auch so eine Sache, die sich da immer irgendwie durchzieht, es ist irgendwie dann immer die Urausführung, die so ein bisschen fragwürdig ist.
Man will das da nämlich erreichen, das ist ja jetzt auch nicht so, dass es nicht Systeme in der Gesellschaft gibt, die es einem ermöglichen, rationaler zu denken.
Man kann in die Schule gehen, man kann studieren, man kann sich mit der Geschichte der Philosophie beschäftigen usw.
Man kann sich vor allem klar machen, dass man eben fehlbar ist in seinen Entscheidungen und die hinterfragen das alles nicht, sondern man soll eine neue Form von Rationalität erlernen, die orientiert sich an der welchen Statistik.
Die Grundannahme ist, geist und menschliches Denken ist eigentlich eine ganz einfache Sache, das ist irgendwie so ein sequenziell belschte Fragestellung, lösen der Rechen-OP-Mechanismus.
Wenn man sich das bewusst macht und Verhedderungen vermeidet, dann wird man rationaler.
Dann gibt es diese völlig skurrilen Sachen rund um das Center for Applied Rationality, also für angewandte Rationalität, das einem beibringen soll, so zu denken, was einem ein Denken beibringen soll, was an der Funktionsweise von KI-Systemen orientiert ist.
Und was sich durch das alles schon ganz erkennbar durchzieht, ist ein sehr, sehr enger Begriff von Rationalität, der sich eben stark an ökonomischem Erfolg ausrichtet und auch das Ganze nicht mehr hinterfragt.
Man ist weit davon entfernt, so etwas aufzumachen, wie man es jetzt beispielsweise aus einer kritischen Theorie kennt, die Einteilung in eine subjektive und objektive Vernunft.
Und das eine ist die Vernunft des Zweck-Rational-Instrumentelle und das andere ist eine Vernunft, die die Frage stellt, sind die Ziele, die wir uns setzen, ist die Gesellschaft, wie wir sie einrichten, ist das überhaupt vernünftig?
Die zweite Art geht gar nicht, das macht die Superintelligenz dann irgendwann.
Und da kommen wir dann auch in den Bereich, wo ich Nähe zu faschistischem Denken sehe, bei dem Intelligenzbegriff, weil die Intelligenz ist das, was der EQ-Test misst.
In der Gedankenwelt, aus der sich der Transhumanismus entwickelt.
Und das ist ja auch eine numerische Größe, die kann größer oder kleiner sein, dann ist auch klar, wenn wir eine Maschine bauen können, die das auch irgendwie mehr hat, dann ist sie auch besser.
Und dadurch ziehen sich dann organische Ideen, die um Intelligenzoptimierung kreisen, weil wir hätten ja auch ein großes existenzielles Risiko in dem Moment, wo sich zu viele dumme Leute vermehren.
Das wäre etwas, wo man aufpassen müsste.
Das ist auch so ein Punkt, wo ich mich dann sehr wundere, warum das ganze Zeug so wenig skandalisiert wird, weil es ist ja jetzt eigentlich keine salonfähige Position.
Und die steht zwar nicht im Vordergrund der Überlegung, aber zum Beispiel durch das Werk von Nick Bostrom zieht sich das an mehreren Stellen.
Also Intelligenz ist eng begriffen, Rationalität ist eng begriffen.
Eine Diskussion, eine philosophische, wird eigentlich vermieden, was das sein soll.
An den Stellen, wo was dazu gesagt wird, ist es irgendwie ein rationaler Agent, hat was mit welcher Statistik zu tun und im Zweifelsfall ist das halt einfach das, was der EQ-Test misst.
Und das ist eben also jetzt nicht nur quasi irgendeine Splitter-Position innerhalb irgendeines Subforums von, wie heißt nochmal dieses Rationality-Wiki?
Das gibt es auch.
Less wrong heißen die.
Less wrong, das ist quasi nicht nur da eben so eine kuriose Sonderposition, sondern das ist was, was du durchaus auch eben bei Nick Bostrom im geschriebenen Wort und gesagtem Wort vor Ort ist.
Bei Nick Bostroms Buch über Superintelligenz ist es zum Beispiel, fasst er das schon auf den ersten paar Seiten als optimaler belgischer Agent.
Da können wir Intelligenz äußerlich bestimmen.
Da brauchen wir nicht.
Und auch diese Schlussfolgerung, dass quasi daraus eigentlich eugenische Handlungen zu setzen sind in irgendeiner Form, wird implizit da dann dann mitgegeben?
Also ich weiß nicht, ob man das in Superintelligenz, also es gibt einerseits diesen Aufsatz rund um existenzielle Risiken.
Da kann man das eigentlich nicht implizit nennen.
Das folgt dann nicht aus diesem aus der spezifischen intelligenten Definition, aber da taucht als eines der eines der Risiken, die wir auf die wir achten müssen, die da verhindern können, dass wir dieses Himmelreich erreichen.
Das Erreichen des Technologischen ist neben Meteoriteneinschlägen, böser Superintelligenz, der globalen Dominanz einer ökologischen Bewegung, auch die dysgenischer Druck, also die Generierung quasi,
durch die, weil Menschen dazu neigen, zu viel Nachkommenschaft zu produzieren, gerade auch die falschen Leute, und man halt dann zu einem Homophiloprogenitus, also das sich fortpflanzenliebend degenerieren würde.
Was dann auch, weil wir dann nicht mehr genug intelligente Leute haben, den Weg in das technologische Paradies verstellt.
Im Superintelligenz Buch werden eugenische Maßnahmen als einer der Wege überhaupt dahin zu kommen, dass wir solche superintelligenten Maschinen haben verhandelt.
Eine Sache, die ich jetzt beschrieben habe, war der Take-off. Es gibt aber auch ein einziges Unterkapitel zu biologischer Kognition, da geht es dann um IQ, und dann, wie wir mit ganz einfachen Maßnahmen dafür sorgen, dass sich besonders kluge Leute und besonders klugen Leute fortpflanzen,
da so eine Dimension haben, weil wir das am besten in vitro machen, eine Dimension von Selektion drin haben, und dann eben in sehr wenigen Generationen dazu kommen, dass wir durch genetische Selektion erstmal eine Elite an Forschern aufbauen, die in der Lage sind, dann uns dahin zu bringen, dass wir die nötigen technologischen Schritte machen können.
Also, das steht nicht im Vordergrund, aber ist jetzt auch nicht, dass da was implizit ist.
Wow, wild. Okay, da muss ich dann jetzt nochmal bei dem, du hattest das zwar eh schon angesprochen, aber da muss ich jetzt trotzdem nochmal nachhaken, wie einflussreich ist denn transhumanistisches Denken und was bedeutet das dann quasi für uns.
Ich meine, in dem, was du gerade gesagt hast, schimmert es ja schon durch, aber also Ray Kurzweil, du hattest ihn angesprochen, ist bei Google, ich kann mich erinnern, der leitet da glaube ich jetzt auch irgendeine Art von internem Think Tank oder sowas.
Peter Thiel, vielleicht für die Hörerinnen und Hörer, der hat mit Elon Musk zusammen PayPal gegründet und Palantir auch, nicht mit Elon Musk zusammen, aber Palantir ist eine Daten-Harvesting-Maschine, die auch mit Geheimdiensten zusammenarbeitet, mit dem Pentagon und so weiter.
Also der ist auf jeden Fall auch eine schillernde Figur, der, versteht er sich auch als transhumanist?
Ich bin mir nicht ganz sicher, ob es irgendwie ein Zitat von ihm gibt, dass er sich explizit selber als transhumanist bezeichnet, aber er fördert Projekte in dieser Richtung.
Er fördert vieles, aber er ist, wenn man sich dann in die entsprechenden Communities und so weiter begibt, ist auf jeden Fall immer, ist ein Ziel, das diskutiert wird, kann ich fördern, ist so eine Möglichkeit, wenn man so ein Projekt hat, das irgendwie so in einem transmonistischen Umfeld, ob man nicht von Peter Thiel auch irgendwie Förderung kriegen könnte.
Stimmt, das kann ich mich erinnern, da habe ich auch mal bei einem Podcast gehört, dass das diskutiert wurde.
Gut, also die Frage wäre, für wie einflussreich hältst du das, sagen diese Formen des Denkens und was bedeutet das für uns?
Ja, da stellt sich dann die Frage, deswegen sprach ich ja schon am Anfang von einem Mainstream, es gibt offensichtlich sehr einflussreiche Leute, die diesem Gedanken gut mindestens nahe stehen oder die große Möglichkeiten haben, auch Einfluss auf die Gesellschaft zu nehmen, weil sie eben über sehr viel Geld verfügen.
Und es hat jetzt in letzter Zeit, also es ist jetzt gerade passiert, es ist eine spannende Sache, was daraus wird, es ist ja jetzt Brexit und Boris Johnson ist Premierminister von Großbritannien und er hat ja diesen auch etwas schillernden Berater Dominic Cummins, der in der transhumanistischen Szene vernetzt ist und der in seiner Zeit schon mal im Bildungsministerium einen recht langen Aufsatz gemacht hat.
Er hat uns geschrieben, einer der Kernaussagen war, dass es bei dem Erfolg von Schülern wesentlich mehr auf ihr genetisches Potenzial ankäme als auf die Umstände ihrer Bildung.
Genau, der ist im Beraterstab des Premierminister von Großbritannien und hat kürzlich auf seiner Webseite Stellengesuche veröffentlicht, bei denen es darum geht, die Verwaltung der englischen Regierung zu optimieren, indem man sich eben Data-Scientists, Leute aus der KI-Programmierung, Unternehmer und was weiß ich was ins Boot holt, eben all diese Personen, die in der Transhumanistischen Szene vernetzt sind.
Dass die in der Transhumanismus so ein bisschen fundierenden, weltsichtentscheidenden Personen sind und da ist die Frage, wo ich jetzt mit etwas Spannung gucke, ob es tatsächlich passiert, dass sich in England transhumanistische Kreise in der Regierungsverwaltung etablieren.
Also das ist nicht ausgemacht, das ist eine völlig chaotische Situation, war aber auf jeden Fall auch so eine Sache, wo ich mir gedacht habe, hui, steht man da vor Stellengesuchen für die Downing Street Number Ten, die so einen transhumanistischen Vibe haben.
Und das ist aber nicht nur jetzt, weil Data-Scientists gibt es ja überall, also das ist ja jetzt noch nicht transhumanistisch irgendwie, da gab es da noch andere Hinweise, die jetzt quasi konkret in diese Richtung deuten, weil jetzt Big-Data-Nutzung gilt ja jetzt quasi schon als, muss man quasi drauf schreiben.
Ich habe jetzt hier die Stellenausschreibung. Das erste Zitat der Stellenausschreibung, also an erster Stelle wird Elisabeth J. Kauski zitiert als KI-Experte, das ist der Leiter des Machine Intelligence Research Institutes.
Das heißt, das hat eine spezifische Richtung aus, wer da abgefragt wird. Und das würde ich schon als einen, also es würde nicht schaden auf diese Stellenausschreibung zu antworten und man hätte schon seine Credentials als Transhumanist, sagen wir es so.
Max, als letztes stelle ich immer noch die Frage, wenn du dir Zukunft vorstellst, was stimmt dich freudig?
Was stimmt mich freudig? Ich sehe gerade, also tatsächlich interessiert mich gerade diese globale Welle, die es ja jetzt irgendwie im letzten Jahr gab von basisdemokratischen Protestbewegungen gegen ökonomische Ungleichheit,
die sich auch in Ländern abgespielt haben, wo man tatsächlich erwartet würde, das landet alles in irgendeinem Islamismus oder was weiß ich was oder in irgendwelchen anderen autokritären Ideologien.
Und dass es zumindest schon mal so aussah, als würde das nicht passieren und das lässt mich mit einer gewissen Erwartung darauf blicken, ob sich nicht die ja doch sehr solide wirkenden Vergesellschaftungsformen der Globalisierung auf so einer wirtschaftlichen Ebene
radikal in Frage stellen lassen. Es gibt ja von Mark Fischer diese Formulierung über capitalist realism, von David Graeber diese Formulierung, dass wir eigentlich für Zukunft wieder kämpfen müssen, Zukunft zu eröffnen, sowas wie Zukunft zu haben.
Und ich glaube, dass da in nächster Zeit sehr viel passieren wird, was den Blick auf Zukunft wieder öffnet, weil sich Möglichkeitenräume plötzlich eröffnen in kollektivem Handeln.
Wunderbar. Das ist doch ein schöner Ausblick. Max, vielen Dank für das Gespräch.
Ich danke dir, dass ich bei dir sein durfte.
Was ihr zu dem Ganzen denkt und wie euch diese Folge hier gefallen hat, unbedingt gut bewerten auf allen Podcast Plattformen, die ihr nutzt.
Für unsere Patreon Unterstützerinnen und Unterstützer gibt es auf www.patreon.com schrägstrich future histories vieles an Zusatzmaterial.
Da könnt ihr also auch vorbeischauen. Bis zum nächsten Mal. Ich freue mich.
