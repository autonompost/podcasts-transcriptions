1
00:00:00,000 --> 00:00:07,000
Welcome to Future Histories. My name is Jan Groos and I am very pleased to welcome Moira Weigel as today's guest.

2
00:00:07,000 --> 00:00:12,000
Moira is an author, researcher and co-founder of Logic magazine.

3
00:00:12,000 --> 00:00:21,000
She currently holds a position as a socio-technical security researcher with the independent research organization Data Society

4
00:00:21,000 --> 00:00:29,000
and will take up a position as an assistant professor of communication studies at Northwestern University in Boston next year.

5
00:00:29,000 --> 00:00:36,000
There are so many interesting works by Moira that we will unfortunately not be able to cover them all today.

6
00:00:36,000 --> 00:00:42,000
Her book, Labor of Love, The Invention of Dating, has been published in 2016.

7
00:00:42,000 --> 00:00:49,000
And her most recent book, which she wrote together with Ben Tarnoff, is called Voices from the Valley.

8
00:00:49,000 --> 00:00:53,000
Tech workers talk about what they do and how they do it.

9
00:00:53,000 --> 00:00:58,000
However, today's interview is going to be about another work from Moira's rich oeuvre.

10
00:00:58,000 --> 00:01:05,000
The essay Palantir goes to Frankfurt School and I'm very excited to talk about it with Moira today.

11
00:01:05,000 --> 00:01:11,000
But before we start, I'd like to thank Fabian, Adrian and Carmen for their donations.

12
00:01:11,000 --> 00:01:15,000
And I would like to welcome Christine as a patron of Future Histories.

13
00:01:15,000 --> 00:01:17,000
Thanks a thousand times.

14
00:01:17,000 --> 00:01:27,000
And now please enjoy today's episode with Moira Weigel about Palantir, tech nationalism and aggression in the life world.

15
00:01:32,000 --> 00:01:34,000
Welcome, Moira.

16
00:01:34,000 --> 00:01:36,000
Thank you so much for having me.

17
00:01:36,000 --> 00:01:41,000
And I should say the book about dating, if anyone's interested, is out in German, actually.

18
00:01:41,000 --> 00:01:45,000
It's called Dating eine Kulturgeschichte in Germany.

19
00:01:45,000 --> 00:01:48,000
But anyway, thank you so much for having me.

20
00:01:48,000 --> 00:01:54,000
Moira, I got hooked as soon as I saw that your essay was about the ideological roots of Palantir.

21
00:01:54,000 --> 00:01:56,000
But not everybody knows this company.

22
00:01:56,000 --> 00:02:02,000
Maybe you could start by describing what Palantir is and what they do.

23
00:02:02,000 --> 00:02:03,000
Yeah, absolutely.

24
00:02:03,000 --> 00:02:11,000
So Palantir is a software company, an enterprise software company that specializes in big data analytics.

25
00:02:11,000 --> 00:02:21,000
And so what that means effectively is that they develop software for large organizations like the US Department of Defense, for example,

26
00:02:21,000 --> 00:02:32,000
but also for banks and large businesses to help them interpret their data, control who accesses data, how, visualize it, and so on.

27
00:02:32,000 --> 00:02:34,000
There are three big parts of the business.

28
00:02:34,000 --> 00:02:41,000
One is called Palantir Gotham, and that mostly works on defense, government contracts, counterterrorism.

29
00:02:41,000 --> 00:02:48,000
One part is called Palantir Metropolis, and they work with hedge funds and banks and businesses.

30
00:02:48,000 --> 00:02:56,000
And one part is called Palantir Foundry, which is used by other corporations like Airbus, for instance.

31
00:02:56,000 --> 00:02:58,000
There's so much more to say about it.

32
00:02:58,000 --> 00:03:03,000
You know, it's founded by Peter Thiel, Alex Karp, Joe Lonsdale, and Stephen Cohen,

33
00:03:03,000 --> 00:03:12,000
this group that comes out of Stanford and was sort of connected through what's sometimes called the PayPal mafia, the company PayPal.

34
00:03:12,000 --> 00:03:20,000
In the early 2000s, it's founded in 2003 and gets its first big funding from Incutel,

35
00:03:20,000 --> 00:03:26,000
which is the counterintelligence agency, CIA's venture capital arm in the United States.

36
00:03:26,000 --> 00:03:31,000
So it's founded in this moment of the beginning of the war on terror in the United States.

37
00:03:31,000 --> 00:03:36,000
But anyway, I'll wait and answer whatever questions you have about it.

38
00:03:36,000 --> 00:03:42,000
But the reason I say all this is I think it's important to understand about them that they're an enterprise software company.

39
00:03:42,000 --> 00:03:51,000
They are not a company like Google or Facebook, whose business model is to gather data about individuals and develop ads and predictions.

40
00:03:51,000 --> 00:03:58,000
They are a company that wins large contracts from large organizations to interpret data partly through humans.

41
00:03:58,000 --> 00:04:00,000
It's not all automatic.

42
00:04:00,000 --> 00:04:07,000
And so I think I say that because I think that difference is very important to their ideology and to the kind of power they represent.

43
00:04:07,000 --> 00:04:15,000
And I think it's it's a difference that has gotten lost in in some of the media coverage of Palantir in the past.

44
00:04:15,000 --> 00:04:20,000
So I'm happy to say more about it. But that's that's my broad overview of what Palantir is.

45
00:04:20,000 --> 00:04:26,000
In your essay, you closely analyze Alex C. Carp's dissertation Aggression in der Lebenswelt.

46
00:04:26,000 --> 00:04:29,000
Alex Carp is the CEO of Palantir.

47
00:04:29,000 --> 00:04:36,000
And I would like to read a small section from the dissertation you also quoted in your essay on Palantir.

48
00:04:36,000 --> 00:04:44,000
So the quote goes, This work began with the observation that many statements have the effect of relieving unconscious drives,

49
00:04:44,000 --> 00:04:50,000
not in spite, but because of the fact that they are blatantly irrational.

50
00:04:50,000 --> 00:04:53,000
That sounds highly relevant for our societies today, I think.

51
00:04:53,000 --> 00:05:00,000
What is Carp's dissertation about and why is it relevant to an understanding of algorithmic governmentality?

52
00:05:00,000 --> 00:05:03,000
It's a really rich question that you ask.

53
00:05:03,000 --> 00:05:08,000
And I just I want to say sort of two quick things before I dive into it.

54
00:05:08,000 --> 00:05:13,000
I think, you know, I study, I'm trained in studying philosophy and the history of ideas.

55
00:05:13,000 --> 00:05:17,000
And we all like to think philosophy and the history of ideas matters a lot.

56
00:05:17,000 --> 00:05:26,000
Right. But I think I just want to say I think this question of how Carp specific philosophy or Peter Thiel specific philosophy

57
00:05:26,000 --> 00:05:31,000
actually comes into play in a company like Palantir is is complexly mediated.

58
00:05:31,000 --> 00:05:36,000
I mean, of course. But I think there's something I worry about in my own work, frankly.

59
00:05:36,000 --> 00:05:45,000
I worry about this as I as I do what I do, where there's such strong myths of genius in Silicon Valley,

60
00:05:45,000 --> 00:05:52,000
this idea that these sort of genius men just come and and, you know, manifest their ideas

61
00:05:52,000 --> 00:05:55,000
and these totally new and futuristic kinds of software.

62
00:05:55,000 --> 00:05:59,000
And I think that's a myth. I think there's it's important to consider the possibility

63
00:05:59,000 --> 00:06:04,000
that Carp was just like Peter's friend who was around when it was time to make this company.

64
00:06:04,000 --> 00:06:08,000
And and it used software and procedures from PayPal.

65
00:06:08,000 --> 00:06:14,000
But it's like there was some stuff that worked at PayPal and there was all this money going into counterterrorism in the U.S. government.

66
00:06:14,000 --> 00:06:17,000
I say all this to to check.

67
00:06:17,000 --> 00:06:26,000
I don't want to overstate the importance of his philosophy, which I think if folks happen to have watched any of the marketing materials

68
00:06:27,000 --> 00:06:32,000
that Palantir puts out around there around there going public, which happened in September 2020,

69
00:06:32,000 --> 00:06:40,000
that is a kind of advertising ploy that they use him for quite a lot, that he's the philosopher, you know, the sort of deep thinker.

70
00:06:40,000 --> 00:06:46,000
And and to put it kindly, I think there are good reasons he didn't go into academia.

71
00:06:46,000 --> 00:06:54,000
But but I think so I did all that to say I think there's this complicated and really fascinating question of like,

72
00:06:55,000 --> 00:06:57,000
how do ideas get translated into technologies?

73
00:06:57,000 --> 00:07:00,000
And it's not a direct not a direct process.

74
00:07:00,000 --> 00:07:09,000
And I think we can think, too, about the technical like how ideas come to shape how actual technologies are deployed.

75
00:07:09,000 --> 00:07:16,000
And we can think, too, about kinds of organizational practices within the firm and what sort of the sociology of the firm

76
00:07:16,000 --> 00:07:21,000
and how that might or might not be influenced by things that the founders believe.

77
00:07:21,000 --> 00:07:28,000
But I think at a very broad level, one thing that was initially interesting to me about the dissertation or grab my attention

78
00:07:28,000 --> 00:07:34,000
is that it seems like almost too close and almost shockingly close analog for what the company does.

79
00:07:34,000 --> 00:07:45,000
Right. The dissertation uses this concept of jargon from Adorno to ask how, you know, again, really oversimplifying.

80
00:07:45,000 --> 00:07:52,000
But Adorno has this reading of Heidegger's language where he says that an existentialist language broadly where Adorno says

81
00:07:52,000 --> 00:08:02,000
these kinds of terms that are that are popular in existentialist language like dwelling and home and being are actually a kind of mystification.

82
00:08:02,000 --> 00:08:11,000
And what is hidden in those terms is are all these realities of modern domination and violence and exploitation.

83
00:08:11,000 --> 00:08:18,000
And so for it to pick just one example for Adorno, when Heidegger talks about dwelling and dwelling and being

84
00:08:18,000 --> 00:08:28,000
and the sort of precarity of being for Adorno says, you know, this is a mystification of the fact, and I'm simplifying,

85
00:08:28,000 --> 00:08:35,000
but it's sometimes useful to simplify Adorno, that in a modern society, you know, all kinds of people are at risk of being homeless

86
00:08:35,000 --> 00:08:43,000
and precarious and losing their homes, losing their jobs, being actually exposed to to second nature, if not nature.

87
00:08:43,000 --> 00:08:54,000
And and Heidegger's language sort of wraps this up in a in a misty sheen and makes it seem like some sort of spiritual transhistorical unchangeable thing.

88
00:08:54,000 --> 00:08:57,000
Right. So Adorno has this analysis of language.

89
00:08:58,000 --> 00:09:08,000
Karp wants to take this concept and say, how can we look at specific language acts and see, take them apart,

90
00:09:08,000 --> 00:09:15,000
see how language works in particular instances to find not realities of social domination,

91
00:09:15,000 --> 00:09:23,000
you know, what Adorno would call objective forms of social domination, but rather unconscious aggression that people can't admit.

92
00:09:23,000 --> 00:09:30,000
The case study that Karp uses for this, which may be more known to a German speaking audience,

93
00:09:30,000 --> 00:09:37,000
is the speech that Martin Weiser gave in 1996 in Frankfurt when he got the prize of the German book trade,

94
00:09:37,000 --> 00:09:44,000
which is this famous or notorious speech where Martin Weiser says, in short, you know,

95
00:09:44,000 --> 00:09:49,000
all of you listeners expect me to get up and apologize for the Holocaust, but I'm not going to.

96
00:09:49,000 --> 00:09:53,000
And this is kind of sanctimonious and insincere.

97
00:09:53,000 --> 00:09:59,000
And I think he calls it the Holocaust pletigo or something that there are these pressures to be sort of preachers of disaster.

98
00:09:59,000 --> 00:10:01,000
And I'm not going to do it.

99
00:10:01,000 --> 00:10:09,000
And this, of course, is a famous episode in the cultural history of Germany in the 1990s and debates about political correctness and historical memory.

100
00:10:09,000 --> 00:10:21,000
But Karp wants to take this language act and ask what kinds of unconscious aggression or violence are expressed in this language act.

101
00:10:21,000 --> 00:10:29,000
We might say it's maybe not that subtle, actually, but Karp does a reading of Weiser where he says,

102
00:10:29,000 --> 00:10:39,000
you know, we can follow the linguistic moves he makes about his speech and see how his language expresses an unconscious aggression,

103
00:10:39,000 --> 00:10:47,000
you know, an anger at being made guilty, collectively guilty, having to remember the Holocaust publicly that he can't say directly.

104
00:10:47,000 --> 00:10:50,000
He says indirectly through these ways.

105
00:10:50,000 --> 00:10:55,000
And one thing that's amazing about it is Karp then steps back and makes no normative assessment of this at all.

106
00:10:55,000 --> 00:10:57,000
He just leaves it. He sort of says, well done.

107
00:10:57,000 --> 00:11:00,000
It was a good, an effective piece of jargon.

108
00:11:00,000 --> 00:11:05,000
But I think so what's quite interesting inside this is a very long answers.

109
00:11:05,000 --> 00:11:15,000
But what's quite interesting is that when I first look at it, at this dissertation, I think this is almost too perfect as like an analogy to what the work of data analytics is.

110
00:11:15,000 --> 00:11:26,000
Right. Like what Palantir does is analyzes these or builds software to analyze and make sense of all this dispersed data that purports to reveal.

111
00:11:27,000 --> 00:11:39,000
Dangers of terrorism or, you know, much more banal things for their corporate clients like, you know, which airplane door is going to have a problem with it coming out of this factory, something like that.

112
00:11:39,000 --> 00:11:43,000
But the majority of their business, at least until recently, has been for government.

113
00:11:44,000 --> 00:11:58,000
So we can say, you know, what the company does is analyze large data sets and actually, in a sense, try to build diagnostic tools that make visible different aspects of relationships within data.

114
00:11:58,000 --> 00:12:08,000
So to the extent that we can think of big data as a kind of unconscious, which I think is its own philosophically complicated question, which we can get into.

115
00:12:08,000 --> 00:12:17,000
But I think that there is a kind of suggestive resonance or analogy between between the dissertation and the work Palantir does.

116
00:12:17,000 --> 00:12:30,000
And if his dissertation basically is saying, you know, all social groups are formed through unconscious violence, looking at jargon lets us see how if we wanted to be reductive, we could say Palantir assumes, you know, the world is

117
00:12:30,000 --> 00:12:41,000
driven by terrorism and kinds of violence. That's a fact. And this company is going to build tools to let the U.S. Department of Defense see how and that's a simplification.

118
00:12:41,000 --> 00:12:49,000
But I do think that there is a striking analogy between that academic work and then what the company actually does.

119
00:12:49,000 --> 00:13:00,000
I think the question that I was asking myself is, is there a fundamental difference in how it is being done in Palantir than how it is being done with Google?

120
00:13:00,000 --> 00:13:16,000
Because they just to pick some other huge company, you know, because in the way that that Alex Karp is trying to present himself in the public sphere, he's always acting as if they are doing these things differently and that the others are, you know, kind of harshly

121
00:13:16,000 --> 00:13:26,000
going over this nuances of how you should actually handle data and what you can actually understand through the analysis of data.

122
00:13:26,000 --> 00:13:33,000
And I mean, in the way he's presenting himself, he's acting as if they are kind of doing it fundamentally different.

123
00:13:33,000 --> 00:13:49,000
But as just as far as I understood it from the information that I got, I kind of got the impression that they are more or less kind of acting in a similar way when it comes to the presumed production of truth.

124
00:13:50,000 --> 00:13:53,000
Yeah, that's a great, a great question.

125
00:13:53,000 --> 00:13:57,000
I think there's both an empirical and a philosophical question.

126
00:13:57,000 --> 00:14:04,000
And your question, I will start with the empirical and see if I can work out to the philosophical.

127
00:14:04,000 --> 00:14:10,000
So I think there actually are very important differences between Google and Palantir.

128
00:14:10,000 --> 00:14:15,000
And I think since a theme of this, this program is hegemony and power.

129
00:14:15,000 --> 00:14:27,000
I'm keen to talk about them because I think it's something that's not very well understood, but it's going to be really important to understandings for techno power going forward.

130
00:14:27,000 --> 00:14:32,000
We can think of the business model, you know, to be very vulgar materialist about it.

131
00:14:32,000 --> 00:14:34,000
Let's think of the business model.

132
00:14:34,000 --> 00:14:36,000
How does Google make money?

133
00:14:36,000 --> 00:14:44,000
Google mostly makes money through search and keywords and through advertising through targeted advertising.

134
00:14:44,000 --> 00:15:03,000
And so what Google does for the most part is that, you know, what they are incentivized to do through their business model is to collect as much information as possible about individual users so that, you know,

135
00:15:03,000 --> 00:15:12,000
They predict very specifically when I search for baby diapers, you know, that this is the kind of baby diaper I want and I will buy it or something.

136
00:15:12,000 --> 00:15:14,000
You know, this is the ad tech model.

137
00:15:14,000 --> 00:15:17,000
There's a whole other question of how good ad tech actually is.

138
00:15:17,000 --> 00:15:21,000
I've just published a book through Logic magazine.

139
00:15:21,000 --> 00:15:23,000
Not that I wrote that Tim Huang wrote.

140
00:15:23,000 --> 00:15:29,000
Someone else wrote called, in fact, I have it right here, but I guess it's a podcast, so no one can actually see me.

141
00:15:29,000 --> 00:15:45,000
Call the subprime attention crisis, which is by someone who used to work at Google and is arguing that actually the value of the attention that companies like Facebook and Google sell has been hugely inflated and is going to crash and cause systemic economic problems like the housing

142
00:15:45,000 --> 00:15:57,000
But so the Google business model is premised on and incentivizes Google's gathering as much information as possible about us as individuals and about other matters.

143
00:15:57,000 --> 00:16:02,000
We think about, you know, Facebook to Facebook is obsessed with what they call time spent.

144
00:16:02,000 --> 00:16:10,000
They're incentivized to keep everyone in their application for as long as possible so they can gather as much data as possible to spin into these ad tech products.

145
00:16:10,000 --> 00:16:23,000
I don't have the figures ready to mine, but like 90 to 100% of their revenue I want to say like 97% of Google's revenue is still ad tech, even though they do all these other things now too.

146
00:16:23,000 --> 00:16:26,000
Palantir is a very different business model.

147
00:16:26,000 --> 00:16:33,000
That's not how Palantir makes money, gathering information about individuals and selling it to other other people.

148
00:16:33,000 --> 00:16:51,000
Palantir makes money because they go, let's say, to the US military and say to the US military, look, you have at this point, nearly 20 years of, I'm making this up, I don't know if this is a real example, but 20 years of camera footage of all these places in Iraq and Afghanistan that you invaded.

149
00:16:51,000 --> 00:16:54,000
You have all this battlefield data.

150
00:16:54,000 --> 00:16:59,000
It's a mess. It's not cleaned up. It's not able to be interpreted very easily.

151
00:16:59,000 --> 00:17:14,000
And so what we're going to do for, I think their most recent contract with the US military is something like $850 million over 10 years, is we are going to work with you, and we will send engineers like consultants to work with you, to build the best of the best of the best.

152
00:17:14,000 --> 00:17:30,000
The US military is something like $850 million over 10 years, is we are going to work with you, and we will send engineers like consultants to work with you, to build software systems for your company that help you make sense of that data.

153
00:17:30,000 --> 00:17:33,000
That's what Palantir does. That's how they make their money.

154
00:17:34,000 --> 00:17:49,000
There was this debate I saw among many investors about whether they were overvalued because actually their products rely on a lot of human labor, like a lot of what they're selling are these, what they call forward deployed engineers who go to work with a particular company.

155
00:17:49,000 --> 00:18:08,000
So, and this is a fundamentally different business model, and it creates different incentives. And it's also why, you know, if listeners have paid some more detailed attention to Palantir, CARP and others around Palantir often make this big deal about how they care about privacy and they care about civil liberties.

156
00:18:08,000 --> 00:18:18,000
And again, I think that's something when people who, for good reasons, do not spend as much time thinking about Palantir as I do, say sort of like, what are they talking about?

157
00:18:18,000 --> 00:18:29,000
That's real. That's not a lie. They don't. Their business model isn't to know everything about me so that they can sell, you know, a diaper company the chance to sell me diapers.

158
00:18:29,000 --> 00:18:41,000
Their business model is to build, so sell to large enterprises, the tools that, or governments, the tools that those large entities will use to interpret all the data they have.

159
00:18:41,000 --> 00:18:57,000
And this is a very different model. And so I think, and then I'll stop going on, but I think part of why Palantir, Amazon is also very interesting to me in this way, and I'm writing about Amazon a bit at the present.

160
00:18:57,000 --> 00:19:03,000
But I think that Palantir represents this other mode of techno power.

161
00:19:03,000 --> 00:19:18,000
I've been playing with calling it ontological power, where, you know, we now have, in contrast to when I started Logic magazine and started working on this, there's actually now quite a lot of public tech criticism.

162
00:19:18,000 --> 00:19:23,000
You know, we have had Shoshana Zuboff's book Surveillance Capitalism.

163
00:19:23,000 --> 00:19:39,000
We have a lot more public awareness and critique, I think, of how companies may be infringed on individual privacy, how they may be, you know, the way Shoshana Zuboff describes it, interfere with our free will.

164
00:19:39,000 --> 00:19:55,000
What Palantir does is not the same as that. And it represents this other form of power that is no less threatening, I would say more threatening, but where they will build the ways that government agencies communicate with each other and share data and so on.

165
00:19:55,000 --> 00:20:07,000
I think it's also important to see this because I think it helps us start to understand why their philosophy, ideology, culture is so different from a company like Google.

166
00:20:07,000 --> 00:20:17,000
If the way you make money is that you let everyone in the world, except in the PRC, except in the People's Republic of China, although they'd love to be there, I'm sure.

167
00:20:17,000 --> 00:20:32,000
But if you let everyone in the world use your search platform and then build ad tools based on the data you gather about them to sell to people who sell them products, you don't have a strong ideology.

168
00:20:32,000 --> 00:20:51,000
You can make money off of everyone. Maybe I want to buy a socialist book. Maybe my neighbor wants to buy a neo-fascist book. It doesn't matter to Google. You can have this very sort of libertarian open philosophy and culture, which Google historically has had, changing a bit now that they're trying to get into cloud and defense contracts, which I think is important.

169
00:20:51,000 --> 00:21:11,000
But Palantir is not doing that. They're selling to very large clients, corporate clients, government clients, and what they're selling is different. And once I think you see that, once I could see that after doing more research on them, the things that had bewildered me about how culturally different they were from companies like Google and Facebook start to make more sense.

170
00:21:11,000 --> 00:21:31,000
They have a material basis based on the business model and based on the form of power they represent. And there's a good reason why they seem to represent something different from the Californian ideology, sort of libertarian open liberal libertarian consensus that I think historically we've associated with Silicon Valley. They're very different.

171
00:21:31,000 --> 00:21:56,000
In what I understand as algorithmic governmentality, there's a certain way of trying to extrapolate from the behavior of others, your behavior, for example. So the assumption is that through collecting enough data on people who have similar past behaviors than you do, they're trying to extrapolate a possible future of Moira.

172
00:21:57,000 --> 00:22:17,000
Within this action, there's a transition from prediction to prescription, actually, because based on the assumptions that are made, then their architectures are built based on this assumption. And it might be totally incorrect, you know, because the past of others is not Moira's future.

173
00:22:18,000 --> 00:22:27,000
It might be self-fulfilling, you know, if you can't get a loan and then she goes further into debt, so her credit score is worse and worse, it will get harder to get a loan.

174
00:22:27,000 --> 00:22:56,000
Absolutely, exactly. And there's a, and I think that's it's a specific epistemology of how to approach the idea of data. And I'm wondering whether this is being shared at Palantir as well, because, and I'm asking this because they are acting as if they are handling things differently, not only when it comes to the question of privacy, not only when it comes to the business model, which

175
00:22:56,000 --> 00:23:23,000
is not collecting these huge amounts of data. But to me, it seemed when I was researching the topic a bit, you know, in preparation for this episode, to me, it seemed as if what Alex Carp actually is trying to say is that those people, they don't understand what data actually is. And when they are acting in the way we just talked about this specific idea of

176
00:23:23,000 --> 00:23:41,000
governmentality, they are getting things wrong. Yeah. And I'm asking myself, what is he saying then? What is he saying? Is he saying that they are having a different approach and a different epistemological approach on how to handle data within Palantir? And that's actually a big question.

177
00:23:41,000 --> 00:24:07,000
And it's totally related to the dissertation, I guess, in some way, because there's this kind of behaviorist idea within the dissertation that people do actually not know themselves correctly, and that their behavior is telling more about them and that the drives that are within them are so subconscious that they are not actually able to be aware of it.

178
00:24:07,000 --> 00:24:18,000
And that through the correlation within the data, you might be able to see the form and that the form is actually the message.

179
00:24:19,000 --> 00:24:38,000
I certainly see the connection to the dissertation, and I'm trying to think about how it connects to Palantir about what kind of attitude is it toward data to assume that certain kind of inferences can be made and that they are descriptive as opposed to prescriptive.

180
00:24:38,000 --> 00:24:54,000
You know, I think I talk about in my article, this idea which others like Cathy O'Neill and Wendy Choon have written about, but this idea that machine learning can only replicate the past is a pretty familiar idea, right?

181
00:24:54,000 --> 00:25:04,000
Because any machine learning algorithm is trained on existing data sets and how you train the algorithm to be right, as you see, did it reproduce the past results from this data set we have?

182
00:25:04,000 --> 00:25:24,000
This is why, to use an example that I like to use with engineering students, when I speak to them, I sometimes ask, how would you design a non-racist predictive policing algorithm or really any kind of algorithm having to do with policing in the United States?

183
00:25:24,000 --> 00:25:38,000
And often, you know, they think about it like, oh, well, could you use this? Could you exclude this and control it? And the point of the exercise is to show, in my view, there is no way you possibly could because everything about any data in the United States is

184
00:25:38,000 --> 00:25:48,000
riven by the history of chattel slavery and economic inequality and so on and so forth. There's no data you could use to train something to predict a different future.

185
00:25:48,000 --> 00:26:04,000
Wendy Choon, the theorist of technology, has this line I like where she says, what if we looked at machine learning systems the way we look at climate change models? Not like they're predicting something true, but that they're predicting something we want to prevent.

186
00:26:04,000 --> 00:26:13,000
You know, this is like a bad thing about the past. We don't want to replicate. So I think you're phrasing, and this is very interesting in the context of Karp's dissertation.

187
00:26:13,000 --> 00:26:28,000
I talk about this a bit in my piece because the dissertation suggests, actually, or at least Adorno's reading of jargon suggests a critique of precisely this kind of decontextualization, right?

188
00:26:28,000 --> 00:26:47,000
So, you know, if Adorno's talking in a rather different context, but if he says, you know, dwelling has this meaning for Heidegger, but it's been totally abstracted from the context of people losing their homes and being homeless.

189
00:26:47,000 --> 00:27:00,000
And therefore it sort of mystifies the way Heidegger talks about dwelling, mystifies and spiritualizes this fact, which is actually about the history of human oppression and exploitation.

190
00:27:00,000 --> 00:27:10,000
I think we could see some really interesting analogies to the way machine learning and algorithmic governmentality might work.

191
00:27:10,000 --> 00:27:19,000
There are some interesting experiments I want to mention in trying to do something closer to what Wendy Choon is talking about.

192
00:27:19,000 --> 00:27:35,000
I think, for instance, suggesting what if you took a predictive policing algorithm, but then instead of using it to say, oh, you should police these people more heavily, used it to say, oh, you need more social services here.

193
00:27:35,000 --> 00:27:38,000
You know, to allocate resources for redistribution or something.

194
00:27:38,000 --> 00:27:40,000
This is just speculative.

195
00:27:40,000 --> 00:27:47,000
But I think, so the analogy, if Adorno says about Heidegger, he takes this concept and decontextualizes it and spiritualizes it.

196
00:27:47,000 --> 00:27:53,000
We could say that's what forms of algorithmic governmentality do with the data they're built on, right?

197
00:27:53,000 --> 00:28:18,000
They're built on, say, a particular history of structural racism in the US and decontextualized from that become sort of an engine for predicting who is more likely to commit a crime or something in a way that makes that seem as if it's just true and reliable and not connected to these very specific histories of oppression and exploitation.

198
00:28:18,000 --> 00:28:30,000
In terms of how closely that relates to the work Palantir does, to be quite honest, I think this is a question I'm still trying to answer in some of my research because they've done work with police departments.

199
00:28:30,000 --> 00:28:49,000
And there's a horrifying, when you were making a joke earlier, Jan, about someone else's past predicting Moira's future, I was thinking of this horrifying interview that some Palantir representative gave in 2015 about their predictive policing program in New Orleans.

200
00:28:49,000 --> 00:29:12,000
And a critic, it was on national public radio in the United States, NPR, and a critic pointed out the kinds of predictive analytics you're using are well known to disproportionately lead to policing of Black people and Hispanic people and poor people.

201
00:29:12,000 --> 00:29:22,000
And isn't that a problem? And this spokeswoman, whose name I'm forgetting, said, well, as long as your cousin isn't a drug dealer, you'll be fine.

202
00:29:22,000 --> 00:29:30,000
Which was amazing to me to have someone say that on public radio, as long as your cousin isn't a drug dealer, you'll be fine.

203
00:29:30,000 --> 00:29:38,000
And so a very different idea of innocence and guilt and justice and will than the US legal system is supposed to operate with.

204
00:29:38,000 --> 00:29:53,000
And so I think there is evidence that they have done some work historically that corresponds to the kind of epistemology of data we're talking about.

205
00:29:53,000 --> 00:30:09,000
But I do, as I understand it, think that a lot of their business is not so much about building these tools of prediction as building means of interpretation for large entities.

206
00:30:09,000 --> 00:30:18,000
And what kind of epistemology that involves is a really interesting question that I want to think more about, right?

207
00:30:18,000 --> 00:30:27,000
Because it involves all kinds of assumptions about what data matters, about what is connected to what, who should be able to access what.

208
00:30:27,000 --> 00:30:36,000
I used earlier the example of immigration authorities accessing health data, like what parts of government should be able to speak to one another.

209
00:30:36,000 --> 00:30:42,000
But my first reaction is that it may be a bit different than that algorithmic governmentality.

210
00:30:42,000 --> 00:30:47,000
It may be more about the power to set standards and set terms.

211
00:30:47,000 --> 00:30:56,000
I'm thinking now of James C. Scott's famous book Seeing Like a State, but it's like, you know, who gets to set how the state sees people's data?

212
00:30:56,000 --> 00:31:02,000
Seems like maybe it introduces a slightly different problematic, but that's something I'm still thinking through very much.

213
00:31:02,000 --> 00:31:05,000
So that's a provisional answer.

214
00:31:05,000 --> 00:31:12,000
Let's dig a little deeper into these alt-right tech liaisons.

215
00:31:12,000 --> 00:31:16,000
Maybe for our listeners, just some general information.

216
00:31:16,000 --> 00:31:32,000
In general, the Silicon Valley is thought to be kind of more liberal, and the programming and software developers scene is more or less seen as liberal, libertarian, or maybe even anarchist leaning.

217
00:31:32,000 --> 00:31:34,000
And we will maybe come back to this later.

218
00:31:34,000 --> 00:31:43,000
But there are also other tendencies, like the so-called neo-reactionaries, which formed around a software developer with the pseudonym Menchus Mobock.

219
00:31:43,000 --> 00:31:53,000
His real name is Curtis Jarvin, and Jarvin's neo-reactionary world of thought has also been taken up by the philosopher Nick Land, for example,

220
00:31:53,000 --> 00:32:04,000
who in turn will be known to many people through his work on accelerationism or the CCRU, the Cybernetic Culture Research Unit, which he co-founded in the 1990s.

221
00:32:04,000 --> 00:32:17,000
And then there is Peter Thiel, you already mentioned him, co-founder of PayPal and also co-founder of Palantir, and a prominent figure in the right-wing conservative, in this case, ultra-libertarian camp.

222
00:32:17,000 --> 00:32:29,000
Peter Thiel, for his part, was one of the few who backed Donald Trump when he was running for president in 2016, and subsequently also advised him at least for a time.

223
00:32:29,000 --> 00:32:37,000
And Menchus Mobock, he was a guest of Peter Thiel on the election evening 2016 to view the election results.

224
00:32:37,000 --> 00:32:47,000
So there are very short distances between radical reactionary positions such as those of Mobock and central positions of power.

225
00:32:47,000 --> 00:32:55,000
What kind of worldview is this so-called neo-reactionary dark enlightenment, and what influence does it have in the tech industry?

226
00:32:55,000 --> 00:33:13,000
It's a great question, and I'm so glad you mentioned that Curtis Yarvin and Peter Thiel watched the election together because it was a friend of mine, Joe Bernstein, who first reported that in 2017 when he got access to all these Breitbart emails, and I thought that was a delicious find.

227
00:33:13,000 --> 00:33:33,000
And all of the things that he dug up, interestingly, just to add to it, I remember one of the other emails that this reporter got from Breitbart was an email afterwards between Milo Yiannopoulos and Menchus Mobock, Curtis Yarvin, where Milo was saying, well, Peter Thiel isn't that enlightened, right?

228
00:33:33,000 --> 00:33:41,000
He's not that far, right? And Curtis Yarvin said, oh no, you'd be surprised, he's more enlightened than you'd think, just plays it very close, just is very careful about it.

229
00:33:41,000 --> 00:33:59,000
And I'll add just one more fun fact, which is that when the New Yorker journalist Andrew Marantz went to the deplorable, which was this big party for the alt-right thrown at the Trump inauguration in January 2017, Peter Thiel was apparently there just being very quiet and not socializing in the background.

230
00:33:59,000 --> 00:34:08,000
So it's a funny scene in Andrew's book where he runs into Peter Thiel at the deplorable and tries to talk to him and Peter Thiel sort of vanishes and avoids him.

231
00:34:09,000 --> 00:34:19,000
I think these social networks are complex and it's an area I'm still doing research in.

232
00:34:19,000 --> 00:34:25,000
I think it's hard to generalize because there are a lot of different ideologies floating around.

233
00:34:25,000 --> 00:34:47,000
And my macroscopic view is that, you know, since roughly the 1990s, if there was this kind of liberal-libertarian consensus that maybe reflected a certain neoliberal hegemony or compromise, which, you know, we saw throughout U.S. politics, the third way, Democratic, Republican, getting along.

234
00:34:48,000 --> 00:34:57,000
Sort of cultural liberalism, economic, old-fashioned liberalism, compromise that Richard Barber, who you've had on, I'm sure can speak to better than I will.

235
00:34:57,000 --> 00:35:03,000
That since 2016, we've seen a kind of crap up of this common sense in the United States.

236
00:35:03,000 --> 00:35:07,000
And I think a lot of different formations are coming out of that.

237
00:35:07,000 --> 00:35:22,000
And it's not entirely clear, especially under a Biden administration or whatever strange era we're entering now, preferable to another Trump administration, but nonetheless, strange era, in my view, we're entering how that will play out.

238
00:35:22,000 --> 00:35:51,000
I think that there's a very strong, I don't know whether to call it elitist, monarchist kind of tendency that someone like Mentis Moebuck represents that takes, again, from critical theory aspects of the critique of liberal modernity, but rather than saying, as, for instance, Habermas does, that then the project of philosophy has to be to help achieve democracy, you know, achieve liberal democracy.

239
00:35:51,000 --> 00:36:01,000
A promise of it that's never been achieved, instead just takes the critique and says, and this is why modernity and liberal democracy are bankrupt projects.

240
00:36:01,000 --> 00:36:10,000
I think that there is a strong tendency and one could be philosophical about it.

241
00:36:10,000 --> 00:36:21,000
You know, there are these Leo Strauss reading groups among venture capitalists in Silicon Valley where they read Strauss and talk about Plato and the Republic of Philosopher Kings.

242
00:36:21,000 --> 00:36:32,000
You know, I think one could be, and Thiel has a version of this in his reading of Girard, you know, in terms of how the powerful visionaries will always be treated as scapegoats and so on.

243
00:36:32,000 --> 00:36:37,000
I think that one can go to a lot of philosophy and talk about their ideas.

244
00:36:37,000 --> 00:36:46,000
One could also, you know, say these are a bunch of mostly white male engineers who think they know what to do and shouldn't have to listen to anyone else.

245
00:36:46,000 --> 00:36:51,000
I sometimes question how much philosophy we need to explain the worldview.

246
00:36:51,000 --> 00:37:04,000
But I think, yeah, there are, there's certainly a lot of social proximity between the Thiel Founders Fund Network and these sort of right wing networks in the tech industry.

247
00:37:04,000 --> 00:37:11,000
A number of sort of prominent figures have turned out to be software engineers or come from that world.

248
00:37:11,000 --> 00:37:15,000
I think, yeah, that's interesting.

249
00:37:15,000 --> 00:37:21,000
I think there's this question about Thiel that's come up specifically and about his nationalism.

250
00:37:21,000 --> 00:37:24,000
You know, what's his name?

251
00:37:24,000 --> 00:37:40,000
Lucky Palmer, who started Oculus and left, he left Facebook basically over, I forget what the company officially said, but afterwards Lucky Palmer said publicly, you know, it was because they were angry that I was an open Trump supporter and I left.

252
00:37:40,000 --> 00:37:43,000
He has called it tech nationalism.

253
00:37:43,000 --> 00:37:46,000
There's a new kind of tech conservatism or tech nationalism.

254
00:37:46,000 --> 00:38:03,000
I think given Peter Thiel's public support of Trump and the funding he's put into campaigns like those of Chris Kobach, who's a very anti-immigration politician in Kansas and the United States, and the ways he sort of placed people in the White House.

255
00:38:03,000 --> 00:38:16,000
He placed this chief technical technology officer, CTO, in the White House, that there's been this question of like, well, how does nationalism fit with that kind of libertarianism, which is supposedly anti-statist and anti-government?

256
00:38:16,000 --> 00:38:38,000
I think the idea that corporations should take over government is actually pretty consistent with this kind of libertarianism and pretty consistent with the sort of monarchist view of someone like mold bug that, you know, the state is failing and you need these engineers who are more competent or sort of philosopher king types to take it over.

257
00:38:38,000 --> 00:38:49,000
So I think it's actually maybe less contradictory than it appears on its surface, but I do think there's this new nationalist emphasis or has been under the Trump administration.

258
00:38:49,000 --> 00:38:55,000
Again, what will happen under the Biden administration will be interesting to see.

259
00:38:55,000 --> 00:39:01,000
I think my prediction is that it won't make that much difference actually to their business.

260
00:39:02,000 --> 00:39:07,000
But maybe in terms of ideological tone or how they talk about it, it'll be different.

261
00:39:07,000 --> 00:39:09,000
I don't know. Yeah.

262
00:39:09,000 --> 00:39:11,000
But there's certainly lots of proximity.

263
00:39:11,000 --> 00:39:19,000
And I think a similar partial appropriation of critical theories, critique of modernity, is a continuity with the Karp thesis.

264
00:39:19,000 --> 00:39:23,000
I've gone on way too long as usual, so sorry.

265
00:39:24,000 --> 00:39:42,000
Ultimately, I'm interested in the various ideological shades of these questions because I think there's actually quite obviously a new competition of meta narratives, you know, the spot that has been taken by neoliberal variants of different sorts.

266
00:39:42,000 --> 00:39:53,000
It's quite open now because I mean, of course, now we have Biden and it seems as if a neoliberal approach kind of again got their way somehow.

267
00:39:53,000 --> 00:40:04,000
But I don't think that like in the medium and long run, it is a narrative that is still convincing to the majority so that there is an open spot in terms of meta narratives.

268
00:40:04,000 --> 00:40:18,000
And that's why I'm also interested in the way that Palantir, Karp and Thiel and others like that try to present themselves as if they have a different kind of meta narrative, also for people in the tech industry specifically.

269
00:40:18,000 --> 00:40:29,000
And we already have given much space to this reactionary and near reactionary positions, which I think is important because there are many dangers arising there.

270
00:40:30,000 --> 00:40:34,000
But I think it's also important to look to different positions.

271
00:40:34,000 --> 00:40:39,000
And that is actually what you're doing with Ben Tarnoff in Voices from the Valley.

272
00:40:39,000 --> 00:40:43,000
And I have to say I didn't yet read it. I didn't have the time to read it.

273
00:40:43,000 --> 00:40:51,000
I only listened to interviews with Ben and you and I'm highly excited about the book and I'm very much looking forward to reading it.

274
00:40:51,000 --> 00:40:59,000
What other positions have you encountered in your work on the book, perhaps especially with regard to this open question of meta narratives?

275
00:40:59,000 --> 00:41:04,000
It's absolutely this moment of new openings for opening for new meta narratives.

276
00:41:04,000 --> 00:41:08,000
And we're seeing those on the left and the right and somewhere between.

277
00:41:08,000 --> 00:41:18,000
And I think also it's important for me as a materialist anyway to attend to what this has to do with changes in the technology and in the business of Silicon Valley.

278
00:41:18,000 --> 00:41:21,000
Silicon Valley started as a government project, right?

279
00:41:21,000 --> 00:41:24,000
It was funded by the US military in the beginning.

280
00:41:24,000 --> 00:41:30,000
It went through this process of privatization and deregulation in the 1990s.

281
00:41:31,000 --> 00:41:38,000
In that moment of neoliberal consensus forming, if we want to call it that.

282
00:41:38,000 --> 00:41:41,000
But it may be entering another phase.

283
00:41:41,000 --> 00:41:49,000
And I think with the rise of new machine learning technologies and particularly new cloud computing technologies,

284
00:41:49,000 --> 00:41:57,000
the sort of competition for the large government contracts that the client Palantir has may represent a new iteration technologically

285
00:41:57,000 --> 00:42:00,000
and in terms of business model as well as ideologically.

286
00:42:00,000 --> 00:42:03,000
And of course, ideology is always tied up in these matters.

287
00:42:03,000 --> 00:42:15,000
And I think I say this just because I think this battle of meta narratives is often reported on in the media about Silicon Valley or has been reported on as a kind of culture war.

288
00:42:15,000 --> 00:42:19,000
You know, it's like, oh, these are just different things people think at these companies.

289
00:42:19,000 --> 00:42:26,000
And I think that it's actually really important to keep paying attention to how it interacts with the business and the technology.

290
00:42:26,000 --> 00:42:30,000
So to take and then I will answer the question about socialism and more optimistic alternatives.

291
00:42:30,000 --> 00:42:38,000
But to take a more current example, Amazon recently brought onto its board Keith Alexander,

292
00:42:38,000 --> 00:42:48,000
who was an architect of the National Security Administration, architect of the spying, you know, global spying that that Edward Snowden revealed in 2013.

293
00:42:49,000 --> 00:42:58,000
And there is a faction within, you know, this is very clearly, in my view, a bid by Amazon to win more government contracts.

294
00:42:58,000 --> 00:43:02,000
They lost this cloud computing contract to Microsoft a year or two ago.

295
00:43:02,000 --> 00:43:08,000
So hiring the chief NSA guy is a bid to try to win those government contracts.

296
00:43:08,000 --> 00:43:17,000
There is a battle about it happening within Amazon because some engineers are very who are more sort of libertarian digital rights privacy type.

297
00:43:17,000 --> 00:43:25,000
People are pissed that the sort of most famous guy, most famous for spying on the entire world is is now on their board of directors.

298
00:43:25,000 --> 00:43:36,000
But this isn't just a matter of competing beliefs, although it is that it also is making it a lot harder for Amazon Web Services to do business in Europe,

299
00:43:36,000 --> 00:43:48,000
where there's the general data protection regulation and there's just been this new lawsuit knocking down Privacy Shield, which was an old privacy framework for managing data sovereignty and privacy in Europe.

300
00:43:48,000 --> 00:43:59,000
So it's one of these cases where it is the advent of cloud computing, you know, the growth of cloud computing and these new virtualization technologies creates new business opportunities or the client for that is the state.

301
00:43:59,000 --> 00:44:06,000
The client for that isn't the individual user who is the user of Google search or Facebook.

302
00:44:06,000 --> 00:44:12,000
That kind of technology lies on large enterprise customers to make its money.

303
00:44:12,000 --> 00:44:17,000
And you see this extensively political or cultural clash where you have some people in Amazon.

304
00:44:17,000 --> 00:44:22,000
A lot of people in Amazon used to work in the military and police who say, you know, maybe they're patriotic.

305
00:44:22,000 --> 00:44:26,000
And then you have people saying how terrible to have someone who's a famous spy.

306
00:44:26,000 --> 00:44:35,000
And that looks like a culture where but it is also fundamentally wrapped up in these business questions about whether how they're going to sell cloud to the US or the rest of the world.

307
00:44:35,000 --> 00:44:37,000
And so that's just a random example.

308
00:44:37,000 --> 00:44:48,000
But I think as we think about these new meta narratives, it's also really important to always keep in mind how they link up with the state of technologies and how they link up with the business opportunities.

309
00:44:48,000 --> 00:45:10,000
Similarly, in Google, if we look at these worker struggles or sort of high profile conflicts that happened in the past few years, for instance, over Project Maven, which was a contract to provide a vision recognition software to the US military that some engineers and others within the company objected to.

310
00:45:10,000 --> 00:45:12,000
And there was sort of a public struggle over this.

311
00:45:12,000 --> 00:45:15,000
And Google did not end up competing for that contract.

312
00:45:15,000 --> 00:45:20,000
This was often reported as a kind of culture war between liberal Silicon Valley and more national Silicon Valley.

313
00:45:20,000 --> 00:45:30,000
But it's fundamentally tied up with how Google is going to continue to find new ways to make money off of all the data they have and ways they know how to read and manage data.

314
00:45:30,000 --> 00:45:39,000
So all that to say, I just always want to reemphasize that these meta narratives are very tied to business and to, you know, ideology is material.

315
00:45:39,000 --> 00:46:04,000
But that said, I think as much attention as the neo reaction and the right wing folks have drawn, there also has been, and I sort of alluded to it indirectly there, a lot of left wing organizing and activity within Silicon Valley in the past four or five years, not solely in reaction to the election of Trump, but certainly catalyzed by the election of Trump.

316
00:46:04,000 --> 00:46:16,000
But it's quite striking, you know, often for many reasons when we hear about Silicon Valley in the media, what we're really hearing about is the ownership class of Silicon Valley, the CEOs of Silicon Valley.

317
00:46:16,000 --> 00:46:20,000
They're the only ones who are allowed to talk to the press most of the time.

318
00:46:20,000 --> 00:46:26,000
And but actually Silicon Valley is made up of a bunch of different kinds of people.

319
00:46:26,000 --> 00:46:36,000
And one, and a lot of them like young people in the United States in general are left of center sort of left even of the neoliberal consensus and one statistic.

320
00:46:36,000 --> 00:46:45,000
I like to cite, which Ben Tarnoff, who I did this book with, wrote about back in 2016 is that more.

321
00:46:45,000 --> 00:46:52,000
Let me put this the right way of the top organizations by number of employees who donated to Bernie Sanders.

322
00:46:52,000 --> 00:46:57,000
So employers were the largest number of employees donated to Bernie Sanders.

323
00:46:57,000 --> 00:47:00,000
The top five were all tech companies and the nurses union.

324
00:47:00,000 --> 00:47:03,000
So his nurses and then software engineers.

325
00:47:03,000 --> 00:47:07,000
And I think that represents a real thing.

326
00:47:07,000 --> 00:47:08,000
And part of it's about age.

327
00:47:08,000 --> 00:47:13,000
You know, the tech industry is young and young people in this country tend to be tend to be more left.

328
00:47:13,000 --> 00:47:17,000
But I also think there's room.

329
00:47:17,000 --> 00:47:28,000
I think there are these laughter and energies in the tech industry to that logic magazine and my new book have tried to tap into and document and show to people because it's so rarely shows up in the press.

330
00:47:28,000 --> 00:47:34,000
And and that we can it's interesting to think too about how those connect to new technological possibilities.

331
00:47:34,000 --> 00:47:37,000
I mean, I personally actually don't think this is a perfect answer.

332
00:47:37,000 --> 00:47:42,000
But some people say, you know, Amazon has a perfect planned economy, just nationalize it.

333
00:47:42,000 --> 00:47:45,000
You know, it's not it's not an inevitability.

334
00:47:45,000 --> 00:47:47,000
I don't think that that's a serious idea necessarily.

335
00:47:47,000 --> 00:47:56,000
But but there are all kinds of potentialities and possibilities in these technologies and the scale at which they they draw people together.

336
00:47:57,000 --> 00:48:04,000
And it's not a given that they lead to a handful of tech emperors running everything.

337
00:48:04,000 --> 00:48:08,000
So our book talks to ordinary people in the tech industry about different experiences.

338
00:48:08,000 --> 00:48:10,000
And I could certainly say more about that.

339
00:48:10,000 --> 00:48:14,000
But I already talked quite long enough in answer to that question.

340
00:48:14,000 --> 00:48:21,000
What I find incredibly exciting is the newly found self-confidence of tech workers as a key industry.

341
00:48:21,000 --> 00:48:30,000
And the sociopolitical power and ability that arises from from this fact, how can this power be activated and for what?

342
00:48:31,000 --> 00:48:33,000
It's a good question.

343
00:48:33,000 --> 00:48:44,000
You know, through Logic magazine, somewhat in my new book, and then through the magazine over the years, we've really tried to document some of these different political actions by tech workers.

344
00:48:44,000 --> 00:48:52,000
And this term tech worker in English is itself a kind of polemical term, right?

345
00:48:52,000 --> 00:48:58,000
Historically, the industry has been very segregated or separated by different skill levels.

346
00:48:58,000 --> 00:49:03,000
There's been this strong ideology that highly paid workers in the tech industry aren't really workers.

347
00:49:03,000 --> 00:49:09,000
You know, they're creatives, creatives, knowledge workers doing their knowledge thing.

348
00:49:10,000 --> 00:49:21,000
And and, you know, don't mind the thousands of people working in the cafeteria or on the campus or indeed, you know, in in customer support who make this all run.

349
00:49:21,000 --> 00:49:28,000
I think so this term tech worker itself has a kind of polemical aim, which is to say everyone who works in technology is a tech worker.

350
00:49:28,000 --> 00:49:30,000
They can have material interests in common.

351
00:49:30,000 --> 00:49:39,000
It really came out of this organization called Tech Workers Coalition founded in the Bay Area in 2014.

352
00:49:39,000 --> 00:49:44,000
And it was, at least to my knowledge, that's and I'm pretty sure it's correct.

353
00:49:44,000 --> 00:49:48,000
That's the sort of first time that a lot of people use use this term.

354
00:49:48,000 --> 00:49:53,000
It's an organization co founded by a former cafeteria worker and an engineer.

355
00:49:53,000 --> 00:50:07,000
And from the very beginning, a big part of their ethos is about bringing together sort of higher paid tech workers and the people who are usually thought of tech workers with all the other folks whose whose labor makes it possible.

356
00:50:07,000 --> 00:50:09,000
And there was a lot of knowledge sharing.

357
00:50:09,000 --> 00:50:27,000
I think the blue collar workers tended to have a lot more experience in agitation and organization than the engineers did and unionizing campaigns for those workers, which happened in Silicon Valley in 2015, 2016, 2017 at Cisco, Facebook, IBM, some other big companies.

358
00:50:27,000 --> 00:50:30,000
There were unions for the for the blue collar staff.

359
00:50:30,000 --> 00:50:36,000
That was a really big learning experience, I think, for a lot of the engineers.

360
00:50:36,000 --> 00:50:41,000
I think that it's it's a new kind of consciousness that can go in all kinds of directions.

361
00:50:41,000 --> 00:50:52,000
One early thing we saw after the Trump election and actually the very first Tech Workers Coalition protest I ever went to was at Palantir headquarters.

362
00:50:52,000 --> 00:51:00,000
I was there to write about it, not as a protester, but was at Palantir headquarters in Palo Alto the day before the Trump inauguration.

363
00:51:00,000 --> 00:51:05,000
So I guess that was January 20th, 2017, I think.

364
00:51:05,000 --> 00:51:22,000
One concern that a lot of tech workers, engineers expressed, was concerned about being made to gather and use data or building products that would be used to police and harass people in ways that they did not support and did not intend.

365
00:51:22,000 --> 00:51:39,000
So for instance, at that Palantir protest, people spoke about this idea of a Muslim database, which Trump had been talking about on the campaign trail and Palantir's possible complicity in building such a database.

366
00:51:39,000 --> 00:51:45,000
I think one thing that's been very interesting to watch is that, you know, engineering work is so modular generally.

367
00:51:45,000 --> 00:51:50,000
It's like engineers work on these very little pieces of software that plug into these very big systems.

368
00:51:50,000 --> 00:52:03,000
And I think, and I don't mean this as a criticism at all, but I think in that kind of work system, it's very easy to kind of lose sight of what the bigger picture is, or it's very easy for a worker not to know what the bigger picture is.

369
00:52:03,000 --> 00:52:13,000
But I think that, A, having this threat of Trump using these tools to political ends that people didn't want was politicizing.

370
00:52:13,000 --> 00:52:26,000
And I think also, and person after person I talked to, talked about this specific meeting that happened between Trump and the tech CEOs, and that happened on December 9th, 2016, and Peter Thiel arranged it.

371
00:52:26,000 --> 00:52:38,000
But person after person I talked to separately brought up these photographs that came out of this meeting, and these photographs of their liberal bosses sort of going to kiss up to Trump after he was elected.

372
00:52:38,000 --> 00:52:49,000
And I think that, as silly as it might sound, sort of just this awareness that it's like they were workers in a business, and those businesses were going to comply with who was in power.

373
00:52:49,000 --> 00:53:03,000
And there's all this ideology about engineers as creative workers who aren't really like workers, they're doing their own thing, but actually, at the end of their day, their bosses would say, no, you're going to make that, that they were actually workers.

374
00:53:03,000 --> 00:53:08,000
I think the election of Trump had sort of a catalyzing effect on that in all sorts of ways.

375
00:53:08,000 --> 00:53:15,000
And I mentioned earlier the protests at Google, there were these actions at Google, even before the Google walkout about sexual harassment.

376
00:53:15,000 --> 00:53:28,000
There were these actions around Project Maiden, which was this vision recognition software in development, potentially for the military didn't happen, and then Project Dragonfly, which was a censored Chinese search engine.

377
00:53:28,000 --> 00:53:34,000
And there have been a lot of other actions like that against specific projects at tech firms.

378
00:53:34,000 --> 00:53:38,000
So I think there are a lot of directions it can go. That's one way it's gone.

379
00:53:38,000 --> 00:53:47,000
I think another key insight, at least at the beginning of that movement, was that engineers realized they could use their sort of privileged position to advocate for certain things.

380
00:53:47,000 --> 00:53:56,000
I think Google has shut down on it a lot since then, but I used to hear people say, you know, how many engineers did it take to shut down Google search for a day? Not that many.

381
00:53:56,000 --> 00:54:06,000
They're like a trillion searches a day. You know, if you think about like a dock worker, like where the points of leverage and economy are, it's actually not very many people right there.

382
00:54:06,000 --> 00:54:11,000
So anyway, yeah, those are some thoughts on it. I think that it's evolved in different directions and will continue to.

383
00:54:11,000 --> 00:54:22,000
I think what COVID means for it is a really open question because, of course, yeah, it's had a lot of implications. People aren't on their campuses anymore. More work is remote.

384
00:54:22,000 --> 00:54:31,000
I think it's possible it'll lead to permanent sort of de-skilling of even certain technical jobs or outsourcing of that at a more rapid pace.

385
00:54:31,000 --> 00:54:37,000
But yeah, there's been a lot of new activity in that area since 2016, for sure.

386
00:54:37,000 --> 00:54:45,000
Maura, there's a last question that I ask everybody who's on the show. If you think about the future, what makes you joyful?

387
00:54:45,000 --> 00:54:50,000
Ah, what makes me joyful about the future?

388
00:54:50,000 --> 00:55:10,000
I think, you know, I think in my personal life, I could have my own answers. I think looking at the political scene, I think that this sort of crack up of a neoliberal hegemony that was in many ways harmful and felt constricting.

389
00:55:10,000 --> 00:55:18,000
And that's happened since 2016 has opened up a lot of new possibilities. And some of those are frightening and bad.

390
00:55:18,000 --> 00:55:29,000
But some of those are really promising and exciting. And so I think with a measured kind of optimism, because I don't want to play down all the suffering and sort of frightening things that have happened.

391
00:55:29,000 --> 00:55:42,000
I think that looking at young people, including tech workers, but also including, you know, young climate strikers and sort of these young feminist activists and so on.

392
00:55:42,000 --> 00:55:50,000
Looking at those folks and how they're using technological tools or when they work in the industry, building new kinds of tools.

393
00:55:50,000 --> 00:56:01,000
These are things that make me feel optimistic, if I'm having to be optimistic. I'm very pessimistic all the time.

394
00:56:01,000 --> 00:56:13,000
But yeah, those are things that make me feel joyful, as well as the thought of whenever it is we get to be in large rooms for a dance party or a bookstore reading.

395
00:56:13,000 --> 00:56:24,000
Again, I have dreams about going to a bookstore full of people to hear someone read. So yeah, those are some things that make me feel joyful.

396
00:56:24,000 --> 00:56:29,000
Nice. Moira, thanks a lot for being part of Future Histories.

397
00:56:29,000 --> 00:56:36,000
Yeah, thanks for having me. This was fun.

398
00:56:36,000 --> 00:56:43,000
That was our show for today. If you want to know more about Future Histories, please visit futurehistories.today.

399
00:56:43,000 --> 00:56:54,000
You can support Future Histories at Patreon. For this, go to patreon.com slash future histories and let me know what you think about this episode and the show in general.

400
00:56:54,000 --> 00:57:06,000
For this, use Twitter, hashtag Future Histories or Reddit or send me an email to future underscore histories at protonmail.com.

401
00:57:06,000 --> 00:57:07,000
See you next time.

