1
00:00:00,000 --> 00:00:04,960
Herzlich willkommen bei Future Histories, dem Podcast zur Erweiterung unserer Vorstellung von

2
00:00:04,960 --> 00:00:11,120
Zukunft. Mein Name ist Jan Groß und ich freue mich sehr, heute Robert Seifert begrüßen zu dürfen.

3
00:00:11,120 --> 00:00:17,460
Er ist Professor für Soziologie an der Christian-Albrechts-Universität zu Kiel und arbeitet unter

4
00:00:17,460 --> 00:00:23,880
anderem an einer Theorie algorithmischer Sozialität. Gemeinsam mit Jonathan Robert hat er den

5
00:00:23,880 --> 00:00:30,720
Sammelband Algorithmus Kulturen herausgebracht, den man als Open Access Publikation bei Transkript

6
00:00:30,720 --> 00:00:38,080
finden kann. Und Full Disclosure, Robert betreut meine Doktorarbeit, was mich sehr freut. Ich habe

7
00:00:38,080 --> 00:00:42,160
diese Folge hier ein wenig vorproduziert, weshalb ich jetzt auch gar nicht großartig

8
00:00:42,160 --> 00:00:48,440
irgendwelche Danksagungen einbauen kann, was ich natürlich sonst immer sehr gerne mache und wünsche

9
00:00:48,440 --> 00:00:54,680
euch deswegen einfach viel Freude bei der heutigen Episode mit Robert Seifert zu algorithmischer

10
00:00:54,680 --> 00:01:08,240
Sozialität. Herzlich willkommen, Robert. Hallo, Jan. Fangen wir mit der Definitionsfrage an. Was

11
00:01:08,240 --> 00:01:14,080
sind Algorithmen und inwiefern unterscheidet sich vielleicht dein Zugang zu dieser Frage von anderen?

12
00:01:14,080 --> 00:01:18,040
Wogegen kennst du dich vielleicht auch ab? Ja, Algorithmen, das ist natürlich immer eine

13
00:01:18,040 --> 00:01:24,480
sehr schwierige Frage, weil es dafür ganz unterschiedliche Definitionen gibt. Vor allem

14
00:01:24,480 --> 00:01:30,640
gibt es eben auch unterschiedliche Definitionen in den Sozialwissenschaften oder in der Soziologie.

15
00:01:30,640 --> 00:01:37,440
Ich habe mir da ein ganz spezifisches Verständnis angeeignet oder übernommen,

16
00:01:37,440 --> 00:01:43,240
eine Definition, also dass man Algorithmen als relational und prozessual versteht,

17
00:01:43,360 --> 00:01:49,440
diese doppelte Charakteristik ist mir wichtig und das kann man vielleicht am besten verstehen,

18
00:01:49,440 --> 00:01:54,520
wenn man das mal versucht, mit anderen gängigen Definitionen von Algorithmen in eine Beziehung zu

19
00:01:54,520 --> 00:02:02,440
setzen. Also häufig werden Algorithmen definiert als formale Handlungsanweisungen oder als die

20
00:02:02,440 --> 00:02:09,880
Gesamtheit von Regeln von Input und Output, also der Verarbeitung von Input und Output oder als

21
00:02:09,880 --> 00:02:15,640
eine Art Symboltabelle, eine klassische Analogie ist auch immer das Rezept. Also das Rezept sagt

22
00:02:15,640 --> 00:02:22,800
sozusagen genau, was man in welchen Schritten tun, machen soll, wie sie kramen, Zutaten,

23
00:02:22,800 --> 00:02:29,200
in welchen Zeitraum und so weiter und so fort. Und das hat eigentlich zwei Implikationen oder

24
00:02:29,200 --> 00:02:36,120
das hat zwei Voraussetzungen, solche Definitionen. Einerseits wird davon die Hardware von der

25
00:02:36,120 --> 00:02:40,080
Software getrennt, also man sagt eben der Algorithmus ist sozusagen eine formale

26
00:02:40,080 --> 00:02:44,880
Handlungsanweisung, die mit einer Software dann zum Beispiel in ein Programm in Gang gesetzt wird

27
00:02:44,880 --> 00:02:51,160
und man trennt eben die Handlungsanweisung dadurch auch von der Implementierung. Also

28
00:02:51,160 --> 00:02:55,720
die Implementierung des Algorithmus muss dann nochmal durch einen anderen Prozess

29
00:02:55,720 --> 00:03:00,800
vonstatten gehen und für mich ist eben wichtig, dass diese Definition des Algorithmus selbst

30
00:03:00,800 --> 00:03:04,560
schon prozessual ist, dass er sozusagen diese Implementierung selbst schon enthält. Das wäre

31
00:03:04,560 --> 00:03:13,320
sozusagen wichtig, das sind selbst Prozesse. Der Techniksoziologe Roger Häusling sagt,

32
00:03:13,320 --> 00:03:18,560
dass Daten die Produkte von Relationierungen sind und das finde ich eigentlich insofern eine ganz

33
00:03:18,560 --> 00:03:23,120
gute Definition, weil man eben sagen kann, dass Algorithmen eben genau diese Relationierungsprozesse

34
00:03:23,120 --> 00:03:28,960
sind. Also die Algorithmen relationieren Daten und produzieren sie oder co-produzieren sie dabei.

35
00:03:28,960 --> 00:03:32,960
Das heißt, insofern geht eben die Definition des formalen Rezepts ein bisschen an der Realität

36
00:03:33,080 --> 00:03:40,120
vorbei, weil das eben der Algorithm und der Ablauf dieses Prozesses selbst ist. Schreiben sozusagen

37
00:03:40,120 --> 00:03:45,200
Temporalität und Zeitlichkeit in die soziale Welt ein. So kennen wir sie als Soziologen ja auch die

38
00:03:45,200 --> 00:03:51,440
Algorithmen. Wenn wir sie jetzt nicht computerseitig uns anschauen, wie sie geschrieben sind, im Alltag

39
00:03:51,440 --> 00:03:55,760
begegnet man Algorithmen in erster Linie auch durch eine sehr spezifische Zeitlichkeit,

40
00:03:55,960 --> 00:04:06,280
indem sie eben unsere Handlungen mit strukturieren. Bei Shintaro Miyazaki, so ein Medienwissenschaftler,

41
00:04:06,280 --> 00:04:12,320
hat einen sehr schönen Begriff dafür, dass die sich entfalten müssen und sich in Zeit verkörpern.

42
00:04:12,320 --> 00:04:18,120
Das finde ich eine sehr gute Beschreibung dafür. Wie gesagt, also nicht nur ein formales Schema,

43
00:04:18,120 --> 00:04:25,320
ein real ablaufender Prozess und man würde eben sogar sagen, es handelt sich genau genommen um

44
00:04:25,320 --> 00:04:30,720
eine Art Maschine, nicht eine algorithmische Maschine. Diese Definition wiederum ist jetzt

45
00:04:30,720 --> 00:04:38,120
inspiriert von einem Computerwissenschaftler Juri Gurevich, der eben zugleich noch darauf

46
00:04:38,120 --> 00:04:42,640
hinweist, dass diejenigen, die Algorithmen von ihrer Implementierung trennen wollen,

47
00:04:42,640 --> 00:04:46,920
die also sagen Algorithmen und Computerprogramme oder Codes sind verschiedene Dinge, dass die

48
00:04:46,920 --> 00:04:51,800
eben den Begriff der Implementierung für Dinge verwenden, die selber Algorithmen sind. Also die

49
00:04:51,800 --> 00:04:59,120
Implementierung selbst ist eigentlich wieder ein algorithmischer Prozess und das wäre sozusagen,

50
00:04:59,120 --> 00:05:04,200
wenn ich das mal versuchen würde, so knapp zusammenzufassen, einerseits die Spezifik meines

51
00:05:04,200 --> 00:05:10,200
Algorithmusverständnis herauszuarbeiten, aber auch eben da eine Unterscheidung vorzunehmen von dem,

52
00:05:10,200 --> 00:05:15,680
was andere, die zentrale Unterscheidung, wie gesagt, besteht darin, dass glaube ich der Begriff des

53
00:05:15,680 --> 00:05:21,000
Rezeptes extrem irreführend ist, weil das so tatsächlich aus einer technisch-soziologischen

54
00:05:21,000 --> 00:05:26,320
Perspektive eigentlich überhaupt nicht, auf diese Art und Weise Algorithmen eigentlich nicht begegnen.

55
00:05:26,320 --> 00:05:32,520
Und wir kommen später dann auch noch zu ganz konkreten Beispielen, anhand derer, finde ich,

56
00:05:32,520 --> 00:05:37,600
man das wirklich sehr schön auch sehen kann, inwiefern dieses relationale Verständnis von

57
00:05:37,600 --> 00:05:43,920
Algorithmen eigentlich auch nachvollziehbar ist. Bevor wir dahin kommen, interessiert mich aber

58
00:05:43,920 --> 00:05:48,920
noch ein bisschen eine Beschreibung der Ausgangslage, denn du gehst von einer umfassenden,

59
00:05:48,920 --> 00:05:53,760
im Grunde sogar allumfassenden, Zitat, algorithmischen Transformation der Gesellschaft,

60
00:05:53,760 --> 00:06:01,120
Zitat Ende, aus und schreibst da, nochmal zitiert, es gibt keinen Weltzugang und keine

61
00:06:01,120 --> 00:06:07,680
soziale Beziehung, keine sozialen Beziehungen, in denen keine algorithmischen Elemente

62
00:06:07,680 --> 00:06:13,320
eingebunden sind, Zitat Ende. Worum handelt es sich bei dieser algorithmischen Transformation

63
00:06:13,320 --> 00:06:18,280
der Gesellschaft und was bedeutet sie für den Versuch eines Verstehens der Gesellschaften?

64
00:06:18,280 --> 00:06:24,720
Also das ist natürlich eine Formulierung, die gerichtet ist sozusagen von einem Soziologen

65
00:06:24,720 --> 00:06:30,280
an die Soziologie. Das ist einerseits sozusagen ein Gesellschaftsbefund, nicht die Gesellschaftsanalyse

66
00:06:30,280 --> 00:06:35,360
und zugleich aber eben auch eine Herausforderung für die Soziologie, insbesondere die soziologische

67
00:06:35,360 --> 00:06:42,200
Theorie. Dass man eben sagt, die Zentralen für die Intersoziologie, auch wenn es natürlich

68
00:06:42,200 --> 00:06:46,080
mit der Techniksoziologie und den Science and Technology Studies und Medienwissenschaften

69
00:06:46,080 --> 00:06:52,320
natürlich viele oder Mediensoziologie viele spezielle oder Formen der Soziologie gibt,

70
00:06:52,320 --> 00:06:56,480
die sich mit der Auseinandersetzung mit Dingen und Medien beschäftigen, ist eben doch, kann man schon

71
00:06:56,480 --> 00:07:02,080
sagen, die Mainstreamsoziologie beschäftigt mit der Interaktion in der Beziehung von menschlichen

72
00:07:02,080 --> 00:07:06,880
Wesen. Das ist tatsächlich eben so, wenn Sie das jetzt studieren oder den Einführungspunkt

73
00:07:06,880 --> 00:07:14,080
zur Soziologie nehmen, ist eine soziale Beziehung eben Menschen zwischen Menschen. Vielleicht kann

74
00:07:14,080 --> 00:07:21,680
man auch, wenn man Beziehungen thematisiert oder sich dazu anschaut, wie die Soziologie sich

75
00:07:21,680 --> 00:07:25,840
dazu positioniert, kann man vielleicht noch einen Kommunikationsbegriff finden, der das vielleicht

76
00:07:25,840 --> 00:07:30,640
noch abstrakter macht und sich nicht allein auf menschliche Interaktion bezieht, aber die sind

77
00:07:30,640 --> 00:07:36,640
dann meistens eben so abstrakt, dass es dann eigentlich keinen Unterschied mehr macht, wer

78
00:07:36,640 --> 00:07:40,120
sozusagen mit wem kommuniziert. Das können dann Systeme und Menschen und alles Mögliche

79
00:07:40,160 --> 00:07:46,080
miteinander sein. Und wenn man das dann sozusagen so verallgemeinert und sagt, die soziale Beziehung

80
00:07:46,080 --> 00:07:50,680
ist jede Form von Kommunikation, dann kommt man natürlich zu dem nicht sehr überraschenden

81
00:07:50,680 --> 00:07:55,720
Befund, dass wir eigentlich immer schon in einer digitalen Gesellschaft gelebt haben. Das liegt da

82
00:07:55,720 --> 00:08:01,960
einfach sozusagen an dem theoretischen Zuschnitt. Aber nicht, wie gesagt, der dominante

83
00:08:01,960 --> 00:08:07,400
soziologische Zugang, das kann man, mein Lieblings, wenn ich das in der Lehre mache, nehme ich immer

84
00:08:07,400 --> 00:08:12,920
Wikipedia Seite zur Soziologie heraus, steht eben dann auf der ersten Zeile, ist die Wissenschaft

85
00:08:12,920 --> 00:08:18,720
von Menschen und den sozialen Beziehungen. Das steht da ganz selbstverständlich. Und jetzt ist

86
00:08:18,720 --> 00:08:24,480
eben der Befund, dass wenn man sich das anschaut, schon seit einigen Jahren eigentlich davon keine

87
00:08:24,480 --> 00:08:29,600
Rede mehr sein kann, dass das sozusagen der dominante Weltzugang ist, sondern der dominante

88
00:08:29,600 --> 00:08:36,560
Zugang ist eben über technische Dinge oder mit technischen Dingen, dass man sagen kann, die

89
00:08:36,560 --> 00:08:41,200
Face-to-Face, die reine menschliche Interaktion ist sozusagen fast schon ein Spezialfein geworden,

90
00:08:41,200 --> 00:08:45,440
umso mehr jetzt in der ganzen pandemischen Situation, denn auch wir sitzen ja jetzt im

91
00:08:45,440 --> 00:08:49,760
Studio nicht uns gegenüber, sondern auch wir sind hochgradig algorithmisch vermittelt.

92
00:08:49,760 --> 00:08:57,000
Insofern hat uns das sozusagen jetzt diese pandemische Situation das noch stärker sozusagen

93
00:08:57,000 --> 00:09:02,720
transparent gemacht, wie weit wir vermittelt sind und das einerseits sozusagen diese digitale

94
00:09:02,720 --> 00:09:05,960
Vermittlung, aber eben auf der anderen Seite, was du angesprochen hast, diese Frage der

95
00:09:05,960 --> 00:09:12,280
algorithmischen Sozialität, dass es eben nicht allein irgendwie einfach nur technische Gerätschaften

96
00:09:12,280 --> 00:09:16,440
sind, die uns miteinander verbinden, wie eine Wippe oder eine Schaukel oder irgend sowas,

97
00:09:16,440 --> 00:09:22,840
sondern dass es eben immer Techniken sind, die mit algorithmischen Prozessen uns in Verbindung

98
00:09:22,840 --> 00:09:29,480
miteinander setzen. Also ein Beispiel, immer wenn man mit älteren Menschen spricht, heißt es dann

99
00:09:29,520 --> 00:09:34,360
immer, früher konnte man einfach mal bei jemandem vorbeiklingen und klingeln, kommst du runter oder

100
00:09:34,360 --> 00:09:39,120
nicht, machen wir was. Mittlerweile ist das sozusagen völlig undenkbar, dass ich bei jemandem

101
00:09:39,120 --> 00:09:45,400
vor der Tür stehe. Es ist halt immer irgendwie verabredet über Messenger-Apps oder E-Mails oder

102
00:09:45,400 --> 00:09:50,720
wie auch immer. Man sieht halt sozusagen, dass zwischen diese zwischenmenschlichen Beziehungen

103
00:09:50,720 --> 00:09:57,360
eigentlich in fast jeder Form, wie wir uns begegnen, eigentlich algorithmische Prozesse

104
00:09:57,360 --> 00:10:04,040
dazwischengeschaltet sind. Dass es möglicherweise von der Partnersure bis zur Orientierung im Raum,

105
00:10:04,040 --> 00:10:09,720
Landkarten oder jetzt diese Situation hier sich dem Podcast anzuhören oder hinziehen zu produzieren,

106
00:10:09,720 --> 00:10:16,280
alles hochgradig algorithmische Prozesse sind, die ohne das gar nicht erklärbar sind und insofern

107
00:10:16,280 --> 00:10:21,680
eben nicht die Herausforderung an der Soziologie, diesen Interaktions- oder Beziehungsbegriff da

108
00:10:21,680 --> 00:10:26,760
umzustellen. Und da ist jetzt die algorithmische Sozialität eben der Versuch, dem Umstand gerecht

109
00:10:26,760 --> 00:10:32,280
zu werden, dass jede Form der Beziehung in erster Linie über algorithmische Systeme läuft.

110
00:10:32,280 --> 00:10:40,640
Und allgemein bekannter, würde ich sagen, ist ja der Begriff der digitalen Transformation. Du schreibst

111
00:10:40,640 --> 00:10:44,760
eben von der algorithmischen Transformation. Ich erinnere mich da an einen Paper von dir,

112
00:10:44,760 --> 00:10:49,600
wo du zu Beginn eben auch eine Unterscheidung einführst zwischen Digitalisierung 1.0 und

113
00:10:49,600 --> 00:10:54,920
Digitalisierung 2.0. Das scheint mir so ein bisschen in eine ähnliche Richtung zu gehen. Also es ist

114
00:10:54,920 --> 00:10:59,040
schon so zu verstehen, dass du das untereinander auch nochmal unterscheiden würdest, die Frage

115
00:10:59,040 --> 00:11:02,960
der digitalen Transformation und die Frage der algorithmischen Transformation. Genau,

116
00:11:02,960 --> 00:11:06,840
das ist natürlich eine Unterscheidung, die heute von manchen natürlich gemacht wird,

117
00:11:06,840 --> 00:11:13,320
häufig nicht gemacht wird, dass man, wenn man alt genug ist, eigentlich den Digitalisierungsbegriff

118
00:11:13,320 --> 00:11:19,960
nochmal von Begriff der 90er, 2000er Jahre hielt, dann ist der verschwunden. Und seit einigen Jahren

119
00:11:20,240 --> 00:11:25,720
ist er wieder als sozusagen der heißeste Begriff wieder aufgetaucht. Was eigentlich überraschend ist,

120
00:11:25,720 --> 00:11:28,960
dass sozusagen so ein Revival so schnell wieder auftaucht, eine Mode jetzt nicht,

121
00:11:28,960 --> 00:11:33,960
dass er keine 40, 50 Jahre gedauert, sondern der kommt irgendwie nach 20 Jahren wieder raus und

122
00:11:33,960 --> 00:11:40,760
alles ist sozusagen digital. Und da ist jetzt der Versuch, die Unterscheidung hier zu machen,

123
00:11:40,760 --> 00:11:47,880
dass man sagt, Digitalisierung 1.0 ist eben tatsächlich unterschieden von Digitalisierung 2.0,

124
00:11:47,920 --> 00:11:51,760
ganz einfach gesagt durch den Grad der Verallgemeinerung. Dass man eben sagen kann,

125
00:11:51,760 --> 00:11:57,040
in den 90ern, Anfang 2000er Jahre war, wie unsere ehemalige Bundeskanzlerin gesagt hatte,

126
00:11:57,040 --> 00:12:02,760
das Internet tatsächlich noch Neuland. Das war für sie natürlich viel länger Neuland als für andere,

127
00:12:02,760 --> 00:12:06,320
aber da ist es eben tatsächlich eben eine Nischenkultur, da gibt es auch in der Soziologie

128
00:12:06,320 --> 00:12:11,400
oder in Sozialwissenschaften dann auch sozusagen spezielle Soziologien, Internet-Studies,

129
00:12:11,400 --> 00:12:20,200
Software-Studies, die sich sozusagen in irgendwelche Expertennischen einarbeiten und dort die

130
00:12:20,200 --> 00:12:27,480
verwunderlichsten Dinge an die Gesellschaft hervorbringen und davon berichten. Und mittlerweile,

131
00:12:27,480 --> 00:12:31,400
und das ist der Unterschied zur Digitalisierung 2.0, würde ich sagen, hat es einen Grad der

132
00:12:31,400 --> 00:12:38,400
Verallgemeinerung und Verselbstständigung und Selbstverständlichung gefunden, dass man da eben

133
00:12:38,440 --> 00:12:42,600
nicht mehr von einer Nischen- und einer Expertenkultur sprechen kann, sondern das betrifft eben,

134
00:12:42,600 --> 00:12:46,760
wie gesagt, alle. Das, was ich jetzt versucht habe, mit der algorithmischen Sozialität zu

135
00:12:46,760 --> 00:12:51,280
beschreiben, also dass man jetzt sagen kann, in der Digitalisierung 2.0 ist der Punkt erreicht,

136
00:12:51,280 --> 00:12:57,720
wo es eben keine Experten, sondern eine Alltagskultur ist. Was natürlich Konsequenzen hat,

137
00:12:57,720 --> 00:13:03,000
weil was auch viel übersehen wird, weil eben in der Digitalisierung 1.0 in erster Linie

138
00:13:03,560 --> 00:13:09,840
Innenexperten sich damit beschäftigt haben, die einen ganz anderen, ja auch vielleicht technikaffineren,

139
00:13:09,840 --> 00:13:15,920
technisch alphabitisierten Zugang zu den Technologien hatten, wo ich als Soziologe heute sagen würde,

140
00:13:15,920 --> 00:13:20,680
heutzutage ist der Zugang ganz anders, der ist eher implizit. Meistens Leute wissen überhaupt nicht,

141
00:13:20,680 --> 00:13:26,080
wie Algorithmen funktionieren und wie die Technik funktioniert und trotzdem funktionieren und

142
00:13:26,080 --> 00:13:31,280
benutzen sie es richtig. Aber das heißt natürlich, dass eine solche Analyse an diesen beiden Fällen

143
00:13:31,280 --> 00:13:37,200
ganz anders untersuchen muss. Ich kann jetzt nicht so tun, als seien wir alle Softwareingenieure und

144
00:13:37,200 --> 00:13:41,600
daran scheint mir auch das Problem der Aufforderung derjenigen zu sein, die sagen, wir sollten jetzt

145
00:13:41,600 --> 00:13:46,440
alle schon in der Schule Programm schreiben lernen. Das geht eigentlich an der Realität vorbei,

146
00:13:46,440 --> 00:13:52,320
weil das faktisch einfach mit Alltagsgegenständen nie der Fall ist. Die kennt man nie und ein

147
00:13:52,320 --> 00:13:57,520
Phänomen der Veralltäglichung und Selbstständigung besteht immer auch darin, dass man es eben benutzt

148
00:13:57,520 --> 00:14:02,280
wie ein Toaster. Man weiß zwar, wie er angeht, aber sonst interessiert einen auch dazu nicht

149
00:14:02,280 --> 00:14:07,160
sonderlich viel mehr. Und das wäre jetzt nochmal das Argument einerseits, sozusagen analytisch die

150
00:14:07,160 --> 00:14:12,440
beiden zeitlich-historisch zu trennen und zweitens analytisch darauf hinzuweisen, dass man dafür eine

151
00:14:12,440 --> 00:14:17,600
ganz andere Form der soziologischen Untersuchung braucht, weil es eben eine Veralltäglichung ist

152
00:14:17,600 --> 00:14:23,000
und einen anderen Rezipienten und Konsumenten voraussetzt als den noch in der früheren

153
00:14:23,000 --> 00:14:30,600
Phase der Digitalisierung 1.0. Also das würde jetzt ja diese Differenzierung zwischen Digitalisierung

154
00:14:30,600 --> 00:14:36,920
1.0 und Digitalisierung 2.0 nachvollziehbar machen. Inwiefern steht das in Zusammenhang mit diesen

155
00:14:36,920 --> 00:14:43,520
beiden Begriffsparen von algorithmischer Transformation und digitaler Transformation? Gibt es da sozusagen

156
00:14:43,520 --> 00:14:48,960
auch noch eine differenziertere Unterscheidung, was unter dem einen zu verstehen ist und was unter dem

157
00:14:48,960 --> 00:14:56,360
anderen? Das ist eine gute Frage. Also da würde ich bei digitaler Transformation eben schon eher auch

158
00:14:56,360 --> 00:15:03,920
noch an Frage einer vernetzten Gesellschaft denken. Also die Frage der Vernetzung, der Frage der

159
00:15:03,920 --> 00:15:10,200
Kommunikation. Der Vernetzungsbegriff bezieht sich sozusagen nicht notwendigerweise auf die Art und

160
00:15:10,200 --> 00:15:15,600
Weise, wie man in den Netzen prozessiert, sondern der bezieht sich in erster Linie darauf, auf diese

161
00:15:15,600 --> 00:15:21,280
Idee der Weltgesellschaft, dass man sozusagen an allen Formen, an allen Enden miteinander kommunizieren

162
00:15:21,280 --> 00:15:26,960
kann. Und da habe ich den Eindruck, dass der Begriff der digitalen Transformation auch häufig so

163
00:15:26,960 --> 00:15:32,880
verwendet wird. Aber es ist natürlich auch klar, dass mit Digitalisierung auch gemeint ist, häufig

164
00:15:32,880 --> 00:15:37,960
das, was ich sozusagen algorithmische Sozialität beschreibe. Dass man eben bestimmte Apps entwickelt,

165
00:15:37,960 --> 00:15:43,720
die mir erlauben, bestimmte Dinge zu tun. Aber ich würde sagen, dass digitale Transformation sich

166
00:15:43,720 --> 00:15:49,800
sozusagen noch auf noch anderes bezieht. Das heißt, ich mache ja auch die Unterscheidung

167
00:15:49,800 --> 00:15:55,120
zwischen Infrastrukturen und Systemen. Das heißt nicht nur ich, das machen relativ viele. Und eben

168
00:15:55,120 --> 00:15:59,880
digitalen oder algorithmischen Systemen. Und die gesamte digitale Transformation umfasst natürlich

169
00:15:59,880 --> 00:16:05,960
auch den kompletten gesellschaftlichen Umbau der Infrastrukturen. Also Einbau von Kabeln und

170
00:16:05,960 --> 00:16:13,440
Sensoren. Und das ist sozusagen eine massive gesellschaftliche Transformation. Wobei sich

171
00:16:13,720 --> 00:16:21,760
dagegen sich diese algorithmische Sozialität eben eher um die Art und Weise des Prozesses,

172
00:16:21,760 --> 00:16:26,920
sozusagen des Lebensprozesses bezieht. Wie wir miteinander in Verbindung treten. Einfach,

173
00:16:26,920 --> 00:16:32,600
dass man auch sagen kann, dass bestimmte algorithmische Systeme, Apps oder was auch

174
00:16:32,600 --> 00:16:40,560
immer eine gewisse Zeitlichkeit einschreiben. Das ist ja immer ein Navigationsgerät. Ich würde

175
00:16:40,560 --> 00:16:45,160
schon sagen, dass das Phänomen, das da dransteht, immer die Navigationsgerät, wie viel Zeit es bis

176
00:16:45,160 --> 00:16:50,240
zu Ziel ist, die Art und Weise des Fahrerlebens schon grundlegend ändert. Nicht, dass da da steht

177
00:16:50,240 --> 00:16:56,120
nach dreieinhalb Stunden bis zur Ankunft. Das ist, glaube ich, schon eine ganz andere Form. Das

178
00:16:56,120 --> 00:17:01,880
kennen wir aber in anderen Formen auch der Arbeit mit Apps oder mit Software. Dass da immer eine

179
00:17:01,880 --> 00:17:08,400
bestimmte Zeitlichkeit implementiert ist. Nicht, dass wir, wenn wir jetzt Messenger-Apps benutzen,

180
00:17:08,400 --> 00:17:15,680
wir eine bestimmte Erwartung haben, wie lange jemand eine Nachricht schreibt. Das wird häufig

181
00:17:15,680 --> 00:17:19,800
gebrochen, wenn man mit älteren Leuten oder Leuten kommuniziert, die sich damit nicht auskennen.

182
00:17:19,800 --> 00:17:23,480
Dann kann man beobachten, wie der andere oder die andere schreibt und schreibt und schreibt und die

183
00:17:23,480 --> 00:17:28,120
Nachricht wird nicht fertig. Und dann gibt es irgendwie einen Punkt, wo diese Erwartung sozusagen,

184
00:17:28,120 --> 00:17:35,960
was man als normale Antwortzeitraum hinnehmen würde, durchbrachen würde. Ähnlich kennen wir das ja

185
00:17:35,960 --> 00:17:40,160
auch bei E-Mails. Das hat auch sozusagen die Zeitlichkeit, wie wir miteinander interagieren,

186
00:17:40,160 --> 00:17:47,280
vollständig verändert. Eine E-Mail nach 14 Tagen zu beantworten, kann man machen, aber es eben auch

187
00:17:47,280 --> 00:17:54,720
dann meistens fällt auf. Also insofern wäre das vielleicht noch mal der Unterschied zur digitalen

188
00:17:54,720 --> 00:17:58,400
und transformativen algorithmischen Sozialität. Das ist bei der algorithmischen Sozialität eben

189
00:17:58,400 --> 00:18:05,160
wirklich um diese Form, Art und Weise der prozessualen Führung des Lebens auch geht.

190
00:18:05,160 --> 00:18:09,360
Aber da kommen wir vielleicht noch darauf, dass man das eben jetzt nicht, das muss man vielleicht

191
00:18:09,360 --> 00:18:13,560
schon warnen sozusagen, wieder sagen, nicht notwendigerweise nur kritisch versteht und sagt,

192
00:18:13,560 --> 00:18:20,160
wir werden von den algorithmischen Maschinen diszipliniert und auf Linie gebracht. Das ist

193
00:18:20,160 --> 00:18:25,200
damit eigentlich nicht gemeint, sondern eher erstmal ganz analytisch der Umstand, dass es so

194
00:18:25,200 --> 00:18:30,120
eine Art von Interaktion zwischen individuellen, menschlichen Individuen, algorithmischen Systemen

195
00:18:30,120 --> 00:18:37,400
als dominantes Beziehungssystem gibt. Du hast jetzt schon ganz selbstverständlich immer mal

196
00:18:37,400 --> 00:18:42,640
wieder eben dieses Begriff par algorithmische Sozialität auch verwendet. Ich hatte ja zuvor

197
00:18:42,640 --> 00:18:51,840
noch heranbahnend von algorithmischer Transformation gesprochen, aber es ist, glaube ich, trotzdem

198
00:18:51,840 --> 00:18:56,880
jetzt eben dann vielleicht noch mal sinnvoll, sich eben diesen Ausdruck ein bisschen genauer

199
00:18:56,960 --> 00:19:02,440
anzugucken oder was du darunter verstehst unter algorithmischer Sozialität. Es gibt da also

200
00:19:02,440 --> 00:19:08,200
andere Beziehungen, die es in den Blick zu nehmen gilt. Das hattest du jetzt schon beschrieben,

201
00:19:08,200 --> 00:19:14,040
Beziehungen, die auch in dieser Form eben vor der digitalen Transformation der Gesellschaften eben

202
00:19:14,040 --> 00:19:19,160
auch noch nicht existiert haben in dieser Form. Und du fasst es eben dann unter den Begriff der

203
00:19:19,160 --> 00:19:26,720
algorithmischen Sozialität zusammen. Was sind das für Beziehungen, die da eben neu auftreten,

204
00:19:26,720 --> 00:19:31,760
die durch eine algorithmische Sozialität beschrieben werden können? Was haben wir

205
00:19:31,760 --> 00:19:36,520
uns darunter vorzustellen? Ja, wie gesagt, das habe ich vielleicht jetzt schon ein bisschen

206
00:19:36,520 --> 00:19:42,800
zusammen gemischt. Also es geht in erster Linie auch um diese Art von grundlegender Beziehung.

207
00:19:43,440 --> 00:19:49,640
Das geht auch auf den Anfangspunkt zurück, dass wir mit jeder Art der sozusagen

208
00:19:49,640 --> 00:19:58,080
Alltagsorientierung mit Algorithmen, ja, dass wir damit operieren. Also das geht ja sogar so weit,

209
00:19:58,080 --> 00:20:06,920
dass wir sagen würden, wenn ich jetzt entnetzen will und Digital Detox betreiben will, brauche

210
00:20:06,920 --> 00:20:12,000
ich da wahrscheinlich auch ein Navigationsgerät, was mich zum Kampf fährt. Ich also auch dieser

211
00:20:12,000 --> 00:20:16,680
Ausstieg ist sozusagen wieder über solche Orientierungen und Zeitlichkeitsphänomene

212
00:20:16,680 --> 00:20:23,280
vermittelt. Und nicht diese Zeitlichkeit ist mir dabei relativ wichtig. Also die Frage,

213
00:20:23,280 --> 00:20:29,160
was ich jetzt schon beschrieben habe, eben einerseits, dass wir, das kann man sich eben

214
00:20:29,160 --> 00:20:34,160
hier nochmal deutlich klar machen, der Unterschied zwischen Algorithmus und Rezept, ein Navigationsgerät

215
00:20:34,160 --> 00:20:38,600
ist eben nicht allein nur ein Rezept. Da kann man natürlich sagen, da steht sozusagen, jetzt fahren

216
00:20:38,600 --> 00:20:42,560
sie rechts und danach 300 Metern fahren sie wieder links und danach fahren sie wieder links

217
00:20:42,560 --> 00:20:45,640
und dann fahren sie wieder rechts. Das wäre sozusagen, das könnte man aufschreiben, dann

218
00:20:45,640 --> 00:20:50,080
könnte ich mir das durchlesen, dann könnte ich da sozusagen danach folgen. Aber wir wissen eben aus

219
00:20:50,080 --> 00:20:56,880
der eigenen Erfahrung, dass das Navigationsgerät eben ganz anders funktioniert, weil das eben

220
00:20:56,880 --> 00:21:04,920
permanent uns begleitet, das Gerät und uns sozusagen in gewisser Weise auch antreibt oder

221
00:21:05,040 --> 00:21:11,520
zurückhält, bestimmte Dinge zu tun. Oder wenn wir noch weitere Assistenzsysteme in Beziehung setzen,

222
00:21:11,520 --> 00:21:14,840
eben vielleicht auch sagt, dass wir immer mal eine Pause machen sollen und so weiter und so fort nicht.

223
00:21:14,840 --> 00:21:19,160
Aber beim Messenger, das hatte ich beschrieben, dieses Phänomen der Zeitlichkeit, aber auch bei

224
00:21:19,160 --> 00:21:28,120
sozialen Medien ist es eben auch so, wir reden häufig davon, dass es eben darauf ankommt, mit

225
00:21:28,120 --> 00:21:36,520
besonders aufmerksam reichens reichen Bildern oder Posts sozusagen Aufmerksamkeit zu generieren,

226
00:21:36,520 --> 00:21:40,320
auf welchen Medien auch immer, Instagram vielleicht mit Bildern, Twitter muss es halt

227
00:21:40,320 --> 00:21:44,280
besonders witzige Tweets sein. Also das ist sozusagen mit sozialen Medien sehr unterschiedlich,

228
00:21:44,280 --> 00:21:49,320
aber es kommt nicht nur sozusagen darauf an, was man und wie man es postet, sondern eben auch in

229
00:21:49,320 --> 00:21:56,720
welcher Zeitlichkeit. Also dass man das bei diesen sozialen Medien eben auch berücksichtigt, dass

230
00:21:56,720 --> 00:22:02,480
man das mit einer relativ großen Regelmäßigkeit macht. Und wenn man jetzt wenig postet, dann wird

231
00:22:02,480 --> 00:22:07,640
der nächste Post, den man nach zwei Wochen postet, auch entsprechend runtergerankt. Und auch hier haben

232
00:22:07,640 --> 00:22:11,760
wir diese Zeitlichkeit, die ich vorhin bei den E-Mails oder Messenger beschrieben habe, eine

233
00:22:11,760 --> 00:22:17,440
bestimmte erwartbare Zeit überschritten ist, dann wird man sozusagen taucht das nicht mehr in der

234
00:22:17,440 --> 00:22:21,920
Aufmerksamkeit auf, ist auch nicht mehr in der Aufmerksamkeit der eigenen Person auf, weil man

235
00:22:21,920 --> 00:22:27,120
sagt, das ist eine E-Mail, da war vor 14 Tagen, das ist jetzt nicht mehr relevant. Oder eben der

236
00:22:27,120 --> 00:22:34,000
Algorithmus sagt, das, was Sie jetzt hier posten, ist nicht mehr aufmerksamkeitsrelevant, weil du als

237
00:22:34,000 --> 00:22:39,280
Person viel zu selten. Was machst du nicht? Also diese Zeitlichkeit, diese Spezifische findet

238
00:22:39,280 --> 00:22:45,480
man eben da. Und das ist mit dem Begriff der Sozialität eben gemeint, nicht dieser Doppelbegriff

239
00:22:45,480 --> 00:22:50,240
Relationalität, also Beziehungen nicht nur zu Menschen, sondern auch zu Dingen und Prozesse,

240
00:22:50,240 --> 00:22:55,080
das heißt, die Form der Art und Weise, wie man in eine Beziehung miteinander dreht,

241
00:22:55,080 --> 00:23:02,560
ist hochgradig von algorithmischen Prozessen mitgeprägt und umgekehrt. Da nochmal anzuschließen an

242
00:23:02,560 --> 00:23:06,880
dem vorhergehenden Kommentar, prägt man diese Prozesse eben auch mit, nicht? Also das ist

243
00:23:06,880 --> 00:23:11,440
sozusagen auch eine Wechselseitigkeit und nicht nur eine einseitige, determinierende, bestimmte

244
00:23:11,440 --> 00:23:20,960
Beziehung. Ein Aspekt oder ein Teil dessen, was du als algorithmische Sozialität beschreibst,

245
00:23:20,960 --> 00:23:26,440
sind eben verschiedene Formen der Beziehung. Und das sind eben nicht immer nur Formen zwischen

246
00:23:26,440 --> 00:23:35,800
Menschen und Algorithmen, sondern es gibt da eben verschiedenste Interaktionen, die durch diese

247
00:23:36,640 --> 00:23:41,200
algorithmische Sozialität mit in den Blick kommen können. Und das sind eben auch unter anderem

248
00:23:41,200 --> 00:23:46,200
interalgorithmische Beziehungen. Also es geht eben nicht nur darum, damit jetzt beschreibbar zu machen,

249
00:23:46,200 --> 00:23:54,840
dass all unsere gemeinten Menschen leben und unser Handeln von Algorithmen, sagen, auch betroffen

250
00:23:54,840 --> 00:24:01,200
und mitbestimmt und co-produziert sind, sondern es gibt eben auch andere Beziehungen, die damit in

251
00:24:01,200 --> 00:24:05,760
den Blick geraten, unter anderem eben auch interalgorithmische Beziehungen. Interessant

252
00:24:05,760 --> 00:24:10,080
fände ich vielleicht noch, dass du das einmal kurz skizzierst, was das für Formen der Beziehungen

253
00:24:10,080 --> 00:24:14,680
sind und wie man sich denen vielleicht auch nähert. Der Begriff stammt eigentlich von

254
00:24:14,680 --> 00:24:24,560
Karin Knut-Zetina und Donald McKenzie, die das im Zusammenhang der Finanzmarktsozialogie erforscht

255
00:24:24,560 --> 00:24:29,520
haben. Und vor allem bei Donald McKenzie kann man eben schön sehen, also der hat sozusagen diesen

256
00:24:29,520 --> 00:24:35,880
Begriff von Goffman der Interaktion jetzt diese Herausforderung angenommen und hat gesagt, wenn

257
00:24:35,880 --> 00:24:40,800
das stimmt, dass wir nicht mehr wie bei Goffman dominant mit menschlichen Interaktionsbeziehungen

258
00:24:40,800 --> 00:24:46,920
zu tun haben, sondern auch mit Interaktionen, mit technischen Dingen, mit Algorithmen, dann muss man

259
00:24:46,920 --> 00:24:51,280
sich überlegen, ob wir vielleicht sogar so weit gehen könnten, eben die Beziehungen zwischen

260
00:24:51,280 --> 00:24:55,480
Algorithmen, das was du gerade beschrieben hast, selbst als eine Form des Sozialen, als Gegenstand

261
00:24:55,480 --> 00:25:01,200
der Sozialen der Soziologie zu betrachten. Er bejaht das, versucht das und das Argument ist hier, da

262
00:25:01,200 --> 00:25:07,640
geht es um den Hochfrequenzhandel, dass wenn man sich anschaut, wie geht es um Order-Bücher,

263
00:25:07,640 --> 00:25:14,400
Auftragsbücher in elektronischen Börsenplätzen, dass wie da Aufträge gestellt werden, wie die

264
00:25:14,400 --> 00:25:21,320
verkauft und gekauft werden, dass die eben hochgradig Ergebnisse politischer Entscheidungen sind,

265
00:25:21,320 --> 00:25:29,240
hochgradig sozial vermittelt sind. Also schon die Frage, ob ich Preisbruchteile von einem Dollar

266
00:25:29,240 --> 00:25:36,880
oder von zehn Cent festlege, ist eine politische Entscheidung, eine soziale Norm und das materialisiert

267
00:25:36,880 --> 00:25:43,280
sich dann sozusagen in diesen Handelsalgorithmen, die dann eben entscheiden, ob jetzt der Verkauf

268
00:25:43,280 --> 00:25:48,320
noch relevant ist oder eben dann sagen, hier wird an dem Börsenplatz nur in Bruchteilen von zehn

269
00:25:48,320 --> 00:25:53,960
Cent gehandelt, da kann ich sozusagen diesen drei Cent Gewinn gar nicht materialisieren,

270
00:25:53,960 --> 00:26:00,920
weil das auf dem Börsenplatz eben nicht strukturell möglich ist. In diesen Art von Beispielen zeigt

271
00:26:00,920 --> 00:26:07,160
Mackenzie, dass das sozusagen auch jede Form der algorithmischen Interaktivität oder was eben als

272
00:26:07,160 --> 00:26:12,960
algorithmische Interaktivität bezeichnet, eben generell als auch eine soziale Beziehung beschrieben

273
00:26:12,960 --> 00:26:19,720
werden kann, weil sie eben soziale Normen hat, Erwartungen eingeschrieben sind. Auch die Frage,

274
00:26:19,720 --> 00:26:27,280
wenn man sich anschaut, wie Aufträge in den Auftragsbüchern sortiert werden, gibt es da

275
00:26:27,280 --> 00:26:33,000
Warteschlangen und diese Warteschlangen erinnern eben an all das, was wir sonst im normalen Leben an

276
00:26:33,000 --> 00:26:38,560
Warteschlangen auch so kennen und wir kennen dort auch Marktmanipulationsstrategien, wo man

277
00:26:38,560 --> 00:26:43,320
versucht, die Warteschlange auszutricksen, wo man sich vorzudrängelt, wo Hochfrequenzhändler

278
00:26:43,320 --> 00:26:48,480
bestimmte Strategien entwickeln, um sozusagen in eine bessere Position im Auftragsbuch zu kommen,

279
00:26:48,480 --> 00:26:57,440
dass ihre Aufträge eben dann schneller behandelt werden. Das ist jetzt eine Perspektive, die vor

280
00:26:57,440 --> 00:27:02,480
allen Dingen eben in den Science and Technology Studies sehr prominent ist, also diese Form der

281
00:27:02,480 --> 00:27:07,520
Einschreibung der sozialen oder selbst die digitalen Technologien eben so weit zu gehen,

282
00:27:07,680 --> 00:27:12,960
zu sagen, das sind jetzt nicht einfach nur Technologien, sondern selbst das Signal des

283
00:27:12,960 --> 00:27:19,400
WLAN Router Routers ist eben ein sozialer Tatbestand. Das ist natürlich eine Provokation,

284
00:27:19,400 --> 00:27:25,880
aber ich finde das auch an den Beispielen, wie Mackenzie das deutlich macht, schon sehr illustrativ,

285
00:27:25,880 --> 00:27:30,280
weil er eben wirklich zeigen kann, wie all diese Dinge gar nicht entstehen würden. Die sind nicht

286
00:27:30,280 --> 00:27:34,840
einfach nur sozusagen ein Ergebnis einer technischen Evolution, wie uns manche Technik-Freaks

287
00:27:34,840 --> 00:27:38,960
immer einreden wollen, dass das sozusagen aus sich selbst sich heraus gebiert, sondern das sind

288
00:27:38,960 --> 00:27:45,480
hochgradig pfadabhängige Entwicklungen, die auch das Ergebnisse von sozialer und politischen

289
00:27:45,480 --> 00:27:50,240
Entscheidungen sind. Genau das ist ja vielleicht ein Beispiel dafür, wenn man diese interalgorithmischen

290
00:27:50,240 --> 00:27:56,800
Beziehungen vielleicht illustrieren könnte. Ja, das würde ich auch ja sofort unterschreiben.

291
00:27:56,800 --> 00:28:03,680
Interessanterweise ist es für dich dann aber trotzdem sind die subjektalgorithmischen Beziehungen,

292
00:28:03,840 --> 00:28:07,600
eben nicht die interalgorithmischen Beziehungen, sondern die subjektalgorithmischen Beziehungen

293
00:28:07,600 --> 00:28:15,240
für dich eigentlich dann zentral, wenn es um die algorithmische Sozialität geht. Also das ist da

294
00:28:15,240 --> 00:28:21,880
doch in deinem Paper recht stark rübergekommen, dass das ist das, was dich am allermeisten interessiert.

295
00:28:21,880 --> 00:28:29,240
Und was ich auch in diesem Zusammenhang interessant fand, war, dass du da eine Abfolge von

296
00:28:29,320 --> 00:28:35,000
Wechselbeziehungen beschreibst, die eben also Prozesse, die diese Wechselbeziehungen eben

297
00:28:35,000 --> 00:28:42,440
darstellen in der subjektalgorithmischen Beziehung. Und das beginnt mit einer algorithmischen

298
00:28:42,440 --> 00:28:47,120
Mustererkennung, dann zweitens der Versuch einer behavioristischen Verhaltenssteuerung,

299
00:28:47,120 --> 00:28:54,920
dann drittens ein vernarkulares Erkennen und viertens dann digitale Praktiken,

300
00:28:54,920 --> 00:29:01,440
die wiederum das Material für dann eine erneute algorithmische Mustererkennung bilden. Also diese

301
00:29:02,360 --> 00:29:08,080
Abfolge von Prozessen fand ich interessant. Vielleicht wäre es hilfreich, den Zuhörerinnen

302
00:29:08,080 --> 00:29:16,320
das an einem Beispiel einmal zu erläutern. Und im Anschluss würde mich dann interessieren, ob diese

303
00:29:16,320 --> 00:29:24,480
Prozessabfolge für dich bei allen Formen von subjektalgorithmischen Beziehungen so gegeben

304
00:29:24,480 --> 00:29:29,800
sind. Weil ich mich zum Beispiel gefragt habe, ob der Schritt des Versuchs einer behavioristischen

305
00:29:29,800 --> 00:29:35,800
Verhaltenssteuerung, ob der zum Beispiel zwingend vorkommen muss, weil das ist ja eine sehr spezifische

306
00:29:35,800 --> 00:29:45,600
Form des Versuches der Verhaltenslenkung, eine bestimmte Rationalität, die da eingeschrieben ist.

307
00:29:45,600 --> 00:29:53,520
Und auf eine Art fragt man sich ja, ob das zwingend an Algorithmus als Formen gebunden sein muss. Ich

308
00:29:53,520 --> 00:30:00,120
würde eher intuitiv jetzt sagen, nein, hoffentlich nicht. Das geht sozusagen schon mit der letzten

309
00:30:00,120 --> 00:30:07,400
Frage anzufangen auf das, was man eben soziale Algorithmen nennen kann. Also ich würde jetzt

310
00:30:07,400 --> 00:30:11,320
gar nicht sagen, das sind notwendigerweise maschinell lernende Algorithmen, die in der

311
00:30:11,320 --> 00:30:16,440
Lage sind, Feedback zu verarbeiten, sondern es geht hier wirklich um soziale Algorithmen,

312
00:30:16,440 --> 00:30:24,360
die sozusagen in irgendeiner Art und Weise das Verhalten des männlichen Akteurs in der Lage sind

313
00:30:24,360 --> 00:30:30,840
zu verarbeiten. Also nochmal das Navigationsgerät beobachtet einen, wenn man sich verfährt oder

314
00:30:30,840 --> 00:30:35,480
nicht folgt, wird neu ausgerechnet und wird sozusagen eine neue Route vorgeschlagen. Kann

315
00:30:35,480 --> 00:30:41,160
man jetzt natürlich begriffsanalytisch fragen, ob es sich streng genommen dabei bei einer

316
00:30:41,160 --> 00:30:48,120
Verhaltenssteuerung handelt. Aber ich würde schon sagen, dass eben das Navigationsgerät,

317
00:30:48,120 --> 00:30:54,520
das hat natürlich jetzt relativ geringe Möglichkeiten Anreize zu schaffen oder die

318
00:30:54,520 --> 00:30:59,040
Leute zu disziplinieren oder zu strafen, aber ich denke schon, dass der Vorschlag selbst,

319
00:30:59,040 --> 00:31:03,520
den das Navigationsgerät macht, eben zu sagen, jetzt hier links und rechts, wieder rechts,

320
00:31:03,520 --> 00:31:10,880
dass das schon auch Elemente einer Verhaltenssteuerung involviert, würde ich schon

321
00:31:10,920 --> 00:31:21,280
denken. Die erste Frage war genau dieser Kreislauf. Der Impuls dahinter ist der, der Theorie, zwei

322
00:31:21,280 --> 00:31:28,240
Fallen zu entkommen. Die eine Falle ist das, was man jetzt oft mit Interviews mit Ingenieuren und

323
00:31:28,240 --> 00:31:33,880
Computerwissenschaftlern spricht, eben diese Vorstellung, das ist ein Werkzeug, das ist eine

324
00:31:33,880 --> 00:31:38,240
Technik wie alles andere auch, das macht, ich programmiere das, hier der Algorithmus und letztlich

325
00:31:38,240 --> 00:31:43,280
mache da genau das, was ich da reinschreibe. Das sind Produkte menschlichen Intentionen und die

326
00:31:43,280 --> 00:31:48,880
setzen sich da um und dann kann ich dann eben sehen, das hört man häufig, das hört man auch

327
00:31:48,880 --> 00:31:54,440
sogar noch von Börsenhändlern, wo man, wenn man jetzt da ethnographisch beobachtet, sagen kann,

328
00:31:54,440 --> 00:31:58,960
dass das mit Sicherheit nicht der Fall ist, weil die hochgradig auch von den Algorithmen gesteuert

329
00:31:58,960 --> 00:32:07,560
werden in ihrem Verhalten. Aber nicht, das ist sozusagen ein Aspekt und der andere Aspekt ist

330
00:32:07,560 --> 00:32:10,200
natürlich das, was man aus der kritischen Soziologie oder der kritischen Theorie kennt,

331
00:32:10,200 --> 00:32:14,680
die natürlich immer sofort den Begriff der Disziplinierung und Steuerung kennt, das kennen

332
00:32:14,680 --> 00:32:19,040
wir auch aus der populären Literatur, da brauchen wir wahrscheinlich jetzt nicht in dem Medium hier

333
00:32:19,040 --> 00:32:25,200
länger darauf eingehen, das habt ihr wahrscheinlich schon in anderen Formen diskutiert. Also die

334
00:32:25,200 --> 00:32:30,320
Vorstellung nicht, dass wir sozusagen von den sozialen Medien oder was auch immer wieder alle

335
00:32:30,320 --> 00:32:37,280
sozusagen gesteuert sind. Also die beiden Extreme, das zu entgehen und da ist diese Idee eben dieses

336
00:32:37,280 --> 00:32:43,400
Algorithmischen oder diese Objektsubjektiven oder Subjektalgorithmischen Beziehungen,

337
00:32:43,400 --> 00:32:49,200
die auf der einen Seite dem Umstand gerecht wird, dass es natürlich Versuche der Verhaltenssteuerung

338
00:32:49,200 --> 00:32:55,680
gibt, also ich war selbst angefangen bei Navigationsgeräten oder eben Banalität immer

339
00:32:55,680 --> 00:33:00,600
wieder Amazon, sie haben das gekauft, also wollen sie auch das kaufen, kennen wir ja mittlerweile

340
00:33:00,600 --> 00:33:07,960
schon und jetzt ist das umgekehrte Argument, dass wir eben das wahrnehmen und das hatte ich am

341
00:33:07,960 --> 00:33:13,280
Anfang gesagt, das kennzeichnende Digitalisierung 2.0 ist eben, dass wir nicht technikaffin sind

342
00:33:13,280 --> 00:33:18,080
und auch nicht technikalphabetisiert, das heißt wir wissen nicht, warum die Entscheidungen

343
00:33:18,080 --> 00:33:21,520
zustande kommen, wie sie zustande kommen, das hat verschiedene Gründe, weil wir wie gesagt

344
00:33:21,520 --> 00:33:26,880
entweder vielleicht nicht alphabetisiert sind technologisch oder andersrum, weil eben viele

345
00:33:26,920 --> 00:33:31,560
Unternehmen auch ein Interesse daran haben, das intransparent zu halten, wie die Algorithmen zu

346
00:33:31,560 --> 00:33:38,640
ihren Entscheidungen kommen, aber nichtsdestotrotz ist es eben tatsächlich so, dass wir irgendwie aus

347
00:33:38,640 --> 00:33:44,840
diesem Verhalten einen Sinn für uns machen und das ist mit dem Begriff des vernackularen Wissens

348
00:33:44,840 --> 00:33:52,320
beschrieben, das heißt, dass wir implizit irgendwie, wenn wir damit umgehen, ein Verständnis

349
00:33:52,320 --> 00:33:59,760
dafür entwickeln, wie die Dinge funktionieren und hier ist es eben wichtig, nicht den Fehler zu

350
00:33:59,760 --> 00:34:05,920
machen, eine Korrespondenz zu vermuten, also dass man sagt, das könnte man jetzt dieses implizite

351
00:34:05,920 --> 00:34:10,280
Wissen explizit machen, dann wäre ich Computerwissenschaftler, also das wäre jetzt

352
00:34:10,280 --> 00:34:14,640
die Falle, dieses implizite Wissen, deswegen nenne ich es eben vernackulares Wissen und nicht

353
00:34:14,640 --> 00:34:21,520
implizites Wissen, weil es eben eine eigenständige Form, eine Volkssprache, sozusagen eine

354
00:34:21,520 --> 00:34:25,920
eigenständige Sprache oder Kultur ist des Umgangs, für uns zum Beispiel ist bei

355
00:34:25,920 --> 00:34:31,280
etwa der Hashtag, der sozusagen durch die NutzerInnen erfunden wurde, das ist wieder

356
00:34:31,280 --> 00:34:34,680
eine Plattform eingeschrieben, noch hat das irgendjemand mal vorgesehen, hat jemand mal

357
00:34:34,680 --> 00:34:40,440
angefangen, hat das benutzt und es hat sich halt durchgesetzt, das heißt, da wurde nichts

358
00:34:40,440 --> 00:34:47,680
verstanden, da wurde auch nichts irgendwie, ja, kein Code offen gelegt oder keine Schulung

359
00:34:47,720 --> 00:34:52,320
durchgeführt, das hat sich eben sozusagen so entwickelt und egal wie wir jetzt damit

360
00:34:52,320 --> 00:34:59,520
umgehen, jede Form, auch den sozialen Medien entwickeln wir vernäckulare Form des Wissens

361
00:34:59,520 --> 00:35:05,160
darüber, wie gesagt wichtig ist eben, dass die eigentlich überhaupt nicht mit den eigentlichen

362
00:35:05,160 --> 00:35:10,840
Intentionen oder Absichten dieser Technologie viel zu tun haben müssen und dann kommen

363
00:35:10,840 --> 00:35:17,160
wir zum Begriff der digitalen Praktik, weil natürlich Wissen diese Wisseneffekte hat,

364
00:35:17,160 --> 00:35:22,560
man macht dann wieder irgendwas damit und jetzt muss eben dieses System entweder, indem

365
00:35:22,560 --> 00:35:28,160
es jetzt sozusagen aktualisiert wird von Programmieren oder weil es selbst durch machinelle

366
00:35:28,160 --> 00:35:32,080
Lernprozesse vielleicht versucht sich wieder darauf einzustellen, wieder darauf reagieren

367
00:35:32,080 --> 00:35:36,160
und wird da wieder irgendwie was versuchen zu erkennen und das dann wieder einen neuen

368
00:35:36,160 --> 00:35:42,600
Input geben und das Interessante ist sozusagen an diesen Subjekt-Algorithmienbeziehungen,

369
00:35:42,600 --> 00:35:47,120
dass sie eben mit hochgradigen Prozessen sozusagen des wechselseitigen Nicht-Verstehens

370
00:35:47,120 --> 00:35:52,520
zu tun haben, das würde ich sozusagen auch immer den Leuten sozusagen entgegenhalten,

371
00:35:52,520 --> 00:35:59,280
die die Algorithmen sozusagen vermuten, dass die irgendwas aushecken, dass die eben tatsächlich

372
00:35:59,280 --> 00:36:05,400
und das ist mit der Begriff der behavioristischen Modelle schon relativ wichtig, weil diese

373
00:36:05,400 --> 00:36:10,920
Vermutung, dass sich sozusagen in den digitalen Systemen Gesellschaft nur einfach ausdifferenziert

374
00:36:11,000 --> 00:36:17,880
glaube ich relativ naiv ist, weil eben diese Systeme ja nicht differenzierungstheoretisch

375
00:36:17,880 --> 00:36:21,920
oder systemtheoretisch komplex gedacht sind, sondern die auf den einfachsten behavioristischen

376
00:36:21,920 --> 00:36:28,040
Modellen beruhen und wenn ich das als Soziolog sage, dass ich diese behavioristischen Modelle

377
00:36:28,040 --> 00:36:33,120
in der Soziologie als eine unangemessene Beschreibung der gesellschaftlichen Wirklichkeit ansehe,

378
00:36:33,120 --> 00:36:38,480
kann ich ja jetzt nicht mich umgekehrt hinstellen und sagen, jetzt sind sie aber als Algorithmen

379
00:36:38,480 --> 00:36:41,000
auf der Welt und jetzt sind sie auf einmal richtig, das ist natürlich nicht der Fall.

380
00:36:41,000 --> 00:36:46,000
Fakt ist aber dennoch natürlich, dass die uns irgendwie auf eine Art und Weise beeinflussen.

381
00:36:46,000 --> 00:36:53,400
Also irgendwas macht das mit uns und dann muss man jetzt empirisch herausfinden, was das mit uns

382
00:36:53,400 --> 00:36:57,920
macht und interessanter ist eben die Frage nicht, wie die Menschen dann darauf reagieren, welche

383
00:36:57,920 --> 00:37:03,440
neue Formen des vernachlässigen Wissens sie entwickeln und wie sie dann darauf wieder sozusagen

384
00:37:03,440 --> 00:37:10,000
sich wechselseitig irritieren. Ja, also ich muss sagen, mir leuchtet das eh auch total ein. Ich

385
00:37:10,000 --> 00:37:17,000
glaube, was ich mich gefragt habe, war halt vor allem, ob diese spezifische Abfolge von Prozessen

386
00:37:17,000 --> 00:37:24,000
als Wechselbeziehungen, die eben diese Formen des Versuchs behavioristischer Verhaltenssteuerung

387
00:37:24,000 --> 00:37:33,000
mit eingeschrieben hat, ob das nur ein Subset ist der Subjekt algorithmischen Beziehungen oder

388
00:37:33,000 --> 00:37:38,760
ob sich das auf alle Subjekt algorithmischen Beziehungen so umlegen lässt. Das war, glaube

389
00:37:38,760 --> 00:37:42,760
ich, so ein bisschen das, was hast du denn? Du hast ja vielleicht eine Vermutung, wo es nicht passt.

390
00:37:42,760 --> 00:37:48,840
Ja, meine Intuition wäre halt, dass nicht jede Form von Subjekt algorithmischer Beziehung diese

391
00:37:48,840 --> 00:37:56,040
diese Form von behavioristischer Intentionalität sozusagen irgendwie eingeschrieben hat. Was wäre

392
00:37:56,040 --> 00:38:03,400
ein Beispiel? Da müsste ich jetzt nachdenken. Ich glaube, also so ein konkretes Beispiel hätte

393
00:38:03,400 --> 00:38:11,920
ich jetzt nicht sofort. Klar, man kann natürlich immer sagen, man könnte jetzt schon dieses

394
00:38:11,920 --> 00:38:16,600
Beispiel des Navigationsgerätes hinnehmen und schon sozusagen das bestreiten, dass das was mit

395
00:38:16,600 --> 00:38:20,280
einer Verhaltenssteuerung zu tun hat. Man kann natürlich immer sagen, eigentlich ist das nur

396
00:38:20,280 --> 00:38:28,520
eine Form der Beschreibung, sozusagen ein coded space, wo irgendwie eine Art von Prozessualität

397
00:38:28,520 --> 00:38:33,840
eingeschrieben ist. Und das, was ich auf jeden Fall sozusagen umständlich zugestehen würde,

398
00:38:33,840 --> 00:38:38,440
ist, dass das natürlich graduell extrem unterschiedlich ist. Also, dass es natürlich

399
00:38:38,440 --> 00:38:44,560
Formen gibt, wo wir uns viel mehr bereit sind, darauf einzulassen oder auch gar keine andere Wahl

400
00:38:44,560 --> 00:38:50,680
haben, sozusagen auf diese Verhaltenssteuerung und das vielleicht eher hinnehmen. Und in anderen

401
00:38:50,680 --> 00:38:59,200
wurde das halt vielleicht eher weniger. So ist, also man muss vielleicht diese Wechselseitigkeit

402
00:38:59,200 --> 00:39:04,600
kann man vielleicht auch als wechselseitige Beziehung unterschiedlicher Intensität bezeichnen,

403
00:39:04,600 --> 00:39:10,360
nicht die auch graduell unterschiedlich sind. Also ein Beispiel ist immer jetzt bei der empirischen

404
00:39:10,360 --> 00:39:17,920
Forschung zum Hochfrequenzhandel, dass man eben schon sehr unterscheiden kann, wie sehr man sich

405
00:39:17,920 --> 00:39:24,520
bereit ist, von algorithmischen Prozessen zu streuen zu lassen. Das hat eben sehr mit der

406
00:39:24,520 --> 00:39:30,920
Subjektivierungsform auch zu tun. Also wenn ich sozusagen sehr der Vorstellung eines sozusagen

407
00:39:30,920 --> 00:39:37,000
autonomen, selbstermächtigten, reflexiven, rationalen Ichs folge, dann tendiere ich,

408
00:39:37,320 --> 00:39:41,920
eher dazu zu sagen, der Algorithmus ist ein Werkzeug, der das macht, was ich will. Wenn ich

409
00:39:41,920 --> 00:39:48,960
sozusagen eher kooperativ bin, dann bin ich auch eher bereit, mich vielleicht von Prozessen einfach

410
00:39:48,960 --> 00:39:54,600
mal anregen zu lassen, die ich jetzt nicht notwendigerweise verstehe. Das ist feldspezifisch,

411
00:39:54,600 --> 00:40:02,280
glaube ich, auch noch unterschiedlich. Also insofern man das wahrscheinlich jetzt beim Spiel viel

412
00:40:02,280 --> 00:40:09,160
eher bereit ist, sozusagen diese Transgression des Subjekts einzugehen, Immersion, Immersions in

413
00:40:09,160 --> 00:40:14,760
der Kunst und im Spiel nicht. Dort, wo es ernst wird, dann nicht. Bei dem Kapitalismus, da ist es

414
00:40:14,760 --> 00:40:20,920
dann alles ganz, ganz furchtbar. Nini, ich hatte mich ja wirklich gefragt, ob es nicht einfach auch

415
00:40:20,920 --> 00:40:27,720
Kontexte gibt, in denen eben nicht nur ich als Subjekt, willentlicher da mich hineinbegebe,

416
00:40:27,720 --> 00:40:33,320
sondern Kontexte gibt, in denen Algorithmen per se gar nicht aufbauen auf eine Rationalität,

417
00:40:33,320 --> 00:40:39,480
die als eine solche behavioristische Verhaltenssteuerung korrekt geframt wäre.

418
00:40:39,480 --> 00:40:46,200
Das war quasi das. Aber nachdem ihr ohnehin sozusagen ein Beispiel jetzt nicht sofort in

419
00:40:46,200 --> 00:40:56,160
den Kopf springt, gehen wir einfach weiter, würde ich sagen. Also diese verschiedenen Prozesse und

420
00:40:56,200 --> 00:41:03,080
Wechselbeziehungen, die lassen sich, finde ich, sehr schön an einem Beispiel verständlich machen,

421
00:41:03,080 --> 00:41:12,600
das du aufzeigst und zwar anhand des autonomen Fahrens bzw. eigentlich des assistierten Fahrens.

422
00:41:12,600 --> 00:41:19,960
Vielleicht kannst du uns anhand dessen einmal kurz erläutern, wie denn diese Subjekt-Algorithmus-

423
00:41:19,960 --> 00:41:26,560
Beziehungen vonstatten gehen und wie sie auch durch die Perspektive einer algorithmischen

424
00:41:26,560 --> 00:41:33,400
Sozialität eigentlich besonders gut in den Blick genommen werden können und dabei dann vielleicht

425
00:41:33,400 --> 00:41:37,080
auch nochmal diesen Unterschied klarmachen zwischen autonomem Fahren und assistierten

426
00:41:37,080 --> 00:41:42,880
Fahren, weil wir haben oft autonomes Fahren schon gehört und viele nehmen das natürlich

427
00:41:42,880 --> 00:41:49,800
einfach als gegeben voraus, dass das jetzt dann bald ins Haus steht, aber dem scheint ja nicht

428
00:41:49,800 --> 00:41:55,800
wirklich so zu sein. Genau, das ist sozusagen auch eine Beobachtung gewesen, die mir dann

429
00:41:55,800 --> 00:42:00,360
relativ schnell bei dem Phänomen des autonomen bzw. assistierten oder auch vernetzten Fahrens

430
00:42:00,360 --> 00:42:07,240
aufgefallen ist, dass da eben sehr viele Dinge durcheinander kommen. Einerseits strategisch,

431
00:42:07,240 --> 00:42:10,600
von denen diejenigen, die solche Systeme produzieren, das kennen wir ja, die versuchen

432
00:42:10,600 --> 00:42:15,960
sozusagen immer natürlich auch ihre Technologie als viel fähiger zu verkaufen, als sie tatsächlich

433
00:42:16,440 --> 00:42:22,000
ist. Elon Musk ist da ja sozusagen das Paradebeispiel dafür, Sachen zu behaupten, die

434
00:42:22,000 --> 00:42:27,120
faktisch eben nicht umgesetzt sind. Das kann man kritisieren, aber da würde man sagen, gut,

435
00:42:27,120 --> 00:42:33,960
das ist eben DNA-Geschäft. Viel problematischer ist es eben tatsächlich auch bei der Analyse im

436
00:42:33,960 --> 00:42:40,600
Journalismus, aber auch in der Sozialwissenschaft, dass man diese Begriffe nicht sauber trennt. Es

437
00:42:40,600 --> 00:42:44,600
gibt so einen Lieblingsartikel in der FAZ, den Namen jetzt des Autos habe ich vergessen, da heißt

438
00:42:44,600 --> 00:42:50,320
es eben, es gibt sozusagen eine Road-Driver-Kamera, die die Augenbewegungen des Fahrers oder der

439
00:42:50,320 --> 00:42:57,920
Fahrerin beobachtet und dadurch den Zitat, das autonome Fahren sicherer machen will. Diese Kamera

440
00:42:57,920 --> 00:43:03,680
dient dazu, die Person sozusagen dafür zu warnen, wenn sie in den Sekundenschlaf einschläft, also

441
00:43:03,680 --> 00:43:08,600
wie sie einschläft, gibt es ein Signal, wacht man auf. Und wenn man diesen Satz jetzt liest,

442
00:43:08,600 --> 00:43:16,680
fragt man sich natürlich schon, warum muss ein autonomes Fahrzeug die Wachheit des Fahrers oder

443
00:43:16,680 --> 00:43:22,840
der Fahrerin überwachen, wenn es doch heißt, dass diese autonomen Fahrzeuge uns irgendwann mal selbst

444
00:43:22,840 --> 00:43:27,480
fahren sollen. Und wieso ist das eine Entwicklung, eine Technologie auf dem Weg hin zum autonomen

445
00:43:27,480 --> 00:43:33,400
Fahren? Es scheint doch eigentlich eine Entwicklung auf dem Weg zu etwas ganz anderem zu sein, nämlich

446
00:43:33,400 --> 00:43:42,000
nicht zum autonomen Fahren, sondern eben zu einem viel stärkeren Vernetzung von Fahrzeug und Fahrerin.

447
00:43:42,000 --> 00:43:46,440
Das ist sozusagen die Beobachtung, dass hier also zwei Dinge durcheinander geworfen werden,

448
00:43:46,440 --> 00:43:51,840
dass also autonomes Fahren eigentlich in erster Linie nur als ein imaginäres existiert, also

449
00:43:51,840 --> 00:43:57,520
ein imaginäres Versprechen der Produzenten oder eben auch in einer unreflektierten Übernahme von

450
00:43:57,520 --> 00:44:01,720
Journalisten oder sozialwissenschaftlichen Analysen. Und auf der anderen Seite, das assistierte Fahren

451
00:44:02,040 --> 00:44:09,400
tatsächlich eine Praktik ist bzw. eben auch eine Politik. Eine Politik insofern, als man ja

452
00:44:09,400 --> 00:44:14,600
kürzlich lesen konnte, dass verschiedene Assistenzsysteme seit Anfang dieses Jahres in

453
00:44:14,600 --> 00:44:21,440
der Europäischen Union verpflichtend in jedem verkauften Fahrzeug einzubauen sind. Da kann man

454
00:44:21,440 --> 00:44:26,080
eben tatsächlich sehen, dass es nicht nur ein imaginäres ist, eine Imagination oder eine Fantasie,

455
00:44:26,080 --> 00:44:31,120
sondern es ist eben tatsächlich, kann man als Technologie jetzt sagen, die hat sich auf dem

456
00:44:31,120 --> 00:44:35,440
Markt verbreitet und ist insofern auch für die Soziologie eben interessant, weil natürlich die

457
00:44:35,440 --> 00:44:41,600
Individualmobilität, ob man das jetzt gut findet oder nicht, eines der weit verbreitesten Phänomene

458
00:44:41,600 --> 00:44:46,280
oder Gesellschaftsphänomene sind. Da kann man jetzt sehen, dass diese Umsetzung der Assistenzsysteme

459
00:44:46,280 --> 00:44:53,000
natürlich wichtig sind. Und wenn man sich das anschaut, stellt man eben fest, dass diese

460
00:44:53,000 --> 00:45:00,560
Assistenzsysteme eigentlich alle dazu dienen, die Aufmerksamkeit des Fahrers oder der Fahrerin auf

461
00:45:00,560 --> 00:45:05,240
den Verkehr und das auf Fahrzeug zu lenken. Ablenkungsassistenten, Einschlafwarnungen,

462
00:45:05,240 --> 00:45:12,760
alle solche Formen dienen nicht dazu, dass das Auto besser fährt, sondern Spurhalteassistenten,

463
00:45:12,760 --> 00:45:22,160
Notbremsassistenten, alles solche Sachen dienen letztlich nur dazu, den Fahrenden nicht davon

464
00:45:22,160 --> 00:45:25,760
abzuhalten, dass sie einschlafen oder aus dem Fenster gucken oder sie mit ihren Nachbarn

465
00:45:25,760 --> 00:45:29,640
unterhalten. Und da sieht man eben, das sind ganz verschiedene, nochmal um den Begriff der

466
00:45:29,640 --> 00:45:33,040
algorithmischen Sozialität oder auch der Beziehung zurückzukommen, das sind eben ganz andere

467
00:45:33,040 --> 00:45:36,680
Beziehungsformen. Und jetzt wäre der Vorschlag zu sagen, mit dem Begriff der algorithmischen

468
00:45:36,680 --> 00:45:41,640
Sozialität bekommt man dieses Phänomen in den Griff, weil man eben sehen kann, dass hier diese

469
00:45:41,640 --> 00:45:48,960
Beziehung zwischen Fahrzeug und Fahrerin immer weiter intensiviert wird auf der visuellen Ebene,

470
00:45:49,200 --> 00:45:57,160
also die Überwachung der Augen, nicht der Lied, ob man einschläft oder eben ob man von der Bahn

471
00:45:57,160 --> 00:46:03,840
abkommt. Und da finde ich eben dieser relationale Begriff der algorithmischen Sozialität analytisch

472
00:46:03,840 --> 00:46:10,640
viel hilfreicher, weil er eben nicht zu diesen Verwirrungen führt, dass wir irgendwie von autonomen

473
00:46:10,640 --> 00:46:16,600
Fahrzeugen ständigen reden, die weder sozusagen entwickelt sind noch marktfähig sind. Und diese

474
00:46:16,640 --> 00:46:22,360
ganze Rede auch sozusagen dazu führt, dass wir faktisch überhaupt nicht erkennen, mit welcher

475
00:46:22,360 --> 00:46:27,640
gesellschaftlichen Veränderung wir es eigentlich zu tun haben. Dass die gesellschaftliche, relevante

476
00:46:27,640 --> 00:46:34,560
gesellschaftliche Veränderung nicht darin besteht, autonome Fahrzeuge zu haben, die uns es erlauben,

477
00:46:34,560 --> 00:46:39,320
irgendwann einfach mal ein Buch zu lesen im Auto, sondern die eher dazu führen werden,

478
00:46:39,320 --> 00:46:44,600
dass wir mit dem Auto, mit dem Fahrzeug noch auf eine viel intensivere Art und Weise verschmolzen

479
00:46:44,600 --> 00:46:51,040
werden als je zuvor. Also eine Intensivierung, eine Vertiefung der Mensch-Maschine-Beziehung,

480
00:46:51,040 --> 00:46:56,960
würde man eben sagen. Und das wäre jetzt sozusagen das Argument, dass man das nur erkennt, wenn man

481
00:46:56,960 --> 00:47:01,560
eben erstens diese beiden Phänomene unterscheidet. Das eine ist ein imaginäres, das eine ist eine

482
00:47:01,560 --> 00:47:06,520
tatsächliche empirische umgesetzte Praktik oder die Technologie. Und zweitens brauche ich dafür

483
00:47:06,520 --> 00:47:11,800
eben eine Theorie, die das überhaupt beschreiben kann. Und dafür wäre dann der Vorschlag, dass ich

484
00:47:11,800 --> 00:47:16,360
jetzt hier mit dieser algorithmischen Sozialität gleich in die Bresche springen kann.

485
00:47:16,360 --> 00:47:21,760
Diesen Prozess, den du eben gerade skizziert hast, den fasst du dann auch als Ironie der

486
00:47:21,760 --> 00:47:28,480
Assistenz bzw. Dialektik der Assistenz zusammen. Und was ich total interessant fand, und das kam

487
00:47:28,480 --> 00:47:32,440
dir eigentlich doch auch in der Art und Weise, wie du das beschrieben hast, ganz gut durch,

488
00:47:32,440 --> 00:47:36,840
ist, dass es am Ende eigentlich in einer gesteigerten Disziplinierung des Subjekts

489
00:47:36,840 --> 00:47:42,640
auch mit untermünden kann. Und zwar nämlich genau eigentlich diametral entgegengesetzt zu dem,

490
00:47:42,640 --> 00:47:48,720
was als Verkaufssprech dann eigentlich eben angefahren wird, nämlich ein Autonomieversprechen.

491
00:47:48,720 --> 00:47:55,360
Was wiederum interessant ist, weil ja da dann eigentlich man auf eine Art von doppelter

492
00:47:55,360 --> 00:48:01,680
Idealisierung der Autonomie stößt. Es ist ja kein Zufall, dass Elon Musk quasi die Person ist,

493
00:48:01,680 --> 00:48:08,320
die in diesem ja in diesem Move quasi federführend ist, denn der ist ja bekanntermaßen

494
00:48:08,320 --> 00:48:15,000
ein Libertärer, der sozusagen die Idee des autonomen Individuums quasi ganz besonders

495
00:48:15,000 --> 00:48:21,960
hoch hält. Und das fand ich auch total spannend, dass da ja dann auf der einen Seite eigentlich dem

496
00:48:21,960 --> 00:48:28,040
Fahrzeug quasi eine Autonomie zugesprochen wird, die nicht eingelöst wird, weil das Fahrzeug ja

497
00:48:28,040 --> 00:48:32,800
am Ende dann eben nicht autonom Fahrt ist, sondern nur eben quasi assistiert immer noch

498
00:48:32,800 --> 00:48:37,680
von einem Menschen gesteuert wird. Und natürlich dann eigentlich im Verkaufssprech wiederum ja auch

499
00:48:37,680 --> 00:48:45,360
dem der Fahrerin und dem dem Fahrer quasi eigentlich eine Idee von Autonomie irgendwie

500
00:48:45,360 --> 00:48:53,400
ja vorgehalten wird oder nicht vorgehalten, eine Art von Autonomie versprochen wird. Insofern

501
00:48:53,400 --> 00:48:58,000
ist, dass sie quasi während der Fahrt sozusagen sich zurücklehnen kann und eben was lesen,

502
00:48:58,000 --> 00:49:03,000
schlafen, was auch immer. Und beides wird eigentlich eben nicht eingehalten, aber trotzdem

503
00:49:03,000 --> 00:49:09,520
mit der Idee der der Autonomie verkauft. Das fand ich irgendwie ganz ganz spannend und das dann am

504
00:49:09,520 --> 00:49:14,640
Ende als Pointe, die wiederum durch die Theorie algorithmischer Sozialität dann in den Blick

505
00:49:14,640 --> 00:49:19,480
kommt als Pointe, eigentlich auch noch als Sahnehäubchen oben draufgesetzt wird, dass im

506
00:49:19,480 --> 00:49:23,880
Grunde das in der Form von gesteigerte Disziplinierung der Subjekte dann würde das,

507
00:49:23,880 --> 00:49:31,440
fand ich eigentlich als Gesamtbild ganz ganz anschaulich und ironisch eigentlich. Deswegen

508
00:49:31,440 --> 00:49:38,280
war Ironie der Assistenz auch ziemlich treffender Begriff fand ich muss ich sagen. Ja das würde man

509
00:49:38,280 --> 00:49:43,960
jetzt überhaupt nicht, also gerade wenn man jetzt jetzt aus Deutschland kommt, kennen wir ja das

510
00:49:43,960 --> 00:49:50,640
Phänomen, dass der oder die Deutsche gern mit manuell schaltenden Fahrzeugen fährt. Also

511
00:49:50,640 --> 00:49:56,320
Deutschland ist ja das einzige Land, wo es kaum automatische Schalter gibt, weil eben alle sozusagen

512
00:49:56,320 --> 00:50:03,080
kleine Versionen von Michael Schumacher sind. Und ausgehend von dieser Beobachtung würde man

513
00:50:03,080 --> 00:50:08,320
natürlich sagen, es gibt eine bestimmte Widerständigkeit gegen die Vorstellung

514
00:50:08,320 --> 00:50:12,240
autonomer Fahrzeuge. Wenn ich jetzt schon nicht bereit bin eine automatische Gangschaltung zu

515
00:50:12,240 --> 00:50:16,320
abzitieren, dann würde ich natürlich vermuten, dass ich auch gegen die Vorstellung autonomer

516
00:50:16,320 --> 00:50:21,720
Fahrzeuge bin. Und gerade in den USA lässt sich halt genau das gegenteilige Phänomen beobachten,

517
00:50:21,720 --> 00:50:28,280
wie du es schon beschrieben hast, dass da eben die Vorstellung des freien unregulierten Individuens,

518
00:50:28,280 --> 00:50:34,200
also unreguliert nicht vor Gesellschaft sozusagen, sich gespiegelt wird in das Fahrzeug. Nicht,

519
00:50:34,200 --> 00:50:41,520
dass man dort aktive Lobbyarbeit betreibt, dass diese Fahrzeuge nicht vernetzt sein sollen. Also

520
00:50:41,600 --> 00:50:47,160
dass man jetzt sagen würde als Regulierungsbehörde, wir setzen jetzt voraus, dass zum Beispiel die

521
00:50:47,160 --> 00:50:51,680
Fahrzeuge miteinander vernetzt sind, dass sie erkennen können, wenn vorne einer abbremst,

522
00:50:51,680 --> 00:50:55,360
kann der hinten schon abbremsen, kommen sie nicht zu Auffahrunfällen, wäre ja technisch

523
00:50:55,360 --> 00:51:02,640
eigentlich liegt auf der Hand. Das wird sozusagen aktiv unterminiert, weil man eben dadurch für

524
00:51:02,640 --> 00:51:07,840
seinen Eingriff in die Freiheit des Individuums befürchtet, was in dem Moment sich sozusagen

525
00:51:07,840 --> 00:51:12,640
als Fahrzeug reinkarniert hat. Eine ziemlich verrückte Wandlung, die man jetzt wirklich gar

526
00:51:12,640 --> 00:51:16,560
nicht erwarten würde. Genau, und das Ergebnis ist dann, das kann man ja auf YouTube und überall

527
00:51:16,560 --> 00:51:21,440
beobachten, diese Videos, wo irgendwelche Leute in Tesla sitzen und schlafen, die also sozusagen

528
00:51:21,440 --> 00:51:25,920
auf dieses Versprechen von Maske reingefallen sind. Das heißt, dieses Assistenzsystem ja auch

529
00:51:25,920 --> 00:51:30,640
glaube ich Autopilot. Also das wird das sozusagen so verkauft. Das heißt, die fahrenden Kaufen

530
00:51:30,640 --> 00:51:35,920
glauben das eben, dass das so ist, nutzen das dann, kommen es dann eben zu entsprechenden Unfällen.

531
00:51:35,920 --> 00:51:43,120
Das wäre sozusagen eben diese Ironie der Assistenz. Gibt es einige Untersuchungen,

532
00:51:43,120 --> 00:51:48,400
die sagen eben mittlerweile sind diese Assistenzsysteme die größten Quellen von

533
00:51:48,400 --> 00:51:53,520
Verkehrsunfällen. Nicht mehr Alkohol oder irgend sowas, sondern diese Ablenkung durch

534
00:51:53,520 --> 00:51:58,000
Assistenzsysteme oder einfach, indem man die Assistenzsysteme überschätzt oder indem man sie

535
00:51:58,000 --> 00:52:02,600
unterschätzt oder indem man abgelenkt wird, wenn man nicht versteht, nicht wenn man überlastet ist,

536
00:52:02,600 --> 00:52:05,840
man demietet sich vielleicht ein Auto oder jemand ist in entsprechendes Alter erreicht,

537
00:52:05,840 --> 00:52:09,800
versteht nicht so schnell, was diese ganzen Displays alles machen, guckt da drauf und

538
00:52:09,800 --> 00:52:14,040
sozusagen kommt es zu Unfällen. Das wäre sozusagen die Ironie der Assistenz, nicht das

539
00:52:14,040 --> 00:52:19,120
sozusagen jetzt durch Systeme, die eigentlich das Fahren leichter machen sollen, die eigentlich

540
00:52:19,120 --> 00:52:25,800
das verschlimmern. Und die Dialektik der Assistenz besteht eben dann darin, dass man jetzt

541
00:52:25,800 --> 00:52:31,760
Assistenzsysteme einbaut, die genau das wieder verhindern sollen. Also Ablenkung verhindern

542
00:52:31,760 --> 00:52:35,960
sollen, dass ich jetzt eben nicht vor den Assistenzsystemen abgelenkt werde, wird durch

543
00:52:35,960 --> 00:52:43,040
erweiterte Assistenzsysteme verhindert. Das ist eine schon sehr verrückte Entwicklung und hat,

544
00:52:43,040 --> 00:52:49,720
glaube ich, in erster Linie damit zu tun, was ich vorhin angesprochen habe, dass man systematisch

545
00:52:49,720 --> 00:52:55,520
sozusagen diesen Verkaufsversprechen, dass es sich um autonome Fahrzeuge handelt, systematisch auf

546
00:52:55,640 --> 00:53:01,680
Leim gegangen ist. Das ist sozusagen niemand oder es gab natürlich immer Leute, die gewarnt haben

547
00:53:01,680 --> 00:53:06,880
und das wissen nun, aber dass man auch in den zentralen Medien überhaupt nicht versteht, mit

548
00:53:06,880 --> 00:53:12,560
welchen Entwicklungen wir zu tun haben, dass jedes Assistenzsystem eigentlich eine Vertiefung der

549
00:53:12,560 --> 00:53:20,400
Beziehung ist und keine Entlastung in dem Sinne. Aber ich muss das immer ein bisschen beholen,

550
00:53:20,400 --> 00:53:23,080
weil unsere Diskussion jetzt in eine weiche Richtung geht, weil ich eben gerade diese

551
00:53:23,240 --> 00:53:29,520
Disziplinierungsperspektive entkommen will, ist das eben auch eine Disziplinierung des Fahrzeugs.

552
00:53:29,520 --> 00:53:35,640
Das Fahrzeug wird ja jetzt auch gezwungen, ständig aufzupassen, was der Fahrer macht oder die

553
00:53:35,640 --> 00:53:40,800
Fahrerin. Es ist auch eine Subjektivierung des Fahrzeugs gleichzeitig, was jetzt auf einmal sehr

554
00:53:40,800 --> 00:53:47,200
menschlich wird oder zumindest für menschliche Schwächen sich sensibilisieren muss. Da sitzt jetzt

555
00:53:47,200 --> 00:53:51,920
ein Mensch drin, der überschätzt sich, der wird abgelenkt, der tentidiert dazu auch einzuschlafen.

556
00:53:52,080 --> 00:53:58,120
Also haben wir es auch einerseits mit einer wechselseitigen Assistenz, könnte man sagen,

557
00:53:58,120 --> 00:54:03,160
zu tun. Also ich assistiere dem Fahrzeug, das Fahrzeug assistiert mir. Umgekehrt ist das aber

558
00:54:03,160 --> 00:54:08,160
eben auch eine doppelseitige Disziplinierung, insofern als das autonome Fahrzeug ja auch Autonomie

559
00:54:08,160 --> 00:54:15,040
verliert. Es wird immer mehr abhängig davon, dass da jemand drin sitzt und auch das Richtige tut und

560
00:54:15,040 --> 00:54:21,600
die richtigen Knöpfe drückt. Damit haben wir jetzt im Grunde eigentlich einen Weg, der derzeit

561
00:54:21,600 --> 00:54:28,840
beschritten wird, beschrieben. Also diese Abfolge von eigentlich Ironie der Assistenz und dann

562
00:54:28,840 --> 00:54:35,760
Dialektik der Assistenz, dass man also eben in dem Fall mit dem Versprechen der Autonomie

563
00:54:35,760 --> 00:54:42,360
nicht autonome Systeme einbaut, die dann wiederum von den Menschen falsch verstanden werden oder

564
00:54:42,360 --> 00:54:48,640
für bare Münze falsch verstanden werden als autonome Systeme. Daraufhin ihr Verhalten darauf

565
00:54:48,640 --> 00:54:53,880
ausrichten, als wären sie wirklich autonom, was wiederum Dysfunktionalitäten erzeugt,

566
00:54:53,880 --> 00:54:59,680
weshalb man eben die Dialektik der Assistenz wieder neu erweiterte Assistenzsysteme einbauen

567
00:54:59,680 --> 00:55:07,680
muss, die das wiederum versuchen zu adressieren. Das ist sozusagen der eine Strang, der wirklich

568
00:55:07,680 --> 00:55:12,400
gelebte Realität ist, muss man da auch jetzt hinzu sagen, weil wie Roberto ja eben gesagt hast,

569
00:55:12,400 --> 00:55:18,880
das ist jetzt auch quasi schon in die Politik eingegangen. Das ist sozusagen mal der eine Zugang,

570
00:55:18,880 --> 00:55:27,920
der derzeit auch absolut so praktiziert wird und ein anderer Zugang auf dieses Problem in

571
00:55:27,920 --> 00:55:34,200
Anführungsstrichen der störenden Subjekte oder der sozusagen in dem Fall dysfunktional agierenden

572
00:55:34,200 --> 00:55:41,960
Subjekte einzugehen, den beschreibst du als Prinzip des Ausschlusses der eigentlich der

573
00:55:41,960 --> 00:55:48,160
menschlichen Subjekte. Ja, und also mich hat das sehr stark erinnert an einen Vortag, den ich

574
00:55:48,160 --> 00:55:54,360
mal gesehen habe von Benjamin Bretton, in dem er eine Zukunft beschreibt, in der menschliche

575
00:55:54,360 --> 00:56:01,560
AutofahrerInnen als ignorant-individualistische Bedrohungsfaktoren beschreibt, die entgegen jeder

576
00:56:01,560 --> 00:56:08,320
empirischen Fakten lagen, darauf beharren, ein freiheitliches Verkehrsrisiko darstellen zu dürfen.

577
00:56:08,320 --> 00:56:15,440
Ja, und Bretton nimmt da dann halt relativ klar Stellung für ein umfassendes, kollektiv

578
00:56:15,440 --> 00:56:21,320
vernetztes, nicht-menschliches Fahren. Ja, also das ist eben das, was du dann wiederum in deinem

579
00:56:21,320 --> 00:56:30,360
Text mit Verweis auf einen Konzern namens Waymo, der wohl eben auch versucht, autonome Systeme,

580
00:56:30,360 --> 00:56:37,000
also Systeme autonome Fahrens beziehungsweise erweitert assistierenden Fahrens zu entwickeln.

581
00:56:37,000 --> 00:56:44,320
Mit Verweis auf diesen Konzern gehst du eben auch auf diese Variante ein, denn eben auch Waymo

582
00:56:44,320 --> 00:56:50,280
beschreibt, dass im Grunde sagen, in ihrer Perspektive der Mensch eigentlich das größte

583
00:56:50,280 --> 00:56:59,160
Problem ist, denn der Mensch, der ist eben nicht so regelkonform, wie es die programmierten Autos

584
00:56:59,160 --> 00:57:05,800
eben sind, und denen fällt es wiederum unglaublich schwer, auf dieses Abweichen von der Regel wiederum

585
00:57:05,800 --> 00:57:11,960
einzugehen, weswegen dann wiederum die Schlussfolgerung von Waymo ist, man müsse doch eigentlich dann

586
00:57:11,960 --> 00:57:19,480
dafür sorgen, dass dieser Störfaktor nicht mehr partizipiert am Verkehr, am Straßenverkehr und

587
00:57:19,480 --> 00:57:27,480
erst dann könne sozusagen ein Maß an Sicherheit erzeugt werden, dass irgendwie der Sache gerecht

588
00:57:27,480 --> 00:57:32,520
wäre, vielleicht kannst du das uns ein bisschen näher bringen, diesen Zugang des Ausschlusses und

589
00:57:32,520 --> 00:57:38,560
auch was das bedeutet. Die Ausgangslage ist natürlich wieder wie vorhin, also sozusagen die

590
00:57:38,560 --> 00:57:44,400
völlige Verwirrung, was autonomes Fahren tatsächlich in seiner Umsetzung bedeutet, dass wir eben

591
00:57:44,400 --> 00:57:51,360
angefixt sind, Lucy Satchman nennt das sozusagen die Privilegierung der Maschine, also wir schauen

592
00:57:51,360 --> 00:57:56,240
uns an Möglichkeiten und Grenzen, Big Data und maschinelles Lernen und autonomes Fahren, aber

593
00:57:56,240 --> 00:58:02,640
dass das natürlich eigentlich immer nur in Interaktionsbeziehungen auftaucht und als

594
00:58:02,640 --> 00:58:09,400
Technologie völlig uninteressant ist oder nicht sehr aufschlussreich ist, wird halt übersehen und

595
00:58:09,400 --> 00:58:16,200
das sind sozusagen jetzt, kommen diese Unternehmen, stellen halt fest in ihren Analysen, also bestimmte

596
00:58:16,200 --> 00:58:22,280
Abläufe können wir das Fahrzeug sozusagen semi-autonom durch die Stadt führen, aber dann gibt es Leute,

597
00:58:22,280 --> 00:58:27,920
die laufen dabei rot über die Ampel und zum Schluss fahren die Autos sozusagen super vorsichtig im

598
00:58:27,920 --> 00:58:32,160
Schritttempo durch die Stadt, die menschlichen Fahrerinnen, die dahinter sind, verlieren den

599
00:58:32,160 --> 00:58:37,240
Nervenfang an zu hupen und es kam sozusagen auch zu Gewaltausbrüchen gegen diese Testfahrzeuge,

600
00:58:37,240 --> 00:58:44,640
einfach weil man hier ist und das ist eigentlich das, wo ich auch mit diesem ganzen Gemengelage der

601
00:58:44,640 --> 00:58:49,000
algorithmischen Sozialität hinweisen will, dass das Problem, das größte Problem eigentlich nicht

602
00:58:49,040 --> 00:58:53,360
die technische Umsetzung des autonomen Fahrers ist, sondern eben eigentlich die

603
00:58:53,360 --> 00:59:01,240
Subjekt-algorithmische Beziehung, nicht dass man es schafft, Fahrzeuge und menschliche Personen

604
00:59:01,240 --> 00:59:08,720
in Beziehung zu setzen und ähnlich, ich habe das mal auf einer Industriekonferenz mal jemanden,

605
00:59:08,720 --> 00:59:13,920
so ein Ingenieur, gefragt, die sozusagen ihre Präsentation machen, da ist ja dann das autonome

606
00:59:13,920 --> 00:59:20,640
Fahrer immer in fünf Jahren umgesetzt in diesen Präsentationen und dann habe ich eben mal gefragt,

607
00:59:20,640 --> 00:59:25,880
wie das jetzt eigentlich man sich das als Soziologe vorstellen soll, dass wir eben auf der einen Seite

608
00:59:25,880 --> 00:59:32,000
als Soziologe in dem Wissen, dass Menschen eben Normenbrecher sind, sich eben nicht an Normen

609
00:59:32,000 --> 00:59:40,400
halten oder eben dazu tendieren, die Normen immer ein bisschen zu schieben und eben ein Algorithmus,

610
00:59:40,480 --> 00:59:45,040
das halt nicht kann, eine Höchstgeschwindigkeit ist halt eine Höchstgeschwindigkeit, die kann man

611
00:59:45,040 --> 00:59:49,520
jetzt zwar irgendwie manuell diskretionär überschreiben, aber dazu braucht es auch wieder

612
00:59:49,520 --> 00:59:56,400
einen menschlichen Fahrer, das Fahrzeug wird das nicht können und ist eben die Frage, wie können

613
00:59:56,400 --> 01:00:03,440
Regelfolger und Regelbrecher zusammenleben? Das läuft, hat ja Ingenieur dann auch zugegeben,

614
01:00:03,440 --> 01:00:10,640
dass das eigentlich faktisch empirisch nur geht, indem man die Menschen das Fahren verbietet,

615
01:00:10,640 --> 01:00:17,720
also das, was du auch geschrieben hast, indem man sie aus dem Verkehr ausschließt. Wir kennen das

616
01:00:17,720 --> 01:00:22,840
und daran zeigt sich auch in gewisser Weile die Banalität des gesamten Phänomens. Wir kennen

617
01:00:22,840 --> 01:00:28,200
das ja auch von anderen autonomen Fahrzeugen, nämlich zum Beispiel so Zubringer-Shuttle am

618
01:00:28,200 --> 01:00:32,280
Flughafen, die sozusagen autonomig verankert werden. Was man da beobachten kann, ist genau das

619
01:00:32,320 --> 01:00:38,680
Phänomen, das wird halt in hochgradig isolierten Umwelten findet das statt, da gehen Türen zu,

620
01:00:38,680 --> 01:00:44,400
gehen Türen auf, es gibt eine Schiene, auf der das langfährt, das sind sozusagen hochgradig

621
01:00:44,400 --> 01:00:50,000
künstliche Umwelten und auch man sieht man jetzt bei dem autonomen Fahren, die angeblich in diese

622
01:00:50,000 --> 01:00:54,360
Alltagswelt gebracht werden soll, das ist ja Anspruch, dass das autonome Fahrzeug eigentlich

623
01:00:54,360 --> 01:01:00,520
faktisch überall fahren kann auf dem Mond, dass das eben eigentlich nur zufangs funktioniert,

624
01:01:00,520 --> 01:01:07,000
wenn man die eben umweltextrem artificiell zurichtet, also entweder vollpackt mit Sensoren,

625
01:01:07,000 --> 01:01:13,440
dass die Fahrzeuge eben wissen, wo sie sind und eben auch eine massive Ausschluss von Menschen.

626
01:01:13,440 --> 01:01:21,480
Und da gab es jetzt verschiedene Testgelände, verschiedene Städte haben eben Experimente zu

627
01:01:21,480 --> 01:01:26,840
technischen Fahrzeugen gemacht und da gab es eben dann auch diese Phänomene, dass dann faktisch die

628
01:01:26,840 --> 01:01:30,920
gesamte Innenstadt für Menschen abgesperrt ist, nicht? Da haben wir dann sozusagen Zäune an der

629
01:01:30,920 --> 01:01:37,840
Straße, es gibt Schranken beim Fußgängerüberweg, da sieht man also genau das Umgesetz, dass sozusagen

630
01:01:37,840 --> 01:01:44,320
der Regelbrecher Mensch aus diesem Phänomen ausgeschlossen ist und es ist ganz offensichtlich,

631
01:01:44,320 --> 01:01:49,560
dass es entweder darauf hinaus läuft, den Menschen auszuschließen, wie sozusagen beim

632
01:01:49,560 --> 01:01:56,680
Flughafenzubringer, beim Shuttle oder dass es in einer Art von intensivierter Beziehung bedarf,

633
01:01:56,800 --> 01:02:03,960
wo man dann aktiv sagen kann, so kennt man das jetzt nicht auch, wenn man sich mal ein neues Auto mit

634
01:02:03,960 --> 01:02:11,080
so Assistenzsystemen mal fährt und man hat diese Abstandsassistenten und man merkt das sozusagen,

635
01:02:11,080 --> 01:02:16,720
man fährt jetzt auf der Überholspur und der Abstandsassistent hält sozusagen diese Mindestabstand ein

636
01:02:16,720 --> 01:02:22,600
und nachdem das vierte oder fünfte Auto in den Sicherheitsabstand gefahren ist, hat man dann auch

637
01:02:22,600 --> 01:02:26,840
keinen Spaß mehr an diesem Assistenzsystem und wird dann entweder eben diese Distanz verringern,

638
01:02:26,840 --> 01:02:33,160
also eigentlich den vorgeschriebenen Sicherheitsabstand unterschreiten oder man wird die

639
01:02:33,160 --> 01:02:37,600
Geschwindigkeit rosseln müssen und irgendwie heranfahren. Das ist jetzt eben nochmal sozusagen ein

640
01:02:37,600 --> 01:02:44,200
Zeichen dafür oder ein Symptom dafür, dass man eben sagen kann, die wahrscheinlichere Form als

641
01:02:44,200 --> 01:02:48,320
Soziologe, würde ich sagen, ist eine Form von Interaktion, dass man komplette Städte und die

642
01:02:48,320 --> 01:02:53,520
gesamte komplette Fahrindustrie oder Infrastruktur von Menschen absperrt, kann ich mir faktisch

643
01:02:53,520 --> 01:02:59,360
eigentlich nicht vorstellen. Insofern eben nochmal eben das Plädoyer, dass man das mit einer

644
01:02:59,360 --> 01:03:05,160
Beziehungsanalyse eigentlich aufschlüsseln muss, diese Transformation, die sich da vor uns in

645
01:03:05,160 --> 01:03:10,040
Augen abspielt. Oder auf eine Art finde ich ist ja oder vielleicht verstehe ich das noch falsch,

646
01:03:10,040 --> 01:03:15,880
das würde mich jetzt interessieren. Auf eine Art klang das ja jetzt so, als ob durch die

647
01:03:15,880 --> 01:03:22,160
Differenzierung von autonomem Fahren oder dem eben in Anführungsstrichen falschen Versprechen,

648
01:03:22,160 --> 01:03:28,200
von autonomem Fahren und dem anderen Paradigma, nämlich dem vernetzten, koordinierten Fahren,

649
01:03:28,200 --> 01:03:33,960
dass da eigentlich schon auch zwei unterschiedliche Paradigmen sich so ein bisschen aufzeigen. Oder

650
01:03:33,960 --> 01:03:40,400
also das ist das vielleicht dann auch eine Form des Verständnisses von subjektalgorithmischen

651
01:03:40,400 --> 01:03:47,560
Beziehungen und Beziehungshaftigkeit eigentlich da so ein bisschen drin versteckt liegt oder ein

652
01:03:47,560 --> 01:03:52,400
Versprechen vielleicht auch eine Möglichkeit, eine Chance drin liegt. Wenn man also weder das

653
01:03:52,400 --> 01:04:00,240
menschliche Subjekt komplett ausschließen will, noch eigentlich einem falschen Versprechen totaler

654
01:04:00,240 --> 01:04:07,320
Autonomie folgen, das dann in einer gesteigerten gegenseitigen Disziplinierung von sowohl in dem

655
01:04:07,320 --> 01:04:12,080
Fall dem Auto als auch dem Menschen mündet, was gäbe es da vielleicht noch für einen anderen Weg,

656
01:04:12,080 --> 01:04:21,960
mit dieser grundsätzlichen Problematik von subjektalgorithmischen Beziehungen umzugehen.

657
01:04:21,960 --> 01:04:31,040
Und auf eine Art hatte ich das Gefühl, dass die Form des vernetzten koordinierten Fahrens

658
01:04:31,040 --> 01:04:36,400
eigentlich da auch schon versucht zu sagen, einem anderen Paradigma zu folgen,

659
01:04:36,400 --> 01:04:41,840
das eben dann nicht auch in dieser Falle der falschen Autonomie tappt und infolgedessen dann

660
01:04:41,840 --> 01:04:47,560
eben auch andere Lösungsvorschläge anstrebt. Aber wie hätten wir uns die vorzustellen? Wie

661
01:04:47,560 --> 01:04:51,680
kann man sich das vielleicht anhand des Beispiels des autonomen Fahrens dann auch ein bisschen

662
01:04:51,680 --> 01:04:56,040
veranschaulichen, wie quasi diese Beziehung anders gedacht werden kann?

663
01:04:56,040 --> 01:05:02,680
Nein, das gibt es natürlich alles. Das sind auch eigentlich, könnte man jetzt nochmal den

664
01:05:02,680 --> 01:05:08,160
Begriff der Algorithmuskulturen schreiben, es sind eben verschiedene Algorithmuskulturen. Das ist

665
01:05:08,160 --> 01:05:11,880
jetzt vielleicht ein bisschen eine steile These, aber ich habe eigentlich auch ehrlich gesagt ein

666
01:05:11,880 --> 01:05:19,160
bisschen die Vermutung, dass das ein Grund ist, warum wir viel von dem autonomen Fahren aus den USA

667
01:05:19,160 --> 01:05:30,960
hören. Vamo, Musk, Tesla und eigentlich aus Europa gar nicht so sehr. Das, was ich mal sozusagen als

668
01:05:30,960 --> 01:05:35,440
explorative Studie auch untersucht habe in Düsseldorf zum vernetzten Fahren, hieß eben

669
01:05:35,440 --> 01:05:40,400
vernetztes Fahren, und das hatte eigentlich einen ganz anderen Zuschnitt. Das ist also nicht das

670
01:05:40,400 --> 01:05:46,600
Interessesprojekt von technolibratären Individualisten und Millionärin, sondern das

671
01:05:46,600 --> 01:05:50,960
ist ein Projekt einer Stadt. Eine Stadt hat natürlich eine ganz andere Form, ein ganz anderes

672
01:05:50,960 --> 01:05:55,520
Problem. Eine Stadt hat ja überhaupt gar kein Interesse daran, dass das Auto und Fahrzeug an den

673
01:05:55,520 --> 01:06:02,000
Umfährt. Die Stadt hat Interesse daran, dass der Verkehr effizient geregelt wird, dass die

674
01:06:02,000 --> 01:06:06,920
Emissionen gesteuert werden, dass das Parkplatzproblem gelöst werde und dieses vernetzte Fahren in

675
01:06:06,920 --> 01:06:12,040
Düsseldorf zielte eben genau darauf ab. Das sind ganz andere Problemarisierungen, die damit betrieben

676
01:06:12,040 --> 01:06:17,000
werden. Wie kann man verhindern, dass in dem Tunnel es zu Auffahrunfällen, wie kann man den

677
01:06:17,000 --> 01:06:22,240
Sperren, wenn es ein Feuerunfall ist, Verkehrsleitssysteme, nicht, da ist sozusagen die

678
01:06:22,240 --> 01:06:27,880
Frage des vernetzten Fahrens immer eine Frage der Kooperation und Kooperationsprobleme. Also würde

679
01:06:27,880 --> 01:06:35,480
man sagen, wenn eine Stadt sowas betreibt, dann wird sie dieses Phänomen ganz anders angehen, als

680
01:06:35,480 --> 01:06:41,880
wenn ein sozusagen irgendwie ein Egomane das betreibt, der am liebsten mit seinem Auto gleich

681
01:06:41,880 --> 01:06:47,200
noch zum Mond fahren würde, wenn es möglich wäre. Also das unterscheidet sich sozusagen schon.

682
01:06:47,200 --> 01:06:51,640
Interessant war bei dieser explorativen Studie in Düsseldorf eben auch, dass die eigentlich

683
01:06:51,760 --> 01:07:00,080
im Grunde eine Infrastruktur gebaut haben. Also wir bauen Sensoren, Kameras, Bluetooth-Schnittstellen,

684
01:07:00,080 --> 01:07:04,840
bieten wir ein in der Stadt, in der Teststrecke und jetzt bieten wir das Privatunternehmen an,

685
01:07:04,840 --> 01:07:10,520
das zu nutzen. Das Einzige, was richtig gut funktioniert hat, ist sozusagen der öffentliche

686
01:07:10,520 --> 01:07:17,080
Nahverkehr, also die Busse. Dass man jetzt sagt, kann die Busfahrer damit optimieren. Das System

687
01:07:17,080 --> 01:07:21,520
findet jetzt heraus, dass in 30 Sekunden davon erholt wird, kann der Busfahrer auch ein bisschen

688
01:07:21,520 --> 01:07:25,600
länger in der Haltestelle warten und stellen und solche Sachen. Aber man hat dann versucht,

689
01:07:25,600 --> 01:07:29,880
auch Privatunternehmen fort und so einzubinden und eigentlich hat man festgestellt, dass viele

690
01:07:29,880 --> 01:07:34,000
von diesen Unternehmen überhaupt kein Interesse haben, weil sie eben ihre Technologien, die sehr

691
01:07:34,000 --> 01:07:38,640
so sehr auf Individualverkehr abgestellt sind, mit diesen Infrastrukturen eigentlich überhaupt

692
01:07:38,640 --> 01:07:42,440
nichts anfangen können oder einfach auch gar nicht daran interessiert sind. Die sind eben daran

693
01:07:42,440 --> 01:07:50,320
interessiert, Individualfahrzeuge herzubauen und dadurch ist diese gesamte Ausrichtung der Technologie

694
01:07:50,320 --> 01:07:57,960
ganz anders orientiert, als wenn ich sagen würde, ich versuche ein Phänomen zu bauen,

695
01:07:57,960 --> 01:08:03,320
wo wir ein gesellschaftliches Problem lösen und nicht nur ein individuelles. Also insofern ist es

696
01:08:03,320 --> 01:08:08,720
auch empirisch jetzt schon so, dass man eben ganz verschiedene, man kann eben vernetztes Fahren,

697
01:08:08,720 --> 01:08:14,680
ist eigentlich wieder ein anderes Phänomen. Also man kann ja sagen, Assistenzsysteme oder assistiertes

698
01:08:14,680 --> 01:08:19,760
Fahren kann auch vernetzt sein, wenn die Fahrzeuge eben miteinander vernetzt sind und sich beobachten

699
01:08:19,840 --> 01:08:25,840
miteinander kommunizieren, aber vernetztes Fahren kann auch viel ganz anders gemeint sein. Also das

700
01:08:25,840 --> 01:08:31,680
ist eben wirklich um eine Funktion des Gemeinschaftslebens, des gemeinsamen Zusammenlebens

701
01:08:31,680 --> 01:08:36,560
geht. Man sagt, wie kann ich das jetzt so gestalten, dass wir alle einen Bus benutzen können, ohne jetzt

702
01:08:36,560 --> 01:08:40,640
sozusagen ständig irgendwie zu spät zu sein und dann lieber doch wieder das eigene Auto nehmen,

703
01:08:40,640 --> 01:08:47,000
solche Sachen. Das ist ja wirklich ein ganz großartiger Ausblick, finde ich, der sich jetzt

704
01:08:47,000 --> 01:08:54,760
da bietet aus der Analyse der algorithmischen Sozialität heraus eigentlich zu einem Plädoyer

705
01:08:54,760 --> 01:09:02,120
für kooperative Algorithmuskulturen. Das ist ja eigentlich wirklich schön und schließt auch super

706
01:09:02,120 --> 01:09:08,720
an an meine letzte Frage, die da ist. Wenn du dir Zukunft vorstellst, was stimmt dich freudig?

707
01:09:08,720 --> 01:09:17,640
Das ist immer eine schwierige Frage. Also wie gesagt, solche Sachen, solche Phänomene,

708
01:09:17,640 --> 01:09:21,440
wie wir sie jetzt gerade zum Schluss geschrieben haben von Kooperation, sind natürlich viel

709
01:09:21,440 --> 01:09:28,920
spannender. Ob das jetzt sozusagen eine Zukunftsentwicklung ist, kann ich schwer absehen.

710
01:09:28,920 --> 01:09:38,280
Ich bin da ehrlich gesagt als Soziologe auch ein bisschen emotionslos. Sowohl was hoffnungsvolle

711
01:09:38,280 --> 01:09:46,360
Zukunftsszenarien als auch Dystopien betrifft. Ich bin da mehr sozusagen an den tatsächlichen

712
01:09:46,360 --> 01:09:53,760
Veränderungen, die man konkret beobachten kann, interessiert. Und da glaube ich schon, dass man,

713
01:09:53,760 --> 01:09:58,480
das versuche ich auch sozusagen in diesem Konzept der algorithmischen Sozialität zu

714
01:09:58,480 --> 01:10:05,520
berücksichtigen, dass das eben keine festgelegte Sache ist. Also da sind extrem viele Unbestimmtheiten

715
01:10:05,840 --> 01:10:19,400
drin in dem Prozess. Ich dekonstruiere jetzt vielleicht eine Frage zu sehr, dass ich eigentlich eher

716
01:10:19,400 --> 01:10:25,320
Interesse habe oder finde, dass wir eine größere Sensibilität entwickeln sollten für die Momente

717
01:10:25,320 --> 01:10:32,120
der Transformation oder das Werten, in dem wir uns gerade befinden, weil wir eben sehr dazu tendieren,

718
01:10:32,120 --> 01:10:37,760
die aktuellen Entwicklungen entgegweder von der Ausgehen von der Vergangenheit zu denken oder eben

719
01:10:37,760 --> 01:10:43,600
irgendwelche kritischen Befürchtungen gleich mal von der Vergangenheit wieder in die Zukunft klappen.

720
01:10:43,600 --> 01:10:50,760
Insofern, da bin ich wahrscheinlich dann auch wieder zu sehr Deleuzianer, finde ich diese

721
01:10:50,760 --> 01:10:55,600
Fokussierung auf die Zukunft vielleicht eher problematisch und würde eher sagen, was wir

722
01:10:55,600 --> 01:11:02,040
eigentlich sehr selten können, ist zu verstehen, dass in den Momenten, in denen wir leben,

723
01:11:02,040 --> 01:11:07,880
was da eigentlich für Momente möglich sind, Transformationsmomente eigentlich drinstecken,

724
01:11:07,880 --> 01:11:14,000
weil wir möglicherweise eben immer schon sozusagen in Zukunftshoffnungen und Zukunfts- oder

725
01:11:14,000 --> 01:11:20,440
Vergangenheitsbefürchtungen leben. Also, weil ich die Frage vielleicht ein bisschen umdrehe,

726
01:11:20,440 --> 01:11:27,280
dass ich immer sozusagen auf diese Transformation im aktuellen Moment eigentlich, das stimmt mich

727
01:11:27,280 --> 01:11:32,920
sozusagen immer wieder freudig, weil ich glaube, dass man, egal was sozusagen passiert, es nie

728
01:11:32,920 --> 01:11:39,680
vollständig bestimmt ist, wie es weitergeht. Das kann, was jetzt freudig ist, damit auch natürlich

729
01:11:39,680 --> 01:11:43,920
schlecht gewählt, weil es kann auch immer alles viel schlimmer kommen als es eh schon ist, aber es

730
01:11:43,920 --> 01:11:50,560
kann auch eben noch wieder besser kommen. Das ist jetzt so eine sehr unbefriedigende sowohl

731
01:11:50,560 --> 01:11:56,320
als auch Antwort, das tut mir jetzt leid. Finde ich überhaupt nicht. Ich finde, das ist, das beharrt

732
01:11:56,320 --> 01:12:02,560
ja einfach vehement auf der Offenheit, die in jeder Gegenwart auch gegeben ist. Das finde ich

733
01:12:02,560 --> 01:12:06,000
eigentlich ganz großartig und mich persönlich stimmt das auch freudig. Insofern kann ich das

734
01:12:06,000 --> 01:12:11,320
total verstehen als eine Antwort. Ja, ganz super. Und danke dir ganz herzlich für dieses Gespräch.

735
01:12:11,640 --> 01:12:16,840
Ja, danke dir, dass ich hier sein durfte. Und ja, ich genieße das immer sehr, den Podcast,

736
01:12:16,840 --> 01:12:23,040
und werde weiter zuhören, neben was hier in Zukunft noch kommt. Das freut mich. Bis dahin, ciao.

737
01:12:28,040 --> 01:12:35,080
Das war Future Histories für heute. Vielen Dank fürs Zuhören, Show-Notizen und vieles mehr findet

738
01:12:35,080 --> 01:12:41,960
ihr auf www.futurehistories.today. Diskutiert mit auf Twitter unter dem Hashtag Future Histories

739
01:12:41,960 --> 01:12:48,280
oder im eigenen Subreddit. Ihr könnt Future Histories nicht nur auf allen großen Podcast-Plattformen

740
01:12:48,280 --> 01:12:54,960
hören und abonnieren, sondern auch auf YouTube, wo ihr neben den Episoden dann auch Kurzvideos

741
01:12:54,960 --> 01:13:02,240
zu Kernbegriffen einzelner Episoden findet. Schreibt mir gerne unter jan at futurehistories.today. Ich

742
01:13:02,240 --> 01:13:07,760
freue mich immer sehr über interessante Rückmeldungen und Hinweise. Wenn ihr Future

743
01:13:07,760 --> 01:13:14,000
Histories unterstützen wollt, dann könnt ihr das auf patreon.com schrägstrich Future Histories oder

744
01:13:14,000 --> 01:13:19,760
auch via Spende auf unserer Homepage. Future Histories ist eine Produktion von MetaLapses

745
01:13:19,760 --> 01:13:25,800
zu finden auf meta-lapses.net. Bis zum nächsten Mal. Ich freue mich.

