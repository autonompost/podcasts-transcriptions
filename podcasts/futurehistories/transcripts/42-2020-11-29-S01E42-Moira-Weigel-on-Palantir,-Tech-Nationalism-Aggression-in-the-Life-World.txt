Welcome to Future Histories. My name is Jan Groos and I am very pleased to welcome Moira Weigel as today's guest.
Moira is an author, researcher and co-founder of Logic magazine.
She currently holds a position as a socio-technical security researcher with the independent research organization Data Society
and will take up a position as an assistant professor of communication studies at Northwestern University in Boston next year.
There are so many interesting works by Moira that we will unfortunately not be able to cover them all today.
Her book, Labor of Love, The Invention of Dating, has been published in 2016.
And her most recent book, which she wrote together with Ben Tarnoff, is called Voices from the Valley.
Tech workers talk about what they do and how they do it.
However, today's interview is going to be about another work from Moira's rich oeuvre.
The essay Palantir goes to Frankfurt School and I'm very excited to talk about it with Moira today.
But before we start, I'd like to thank Fabian, Adrian and Carmen for their donations.
And I would like to welcome Christine as a patron of Future Histories.
Thanks a thousand times.
And now please enjoy today's episode with Moira Weigel about Palantir, tech nationalism and aggression in the life world.
Welcome, Moira.
Thank you so much for having me.
And I should say the book about dating, if anyone's interested, is out in German, actually.
It's called Dating eine Kulturgeschichte in Germany.
But anyway, thank you so much for having me.
Moira, I got hooked as soon as I saw that your essay was about the ideological roots of Palantir.
But not everybody knows this company.
Maybe you could start by describing what Palantir is and what they do.
Yeah, absolutely.
So Palantir is a software company, an enterprise software company that specializes in big data analytics.
And so what that means effectively is that they develop software for large organizations like the US Department of Defense, for example,
but also for banks and large businesses to help them interpret their data, control who accesses data, how, visualize it, and so on.
There are three big parts of the business.
One is called Palantir Gotham, and that mostly works on defense, government contracts, counterterrorism.
One part is called Palantir Metropolis, and they work with hedge funds and banks and businesses.
And one part is called Palantir Foundry, which is used by other corporations like Airbus, for instance.
There's so much more to say about it.
You know, it's founded by Peter Thiel, Alex Karp, Joe Lonsdale, and Stephen Cohen,
this group that comes out of Stanford and was sort of connected through what's sometimes called the PayPal mafia, the company PayPal.
In the early 2000s, it's founded in 2003 and gets its first big funding from Incutel,
which is the counterintelligence agency, CIA's venture capital arm in the United States.
So it's founded in this moment of the beginning of the war on terror in the United States.
But anyway, I'll wait and answer whatever questions you have about it.
But the reason I say all this is I think it's important to understand about them that they're an enterprise software company.
They are not a company like Google or Facebook, whose business model is to gather data about individuals and develop ads and predictions.
They are a company that wins large contracts from large organizations to interpret data partly through humans.
It's not all automatic.
And so I think I say that because I think that difference is very important to their ideology and to the kind of power they represent.
And I think it's it's a difference that has gotten lost in in some of the media coverage of Palantir in the past.
So I'm happy to say more about it. But that's that's my broad overview of what Palantir is.
In your essay, you closely analyze Alex C. Carp's dissertation Aggression in der Lebenswelt.
Alex Carp is the CEO of Palantir.
And I would like to read a small section from the dissertation you also quoted in your essay on Palantir.
So the quote goes, This work began with the observation that many statements have the effect of relieving unconscious drives,
not in spite, but because of the fact that they are blatantly irrational.
That sounds highly relevant for our societies today, I think.
What is Carp's dissertation about and why is it relevant to an understanding of algorithmic governmentality?
It's a really rich question that you ask.
And I just I want to say sort of two quick things before I dive into it.
I think, you know, I study, I'm trained in studying philosophy and the history of ideas.
And we all like to think philosophy and the history of ideas matters a lot.
Right. But I think I just want to say I think this question of how Carp specific philosophy or Peter Thiel specific philosophy
actually comes into play in a company like Palantir is is complexly mediated.
I mean, of course. But I think there's something I worry about in my own work, frankly.
I worry about this as I as I do what I do, where there's such strong myths of genius in Silicon Valley,
this idea that these sort of genius men just come and and, you know, manifest their ideas
and these totally new and futuristic kinds of software.
And I think that's a myth. I think there's it's important to consider the possibility
that Carp was just like Peter's friend who was around when it was time to make this company.
And and it used software and procedures from PayPal.
But it's like there was some stuff that worked at PayPal and there was all this money going into counterterrorism in the U.S. government.
I say all this to to check.
I don't want to overstate the importance of his philosophy, which I think if folks happen to have watched any of the marketing materials
that Palantir puts out around there around there going public, which happened in September 2020,
that is a kind of advertising ploy that they use him for quite a lot, that he's the philosopher, you know, the sort of deep thinker.
And and to put it kindly, I think there are good reasons he didn't go into academia.
But but I think so I did all that to say I think there's this complicated and really fascinating question of like,
how do ideas get translated into technologies?
And it's not a direct not a direct process.
And I think we can think, too, about the technical like how ideas come to shape how actual technologies are deployed.
And we can think, too, about kinds of organizational practices within the firm and what sort of the sociology of the firm
and how that might or might not be influenced by things that the founders believe.
But I think at a very broad level, one thing that was initially interesting to me about the dissertation or grab my attention
is that it seems like almost too close and almost shockingly close analog for what the company does.
Right. The dissertation uses this concept of jargon from Adorno to ask how, you know, again, really oversimplifying.
But Adorno has this reading of Heidegger's language where he says that an existentialist language broadly where Adorno says
these kinds of terms that are that are popular in existentialist language like dwelling and home and being are actually a kind of mystification.
And what is hidden in those terms is are all these realities of modern domination and violence and exploitation.
And so for it to pick just one example for Adorno, when Heidegger talks about dwelling and dwelling and being
and the sort of precarity of being for Adorno says, you know, this is a mystification of the fact, and I'm simplifying,
but it's sometimes useful to simplify Adorno, that in a modern society, you know, all kinds of people are at risk of being homeless
and precarious and losing their homes, losing their jobs, being actually exposed to to second nature, if not nature.
And and Heidegger's language sort of wraps this up in a in a misty sheen and makes it seem like some sort of spiritual transhistorical unchangeable thing.
Right. So Adorno has this analysis of language.
Karp wants to take this concept and say, how can we look at specific language acts and see, take them apart,
see how language works in particular instances to find not realities of social domination,
you know, what Adorno would call objective forms of social domination, but rather unconscious aggression that people can't admit.
The case study that Karp uses for this, which may be more known to a German speaking audience,
is the speech that Martin Weiser gave in 1996 in Frankfurt when he got the prize of the German book trade,
which is this famous or notorious speech where Martin Weiser says, in short, you know,
all of you listeners expect me to get up and apologize for the Holocaust, but I'm not going to.
And this is kind of sanctimonious and insincere.
And I think he calls it the Holocaust pletigo or something that there are these pressures to be sort of preachers of disaster.
And I'm not going to do it.
And this, of course, is a famous episode in the cultural history of Germany in the 1990s and debates about political correctness and historical memory.
But Karp wants to take this language act and ask what kinds of unconscious aggression or violence are expressed in this language act.
We might say it's maybe not that subtle, actually, but Karp does a reading of Weiser where he says,
you know, we can follow the linguistic moves he makes about his speech and see how his language expresses an unconscious aggression,
you know, an anger at being made guilty, collectively guilty, having to remember the Holocaust publicly that he can't say directly.
He says indirectly through these ways.
And one thing that's amazing about it is Karp then steps back and makes no normative assessment of this at all.
He just leaves it. He sort of says, well done.
It was a good, an effective piece of jargon.
But I think so what's quite interesting inside this is a very long answers.
But what's quite interesting is that when I first look at it, at this dissertation, I think this is almost too perfect as like an analogy to what the work of data analytics is.
Right. Like what Palantir does is analyzes these or builds software to analyze and make sense of all this dispersed data that purports to reveal.
Dangers of terrorism or, you know, much more banal things for their corporate clients like, you know, which airplane door is going to have a problem with it coming out of this factory, something like that.
But the majority of their business, at least until recently, has been for government.
So we can say, you know, what the company does is analyze large data sets and actually, in a sense, try to build diagnostic tools that make visible different aspects of relationships within data.
So to the extent that we can think of big data as a kind of unconscious, which I think is its own philosophically complicated question, which we can get into.
But I think that there is a kind of suggestive resonance or analogy between between the dissertation and the work Palantir does.
And if his dissertation basically is saying, you know, all social groups are formed through unconscious violence, looking at jargon lets us see how if we wanted to be reductive, we could say Palantir assumes, you know, the world is
driven by terrorism and kinds of violence. That's a fact. And this company is going to build tools to let the U.S. Department of Defense see how and that's a simplification.
But I do think that there is a striking analogy between that academic work and then what the company actually does.
I think the question that I was asking myself is, is there a fundamental difference in how it is being done in Palantir than how it is being done with Google?
Because they just to pick some other huge company, you know, because in the way that that Alex Karp is trying to present himself in the public sphere, he's always acting as if they are doing these things differently and that the others are, you know, kind of harshly
going over this nuances of how you should actually handle data and what you can actually understand through the analysis of data.
And I mean, in the way he's presenting himself, he's acting as if they are kind of doing it fundamentally different.
But as just as far as I understood it from the information that I got, I kind of got the impression that they are more or less kind of acting in a similar way when it comes to the presumed production of truth.
Yeah, that's a great, a great question.
I think there's both an empirical and a philosophical question.
And your question, I will start with the empirical and see if I can work out to the philosophical.
So I think there actually are very important differences between Google and Palantir.
And I think since a theme of this, this program is hegemony and power.
I'm keen to talk about them because I think it's something that's not very well understood, but it's going to be really important to understandings for techno power going forward.
We can think of the business model, you know, to be very vulgar materialist about it.
Let's think of the business model.
How does Google make money?
Google mostly makes money through search and keywords and through advertising through targeted advertising.
And so what Google does for the most part is that, you know, what they are incentivized to do through their business model is to collect as much information as possible about individual users so that, you know,
They predict very specifically when I search for baby diapers, you know, that this is the kind of baby diaper I want and I will buy it or something.
You know, this is the ad tech model.
There's a whole other question of how good ad tech actually is.
I've just published a book through Logic magazine.
Not that I wrote that Tim Huang wrote.
Someone else wrote called, in fact, I have it right here, but I guess it's a podcast, so no one can actually see me.
Call the subprime attention crisis, which is by someone who used to work at Google and is arguing that actually the value of the attention that companies like Facebook and Google sell has been hugely inflated and is going to crash and cause systemic economic problems like the housing
But so the Google business model is premised on and incentivizes Google's gathering as much information as possible about us as individuals and about other matters.
We think about, you know, Facebook to Facebook is obsessed with what they call time spent.
They're incentivized to keep everyone in their application for as long as possible so they can gather as much data as possible to spin into these ad tech products.
I don't have the figures ready to mine, but like 90 to 100% of their revenue I want to say like 97% of Google's revenue is still ad tech, even though they do all these other things now too.
Palantir is a very different business model.
That's not how Palantir makes money, gathering information about individuals and selling it to other other people.
Palantir makes money because they go, let's say, to the US military and say to the US military, look, you have at this point, nearly 20 years of, I'm making this up, I don't know if this is a real example, but 20 years of camera footage of all these places in Iraq and Afghanistan that you invaded.
You have all this battlefield data.
It's a mess. It's not cleaned up. It's not able to be interpreted very easily.
And so what we're going to do for, I think their most recent contract with the US military is something like $850 million over 10 years, is we are going to work with you, and we will send engineers like consultants to work with you, to build the best of the best of the best.
The US military is something like $850 million over 10 years, is we are going to work with you, and we will send engineers like consultants to work with you, to build software systems for your company that help you make sense of that data.
That's what Palantir does. That's how they make their money.
There was this debate I saw among many investors about whether they were overvalued because actually their products rely on a lot of human labor, like a lot of what they're selling are these, what they call forward deployed engineers who go to work with a particular company.
So, and this is a fundamentally different business model, and it creates different incentives. And it's also why, you know, if listeners have paid some more detailed attention to Palantir, CARP and others around Palantir often make this big deal about how they care about privacy and they care about civil liberties.
And again, I think that's something when people who, for good reasons, do not spend as much time thinking about Palantir as I do, say sort of like, what are they talking about?
That's real. That's not a lie. They don't. Their business model isn't to know everything about me so that they can sell, you know, a diaper company the chance to sell me diapers.
Their business model is to build, so sell to large enterprises, the tools that, or governments, the tools that those large entities will use to interpret all the data they have.
And this is a very different model. And so I think, and then I'll stop going on, but I think part of why Palantir, Amazon is also very interesting to me in this way, and I'm writing about Amazon a bit at the present.
But I think that Palantir represents this other mode of techno power.
I've been playing with calling it ontological power, where, you know, we now have, in contrast to when I started Logic magazine and started working on this, there's actually now quite a lot of public tech criticism.
You know, we have had Shoshana Zuboff's book Surveillance Capitalism.
We have a lot more public awareness and critique, I think, of how companies may be infringed on individual privacy, how they may be, you know, the way Shoshana Zuboff describes it, interfere with our free will.
What Palantir does is not the same as that. And it represents this other form of power that is no less threatening, I would say more threatening, but where they will build the ways that government agencies communicate with each other and share data and so on.
I think it's also important to see this because I think it helps us start to understand why their philosophy, ideology, culture is so different from a company like Google.
If the way you make money is that you let everyone in the world, except in the PRC, except in the People's Republic of China, although they'd love to be there, I'm sure.
But if you let everyone in the world use your search platform and then build ad tools based on the data you gather about them to sell to people who sell them products, you don't have a strong ideology.
You can make money off of everyone. Maybe I want to buy a socialist book. Maybe my neighbor wants to buy a neo-fascist book. It doesn't matter to Google. You can have this very sort of libertarian open philosophy and culture, which Google historically has had, changing a bit now that they're trying to get into cloud and defense contracts, which I think is important.
But Palantir is not doing that. They're selling to very large clients, corporate clients, government clients, and what they're selling is different. And once I think you see that, once I could see that after doing more research on them, the things that had bewildered me about how culturally different they were from companies like Google and Facebook start to make more sense.
They have a material basis based on the business model and based on the form of power they represent. And there's a good reason why they seem to represent something different from the Californian ideology, sort of libertarian open liberal libertarian consensus that I think historically we've associated with Silicon Valley. They're very different.
In what I understand as algorithmic governmentality, there's a certain way of trying to extrapolate from the behavior of others, your behavior, for example. So the assumption is that through collecting enough data on people who have similar past behaviors than you do, they're trying to extrapolate a possible future of Moira.
Within this action, there's a transition from prediction to prescription, actually, because based on the assumptions that are made, then their architectures are built based on this assumption. And it might be totally incorrect, you know, because the past of others is not Moira's future.
It might be self-fulfilling, you know, if you can't get a loan and then she goes further into debt, so her credit score is worse and worse, it will get harder to get a loan.
Absolutely, exactly. And there's a, and I think that's it's a specific epistemology of how to approach the idea of data. And I'm wondering whether this is being shared at Palantir as well, because, and I'm asking this because they are acting as if they are handling things differently, not only when it comes to the question of privacy, not only when it comes to the business model, which
is not collecting these huge amounts of data. But to me, it seemed when I was researching the topic a bit, you know, in preparation for this episode, to me, it seemed as if what Alex Carp actually is trying to say is that those people, they don't understand what data actually is. And when they are acting in the way we just talked about this specific idea of
governmentality, they are getting things wrong. Yeah. And I'm asking myself, what is he saying then? What is he saying? Is he saying that they are having a different approach and a different epistemological approach on how to handle data within Palantir? And that's actually a big question.
And it's totally related to the dissertation, I guess, in some way, because there's this kind of behaviorist idea within the dissertation that people do actually not know themselves correctly, and that their behavior is telling more about them and that the drives that are within them are so subconscious that they are not actually able to be aware of it.
And that through the correlation within the data, you might be able to see the form and that the form is actually the message.
I certainly see the connection to the dissertation, and I'm trying to think about how it connects to Palantir about what kind of attitude is it toward data to assume that certain kind of inferences can be made and that they are descriptive as opposed to prescriptive.
You know, I think I talk about in my article, this idea which others like Cathy O'Neill and Wendy Choon have written about, but this idea that machine learning can only replicate the past is a pretty familiar idea, right?
Because any machine learning algorithm is trained on existing data sets and how you train the algorithm to be right, as you see, did it reproduce the past results from this data set we have?
This is why, to use an example that I like to use with engineering students, when I speak to them, I sometimes ask, how would you design a non-racist predictive policing algorithm or really any kind of algorithm having to do with policing in the United States?
And often, you know, they think about it like, oh, well, could you use this? Could you exclude this and control it? And the point of the exercise is to show, in my view, there is no way you possibly could because everything about any data in the United States is
riven by the history of chattel slavery and economic inequality and so on and so forth. There's no data you could use to train something to predict a different future.
Wendy Choon, the theorist of technology, has this line I like where she says, what if we looked at machine learning systems the way we look at climate change models? Not like they're predicting something true, but that they're predicting something we want to prevent.
You know, this is like a bad thing about the past. We don't want to replicate. So I think you're phrasing, and this is very interesting in the context of Karp's dissertation.
I talk about this a bit in my piece because the dissertation suggests, actually, or at least Adorno's reading of jargon suggests a critique of precisely this kind of decontextualization, right?
So, you know, if Adorno's talking in a rather different context, but if he says, you know, dwelling has this meaning for Heidegger, but it's been totally abstracted from the context of people losing their homes and being homeless.
And therefore it sort of mystifies the way Heidegger talks about dwelling, mystifies and spiritualizes this fact, which is actually about the history of human oppression and exploitation.
I think we could see some really interesting analogies to the way machine learning and algorithmic governmentality might work.
There are some interesting experiments I want to mention in trying to do something closer to what Wendy Choon is talking about.
I think, for instance, suggesting what if you took a predictive policing algorithm, but then instead of using it to say, oh, you should police these people more heavily, used it to say, oh, you need more social services here.
You know, to allocate resources for redistribution or something.
This is just speculative.
But I think, so the analogy, if Adorno says about Heidegger, he takes this concept and decontextualizes it and spiritualizes it.
We could say that's what forms of algorithmic governmentality do with the data they're built on, right?
They're built on, say, a particular history of structural racism in the US and decontextualized from that become sort of an engine for predicting who is more likely to commit a crime or something in a way that makes that seem as if it's just true and reliable and not connected to these very specific histories of oppression and exploitation.
In terms of how closely that relates to the work Palantir does, to be quite honest, I think this is a question I'm still trying to answer in some of my research because they've done work with police departments.
And there's a horrifying, when you were making a joke earlier, Jan, about someone else's past predicting Moira's future, I was thinking of this horrifying interview that some Palantir representative gave in 2015 about their predictive policing program in New Orleans.
And a critic, it was on national public radio in the United States, NPR, and a critic pointed out the kinds of predictive analytics you're using are well known to disproportionately lead to policing of Black people and Hispanic people and poor people.
And isn't that a problem? And this spokeswoman, whose name I'm forgetting, said, well, as long as your cousin isn't a drug dealer, you'll be fine.
Which was amazing to me to have someone say that on public radio, as long as your cousin isn't a drug dealer, you'll be fine.
And so a very different idea of innocence and guilt and justice and will than the US legal system is supposed to operate with.
And so I think there is evidence that they have done some work historically that corresponds to the kind of epistemology of data we're talking about.
But I do, as I understand it, think that a lot of their business is not so much about building these tools of prediction as building means of interpretation for large entities.
And what kind of epistemology that involves is a really interesting question that I want to think more about, right?
Because it involves all kinds of assumptions about what data matters, about what is connected to what, who should be able to access what.
I used earlier the example of immigration authorities accessing health data, like what parts of government should be able to speak to one another.
But my first reaction is that it may be a bit different than that algorithmic governmentality.
It may be more about the power to set standards and set terms.
I'm thinking now of James C. Scott's famous book Seeing Like a State, but it's like, you know, who gets to set how the state sees people's data?
Seems like maybe it introduces a slightly different problematic, but that's something I'm still thinking through very much.
So that's a provisional answer.
Let's dig a little deeper into these alt-right tech liaisons.
Maybe for our listeners, just some general information.
In general, the Silicon Valley is thought to be kind of more liberal, and the programming and software developers scene is more or less seen as liberal, libertarian, or maybe even anarchist leaning.
And we will maybe come back to this later.
But there are also other tendencies, like the so-called neo-reactionaries, which formed around a software developer with the pseudonym Menchus Mobock.
His real name is Curtis Jarvin, and Jarvin's neo-reactionary world of thought has also been taken up by the philosopher Nick Land, for example,
who in turn will be known to many people through his work on accelerationism or the CCRU, the Cybernetic Culture Research Unit, which he co-founded in the 1990s.
And then there is Peter Thiel, you already mentioned him, co-founder of PayPal and also co-founder of Palantir, and a prominent figure in the right-wing conservative, in this case, ultra-libertarian camp.
Peter Thiel, for his part, was one of the few who backed Donald Trump when he was running for president in 2016, and subsequently also advised him at least for a time.
And Menchus Mobock, he was a guest of Peter Thiel on the election evening 2016 to view the election results.
So there are very short distances between radical reactionary positions such as those of Mobock and central positions of power.
What kind of worldview is this so-called neo-reactionary dark enlightenment, and what influence does it have in the tech industry?
It's a great question, and I'm so glad you mentioned that Curtis Yarvin and Peter Thiel watched the election together because it was a friend of mine, Joe Bernstein, who first reported that in 2017 when he got access to all these Breitbart emails, and I thought that was a delicious find.
And all of the things that he dug up, interestingly, just to add to it, I remember one of the other emails that this reporter got from Breitbart was an email afterwards between Milo Yiannopoulos and Menchus Mobock, Curtis Yarvin, where Milo was saying, well, Peter Thiel isn't that enlightened, right?
He's not that far, right? And Curtis Yarvin said, oh no, you'd be surprised, he's more enlightened than you'd think, just plays it very close, just is very careful about it.
And I'll add just one more fun fact, which is that when the New Yorker journalist Andrew Marantz went to the deplorable, which was this big party for the alt-right thrown at the Trump inauguration in January 2017, Peter Thiel was apparently there just being very quiet and not socializing in the background.
So it's a funny scene in Andrew's book where he runs into Peter Thiel at the deplorable and tries to talk to him and Peter Thiel sort of vanishes and avoids him.
I think these social networks are complex and it's an area I'm still doing research in.
I think it's hard to generalize because there are a lot of different ideologies floating around.
And my macroscopic view is that, you know, since roughly the 1990s, if there was this kind of liberal-libertarian consensus that maybe reflected a certain neoliberal hegemony or compromise, which, you know, we saw throughout U.S. politics, the third way, Democratic, Republican, getting along.
Sort of cultural liberalism, economic, old-fashioned liberalism, compromise that Richard Barber, who you've had on, I'm sure can speak to better than I will.
That since 2016, we've seen a kind of crap up of this common sense in the United States.
And I think a lot of different formations are coming out of that.
And it's not entirely clear, especially under a Biden administration or whatever strange era we're entering now, preferable to another Trump administration, but nonetheless, strange era, in my view, we're entering how that will play out.
I think that there's a very strong, I don't know whether to call it elitist, monarchist kind of tendency that someone like Mentis Moebuck represents that takes, again, from critical theory aspects of the critique of liberal modernity, but rather than saying, as, for instance, Habermas does, that then the project of philosophy has to be to help achieve democracy, you know, achieve liberal democracy.
A promise of it that's never been achieved, instead just takes the critique and says, and this is why modernity and liberal democracy are bankrupt projects.
I think that there is a strong tendency and one could be philosophical about it.
You know, there are these Leo Strauss reading groups among venture capitalists in Silicon Valley where they read Strauss and talk about Plato and the Republic of Philosopher Kings.
You know, I think one could be, and Thiel has a version of this in his reading of Girard, you know, in terms of how the powerful visionaries will always be treated as scapegoats and so on.
I think that one can go to a lot of philosophy and talk about their ideas.
One could also, you know, say these are a bunch of mostly white male engineers who think they know what to do and shouldn't have to listen to anyone else.
I sometimes question how much philosophy we need to explain the worldview.
But I think, yeah, there are, there's certainly a lot of social proximity between the Thiel Founders Fund Network and these sort of right wing networks in the tech industry.
A number of sort of prominent figures have turned out to be software engineers or come from that world.
I think, yeah, that's interesting.
I think there's this question about Thiel that's come up specifically and about his nationalism.
You know, what's his name?
Lucky Palmer, who started Oculus and left, he left Facebook basically over, I forget what the company officially said, but afterwards Lucky Palmer said publicly, you know, it was because they were angry that I was an open Trump supporter and I left.
He has called it tech nationalism.
There's a new kind of tech conservatism or tech nationalism.
I think given Peter Thiel's public support of Trump and the funding he's put into campaigns like those of Chris Kobach, who's a very anti-immigration politician in Kansas and the United States, and the ways he sort of placed people in the White House.
He placed this chief technical technology officer, CTO, in the White House, that there's been this question of like, well, how does nationalism fit with that kind of libertarianism, which is supposedly anti-statist and anti-government?
I think the idea that corporations should take over government is actually pretty consistent with this kind of libertarianism and pretty consistent with the sort of monarchist view of someone like mold bug that, you know, the state is failing and you need these engineers who are more competent or sort of philosopher king types to take it over.
So I think it's actually maybe less contradictory than it appears on its surface, but I do think there's this new nationalist emphasis or has been under the Trump administration.
Again, what will happen under the Biden administration will be interesting to see.
I think my prediction is that it won't make that much difference actually to their business.
But maybe in terms of ideological tone or how they talk about it, it'll be different.
I don't know. Yeah.
But there's certainly lots of proximity.
And I think a similar partial appropriation of critical theories, critique of modernity, is a continuity with the Karp thesis.
I've gone on way too long as usual, so sorry.
Ultimately, I'm interested in the various ideological shades of these questions because I think there's actually quite obviously a new competition of meta narratives, you know, the spot that has been taken by neoliberal variants of different sorts.
It's quite open now because I mean, of course, now we have Biden and it seems as if a neoliberal approach kind of again got their way somehow.
But I don't think that like in the medium and long run, it is a narrative that is still convincing to the majority so that there is an open spot in terms of meta narratives.
And that's why I'm also interested in the way that Palantir, Karp and Thiel and others like that try to present themselves as if they have a different kind of meta narrative, also for people in the tech industry specifically.
And we already have given much space to this reactionary and near reactionary positions, which I think is important because there are many dangers arising there.
But I think it's also important to look to different positions.
And that is actually what you're doing with Ben Tarnoff in Voices from the Valley.
And I have to say I didn't yet read it. I didn't have the time to read it.
I only listened to interviews with Ben and you and I'm highly excited about the book and I'm very much looking forward to reading it.
What other positions have you encountered in your work on the book, perhaps especially with regard to this open question of meta narratives?
It's absolutely this moment of new openings for opening for new meta narratives.
And we're seeing those on the left and the right and somewhere between.
And I think also it's important for me as a materialist anyway to attend to what this has to do with changes in the technology and in the business of Silicon Valley.
Silicon Valley started as a government project, right?
It was funded by the US military in the beginning.
It went through this process of privatization and deregulation in the 1990s.
In that moment of neoliberal consensus forming, if we want to call it that.
But it may be entering another phase.
And I think with the rise of new machine learning technologies and particularly new cloud computing technologies,
the sort of competition for the large government contracts that the client Palantir has may represent a new iteration technologically
and in terms of business model as well as ideologically.
And of course, ideology is always tied up in these matters.
And I think I say this just because I think this battle of meta narratives is often reported on in the media about Silicon Valley or has been reported on as a kind of culture war.
You know, it's like, oh, these are just different things people think at these companies.
And I think that it's actually really important to keep paying attention to how it interacts with the business and the technology.
So to take and then I will answer the question about socialism and more optimistic alternatives.
But to take a more current example, Amazon recently brought onto its board Keith Alexander,
who was an architect of the National Security Administration, architect of the spying, you know, global spying that that Edward Snowden revealed in 2013.
And there is a faction within, you know, this is very clearly, in my view, a bid by Amazon to win more government contracts.
They lost this cloud computing contract to Microsoft a year or two ago.
So hiring the chief NSA guy is a bid to try to win those government contracts.
There is a battle about it happening within Amazon because some engineers are very who are more sort of libertarian digital rights privacy type.
People are pissed that the sort of most famous guy, most famous for spying on the entire world is is now on their board of directors.
But this isn't just a matter of competing beliefs, although it is that it also is making it a lot harder for Amazon Web Services to do business in Europe,
where there's the general data protection regulation and there's just been this new lawsuit knocking down Privacy Shield, which was an old privacy framework for managing data sovereignty and privacy in Europe.
So it's one of these cases where it is the advent of cloud computing, you know, the growth of cloud computing and these new virtualization technologies creates new business opportunities or the client for that is the state.
The client for that isn't the individual user who is the user of Google search or Facebook.
That kind of technology lies on large enterprise customers to make its money.
And you see this extensively political or cultural clash where you have some people in Amazon.
A lot of people in Amazon used to work in the military and police who say, you know, maybe they're patriotic.
And then you have people saying how terrible to have someone who's a famous spy.
And that looks like a culture where but it is also fundamentally wrapped up in these business questions about whether how they're going to sell cloud to the US or the rest of the world.
And so that's just a random example.
But I think as we think about these new meta narratives, it's also really important to always keep in mind how they link up with the state of technologies and how they link up with the business opportunities.
Similarly, in Google, if we look at these worker struggles or sort of high profile conflicts that happened in the past few years, for instance, over Project Maven, which was a contract to provide a vision recognition software to the US military that some engineers and others within the company objected to.
And there was sort of a public struggle over this.
And Google did not end up competing for that contract.
This was often reported as a kind of culture war between liberal Silicon Valley and more national Silicon Valley.
But it's fundamentally tied up with how Google is going to continue to find new ways to make money off of all the data they have and ways they know how to read and manage data.
So all that to say, I just always want to reemphasize that these meta narratives are very tied to business and to, you know, ideology is material.
But that said, I think as much attention as the neo reaction and the right wing folks have drawn, there also has been, and I sort of alluded to it indirectly there, a lot of left wing organizing and activity within Silicon Valley in the past four or five years, not solely in reaction to the election of Trump, but certainly catalyzed by the election of Trump.
But it's quite striking, you know, often for many reasons when we hear about Silicon Valley in the media, what we're really hearing about is the ownership class of Silicon Valley, the CEOs of Silicon Valley.
They're the only ones who are allowed to talk to the press most of the time.
And but actually Silicon Valley is made up of a bunch of different kinds of people.
And one, and a lot of them like young people in the United States in general are left of center sort of left even of the neoliberal consensus and one statistic.
I like to cite, which Ben Tarnoff, who I did this book with, wrote about back in 2016 is that more.
Let me put this the right way of the top organizations by number of employees who donated to Bernie Sanders.
So employers were the largest number of employees donated to Bernie Sanders.
The top five were all tech companies and the nurses union.
So his nurses and then software engineers.
And I think that represents a real thing.
And part of it's about age.
You know, the tech industry is young and young people in this country tend to be tend to be more left.
But I also think there's room.
I think there are these laughter and energies in the tech industry to that logic magazine and my new book have tried to tap into and document and show to people because it's so rarely shows up in the press.
And and that we can it's interesting to think too about how those connect to new technological possibilities.
I mean, I personally actually don't think this is a perfect answer.
But some people say, you know, Amazon has a perfect planned economy, just nationalize it.
You know, it's not it's not an inevitability.
I don't think that that's a serious idea necessarily.
But but there are all kinds of potentialities and possibilities in these technologies and the scale at which they they draw people together.
And it's not a given that they lead to a handful of tech emperors running everything.
So our book talks to ordinary people in the tech industry about different experiences.
And I could certainly say more about that.
But I already talked quite long enough in answer to that question.
What I find incredibly exciting is the newly found self-confidence of tech workers as a key industry.
And the sociopolitical power and ability that arises from from this fact, how can this power be activated and for what?
It's a good question.
You know, through Logic magazine, somewhat in my new book, and then through the magazine over the years, we've really tried to document some of these different political actions by tech workers.
And this term tech worker in English is itself a kind of polemical term, right?
Historically, the industry has been very segregated or separated by different skill levels.
There's been this strong ideology that highly paid workers in the tech industry aren't really workers.
You know, they're creatives, creatives, knowledge workers doing their knowledge thing.
And and, you know, don't mind the thousands of people working in the cafeteria or on the campus or indeed, you know, in in customer support who make this all run.
I think so this term tech worker itself has a kind of polemical aim, which is to say everyone who works in technology is a tech worker.
They can have material interests in common.
It really came out of this organization called Tech Workers Coalition founded in the Bay Area in 2014.
And it was, at least to my knowledge, that's and I'm pretty sure it's correct.
That's the sort of first time that a lot of people use use this term.
It's an organization co founded by a former cafeteria worker and an engineer.
And from the very beginning, a big part of their ethos is about bringing together sort of higher paid tech workers and the people who are usually thought of tech workers with all the other folks whose whose labor makes it possible.
And there was a lot of knowledge sharing.
I think the blue collar workers tended to have a lot more experience in agitation and organization than the engineers did and unionizing campaigns for those workers, which happened in Silicon Valley in 2015, 2016, 2017 at Cisco, Facebook, IBM, some other big companies.
There were unions for the for the blue collar staff.
That was a really big learning experience, I think, for a lot of the engineers.
I think that it's it's a new kind of consciousness that can go in all kinds of directions.
One early thing we saw after the Trump election and actually the very first Tech Workers Coalition protest I ever went to was at Palantir headquarters.
I was there to write about it, not as a protester, but was at Palantir headquarters in Palo Alto the day before the Trump inauguration.
So I guess that was January 20th, 2017, I think.
One concern that a lot of tech workers, engineers expressed, was concerned about being made to gather and use data or building products that would be used to police and harass people in ways that they did not support and did not intend.
So for instance, at that Palantir protest, people spoke about this idea of a Muslim database, which Trump had been talking about on the campaign trail and Palantir's possible complicity in building such a database.
I think one thing that's been very interesting to watch is that, you know, engineering work is so modular generally.
It's like engineers work on these very little pieces of software that plug into these very big systems.
And I think, and I don't mean this as a criticism at all, but I think in that kind of work system, it's very easy to kind of lose sight of what the bigger picture is, or it's very easy for a worker not to know what the bigger picture is.
But I think that, A, having this threat of Trump using these tools to political ends that people didn't want was politicizing.
And I think also, and person after person I talked to, talked about this specific meeting that happened between Trump and the tech CEOs, and that happened on December 9th, 2016, and Peter Thiel arranged it.
But person after person I talked to separately brought up these photographs that came out of this meeting, and these photographs of their liberal bosses sort of going to kiss up to Trump after he was elected.
And I think that, as silly as it might sound, sort of just this awareness that it's like they were workers in a business, and those businesses were going to comply with who was in power.
And there's all this ideology about engineers as creative workers who aren't really like workers, they're doing their own thing, but actually, at the end of their day, their bosses would say, no, you're going to make that, that they were actually workers.
I think the election of Trump had sort of a catalyzing effect on that in all sorts of ways.
And I mentioned earlier the protests at Google, there were these actions at Google, even before the Google walkout about sexual harassment.
There were these actions around Project Maiden, which was this vision recognition software in development, potentially for the military didn't happen, and then Project Dragonfly, which was a censored Chinese search engine.
And there have been a lot of other actions like that against specific projects at tech firms.
So I think there are a lot of directions it can go. That's one way it's gone.
I think another key insight, at least at the beginning of that movement, was that engineers realized they could use their sort of privileged position to advocate for certain things.
I think Google has shut down on it a lot since then, but I used to hear people say, you know, how many engineers did it take to shut down Google search for a day? Not that many.
They're like a trillion searches a day. You know, if you think about like a dock worker, like where the points of leverage and economy are, it's actually not very many people right there.
So anyway, yeah, those are some thoughts on it. I think that it's evolved in different directions and will continue to.
I think what COVID means for it is a really open question because, of course, yeah, it's had a lot of implications. People aren't on their campuses anymore. More work is remote.
I think it's possible it'll lead to permanent sort of de-skilling of even certain technical jobs or outsourcing of that at a more rapid pace.
But yeah, there's been a lot of new activity in that area since 2016, for sure.
Maura, there's a last question that I ask everybody who's on the show. If you think about the future, what makes you joyful?
Ah, what makes me joyful about the future?
I think, you know, I think in my personal life, I could have my own answers. I think looking at the political scene, I think that this sort of crack up of a neoliberal hegemony that was in many ways harmful and felt constricting.
And that's happened since 2016 has opened up a lot of new possibilities. And some of those are frightening and bad.
But some of those are really promising and exciting. And so I think with a measured kind of optimism, because I don't want to play down all the suffering and sort of frightening things that have happened.
I think that looking at young people, including tech workers, but also including, you know, young climate strikers and sort of these young feminist activists and so on.
Looking at those folks and how they're using technological tools or when they work in the industry, building new kinds of tools.
These are things that make me feel optimistic, if I'm having to be optimistic. I'm very pessimistic all the time.
But yeah, those are things that make me feel joyful, as well as the thought of whenever it is we get to be in large rooms for a dance party or a bookstore reading.
Again, I have dreams about going to a bookstore full of people to hear someone read. So yeah, those are some things that make me feel joyful.
Nice. Moira, thanks a lot for being part of Future Histories.
Yeah, thanks for having me. This was fun.
That was our show for today. If you want to know more about Future Histories, please visit futurehistories.today.
You can support Future Histories at Patreon. For this, go to patreon.com slash future histories and let me know what you think about this episode and the show in general.
For this, use Twitter, hashtag Future Histories or Reddit or send me an email to future underscore histories at protonmail.com.
See you next time.
