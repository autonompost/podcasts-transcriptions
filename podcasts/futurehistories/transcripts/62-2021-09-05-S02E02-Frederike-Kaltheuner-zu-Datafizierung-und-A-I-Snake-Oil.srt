1
00:00:00,000 --> 00:00:06,000
Herzlich willkommen bei Future Histories, dem Podcast zur Erweiterung unserer Vorstellung von Zukunft.

2
00:00:06,000 --> 00:00:12,000
Mein Name ist Jan Groß und ihr hört heute die zweite Future Histories Live-Episode,

3
00:00:12,000 --> 00:00:15,000
die ich kürzlich in Hamburg aufgezeichnet habe.

4
00:00:15,000 --> 00:00:19,000
In diesem Fall, da war ich beim Mind the Progress-Kongress eingeladen

5
00:00:19,000 --> 00:00:23,000
und hatte die Freude, mit Friederike Kaltheuner als Gast zu sprechen.

6
00:00:23,000 --> 00:00:31,000
Friederike, die forscht, schreibt und denkt zu Fragen der Datafizierung und ist Expertin im Bereich Tech Policy.

7
00:00:31,000 --> 00:00:37,000
Sie hat unter anderem das Corporate Exploitation Program bei Privacy International geleitet,

8
00:00:37,000 --> 00:00:40,000
war Tech Policy Fellow bei der Mozilla Foundation.

9
00:00:40,000 --> 00:00:46,000
Gemeinsam mit Nele Obermüller hat sie auch ein Buch geschrieben mit dem Titel Datengerechtigkeit.

10
00:00:46,000 --> 00:00:50,000
Vielen Dank an dieser Stelle hier an die OrganisatorInnen des Kongresses für die Einladung

11
00:00:50,000 --> 00:00:55,000
und vor allem auch an Sophia für diese wirklich ausgezeichnete Organisation und Betreuung.

12
00:00:55,000 --> 00:01:00,000
1.000 Dank. Bevor wir jetzt aber zum Gespräch mit Friederike kommen,

13
00:01:00,000 --> 00:01:03,000
hatte ich ja in der vergangenen Episode angekündigt,

14
00:01:03,000 --> 00:01:08,000
noch ein paar Worte zur gerade begonnenen zweiten Staffel von Future Histories zu sagen,

15
00:01:08,000 --> 00:01:12,000
ob sich da was geändert hat im Vergleich zur Staffel 1 und wenn ja, was.

16
00:01:12,000 --> 00:01:18,000
Falls euch diese Reflektionen über den Verlauf des Podcasts Future Histories nicht so sehr interessieren

17
00:01:18,000 --> 00:01:23,000
und ihr eigentlich nur Friederike hören wollt, dann nehme ich euch das nicht im geringsten Übel,

18
00:01:23,000 --> 00:01:27,000
sondern habe genau dafür einen Kapitelmarker gesetzt.

19
00:01:27,000 --> 00:01:32,000
Wenn eure App das unterstützt, dann könnt ihr da also jetzt vorspringen.

20
00:01:32,000 --> 00:01:37,000
Für alle, die es sehr wohl interessiert, willkommen bei diesem kurzen Einschub.

21
00:01:37,000 --> 00:01:40,000
Also vielleicht zunächst mal für euch zur Einordnung.

22
00:01:40,000 --> 00:01:45,000
Ich hatte damals, als ich Future Histories vor jetzt über zwei Jahren gestartet habe,

23
00:01:45,000 --> 00:01:49,000
ja eher so eine vage Idee davon, wie ich das ungefähr machen will.

24
00:01:49,000 --> 00:01:53,000
Und der Aufbau in Staffeln, der schien mir damals einfach sinnvoll,

25
00:01:53,000 --> 00:01:58,000
um zum einen eben die Möglichkeit einzubauen, dass man auch mal eine Pause machen kann.

26
00:01:58,000 --> 00:02:03,000
Und zum anderen wollte ich auch nicht, dass die Themen, mit denen ich damals gestartet bin,

27
00:02:03,000 --> 00:02:05,000
auf ewig in Stein gemeißelt sind.

28
00:02:05,000 --> 00:02:11,000
Eine neue Staffel, die bietet also auch die Möglichkeit, thematisch da Neuausrichtungen vorzunehmen.

29
00:02:11,000 --> 00:02:15,000
Und das ist im Fall von Future Histories nicht als harter Bruch zu verstehen,

30
00:02:15,000 --> 00:02:22,000
sondern eher als eine stetige Entwicklung der forschenden Suchbewegung dieses Podcasts hier.

31
00:02:22,000 --> 00:02:25,000
Es wird also auch in der zweiten Staffel drei Überthemen geben,

32
00:02:25,000 --> 00:02:30,000
aber zum einen werden sie auch weiterhin eher als grobe Orientierung dienen

33
00:02:30,000 --> 00:02:34,000
und eine Offenheit zulassen, die ich als essentiell empfinde.

34
00:02:34,000 --> 00:02:39,000
Und zum anderen leiten sie sich auch unmittelbar aus den unterschiedlichen Auseinandersetzungen,

35
00:02:39,000 --> 00:02:42,000
den Pferden und den Entwicklungen der ersten Staffel ab.

36
00:02:42,000 --> 00:02:46,000
Aus dem Thema Homo economicus, da hat sich zum Beispiel,

37
00:02:46,000 --> 00:02:48,000
ich hatte das an anderer Stelle schon erwähnt,

38
00:02:48,000 --> 00:02:53,000
über den Verlauf der ersten Staffel hinweg die Frage entwickelt,

39
00:02:53,000 --> 00:02:57,000
wie alternative Systeme politischer Ökonomie denn aussehen könnten.

40
00:02:57,000 --> 00:03:02,000
Politische Ökonomien der Zukunft wird also ein Überthema der zweiten Staffel sein.

41
00:03:02,000 --> 00:03:07,000
Ich bin ja absolut der Meinung, dass es nicht reicht, in der Kritik zu verharren,

42
00:03:07,000 --> 00:03:12,000
sondern dass es zwingend notwendig ist, konstruktive und produktive Vorschläge zu machen,

43
00:03:12,000 --> 00:03:19,000
wie man es denn anders machen könnte, gerade auch auf einer Makroebene wie der der politischen Ökonomie.

44
00:03:19,000 --> 00:03:25,000
Denn diese Makroebene, die es absolut unterversorgt, wenn es um Alternativen geht, finde ich,

45
00:03:25,000 --> 00:03:32,000
das gilt es zu ändern und ich möchte dabei zum einen bestimmte Stränge der ersten Staffel weiterverfolgen,

46
00:03:32,000 --> 00:03:35,000
die sich als fruchtbare Pferden herausgestellt haben.

47
00:03:35,000 --> 00:03:39,000
Konkret ist, dass die Auseinandersetzung mit dem Themenkomplex,

48
00:03:39,000 --> 00:03:45,000
den ich mittlerweile unter dem spielerisch provokanten Begriffspaar Freie Planwirtschaft zusammenfasse,

49
00:03:45,000 --> 00:03:50,000
da ist noch vieles offen und ich habe sehr, sehr viele Fragen an diesen Themenkomplex,

50
00:03:50,000 --> 00:03:53,000
die definitiv noch umgeklärt sind.

51
00:03:53,000 --> 00:04:00,000
Und daran anschließend gibt es auch eine andere Fragestellung oder einen thematischen Ansatz

52
00:04:00,000 --> 00:04:06,000
zu diesem Komplex zukünftiger politischer Ökonomien, einen anderen Ansatz,

53
00:04:06,000 --> 00:04:09,000
den ich ebenfalls in Staffel 1 wiederholt auch aufgegriffen habe,

54
00:04:09,000 --> 00:04:14,000
der aber auch absolut überhaupt nicht abschließend geklärt ist in irgendeiner Form.

55
00:04:14,000 --> 00:04:20,000
Und das ist die Frage nach alternativen Regierungskünsten, nach alternativer Governamentalität,

56
00:04:20,000 --> 00:04:24,000
wie es Frieda Vogelmann in Episode 11 nennt.

57
00:04:24,000 --> 00:04:28,000
Das sind also so zwei Aspekte, die ich aus der ersten Staffel mitnehmen möchte,

58
00:04:28,000 --> 00:04:35,000
aber ich will natürlich auch neue Pfade eröffnen, die für politische Ökonomien der Zukunft von Bedeutung sind.

59
00:04:35,000 --> 00:04:39,000
Das Ganze wird also explorativ sein, wie man so schön sagt.

60
00:04:39,000 --> 00:04:41,000
Und ich freue mich auch immer sehr über Hinweise.

61
00:04:41,000 --> 00:04:50,000
Also solltet ihr in dieser Richtung irgendwie gute Ansätze kennen, Zugänge, Bücher, TheoretikerInnen und so weiter,

62
00:04:50,000 --> 00:04:55,000
gerne immer mir schreiben unter jan-at-future-histories.today.

63
00:04:55,000 --> 00:05:00,000
Ein wichtiger Bereich, den es für diesen Themenblock in jedem Fall zu erarbeiten gilt,

64
00:05:00,000 --> 00:05:09,000
ist das Blicken über den Tellerrand, um nicht in so einer leider ja immer wieder sehr eurozentristischen Debatte eigentlich stecken zu bleiben.

65
00:05:09,000 --> 00:05:15,000
Ein wichtiger Aspekt wird also sein zu fragen, wie wird denn die Frage nach alternativen politischen Ökonomien

66
00:05:15,000 --> 00:05:18,000
in anderen Teilen der Welt gestellt und verhandelt.

67
00:05:19,000 --> 00:05:26,000
Der zweite Themenblock, der ergibt sich im Grunde unmittelbar aus dem ersten und widmet sich der Frage der Transformation.

68
00:05:26,000 --> 00:05:30,000
Ich finde es unglaublich wichtig, diese Ebenen miteinander zu verbinden,

69
00:05:30,000 --> 00:05:36,000
dass man sich also auf der einen Seite wieder traut, auch über Alternativen auf der Makroebene nachzudenken,

70
00:05:36,000 --> 00:05:40,000
dass aber gleichzeitig auch eine plausible Idee davon entwickelt wird,

71
00:05:40,000 --> 00:05:44,000
wie diese alternativen politischen Ökonomien denn in die Welt gebracht werden könnten,

72
00:05:44,000 --> 00:05:48,000
wie eine Transformation also ganz konkret aussehen könnte.

73
00:05:48,000 --> 00:05:55,000
Nicht das Blaupause, also bitte nicht falsch verstehen, nicht als ein Prozess mit einem finalen Ziel oder sowas,

74
00:05:55,000 --> 00:06:01,000
sondern als ein wechselseitiger Prozess zwischen gelebter alternativer Praxis im Hier und Jetzt

75
00:06:01,000 --> 00:06:08,000
und auch Projekten, die quasi an die Substanz dessen gehen, was es neu zu verhandeln gilt

76
00:06:08,000 --> 00:06:12,000
und im Entwickeln alternativer Zukunft auch jenseits der Nische.

77
00:06:12,000 --> 00:06:19,000
Ich glaube, dass das sich überhaupt nicht irgendwie widerspricht, sondern im Gegenteil einfach ganz fantastisch gegenseitig befruchten kann.

78
00:06:19,000 --> 00:06:25,000
Die Frage der Transformation ist ein sehr weites Feld und auch eine harte Nuss, finde ich.

79
00:06:25,000 --> 00:06:33,000
Da wird es also einiges zu besprechen geben und ich freue mich schon auf viele, viele spannende Episoden zu diesem Thema.

80
00:06:33,000 --> 00:06:37,000
Das dritte Themenfeld, das uns in der zweiten Staffel begleiten wird,

81
00:06:37,000 --> 00:06:45,000
das greift den technopolitischen Strang von Future Histories auf und verdichtet ihn entlang eines Forschungsprojektes,

82
00:06:45,000 --> 00:06:48,000
das ich gemeinsam mit Robert Seifert verfolge.

83
00:06:48,000 --> 00:06:53,000
Das Thema lautet das Regieren der Algorithmen und das freut mich ganz besonders,

84
00:06:53,000 --> 00:06:58,000
nicht nur, weil ich Robert Seifert sehr schätze und das Thema auch unglaublich gut in Future Histories passt,

85
00:06:58,000 --> 00:07:04,000
sondern auch, weil es bedeutet, dass hier zwei Sphären noch enger zueinander finden,

86
00:07:04,000 --> 00:07:09,000
die für mich persönlich einfach von großer Bedeutung sind, nämlich Future Histories als Projekt

87
00:07:09,000 --> 00:07:17,000
und die Forschung im Rahmen meiner Dissertation zu sozio-technischen Imaginationen algorithmischer Regierungskunst.

88
00:07:17,000 --> 00:07:22,000
Diese Dissertation, die ist nämlich Teil des Forschungsprojektes, das Regieren der Algorithmen.

89
00:07:22,000 --> 00:07:30,000
Und weil ich ebenfalls daran arbeite, Podcasten als Teil der erweiterten Forschungspraxis zu etablieren und zu plausibilisieren,

90
00:07:30,000 --> 00:07:36,000
passt das alles natürlich ganz großartig zusammen und freut mich wirklich sehr.

91
00:07:36,000 --> 00:07:41,000
Worum wird es also im Themenstrang das Regieren der Algorithmen gehen?

92
00:07:41,000 --> 00:07:46,000
Das Forschungsprojekt, das trägt den Untertitel eine Soziologie algorithmischer Regierungskunst

93
00:07:46,000 --> 00:07:55,000
und steht somit in ausgezeichneter Korrespondenz zur in Themenblock 1 bereits erwähnten Frage nach alternativen Regierungskünsten.

94
00:07:55,000 --> 00:08:04,000
Der Begriff des Regierens, der ist dabei angelehnt an Foucault weit gefasst und geht über eine rein politische Definition hinaus.

95
00:08:04,000 --> 00:08:10,000
Regieren bezieht sich hier ganz allgemein auf soziale Felder, Technologien und individuelle Handlungsformen,

96
00:08:10,000 --> 00:08:15,000
die der Selbst- und Fremdführung von Menschen dienen und Regierungskünste,

97
00:08:15,000 --> 00:08:20,000
die stellen dabei dann das reflektierte Nachdenken über die beste Form des Regierens dar.

98
00:08:20,000 --> 00:08:29,000
Algorithmische Regierungskünste tun dies wiederum unter Einbezug algorithmischer Verfahren als maßgebliche Elemente des Regierens.

99
00:08:29,000 --> 00:08:35,000
Und wir führen im Namen des Forschungsprojektes das Regieren der Algorithmen ja eine Kecke Zweideutigkeit mit,

100
00:08:35,000 --> 00:08:43,000
denn der Name das Regieren der Algorithmen, das kann sich ja beziehen sowohl auf das Regieren von Algorithmen,

101
00:08:43,000 --> 00:08:52,000
also im Sinne des Regulierens von Algorithmen, als auch auf soziotechnische Imagination des Regierens mit algorithmischen Technologien.

102
00:08:52,000 --> 00:09:00,000
Siehe mein Promotionsprojekt. Das heißt, wir kommen da aus verschiedensten Richtungen, was ich als sehr produktiv empfinde.

103
00:09:00,000 --> 00:09:07,000
Mehr dazu gibt es dann in einer kommenden Episode mit Robert Seifert als Gast. Ich freue mich schon sehr.

104
00:09:08,000 --> 00:09:13,000
Was mich auch sehr freut und das wollte ich auch schon längst ganz fröhlich verkünden,

105
00:09:13,000 --> 00:09:18,000
ist, dass Future Histories eine Förderung der Wiener Medieninitiative erhalten hat,

106
00:09:18,000 --> 00:09:25,000
um das Projekt etwas auszubauen und auch auf solidere Beine stellen zu können, sage ich mal.

107
00:09:25,000 --> 00:09:30,000
Daher stammt zum Beispiel auch der Umstand, dass jetzt zumindest mal für die nächsten 20 Episoden

108
00:09:30,000 --> 00:09:35,000
jede zweite Woche ein Future Histories Kurzvideo veröffentlicht wird auf YouTube.

109
00:09:35,000 --> 00:09:43,000
Gerne auch übrigens den Kanal abonnieren. Das ist definitiv auch eine Hilfe im Ausbau dieses Medienstranges, sage ich mal.

110
00:09:43,000 --> 00:09:48,000
Es gibt jetzt auch ein kleines Studio in einem Gemeindebau direkt beim Wiener Prater

111
00:09:48,000 --> 00:09:54,000
und die Förderung ermöglicht auch, dass ich endlich nicht mehr gezwungen bin, alles alleine zu machen

112
00:09:54,000 --> 00:09:57,000
und mich an den Rand der Überlastung zu treiben.

113
00:09:57,000 --> 00:10:01,000
Denn ich werde jetzt hier bei Future Histories von Clara unterstützt.

114
00:10:01,000 --> 00:10:08,000
Clara, vielen Dank für alles. Ich freue mich total, dass du mitmachst und dass du mich in all diesen Sachen unterstützt.

115
00:10:08,000 --> 00:10:10,000
Ich freue mich auch. Hallo.

116
00:10:10,000 --> 00:10:15,000
Die Förderung der Wiener Medieninitiative bewegt sich im Bereich dessen, was man gemeinhin

117
00:10:15,000 --> 00:10:19,000
wahrscheinlich Kreativwirtschaft nennen würde und so ist es integraler Bestandteil,

118
00:10:19,000 --> 00:10:24,000
dass ich mich im Zuge dessen mit einer Neugründung selbstständig gemacht habe.

119
00:10:24,000 --> 00:10:28,000
Die Firma, die daraus entsprungen ist, die trägt den Namen MetaLapsis

120
00:10:28,000 --> 00:10:34,000
und ihr findet sie unter www.metalapsis.net mit einer sehr gelungenen Homepage vertreten,

121
00:10:34,000 --> 00:10:39,000
für deren Gestaltung und Umsetzung ich Thomas, Daniel und Leon danken möchte.

122
00:10:39,000 --> 00:10:45,000
Und wo wir schon bei Webseiten sind, es gibt jetzt auch Infos zu meiner Person gebündelt unter

123
00:10:45,000 --> 00:10:52,000
jan-groß.de, geschrieben Gustav Richard Otto Otto Siegfried, also mit zwei O und einem normalen S.

124
00:10:52,000 --> 00:10:58,000
Auf eine Art fand ich es zwar auch ganz schick, muss ich sagen, dass es bisher nur so verstreute

125
00:10:58,000 --> 00:11:05,000
Informationen über mich gab im Netz, aber nun hat dann doch der pragmatische Zugang gewonnen.

126
00:11:05,000 --> 00:11:12,000
So, jetzt genug Eigenwerbung. Ich freue mich riesig auf die zweite Staffel, möchte ich noch mal sagen.

127
00:11:12,000 --> 00:11:18,000
Und mir verbleibt noch Marianne, Andrea und Nausi K.A. ganz herzlich in der Gemeinschaft

128
00:11:18,000 --> 00:11:24,000
der Patreon-UnterstützerInnen zu begrüßen. Und ich danke Fabian, Carmen, Rudolf und Wilfried

129
00:11:24,000 --> 00:11:30,000
für ihre Spenden und wünsche euch jetzt viel Freude mit einer neuen Episode Future Histories Live,

130
00:11:30,000 --> 00:11:33,000
diesmal mit Friederike Kalt-Heuner.

131
00:11:41,000 --> 00:11:46,000
Ja, herzlich willkommen auch von meiner Seite. Ich freue mich sehr, dass wir heute hier sein dürfen.

132
00:11:46,000 --> 00:11:48,000
Herzlich willkommen, Friederike.

133
00:11:48,000 --> 00:11:50,000
Danke für die Einladung.

134
00:11:50,000 --> 00:11:56,000
Ich freue mich sehr. Friederike, ich mag gerne mit einer ganz basic Frage einsteigen, die vielleicht

135
00:11:56,000 --> 00:12:02,000
gar nicht so einfach ist, wie sie im ersten Moment scheint. Denn gerade im Zusammenhang mit Daten,

136
00:12:02,000 --> 00:12:07,000
da scheint mir allzu oft zu einer gewissen Naturalisierung am Werke. Man sieht es an Framings,

137
00:12:07,000 --> 00:12:14,000
wie Daten sind, das neue Öl oder auch der Begriff Raw Data, Rotdaten. Also zunächst mal vorab.

138
00:12:14,000 --> 00:12:17,000
Was sind Daten und wie entstehen sie?

139
00:12:19,000 --> 00:12:24,000
Vielleicht ist es am einfachsten, mit einer ganz einfachen Aussage anzufangen.

140
00:12:24,000 --> 00:12:32,000
Wir leben in einer Welt, in der es noch nie so einfach war, so viel über jeden von uns zu wissen.

141
00:12:32,000 --> 00:12:38,000
Und das hat natürlich auch was mit Daten zu tun. Also der Begriff Datafizierung heißt,

142
00:12:38,000 --> 00:12:48,000
die Umwandlung von Räumen, Verhalten, Informationen in Daten. Wenn ich einen Vortrag zu dem Thema halte,

143
00:12:48,000 --> 00:12:53,000
habe ich eine Lieblingsfolie, die ich eigentlich fast immer verwende. Und auf dem Bild sieht man eine,

144
00:12:53,000 --> 00:13:01,000
das ist ein Foto von einer Stasi-Akte. Und die zeigt so ein bisschen, wie auf der einen Seite,

145
00:13:01,000 --> 00:13:07,000
wie arbeitsaufwendig es früher war, Informationen herauszufinden und wie trivial die Informationen

146
00:13:07,000 --> 00:13:13,000
zum Teil auch waren. Also da steht dann in der Akte, welches Buch liest jemand, wer sind so die Freunde,

147
00:13:13,000 --> 00:13:20,000
wo verbringen die so ihren Nachmittag. Und was Datafizierung jetzt ganz grob runtergebrochen heißt,

148
00:13:20,000 --> 00:13:24,000
ist, dass wir jetzt in einer Welt leben, in all diese Dinge, die früher sehr aufwendig

149
00:13:24,000 --> 00:13:30,000
und kostenspielig waren, herauszufinden. Diese Dinge werden automatisch über uns aufgezeichnet

150
00:13:30,000 --> 00:13:37,000
in Form von Daten. Und wenn wir über Daten sprechen, zum öffentlichen Diskurs, geht es eigentlich meistens

151
00:13:37,000 --> 00:13:41,000
um die Daten, die wir mehr oder weniger bewusst teilen. Also die Bilder, die wir teilen,

152
00:13:41,000 --> 00:13:47,000
was wir ins Internet stellen. Aber das ist eigentlich nur die Spitze des Eisbergs. Der viel größere Teil,

153
00:13:47,000 --> 00:13:51,000
der den meisten Menschen, glaube ich, gar nicht so bewusst ist, sind die Daten, die automatisch

154
00:13:51,000 --> 00:13:58,000
über uns aufgezeichnet werden. Unser Bewegungsmuster, nicht nur was wir kaufen, sondern wie sich unsere Maus bewegt,

155
00:13:58,000 --> 00:14:03,000
wo wir vielleicht zögern, was uns vielleicht vorher noch interessiert hat. Und dann die dritte Kategorie,

156
00:14:03,000 --> 00:14:08,000
die mich, ich glaube, das ist vielleicht eigentlich auch der Einstieg für mich persönlich zu dem Thema.

157
00:14:08,000 --> 00:14:15,000
Die dritte Kategorie sind all die Rückschlüsse, die man aus diesen Mustern ableiten kann.

158
00:14:15,000 --> 00:14:21,000
Also aus meinem Bewegungsmuster, aus meinem Netflix-Verhalten kann man sehr viel ablesen,

159
00:14:21,000 --> 00:14:27,000
zum Beispiel, ob jemand depressiv ist, ob jemand jetzt das Haus nicht verlassen hat.

160
00:14:27,000 --> 00:14:32,000
Man kann anhand meiner Telefondaten sehen, bin ich eher jemand, die sofort zurückruft?

161
00:14:32,000 --> 00:14:39,000
Oder muss ich angerufen werden? Und wen rufe ich an? Und das ist eigentlich das, was mich interessiert.

162
00:14:39,000 --> 00:14:46,000
Weil auf der einen Seite ist es so, wenn diese Vorhersagen stimmen, sind sie unheimlich genau

163
00:14:46,000 --> 00:14:52,000
und können Dinge aussagen über uns, die wir vielleicht selber gar nicht wissen oder nicht wahrhaben wollen.

164
00:14:52,000 --> 00:14:57,000
Gleichzeitig ist es aber auch so, dass diese Muster und Vorhersagen oft völliger Quatsch sind.

165
00:14:57,000 --> 00:15:01,000
Und man sieht das ganz gut anhand zum Beispiel der Online-Werbung, die ich sehe.

166
00:15:01,000 --> 00:15:07,000
Da gibt es manchmal Dinge, die sind unheimlich fast hellseherisch genau.

167
00:15:07,000 --> 00:15:11,000
Und dann wiederum gibt es einfach Quatsch.

168
00:15:11,000 --> 00:15:16,000
Also Facebook hat jetzt über 12, 13 Jahre Daten von mir und denkt immer noch, ich wäre ein Mann.

169
00:15:16,000 --> 00:15:19,000
Und dieses Spannungsfeld ist das, was mich interessiert.

170
00:15:19,000 --> 00:15:22,000
Also das Spannungsfeld zwischen Orwell und Kafka.

171
00:15:22,000 --> 00:15:25,000
Auf der einen Seite sind wir sehr lesbar geworden.

172
00:15:25,000 --> 00:15:30,000
Auf der anderen Seite ist das, was gelesen wird, auch oft völlig falsch.

173
00:15:30,000 --> 00:15:35,000
Und manchmal ist es einfach nur irritierend, dass es falsch ist.

174
00:15:35,000 --> 00:15:38,000
Manchmal geht es aber auch um etwas ganz anderes.

175
00:15:38,000 --> 00:15:40,000
Manchmal werden wir in Kategorien gepackt.

176
00:15:40,000 --> 00:15:44,000
Manchmal werden wir falsch einsortiert auf Arten und Weisen, die diskriminierend sind,

177
00:15:44,000 --> 00:15:48,000
die bestehende diskriminierende Strukturen reproduzieren.

178
00:15:48,000 --> 00:15:56,000
Und ich glaube, neben Klimawandel, Ungerechtigkeit, wachsender Ungleichheit,

179
00:15:56,000 --> 00:16:00,000
ist eben einer der wichtigsten und dringendsten Themen unserer Zeit,

180
00:16:00,000 --> 00:16:03,000
wie wir die Spielregeln dieser neuen Welt gestalten,

181
00:16:03,000 --> 00:16:08,000
die in ihren vollen Konsequenzen immer noch nicht viele Leute wirklich verstehen.

182
00:16:08,000 --> 00:16:14,000
Also wir leben einer radikal anderen Welt, als wir 2001 gelebt haben.

183
00:16:14,000 --> 00:16:16,000
Und es liegt an uns, diese Welt zu gestalten.

184
00:16:16,000 --> 00:16:18,000
Und deswegen finde ich das Thema Zukunft so spannend,

185
00:16:18,000 --> 00:16:21,000
weil im Deutschen das Wort Digitalisierung suggeriert manchmal,

186
00:16:21,000 --> 00:16:25,000
als wäre das ein Prozess, der uns passiert, der über uns kommt.

187
00:16:25,000 --> 00:16:28,000
Dabei ist es eigentlich andersrum. Es liegt an uns, ihn zu gestalten.

188
00:16:28,000 --> 00:16:31,000
Alles, was jetzt ist, hätte auch anders sein können.

189
00:16:31,000 --> 00:16:36,000
Es ist viel freier, viel offener.

190
00:16:36,000 --> 00:16:41,000
Und was mich motiviert oder meine persönliche Motivation ist,

191
00:16:41,000 --> 00:16:46,000
ich möchte eben mehr Menschen dazu bewegen und auch selber Teil dieser Gestaltung sein

192
00:16:46,000 --> 00:16:49,000
und das nicht einfach nur mich überkommen lassen.

193
00:16:49,000 --> 00:16:53,000
Da hast du jetzt verschiedene Sachen angesprochen,

194
00:16:53,000 --> 00:16:58,000
unter anderem auch dieses Spannungsfeld zwischen Genauigkeit und danebenliegen.

195
00:16:58,000 --> 00:17:01,000
Ich finde das auch unter einem anderen Aspekt interessant,

196
00:17:01,000 --> 00:17:06,000
weil ja auch auf Seiten der Kritikerinnen und Kritiker dieser Technologien

197
00:17:06,000 --> 00:17:10,000
das mitunter dann fast schon unbeabsichtigt scheint mir irgendwie passiert,

198
00:17:10,000 --> 00:17:15,000
dass die an den Mythen dieser Technologien eigentlich mitarbeiten,

199
00:17:15,000 --> 00:17:21,000
indem sie, ich glaube fast aus Versehen sozusagen,

200
00:17:21,000 --> 00:17:25,000
diese Erzählungen, diese Narrative für bare Münze nehmen

201
00:17:25,000 --> 00:17:30,000
und dann quasi im Aufschrei dagegen, wie genau man uns denn jetzt sehen könne,

202
00:17:30,000 --> 00:17:33,000
wie genau man jetzt Zukunft voraussehen könne und so weiter,

203
00:17:33,000 --> 00:17:37,000
eigentlich auf eine Art eben diese Mythologien auch mit verfestigen.

204
00:17:37,000 --> 00:17:42,000
Wie versuchst du das zu umschiffen? Wie gehst du mit dieser Problematik um?

205
00:17:44,000 --> 00:17:49,000
Kritik ist ja erstmal grundsätzlich positiv, aber es gibt natürlich Kritik und Kritik.

206
00:17:49,000 --> 00:17:55,000
Man sieht das immer anhand dieses Beispiels oder zwei Beispiele finde ich hier besonders gut.

207
00:17:55,000 --> 00:17:59,000
Das eine ist künstliche Intelligenz. Natürlich muss man das kritisch sehen,

208
00:17:59,000 --> 00:18:03,000
aber Kritik heißt nicht, dass wir jetzt Angst vor Terminator haben müssen

209
00:18:03,000 --> 00:18:08,000
oder dass KI jetzt bald alles, wie sagt man das auf Deutsch,

210
00:18:08,000 --> 00:18:12,000
also dass KI jetzt plötzlich unser ganzes Leben entscheiden wird oder bestimmen wird.

211
00:18:12,000 --> 00:18:16,000
Das ist die falsche Sorge, sondern die eigentliche Sorge ist,

212
00:18:16,000 --> 00:18:22,000
dass KI oft nicht funktioniert, oft richtig schlampig gebaut wird,

213
00:18:22,000 --> 00:18:29,000
Vorurteile, Diskriminierung reproduziert, also die Gefahr ist oft viel subtiler als man das denkt.

214
00:18:30,000 --> 00:18:33,000
Und dann die eigentliche Gefahr oder wir müssen gar nicht über Gefahren reden,

215
00:18:33,000 --> 00:18:38,000
aber ich glaube der Bereich, der am meisten kritisiert werden muss,

216
00:18:38,000 --> 00:18:43,000
ist die Tatsache, dass es ganz viele Fragen gibt, die wir nicht beantworten können,

217
00:18:43,000 --> 00:18:49,000
die nur eine ganz kleine Anzahl an Firmen sind überhaupt in der Lage diese Fragen zu beantworten.

218
00:18:49,000 --> 00:18:54,000
Ein Beispiel ist sowas Radikalisierung im Netz von Empfehlungsalgorithmen.

219
00:18:54,000 --> 00:18:57,000
Die Studien dazu sind extrem schwierig durchzuführen,

220
00:18:57,000 --> 00:19:01,000
weil personalisierte Algorithmen eben für jeden anders aussehen.

221
00:19:01,000 --> 00:19:07,000
Alleine schon ist es sehr schwierig, auch kollektiv zum Beispiel über Online-Werbung zu sprechen.

222
00:19:07,000 --> 00:19:11,000
Meine Online-Werbung sieht anders aus als deine, sieht anders aus als ihre.

223
00:19:11,000 --> 00:19:17,000
Und da gibt es oft Kritik in der Öffentlichkeit, sowas wie das Internet macht uns dumm,

224
00:19:17,000 --> 00:19:22,000
oder das Internet macht, oder soziale Medien machen X.

225
00:19:22,000 --> 00:19:26,000
Und ich finde diese Art von Kritik schwierig, weil die Frage ist ja,

226
00:19:26,000 --> 00:19:29,000
was ist denn überhaupt Social Media und was sollte es sein?

227
00:19:29,000 --> 00:19:36,000
Indem wir so monokausale Kritik üben, sehen wir die eigentliche Technik Social Media

228
00:19:36,000 --> 00:19:38,000
als etwas viel starreres als es eigentlich ist.

229
00:19:38,000 --> 00:19:43,000
Wir meinen damit eine Bandbreite an Produkten, die sich permanent verändern,

230
00:19:43,000 --> 00:19:46,000
die selbst vor fünf Jahren anders aussahen als jetzt.

231
00:19:46,000 --> 00:19:49,000
Das heißt, diese kausalen Aussagen sind schwierig.

232
00:19:50,000 --> 00:19:53,000
Also im Kern geht es für mich um zwei Sachen.

233
00:19:53,000 --> 00:19:59,000
Die eine Sache ist, es ist nicht nur, dass wir in einer Welt leben, in der man mehr über uns weiß,

234
00:19:59,000 --> 00:20:03,000
sondern der öffentliche Raum, und das ist ja nicht nur Raum, in dem wir sprechen,

235
00:20:03,000 --> 00:20:07,000
das ist auch der Raum, in dem sich soziale Bewegungen formen,

236
00:20:07,000 --> 00:20:10,000
in dem wir über die Welt lernen, jetzt diese Woche Afghanistan.

237
00:20:10,000 --> 00:20:15,000
Das liest man auf Twitter mit und das lesen auch Journalisten auf Twitter mit.

238
00:20:15,000 --> 00:20:22,000
Und es ist schon sehr erstaunlich, dass diese Räume von einer Handvoll an Firmen kontrolliert und gesteuert werden.

239
00:20:22,000 --> 00:20:25,000
Das ist historisch schon extrem einmalig.

240
00:20:25,000 --> 00:20:29,000
Das ist eine Art von Macht, die diese Firmen haben, die wir, ich glaube,

241
00:20:29,000 --> 00:20:32,000
die wir im vollen Ausmaß noch nicht richtig verstanden haben.

242
00:20:32,000 --> 00:20:39,000
Und die andere Sache, die ich so interessant finde, ist die Informationsungleichheit.

243
00:20:39,000 --> 00:20:42,000
Auf der einen Seite kann man sehr viel über uns wissen,

244
00:20:42,000 --> 00:20:48,000
auf der anderen Seite ist es sehr schwer überhaupt zu verstehen, wie zum Beispiel ein Handy funktioniert.

245
00:20:48,000 --> 00:20:50,000
Was passiert eigentlich auf dem Handy?

246
00:20:50,000 --> 00:20:55,000
Oder sehr wenig Menschen wissen eigentlich, hört mein Handy mir zu, wie wird eigentlich Werbung personalisiert?

247
00:20:55,000 --> 00:21:00,000
Und das kreiert eine Informationsasymmetrie, die natürlich auch ein Machtgefälle ist.

248
00:21:00,000 --> 00:21:04,000
Also das sind für mich diese Fragen um Macht und Gerechtigkeit.

249
00:21:04,000 --> 00:21:08,000
Das sind eigentlich die Kernfragen oder das ist da, wo die Kritik ansetzen muss.

250
00:21:09,000 --> 00:21:15,000
Es gibt so eine Demarkationslinie, die ich noch nie so recht habe begreifen können.

251
00:21:15,000 --> 00:21:19,000
Vielleicht kannst du mir da ein bisschen weiterhelfen und das ist die Wahlwerbung.

252
00:21:19,000 --> 00:21:25,000
Also die Mechanismen, die Tools, die eingesetzt werden in dieser Wahlwerbung,

253
00:21:25,000 --> 00:21:31,000
das sind Tools, die sind im normalen Online-Werbegeschäft total frei verfügbar für jeden.

254
00:21:31,000 --> 00:21:36,000
Also du kannst quasi diese Lookalikes oder sowas, das ist ganz normal,

255
00:21:36,000 --> 00:21:40,000
wenn du eine Facebook-Werbung schaltest, dass du das dann auswählen kannst.

256
00:21:40,000 --> 00:21:45,000
Wenn das aber wiederum dann auf den politischen Bereich übertragen wird,

257
00:21:45,000 --> 00:21:48,000
dann gibt es immer plötzlich so einen Aufschrei.

258
00:21:48,000 --> 00:21:53,000
Dadurch können jetzt irgendwie in einer Art und Weise die öffentliche Meinung manipuliert werden,

259
00:21:53,000 --> 00:21:56,000
wie das vorher noch nicht dagewesen sei.

260
00:21:56,000 --> 00:22:01,000
Es gäbe jetzt Microtargeting und das würde quasi diesen demokratischen Willensbildungsprozess

261
00:22:01,000 --> 00:22:05,000
in einer Art und Weise beeinflussen, die jetzt aber wirklich nicht mehr zulässig wäre.

262
00:22:05,000 --> 00:22:09,000
Und ich frage mich immer, wenn ich das höre, ohne jetzt sagen zu wollen,

263
00:22:09,000 --> 00:22:12,000
dass das eine komplett unberechtigte Kritik wäre oder sowas,

264
00:22:12,000 --> 00:22:15,000
aber ich habe noch nie begriffen, wie man diese Linie zieht.

265
00:22:15,000 --> 00:22:20,000
Also was ist quasi, was ist irgendwie okay?

266
00:22:20,000 --> 00:22:23,000
Früher gab es auch immer schon Medienkonglomerate,

267
00:22:23,000 --> 00:22:28,000
die hatten halt dann irgendwie eine Mehrheit der Tageszeitungen in Besitz

268
00:22:28,000 --> 00:22:34,000
und hatten dadurch eine gewisse Medienherrschaft, wenn man so will.

269
00:22:34,000 --> 00:22:43,000
Warum wird diese Demarkationslinie so gezogen in Bezug auf zum Beispiel Online-Targeting bei politischen Wahlkämpfen?

270
00:22:43,000 --> 00:22:46,000
Erstmal danke für die Frage. Das Thema ist mir extrem wichtig.

271
00:22:46,000 --> 00:22:52,000
Ich arbeite gerade für die EU-Kommission zu der Regulierung von politischer Online-Werbung.

272
00:22:52,000 --> 00:22:55,000
Man muss unterschiedliche Sachen unterscheiden.

273
00:22:55,000 --> 00:23:00,000
Es wurde ganz viel über Online-Werbung diskutiert im Kontext des Brexit-Referendums.

274
00:23:00,000 --> 00:23:02,000
Und ich habe jetzt jahrelang in England gelebt.

275
00:23:02,000 --> 00:23:09,000
Und es ist natürlich nicht so, dass Anti-EU, Anti-Einwanderer-Rechte, Populismus nur online stattfindet.

276
00:23:09,000 --> 00:23:11,000
Man muss nur in einen Kiosk gehen.

277
00:23:11,000 --> 00:23:15,000
Und die Tabloids sind voll von Missinformationen.

278
00:23:15,000 --> 00:23:20,000
Missinformationen und Lügen. Also das ist kein inhärentes Online-Problem.

279
00:23:20,000 --> 00:23:24,000
Was aber spannend ist oder was auch eine wirkliche Gefahr ist,

280
00:23:24,000 --> 00:23:29,000
und das ist so ein Phänomen, was man oft auch in anderen Bereichen sieht, zum Beispiel in der Polizeiarbeit.

281
00:23:29,000 --> 00:23:33,000
Die Regeln, die offline im Wahlkampf gelten, gelten nicht online.

282
00:23:33,000 --> 00:23:37,000
Beispielsweise, wenn ich jetzt hier ein Wahlplakat aufhänge,

283
00:23:37,000 --> 00:23:41,000
muss ich deklarieren, von wem das kommt und wer dafür bezahlt hat.

284
00:23:41,000 --> 00:23:46,000
Wenn ich, ich kann nicht einfach als Milliardär, der ich nicht bin,

285
00:23:46,000 --> 00:23:51,000
im deutschen Fernsehen jetzt einfach alle Werbeplätze kaufen

286
00:23:51,000 --> 00:23:54,000
und nur noch Werbung für Friederike als Kanzlerin machen.

287
00:23:54,000 --> 00:23:56,000
Das ist einfach nicht erlaubt.

288
00:23:56,000 --> 00:24:00,000
Weil es da Kontingente gibt, es gibt Plätze, das ist alles sehr streng reguliert.

289
00:24:00,000 --> 00:24:04,000
Oder man merkt ja auch jetzt im Bundeswahlkampf in Deutschland,

290
00:24:04,000 --> 00:24:06,000
es hängen eigentlich erst jetzt Plakate.

291
00:24:06,000 --> 00:24:08,000
Es waren nicht schon seit Monaten Plakate.

292
00:24:08,000 --> 00:24:13,000
Der Unterschied ist, dass online die Regeln einfach nicht greifen.

293
00:24:13,000 --> 00:24:15,000
Aus ganz unterschiedlichen Gründen.

294
00:24:15,000 --> 00:24:21,000
Was dann dazu führt, dass eben Parteien, und das ist in den USA passiert,

295
00:24:21,000 --> 00:24:27,000
dass zum Beispiel die Trump-Kampagne Millionen verschiedene Versionen

296
00:24:27,000 --> 00:24:29,000
von Werbung ausgestrahlt hat.

297
00:24:29,000 --> 00:24:31,000
Das ist noch ein weiterer Unterschied.

298
00:24:31,000 --> 00:24:35,000
Also offline, was auch immer ich auf Wahlplakaten behaupte,

299
00:24:35,000 --> 00:24:38,000
wen ich beleidige, wie radikal auch immer das ist,

300
00:24:38,000 --> 00:24:40,000
es hängt in der Öffentlichkeit.

301
00:24:40,000 --> 00:24:44,000
Und es kann damit gesehen und im öffentlichen Diskurs diskutiert werden.

302
00:24:44,000 --> 00:24:50,000
Der Unterschied zur Online-Werbung ist, weil die Online-Werbung eben personalisiert ist,

303
00:24:50,000 --> 00:24:54,000
ich kann eine Million Versionen derselben Werbung schalten

304
00:24:54,000 --> 00:24:57,000
und im Endeffekt jedem was anderes erzählen.

305
00:24:57,000 --> 00:25:01,000
Ich kann den einen Wählern das versprechen, den anderen Wählern das versprechen.

306
00:25:01,000 --> 00:25:05,000
Ich kann Gruppen gegeneinander aufhetzen,

307
00:25:05,000 --> 00:25:09,000
ohne dass man überhaupt etwas in der Öffentlichkeit davon mitkriegt,

308
00:25:09,000 --> 00:25:13,000
weil es eben so vergänglich ist und so schnelllebig ist.

309
00:25:13,000 --> 00:25:16,000
Die Werbung wird einen Tag geschaltet und dann wird sie nicht mehr geschaltet.

310
00:25:16,000 --> 00:25:19,000
Und vielleicht noch der letzte Grund ist,

311
00:25:19,000 --> 00:25:22,000
man kann durch, und das wurde auch in den USA gemacht,

312
00:25:22,000 --> 00:25:25,000
man kann durch Online-Werbung ganz gezielt auch testen,

313
00:25:25,000 --> 00:25:29,000
was so die Stimmung ist, worauf die Leute anspringen.

314
00:25:29,000 --> 00:25:33,000
Und Werbung wird ja nicht nur geschaltet, sondern das wird dann optimiert

315
00:25:33,000 --> 00:25:37,000
und das wird dann sehr genau gezielt an bestimmte Gruppen ausgeschaltet.

316
00:25:37,000 --> 00:25:41,000
Was ich damit sagen will, in der öffentlichen Debatte

317
00:25:41,000 --> 00:25:45,000
wurde das oft verkürzt auf politische Werbung ist eine Gefahr,

318
00:25:45,000 --> 00:25:49,000
weil die Menschen manipuliert, die dann anders wählen als sie sonst wählen würden.

319
00:25:49,000 --> 00:25:53,000
Das ist natürlich zu kurz gegriffen, das Problem ist viel komplexer

320
00:25:53,000 --> 00:25:56,000
und das zeigt so ein bisschen auch, wenn man über Polizeiarbeit

321
00:25:56,000 --> 00:25:59,000
oder vorausschauende Polizeiarbeit spricht, das Problem ist häufig,

322
00:25:59,000 --> 00:26:03,000
dass dieselben Regeln, die offline gelten, nicht online gelten.

323
00:26:03,000 --> 00:26:07,000
Und das quote-unquote online, das ist ja auch verschmolzen,

324
00:26:07,000 --> 00:26:10,000
dass plötzlich Dinge möglich sind, die so nicht möglich sind.

325
00:26:10,000 --> 00:26:14,000
Es ist offline nicht möglich, jedem einen anderen Wahlspruch zu zeigen.

326
00:26:14,000 --> 00:26:20,000
Das heißt, es bedarf irgendwie Regeln, es bedarf Intervention

327
00:26:20,000 --> 00:26:24,000
und so wie es momentan aussieht, machen das die Plattformen eben freiwillig

328
00:26:24,000 --> 00:26:28,000
und sie machen das auch nicht konsequent und auch nicht immer sehr zuverlässig.

329
00:26:28,000 --> 00:26:31,000
Und deswegen muss es halt Regeln geben, die das klar definieren.

330
00:26:31,000 --> 00:26:34,000
Auch wenn zum Beispiel dann bei Facebook mal ein anderer Chef ist,

331
00:26:34,000 --> 00:26:37,000
dass sich die Regeln dann nicht ändern.

332
00:26:37,000 --> 00:26:41,000
Aber ich mag die Frage, um nochmal darauf zurückzugehen, was ich an der Frage mag,

333
00:26:41,000 --> 00:26:46,000
ist, es stimmt, es gibt viel Digitalisierungskritik.

334
00:26:46,000 --> 00:26:50,000
Allein die Frage, so Digitalisierung gut oder schlecht, ist die völlig falsche Frage,

335
00:26:50,000 --> 00:26:54,000
sondern die Frage ist, wie, wie Digitalisierung, wie Technologien,

336
00:26:54,000 --> 00:26:58,000
was wollen wir eigentlich kollektiv als Gesellschaft

337
00:26:58,000 --> 00:27:03,000
und nicht das Empfinden als so eine unaufweichbare Bewegung.

338
00:27:04,000 --> 00:27:09,000
Und bei dieser Online-Werbung, also zumindest, das würde mich vielleicht auch noch interessieren,

339
00:27:09,000 --> 00:27:16,000
ich wüsste jetzt auch gar nicht, wie effektiv oder ob die, vielleicht eher so,

340
00:27:16,000 --> 00:27:21,000
ob die Erzählung dessen, wie effektiv das angeblich gewesen sei,

341
00:27:21,000 --> 00:27:25,000
wie sehr das mit der Realität übereinstimmt.

342
00:27:25,000 --> 00:27:29,000
Und tendenziell neige ich dazu, dass ich immer so ein bisschen die Intuition habe,

343
00:27:29,000 --> 00:27:34,000
dass es mir so scheint, als ob andere Aspekte, die eher sozioökonomische,

344
00:27:34,000 --> 00:27:39,000
sozio-politische Aspekte sind, die zu verschiedenen Entwicklungen führen,

345
00:27:39,000 --> 00:27:42,000
dass die unterschlagen werden, wenn man das so darstellt als,

346
00:27:42,000 --> 00:27:46,000
eben als sei quasi Trump, also jetzt ich bin absichtlich polemisch,

347
00:27:46,000 --> 00:27:52,000
Trump wäre gewählt worden, weil er über Social Media quasi die Leute gegeneinander aufgehetzt hat.

348
00:27:52,000 --> 00:27:56,000
Also dafür muss es ja ein Fundament geben, das in dem Fall, glaube ich,

349
00:27:56,000 --> 00:27:59,000
auch mit einer realen Not zum Beispiel zu tun hat,

350
00:27:59,000 --> 00:28:04,000
weil man kann Leute in Not besser gegeneinander aufhetzen, zum Beispiel.

351
00:28:04,000 --> 00:28:08,000
Die, sorry, jetzt habe ich meinen Faden verloren.

352
00:28:08,000 --> 00:28:14,000
Ich habe immer noch was hinterhergeschoben, immer noch was hinterhergeschoben.

353
00:28:14,000 --> 00:28:20,000
Es muss ja noch nicht passiert sein, das heißt aber nicht, dass es nicht doch auch noch passieren könnte.

354
00:28:20,000 --> 00:28:24,000
Also wir wissen, was technisch möglich ist und das Beispiel, was ich immer gerne bringe, ist,

355
00:28:24,000 --> 00:28:28,000
es ist belegt, es gab in den USA Abtreibungsgegner,

356
00:28:28,000 --> 00:28:35,000
die abtreibungswillige Frauen im Klinikbesuch mit Antiabtreibungswerbung bombardiert haben.

357
00:28:35,000 --> 00:28:38,000
Werden die deshalb jetzt nicht abtreiben?

358
00:28:38,000 --> 00:28:44,000
Wahrscheinlich nicht, es ist aber eine ganz perfide Form von Belästigung.

359
00:28:44,000 --> 00:28:47,000
Wahlen werden ja auch oft, es gibt immer mehr Wahlen,

360
00:28:47,000 --> 00:28:52,000
die werden in bestimmten Wahlkreisen durch wenige tausend Stimmen getroffen.

361
00:28:52,000 --> 00:28:55,000
Die Wahlerwerbung muss ja noch nicht mal sagen, wähl Trump,

362
00:28:55,000 --> 00:28:57,000
sondern die Wahlerwerbung kann auch einfach sagen,

363
00:28:57,000 --> 00:29:00,000
der ganze Politikbetrieb ist kaputt, geh doch gar nicht wählen.

364
00:29:00,000 --> 00:29:03,000
Und das macht das halt alles so kompliziert.

365
00:29:03,000 --> 00:29:06,000
Und ich glaube, das ist ein gutes Beispiel, weil das zeigt, wir wissen,

366
00:29:06,000 --> 00:29:12,000
technisch ist es eben möglich, sehr spezifische Gruppen sehr spezifisch anzusprechen.

367
00:29:12,000 --> 00:29:15,000
Keine Ahnung, wie effektiv das ist, aber das Problem ist,

368
00:29:15,000 --> 00:29:18,000
niemand kann es messen, außer die Plattformen selber.

369
00:29:18,000 --> 00:29:21,000
Aber hier geht es ja nicht um irgendwas, hier geht es um Demokratie.

370
00:29:21,000 --> 00:29:26,000
Es ist eigentlich extrem beunruhigend, dass wir gar nicht mal richtig wissen,

371
00:29:26,000 --> 00:29:29,000
welchen Einfluss Plattformen auf Wahlen hatten,

372
00:29:29,000 --> 00:29:32,000
weil nur die Plattformen selber überhaupt die Daten dazu haben.

373
00:29:32,000 --> 00:29:34,000
Das alleine für mich reicht schon aus.

374
00:29:34,000 --> 00:29:38,000
Es muss nicht ausgenutzt werden, wir wissen, dass es ausgenutzt werden könnte.

375
00:29:38,000 --> 00:29:40,000
Und das sollte nicht passieren.

376
00:29:40,000 --> 00:29:43,000
Total, also da würde ich natürlich sofort zustimmen.

377
00:29:43,000 --> 00:29:47,000
Wir sollten dafür sorgen, dass die Dinge eben in einer Art und Weise nachvollziehbar werden,

378
00:29:47,000 --> 00:29:49,000
wie sie es bisher leider nicht sind.

379
00:29:49,000 --> 00:29:52,000
Und das nährt ja am Ende dann eben auch diese Mythologien,

380
00:29:52,000 --> 00:29:56,000
weil die Plattformen haben natürlich ein Eigeninteresse daran zu tun,

381
00:29:56,000 --> 00:30:00,000
als wären ihre Werbung unglaublich effektiv und könnten extrem genau,

382
00:30:00,000 --> 00:30:03,000
also detailliert targetten und so weiter und so fort.

383
00:30:03,000 --> 00:30:08,000
Ich würde aber gerne jetzt dann noch zu einer anderen Demystifisierung übergehen,

384
00:30:08,000 --> 00:30:12,000
nämlich zu etwas, was du in einem Block-Eintrag AI Snake Oil genannt hast.

385
00:30:12,000 --> 00:30:17,000
Das ist ein Ausdruck, der mir sehr gut gefallen hat und sofort ins Auge gesprungen ist.

386
00:30:17,000 --> 00:30:25,000
Und du sprichst da auch vom Jahr 2020 als dem Jahr der AI-Desillusionierung.

387
00:30:25,000 --> 00:30:28,000
Vielleicht kannst du zum einen ein bisschen darauf eingehen, warum du meinst,

388
00:30:28,000 --> 00:30:33,000
dass 2020 dieses Jahr der AI-Desillusionierung war

389
00:30:33,000 --> 00:30:40,000
und vielleicht auch ein konkretes Beispiel geben für AI Snake Oil.

390
00:30:40,000 --> 00:30:46,000
Ich glaube, 2020 war das Jahr, also alleine empirisch das Schöne ist,

391
00:30:46,000 --> 00:30:48,000
wenn man schon so lange an diesem Themenkomplex arbeitet,

392
00:30:48,000 --> 00:30:51,000
dass man sieht, dass die Dinge immer in Wellen kommen.

393
00:30:51,000 --> 00:30:54,000
Also 2014 sprachen wir über Big Data.

394
00:30:54,000 --> 00:30:58,000
Niemand redet mehr von Big Data plötzlich, aber die Phänomene sind ja nicht weg.

395
00:30:58,000 --> 00:31:06,000
Dann kam eine Zeit, wo alles war AI, ohne auch kritische Äußerungen.

396
00:31:06,000 --> 00:31:10,000
Und dann gab es die Welle, wo man gesagt hat, na ja, aber AI kann diskriminieren,

397
00:31:10,000 --> 00:31:13,000
AI hat ethische Probleme etc.

398
00:31:13,000 --> 00:31:17,000
Also es gibt immer diese Wellen. Und für mich war 2020 schon ein interessantes Jahr,

399
00:31:17,000 --> 00:31:23,000
weil es wurde jahrelang behauptet, dass AI eigentlich jedes Problem lösen kann.

400
00:31:23,000 --> 00:31:29,000
Und dann 2020 stand und steht die Welt vor einem wirklichen Problem.

401
00:31:29,000 --> 00:31:33,000
Wir haben ja genug Probleme, aber so einer akuten Krise,

402
00:31:33,000 --> 00:31:38,000
wie die Pandemie. Und der Witz war, dass AI relativ wenig dazu beigetragen hat,

403
00:31:38,000 --> 00:31:43,000
die Pandemie zu lösen, weil nämlich Probleme hochkomplex sind.

404
00:31:43,000 --> 00:31:47,000
Und die Pandemie, es liegt ja nicht daran, AI kann die Pandemie nicht lösen,

405
00:31:47,000 --> 00:31:52,000
sondern es ist eine Kombination aus politischem Willen, auch Desinformation online.

406
00:31:52,000 --> 00:31:55,000
Es sind hochkomplexe gesellschaftliche Probleme.

407
00:31:55,000 --> 00:31:57,000
Es ist kein informationelles Problem.

408
00:31:57,000 --> 00:31:59,000
Wir kriegen die Pandemie nicht in den Griff,

409
00:31:59,000 --> 00:32:03,000
weil wir zu wenig wissen, sondern aus ganz vielen komplexen anderen Gründen.

410
00:32:03,000 --> 00:32:07,000
Und das hat, glaube ich, schon ein bisschen dazu zu der allgemeinen Stimmung beigetragen.

411
00:32:07,000 --> 00:32:13,000
Der Grund, warum mich das Thema interessiert, ich habe eben letztes Jahr ein Fellowship gemacht.

412
00:32:13,000 --> 00:32:19,000
Und das Fellowship-Projekt ist ein Buch, was ich herausgebe zum Thema KI-Pseudo-Wissenschaften.

413
00:32:19,000 --> 00:32:24,000
Und in der Einleitung, das kommt auf Englisch raus, und in der Einleitung des Buches steht,

414
00:32:24,000 --> 00:32:30,000
Also dieses Buch ist eine Intervention, weil es ist ein bisschen mehr abgeflaucht jetzt,

415
00:32:30,000 --> 00:32:34,000
aber es vergeht eigentlich kein Tag oder keine Woche,

416
00:32:34,000 --> 00:32:37,000
in der ich nicht irgendeinen Nachrichtenartikel auf Twitter lese,

417
00:32:37,000 --> 00:32:42,000
wo wieder behauptet, dass KI irgendetwas kann, was es auf keinen Fall kann.

418
00:32:42,000 --> 00:32:47,000
Von KI kann erkennen, ob wir unsere sexuelle Orientierung erkennen,

419
00:32:47,000 --> 00:32:50,000
KI kann erkennen, ob wir unsere sexuelle Orientierung erkennen,

420
00:32:50,000 --> 00:32:53,000
KI kann erkennen, ob wir unsere sexuelle Orientierung erkennen,

421
00:32:53,000 --> 00:33:00,000
KI kann etnie, da kommt man ganz schnell in sehr hochproblematische, rassistische Ecken.

422
00:33:00,000 --> 00:33:06,000
Oder auch ganz am Anfang der Pandemie waren auch so KI Videokameras,

423
00:33:06,000 --> 00:33:09,000
können entdecken, ob jemand Covid hat oder nicht.

424
00:33:09,000 --> 00:33:11,000
Das ist ja völliger Quatsch.

425
00:33:11,000 --> 00:33:16,000
Aber die Frage, die dem Buch oder die das Buch nachgeht,

426
00:33:16,000 --> 00:33:21,000
und ich habe auch ganz bewusst, durch die Pandemie konnte ich auch dieses Fellowship nicht so verbringen,

427
00:33:21,000 --> 00:33:25,000
wie ich es verbringen wollte und dachte, ich möchte dieses Projektgeld dann zumindest verteilen

428
00:33:25,000 --> 00:33:29,000
und dann auch mit anderen Leuten teilen.

429
00:33:29,000 --> 00:33:33,000
Ich habe bewusst unterschiedliche Autoren eingeladen, die eben dieser Frage nachgehen,

430
00:33:33,000 --> 00:33:36,000
warum gibt es einfach so viel KI-Pseudowissenschaften?

431
00:33:36,000 --> 00:33:38,000
Warum ist das so?

432
00:33:38,000 --> 00:33:44,000
Und die Autorinnen haben jeweils eine ganz unterschiedliche Erklärung.

433
00:33:44,000 --> 00:33:48,000
Und die erste Frage, mit der wir uns so ein bisschen auseinandersetzen in dem Buch ist,

434
00:33:48,000 --> 00:33:54,000
was ist eigentlich der Unterschied zwischen Pseudowissenschaften, Schlangenöl und Hype?

435
00:33:54,000 --> 00:33:58,000
Das sind ja ganz unterschiedliche Dinge, die unterschiedliche Kerne des Problems beschreiben.

436
00:33:58,000 --> 00:34:03,000
Also Schlangenöl, das ist im Deutschen weniger gängig, als es so im Englischen ist.

437
00:34:03,000 --> 00:34:10,000
Snake Oil, das beschreibt eigentlich unfaires Marketing, also Produkte.

438
00:34:10,000 --> 00:34:14,000
Schlangenöl, Haut, sehr viel Kosmetik, Frauenkosmetik ist Schlangenöl.

439
00:34:14,000 --> 00:34:19,000
Da steht, hier ist eine Wundercreme, die irgendetwas kann und die Creme kann eigentlich gar nichts.

440
00:34:19,000 --> 00:34:21,000
Das ist Snake Oil runtergebrochen.

441
00:34:21,000 --> 00:34:24,000
Hype ist natürlich nochmal was anderes.

442
00:34:24,000 --> 00:34:27,000
Hype ist gar nicht immer schlecht.

443
00:34:27,000 --> 00:34:30,000
Hype ist einfach, naja, auch Aufregung.

444
00:34:30,000 --> 00:34:35,000
Wir leben ja in einer Welt, in der Aufmerksamkeit das teuerste Gut ist.

445
00:34:35,000 --> 00:34:38,000
Irgendwie muss man ja auch gehört werden.

446
00:34:38,000 --> 00:34:41,000
Und dann die dritte Kategorie ist einfach Pseudowissenschaften.

447
00:34:41,000 --> 00:34:46,000
Also einfach Wissenschaft, die nach Wissenschaft aussieht, aber völliger Quatsch ist.

448
00:34:46,000 --> 00:34:49,000
Und diese drei Kategorien schauen sich eben das Buch an.

449
00:34:49,000 --> 00:34:54,000
Wir haben zum Beispiel einen Beitrag von einem Journalisten, der über KI schreibt für The Verge

450
00:34:54,000 --> 00:35:00,000
und so ein bisschen erzählt, wie es ist, wenn er Produktpitches bekommt.

451
00:35:00,000 --> 00:35:03,000
Zum Beispiel hier ist eine KI-gesteuerte Zahnbürste.

452
00:35:03,000 --> 00:35:07,000
Und in dem Artikel reflektiert er auch darüber, wie kann ich darüber kritisch berichten,

453
00:35:07,000 --> 00:35:10,000
ohne dem gleichzeitig noch mehr Aufmerksamkeit zu geben.

454
00:35:10,000 --> 00:35:12,000
Das ist irgendwie ein Aspekt.

455
00:35:12,000 --> 00:35:22,000
Da ist auch eine Autorin, die spricht, eine Ingenieurin, die sagt, ganz viel ist wirklich einfach verantwortungslos.

456
00:35:22,000 --> 00:35:26,000
Manchmal werden einfach verantwortungslose Produkte gebaut, schlampig gebaut.

457
00:35:26,000 --> 00:35:30,000
Und das fand ich ganz spannend, weil das ist natürlich was anderes, wenn ich das sage,

458
00:35:30,000 --> 00:35:33,000
als wenn das eine Ingenieurin sagt, die in diesen Teams gearbeitet hat.

459
00:35:33,000 --> 00:35:35,000
Also wir schauen uns unterschiedliche Aspekte an.

460
00:35:35,000 --> 00:35:43,000
Aber der Begriff KI AI Snake Oil oder KI Schlangenöl stammt von einem Professor aus Princeton,

461
00:35:43,000 --> 00:35:45,000
den wir auch in dem Buch interviewen.

462
00:35:45,000 --> 00:35:52,000
Der hat diesen Begriff geprägt in einem Vortrag, in dem er irgendwie heißt How to recognize AI Snake Oil.

463
00:35:52,000 --> 00:35:56,000
Und das Tolle an dem Vortrag ist, er nimmt so ein bisschen auseinander.

464
00:35:56,000 --> 00:35:58,000
Was ist eigentlich KI?

465
00:35:58,000 --> 00:36:02,000
Das Problem ist ja, dass das Wort hat eine ganz klare technische Definition,

466
00:36:02,000 --> 00:36:07,000
wird im allgemeinen Sprachgebrauch aber benutzt, um alles Mögliche zu bezeichnen.

467
00:36:07,000 --> 00:36:13,000
Es gab mal eine Studie, die hat gesagt, dass 60 Prozent, die ist glaube ich jetzt zwei Jahre alt,

468
00:36:13,000 --> 00:36:19,000
60 Prozent aller europäischen KI Startups machen eigentlich gar nichts mit KI.

469
00:36:19,000 --> 00:36:22,000
Es gibt halt einfach viel Geld, wenn man KI draufschreibt.

470
00:36:22,000 --> 00:36:29,000
Und was er in diesem Vortrag gemacht hat, er unterscheidet, auf der einen Seite gab es wirklich in den letzten Jahren

471
00:36:29,000 --> 00:36:38,000
in sehr klar und eng definierten Problembeschreibungen hat der Einsatz von KI immense Fortschritte gemacht.

472
00:36:38,000 --> 00:36:41,000
Sowas wie ein Objekt zu erkennen.

473
00:36:41,000 --> 00:36:45,000
Oder wenn man sich anschaut, wie gut sich zum Beispiel Google Translate verändert hat.

474
00:36:45,000 --> 00:36:51,000
Ich war vor einem Jahr in China und man kann einfach in ein Gerät auf Englisch reinsprechen

475
00:36:51,000 --> 00:36:54,000
und das Gerät übersetzt automatisch auf Chinesisch.

476
00:36:54,000 --> 00:36:58,000
Das hätten wir noch vor zehn Jahren für unglaublich gehalten.

477
00:36:58,000 --> 00:37:04,000
Sein Punkt ist aber, das sind sehr klar definierte, eng abgegrenzte Aufgaben.

478
00:37:04,000 --> 00:37:09,000
Das sind keine offenen Probleme wie die Pandemie lösen.

479
00:37:09,000 --> 00:37:14,000
Und er beschreibt so ein bisschen, er unterteilt die Aufgaben in unterschiedliche Kategorien

480
00:37:14,000 --> 00:37:19,000
und sagt eben, es gibt eine Kategorie und das habe ich immer noch nicht raus, wie man das gut auf Deutsch übersetzt,

481
00:37:19,000 --> 00:37:22,000
wo er sagt so Predicting Social Outcomes.

482
00:37:22,000 --> 00:37:32,000
Also KI ist einfach grundsätzlich immer dubios, wenn es dazu angewendet wird, um die Zukunft vorauszusagen.

483
00:37:32,000 --> 00:37:35,000
Sowohl die kollektive als auch die individuelle.

484
00:37:35,000 --> 00:37:38,000
Weil das einfach nicht geht.

485
00:37:38,000 --> 00:37:43,000
Und das heißt sowas wie vorausschauende Polizeiarbeit oder so sagen, wer du bist,

486
00:37:43,000 --> 00:37:46,000
was deine sexuelle Orientierung ist, das ist einfach Quatsch.

487
00:37:47,000 --> 00:37:53,000
Und das hat nichts und nur weil wir Fortschritt darin gemacht haben, Objekte zu erkennen, heißt das nicht,

488
00:37:53,000 --> 00:37:58,000
dass wir automatisch auch in diesen anderen Fragestellen, in diesen inherent normativen

489
00:37:58,000 --> 00:38:01,000
und inherent sozialen Fragestellungen denselben Fortschritt machen werden.

490
00:38:01,000 --> 00:38:07,000
Das heißt, das Buch ist so ein bisschen eine Aufforderung, differenzierter zu betrachten,

491
00:38:07,000 --> 00:38:13,000
was ist eigentlich KI, was ist eigentlich der Fortschritt und was ist auch einfach völliger Unsinn.

492
00:38:14,000 --> 00:38:22,000
Und du hast recht, ich mag deine Frage, weil oft trägt die Kritik dann auch dazu bei, zum Hype, zur Überhöhung der Technik.

493
00:38:22,000 --> 00:38:27,000
Weil man sagt, wir müssen jetzt Angst vor Algorithmen haben, wir müssen Angst vor KI haben, das ist ja Quatsch.

494
00:38:27,000 --> 00:38:30,000
Es kommt nämlich einfach darauf an, worum es geht.

495
00:38:30,000 --> 00:38:35,000
Ja und auch in der Darstellung, was sie eben können, liegt die Kritik manchmal halt dann falsch,

496
00:38:35,000 --> 00:38:39,000
weil sie eben zu sehr quasi diesen Marketing-Sprech eigentlich kauft.

497
00:38:39,000 --> 00:38:46,000
Und zur Frage der Zukunft ist natürlich dann noch das perfide, was da vielleicht noch hinzuzusagen wäre,

498
00:38:46,000 --> 00:38:52,000
dass auf einer anderen Ebene es ja sehr wohl wieder funktioniert, nämlich dadurch, dass so getan wird,

499
00:38:52,000 --> 00:38:59,000
als könne man quasi Zukunft bis zu einem gewissen Grad vorausschauen, dann ja auch Politiken betrieben werden,

500
00:38:59,000 --> 00:39:06,000
die auf Basis dieser Pseudo-Vorhersagen handeln und dadurch dann wiederum de facto

501
00:39:06,000 --> 00:39:10,000
möglichkeitskorridore eingeschränkt werden. Und das ist natürlich schrecklich.

502
00:39:10,000 --> 00:39:19,000
Also es gibt dem der schönste Text, das heißt Cheap AI von einer Wissenschaftlerin, die heißt Abhiba Birhane,

503
00:39:19,000 --> 00:39:25,000
die schreibt, man redet manchmal so in der Linguistik von Cheap Talk.

504
00:39:25,000 --> 00:39:31,000
Also es ist sehr einfach, diese Produkte zu konzipieren und zu bauen, aber für diejenigen, die betroffen sind,

505
00:39:31,000 --> 00:39:39,000
haben sie eben wirkliche Konsequenzen von, weiß ich nicht, trans Menschen, die gar nicht erkannt werden,

506
00:39:39,000 --> 00:39:45,000
die nicht existieren in der Kategorisierung des Systems, bis hin zu wirklich übelst rassistischen Systemen.

507
00:39:45,000 --> 00:39:50,000
Und diese, nur wenn man KI draufschreibt, das ist mehr als nur ein Marketing-Trick.

508
00:39:50,000 --> 00:39:56,000
Das gibt dem den Anschein von Zukunft und von Wissenschaftlichkeit und von Neuheit.

509
00:39:57,000 --> 00:40:03,000
Dabei gibt es bestimmte Anwendungen, wo wirklich einfach nur überholte Bilder repliziert werden.

510
00:40:05,000 --> 00:40:11,000
Auf eine Art leitet das auch über, nochmal zur Frage der Daten.

511
00:40:13,000 --> 00:40:22,000
Eine ein bisschen gewagte Hypothese, die ich mal in den Raum stellen will, ist nochmal zu fragen, ob wir es denn tatsächlich mit Big Data zu tun haben

512
00:40:22,000 --> 00:40:29,000
oder ob es nicht vielleicht angebrachter wäre zu sagen, was wir vor uns sehen ist eher Small Data insofern,

513
00:40:29,000 --> 00:40:37,000
als dass die Player, die in der Lage sind, diese Mengen an Daten zu produzieren, das ist ja nichts, was irgendwie frei da draußen rumliegt

514
00:40:37,000 --> 00:40:42,000
und müsste nur eingesammelt werden, sondern es wird produziert, es wird erschaffen und dann ausgewertet.

515
00:40:42,000 --> 00:40:49,000
Und die Player, die überhaupt die Ressourcen haben, um das effektiv zu tun, sind entweder Staaten oder eben Großkonzerne.

516
00:40:49,000 --> 00:40:56,000
Und die haben ja ein spezifisch vorgefertigtes Eigeninteresse, das im Grunde eigentlich dann ja ein Gebiet erzeugt,

517
00:40:56,000 --> 00:41:03,000
was relativ eng gefasst ist, also deswegen eigentlich Small Data und was dann auf dieses spezifische Gebiet hin

518
00:41:03,000 --> 00:41:10,000
vielleicht immer detaillierter werden kann, das mag ich sehr wohl glauben, aber das eigentlich ein riesengroßes Feld,

519
00:41:10,000 --> 00:41:18,000
das jenseits dieser spezifisch formierten Interessen liegt, dass das damit eigentlich unmöglich erfasst werden kann.

520
00:41:18,000 --> 00:41:24,000
Und daraus schließt sich dann eigentlich die Frage an, brauchen wir andere Daten?

521
00:41:24,000 --> 00:41:30,000
Und dann eigentlich natürlich auch bräuchten wir andere Player, die in der Lage wären, diese Daten zu produzieren.

522
00:41:30,000 --> 00:41:36,000
Also ich würde sagen Big but shallow. Also es gibt wirklich, es gab noch nie so viele Daten.

523
00:41:36,000 --> 00:41:43,000
Dieser Begriff Big Data kommt auch daher, das erfordert ganz andere Verarbeitungsmechanismen und Techniken.

524
00:41:43,000 --> 00:41:50,000
Ich hatte mal ein ganz spannendes Gespräch mit einer Entwicklerin, die für die BBC arbeitet

525
00:41:50,000 --> 00:41:57,000
und für die BBC Empfehlungsalgorithmen erarbeitet. Und die Fragestellung, ich weiß nicht, ob das nie jemals verwirklicht wurde,

526
00:41:57,000 --> 00:42:06,000
aber die Fragestellung, die sie sich gestellt hat, was sind Personalisierungs- oder Empfehlungssysteme, die im öffentlichen Interesse sind?

527
00:42:07,000 --> 00:42:13,000
Und dann die Frage, die aufkam, also weiß ich nicht, ich gehe auf BBC News und dann werden wir auch, wie bei Netflix oder bei Amazon,

528
00:42:13,000 --> 00:42:22,000
empfohlen, was ich als nächstes sehen soll. Der Unterschied ist nur, dass die BBC anders als Netflix eine ganz andere Daseinsberechtigung und auch Mission haben.

529
00:42:22,000 --> 00:42:28,000
Und die Frage, die dann aufkam, war, naja, all diese Firmen, die sammeln ja so viel Daten von uns,

530
00:42:28,000 --> 00:42:33,000
wäre es nicht möglich, dass man seine Netflix-Daten an die BBC spenden könnte?

531
00:42:34,000 --> 00:42:43,000
Und die Antwort darauf ist, es ist Quatsch, weil Netflix eben, weil es auf ein ganz anderes Ziel optimiert ist, ganz andere Arten von Daten sammelt,

532
00:42:43,000 --> 00:42:52,000
die für ein Empfehlungssystem im öffentlichen Interesse oder im Gemeinwohlinteresse gar nicht richtig verwertbar sind.

533
00:42:52,000 --> 00:43:01,000
Ich habe mal nach meinem Studium kurz gedacht, ich möchte Data Scientist werden und habe bei Zeit Online ein Praktikum gemacht,

534
00:43:01,000 --> 00:43:09,000
wo es auch darum ging zu messen, ja, wie kann man Qualitätsjournalismus messen?

535
00:43:09,000 --> 00:43:18,000
Also man will ja nicht nur auf Klicks optimieren, die Zeit ist nicht irgendein Blog, so ein Clickbait-Blog,

536
00:43:18,000 --> 00:43:23,000
sondern eine Zeitung, die auch schwierige Themen behandeln möchte.

537
00:43:23,000 --> 00:43:32,000
Was ich so interessant fand aus dieser Erfahrung heraus war, dass die ganzen Tools, die es damals vor zehn Jahren intern gab,

538
00:43:32,000 --> 00:43:37,000
um überhaupt das Leseverhalten zu messen, alle aus dem E-Commerce-Marketing kamen.

539
00:43:37,000 --> 00:43:43,000
Das heißt, die Zahlen waren dann optimiert für ganz andere Szenarien, also Szenarien,

540
00:43:43,000 --> 00:43:48,000
wo man möchte, dass Leute so viel Zeit wie möglich auf der Seite verbringen, dass sie so oft wie möglich klicken.

541
00:43:48,000 --> 00:43:53,000
Dabei ist das eigentlich keine Erfolgsmetrik für ein Medium wie Zeit Online.

542
00:43:53,000 --> 00:44:00,000
Es ist bestimmt jetzt ganz anders. Aber jedenfalls, genau, es gibt sehr viele Daten, aber es gibt auch sehr viele Daten überhaupt nicht.

543
00:44:00,000 --> 00:44:08,000
Also das ist halt dieses Paradox. Also was ich zum Beispiel spannend fand in der 2015 oder auch jetzt in der Pandemie,

544
00:44:08,000 --> 00:44:13,000
ganz viele Gesundheitsämter haben keine kompatiblen Datensysteme und können die Daten nicht austeilen.

545
00:44:14,000 --> 00:44:17,000
Also genau, zu wenig und zu viele gleichzeitig.

546
00:44:18,000 --> 00:44:25,000
Das bringt mich dann jetzt zu einer anderen Frage, die eher auf diese systemische Ebene abzielt eigentlich,

547
00:44:25,000 --> 00:44:33,000
weil sich ja auch die Frage stellt, wie kommen wir dahin, dass diese anderen Daten auch produziert werden,

548
00:44:33,000 --> 00:44:42,000
dass aus einer anderen Logik heraus Daten produziert werden und dadurch dann eben auch andere technologische Infrastrukturen erzeugt werden können,

549
00:44:42,000 --> 00:44:49,000
die andere Zielmaßgaben haben. Und da möchte ich einen kurzen Satz erwähnen, den Felix Stahlder,

550
00:44:49,000 --> 00:44:55,000
ein Gast aus Future Histories, auch in seinem Buch Kultur der Digitalität geschrieben hat.

551
00:44:55,000 --> 00:45:01,000
Der sagte nämlich, nicht der Algorithmus ist pervers, sondern die Situation, in der er lebt.

552
00:45:01,000 --> 00:45:07,000
Und das lässt sich jetzt wiederum ein bisschen quer schließen zum Thema dieses Kongresses hier,

553
00:45:07,000 --> 00:45:11,000
denn hier geht es ja auch um die Frage konsequent, inkonsequent.

554
00:45:11,000 --> 00:45:20,000
Und was mich immer so ein bisschen gestört hat an diesem Framing ist, dass es für meinen Geschmack zu sehr in diese Richtung tendiert,

555
00:45:20,000 --> 00:45:28,000
dem Individuum diese Frage zu beantworten, ob jetzt das richtig gemacht wird oder nicht.

556
00:45:28,000 --> 00:45:35,000
Aber meiner Meinung nach sind diese Infrastrukturen derzeit ja absichtlich und extra einfach so aufgebaut,

557
00:45:35,000 --> 00:45:44,000
dass wir möglichst inkonsequent im Sinne des zum Beispiel Datenschutzes usw. handeln, wenn wir uns in diesen digitalen Welten bewegen.

558
00:45:44,000 --> 00:45:52,000
Das heißt, um quasi den letzten Schritt dann noch zu machen, sind wir nicht inkonsequent oder konsequent inkonsequent

559
00:45:52,000 --> 00:45:58,000
dann insofern, als dass wir diese systemische Ebene in der Fragestellung eigentlich allzu oft ausblenden

560
00:45:59,000 --> 00:46:06,000
und sollten wir nicht eigentlich auf dieser Ebene auch ansetzen, weil die jetzigen Logiken, die incentivieren das ja.

561
00:46:06,000 --> 00:46:12,000
Die viel wichtigere Frage finde ich, wer ist wir? Und das wir ist nämlich kompliziert.

562
00:46:12,000 --> 00:46:17,000
Also Risiken und sowohl Risiken als auch Chancen sind extrem ungleich verteilt.

563
00:46:17,000 --> 00:46:23,000
Und das ist die Hauptlektion aus fast zehn Jahren Arbeit an dem Thema, ist genau das.

564
00:46:23,000 --> 00:46:36,000
Beispiel, wenn man sagt, Gesichtserkennung für die allermeisten, für weiße Männer ist Gesichtserkennung sehr akkurat.

565
00:46:36,000 --> 00:46:44,000
Für nicht weiße Frauen sieht die Genauigkeit ganz anders aus.

566
00:46:44,000 --> 00:46:49,000
Das ist aber nicht nur da der Fall, das ist wirklich so ein Muster, was sich durch alles zieht.

567
00:46:49,000 --> 00:46:56,000
Beispiel, wir werden ja auch, da geht es oft darum so, es gibt dann diese Daten von mir und dann können die gegen mich verwendet werden.

568
00:46:56,000 --> 00:46:58,000
Das hängt natürlich auch von meiner Lebenssituation ab.

569
00:46:58,000 --> 00:47:07,000
Wenn ich beispielsweise finanziell unabhängig bin, dann kann mir das sehr herzlich egal sein, ob ich damit bewerte, ob meine Kreditwürdigkeit dadurch bewertet wird.

570
00:47:07,000 --> 00:47:16,000
Und besonders risikobehaftete Anwendungen werden auch in der Regel tendenziell immer auf bestimmte Bevölkerungsgruppen angewendet.

571
00:47:16,000 --> 00:47:20,000
Deswegen dieses Wir ist kompliziert.

572
00:47:20,000 --> 00:47:26,000
Und das, was für die einen vielleicht sehr angenehm ist, ist vielleicht auch für jemand ganz anderen lebensgefährlich.

573
00:47:26,000 --> 00:47:28,000
Also das Wir ist kompliziert.

574
00:47:28,000 --> 00:47:31,000
Und das andere ist, ich hasse dieses auf die Individualebene bringen.

575
00:47:31,000 --> 00:47:39,000
Und versuche auch aus dem Grund nicht mehr so viel über Daten zu sprechen, sondern viel mehr über Macht, über Infrastruktur.

576
00:47:40,000 --> 00:47:46,000
Weil sobald man über Daten redet, geht es dann schnell auf das Thema, ja, was soll ich denn mit meinen Daten machen?

577
00:47:46,000 --> 00:47:52,000
Weil jeder fühlt sich schuldig, alle haben das Gefühl, dass sie zu viele Daten irgendwie, das ist so wie recyclen.

578
00:47:52,000 --> 00:47:56,000
Ich sollte mehr recyceln, ich gebe mir sehr viel Mühe, aber manchmal mache ich es doch nicht.

579
00:47:56,000 --> 00:47:58,000
Dabei ist das eigentlich der völlig falsche Ansatz.

580
00:47:58,000 --> 00:48:03,000
Niemand hat Zeit, sich jeden Tag damit auseinanderzusetzen, was mit den eigenen Daten passiert.

581
00:48:03,000 --> 00:48:07,000
Das ist meine Arbeit und ich habe keine Zeit und kein Interesse daran.

582
00:48:07,000 --> 00:48:12,000
Und wir unterschätzen auch oft, dass die, genau, das sind systemische Fragen.

583
00:48:12,000 --> 00:48:19,000
Es ist oft, sogar wird es einem unmöglich gemacht, überhaupt eine Entscheidung zu fällen.

584
00:48:21,000 --> 00:48:28,000
Man sagt so, der Satz ist immer so in dem Security-Bereich, die NutzerInnen sind eigentlich das schwächste Glied in dem ganzen System.

585
00:48:28,000 --> 00:48:31,000
Und wir können nicht die Verantwortung darauf abwälzen.

586
00:48:32,000 --> 00:48:44,000
Und wenn man da jetzt in diese Richtung weitergeht, dann, finde ich, liegt da eigentlich auch noch ein Potential für ein eigentlich uneingelöstes Versprechen, wenn man so will.

587
00:48:44,000 --> 00:48:54,000
Weil also diese Frage in Bezug auf was diese Technologien und Technologiebündel, was die leisten können oder nicht,

588
00:48:54,000 --> 00:48:59,000
die ist natürlich auf der einen Seite wichtig zu verhandeln im Sinne des Demystifizierens.

589
00:48:59,000 --> 00:49:08,000
Auf der anderen Seite ist es ja auch so, dass da gerade unglaublich viele Ressourcen, Kreativität, Gestaltungskraft und so weiter,

590
00:49:08,000 --> 00:49:17,000
jetzt mal ein bisschen polemisch zugespitzt da hineinfließen, dass man Leute dazu bringt, auf irgendwelche idiotischen Online-Ads zu klicken.

591
00:49:17,000 --> 00:49:29,000
Also im Grunde gäbe es da ja auch noch eine Potentialität innerhalb alternativer Technologien und alternativer technologischer Infrastrukturen,

592
00:49:29,000 --> 00:49:39,000
die bis zu einem gewissen Grad innerhalb der jetzigen, auch politökonomischen Formatierung eigentlich nicht eingelöst werden kann.

593
00:49:41,000 --> 00:49:45,000
Wenn du in diese Richtung denkst, was zeigt sich dir dann?

594
00:49:47,000 --> 00:49:55,000
Das Wichtigste ist mir, dass ich nicht deterministisch bin, also auch auf Plattformen, die nach fürchterlichen Logiken funktionieren,

595
00:49:55,000 --> 00:50:01,000
auf denen es Hass gibt, die radikalisieren, gibt es trotzdem wunderbare, interessante, spannende Inhalte.

596
00:50:01,000 --> 00:50:07,000
Also das ist alles viel beweglicher, als es einem so scheint.

597
00:50:07,000 --> 00:50:19,000
Und ich arbeite ja im Thema Regulierung, aber weil ich schon glaube, dass, oder meine Erfahrung ist so ein bisschen das Einzige, was so funktioniert,

598
00:50:19,000 --> 00:50:30,000
wir haben es ja mit wirklich marktdominanten Firmen zu tun. Das ist ja keine normale Situation, in dem es hier gibt ein paar Optionen

599
00:50:30,000 --> 00:50:35,000
und wir können uns dafür entscheiden, welche wir am besten finden, sondern wir haben eine Realität geschaffen,

600
00:50:35,000 --> 00:50:41,000
in der, und das war wirklich diese Woche, ich habe so viel über Afghanistan nachgedacht, man muss sich wirklich fragen,

601
00:50:41,000 --> 00:50:46,000
was ist denn die Außenpolitik der Plattform, die die Taliban jetzt als Regierung anerkennen?

602
00:50:46,000 --> 00:50:53,000
Das zeigt nochmal, wie viel Macht diese Plattformen haben. Das ist ja absurd, dass wir überhaupt diese Frage stellen müssen,

603
00:50:53,000 --> 00:51:01,000
weil das das Medium ist, über das die ganze Welt Dinge erfährt. Und selbst wenn man eine Nachrichtenagentur ist,

604
00:51:01,000 --> 00:51:07,000
aber wenn man eine Nachrichtenseite hat, eine Zeitung, man ist abhängig vom Plattform zu den Lesern zu finden.

605
00:51:07,000 --> 00:51:14,000
Das ist schon nur unglaublich, also das ist völlig absurd. Also wir befinden uns nicht in so einem Markt, wo man sagen kann,

606
00:51:14,000 --> 00:51:20,000
das ist so, was könnten die Alternativen sein, sondern für mich ist schon der erste Punkt, es kann nicht sein,

607
00:51:20,000 --> 00:51:29,000
dass so viel Macht in der Hand ungewählter Firmen liegt, auch wenn die gar nicht nur schlecht sind und auch oft,

608
00:51:29,000 --> 00:51:38,000
das wird oft vergessen in Europa, dass es natürlich auch viele Länder gibt, in denen Social Media Firmen lange Zeit

609
00:51:38,000 --> 00:51:45,000
und auch immer noch Alternativen zu staatlicher Zensur sind. Also das ist komplexer, das sind nicht nur die Bösen,

610
00:51:45,000 --> 00:51:52,000
weil man will ja auch nicht, dass autoritäre Regime die Öffentlichkeit kontrollieren. Also es ist hochkomplex,

611
00:51:52,000 --> 00:51:58,000
aber so ganz grundsätzlich, wenn man nur, wenn ich jetzt einen Schritt zurückgehe und mir darüber nachdenken würde,

612
00:51:58,000 --> 00:52:04,000
in welcher Welt möchte ich leben? Ich möchte nicht in einer Welt leben, in der ich mich fragen muss, was Facebooks Außenpolitik ist

613
00:52:04,000 --> 00:52:14,000
und wie Facebook zur Taliban steht. Das ist einfach absurd. Und um zumindest dieser Macht etwas entgegenzusetzen,

614
00:52:14,000 --> 00:52:26,000
das Einzige, was funktioniert, sind börsennotierte Unternehmen, Regeln, Klagen, Regeln, Klagen und natürlich auch so ein bisschen

615
00:52:26,000 --> 00:52:36,000
veränderte Stimmung in der Gesellschaft, verändertes Nutzungsverhalten, aber ich glaube schon an Regulierung zu einem gewissen Grad.

616
00:52:36,000 --> 00:52:43,000
Das sind aber nicht, das sind nur die Spielregeln. Also Regulierung sind die Spielregeln der Digitalisierung.

617
00:52:43,000 --> 00:52:50,000
Und deswegen ist es unglaublich wichtig. Aber das ist nicht das Endziel, das ist nur so der erste Schritt, um überhaupt irgendwie

618
00:52:50,000 --> 00:52:56,000
die Möglichkeit für Alternativen zu schaffen. Weil wenn jedes Start-up davon träumt, von Google gekauft zu werden,

619
00:52:57,000 --> 00:53:00,000
dann werden wir auch keine Alternativen kriegen.

620
00:53:03,000 --> 00:53:04,000
Und von da aus?

621
00:53:04,000 --> 00:53:05,000
Wie bitte?

622
00:53:05,000 --> 00:53:16,000
Und von da aus? Also ich gehe mit, sozusagen kurzfristig, Regulierung sehr gerne und viel. Aber ich finde trotzdem noch die darüber hinausgehende Frage

623
00:53:16,000 --> 00:53:27,000
auch interessant. Also was für digitale Infrastrukturen bräuchten wir, jenseits der Frage, ob man jetzt irgendwie eine Marktkonkurrenz

624
00:53:27,000 --> 00:53:35,000
wieder herstellen kann, in Anführungsstrichen. Weil meine Vermutung wäre, dass auch das Herstellen einer mutmaßlichen Marktkonkurrenz

625
00:53:35,000 --> 00:53:44,000
trotzdem natürlich ja dann wieder Player zurücklassen würde, die am Ende über monetäre Incentivierung funktionieren

626
00:53:44,000 --> 00:53:56,000
und letztlich eigentlich auf einen Profitstreben hinaus ausgerichtet sind. Was ja wieder die Logik eigentlich gewaltsam sehr stark beschränkt oder begrenzt.

627
00:53:56,000 --> 00:54:08,000
Wir leben halt im Kapitalismus, ne? Wir leben halt im Kapitalismus. Also, for better or worse, da geht es dann um ganz Grundsätzliche.

628
00:54:08,000 --> 00:54:16,000
Grundsätzliche, klar. Sollte der Raum, und Raum ist ja schon schwierig. Was ist denn eigentlich, ich war so fasziniert von dem Vortrag, der eben war,

629
00:54:16,000 --> 00:54:25,000
wo es darum ging, da hatte eine Zuhörerin gefragt, ist denn WhatsApp Social Media? Das fand ich großartig, weil oft einfach diese Begriffe verendet werden.

630
00:54:25,000 --> 00:54:35,000
Was ist denn das überhaupt? Also was ist denn die Alternative, ich gehe so ein bisschen deiner Frage aus, aber über die Alternative von was sprechen wir denn?

631
00:54:35,000 --> 00:54:47,000
Also so Amazon, Facebook, das sind ja nicht Social Media Firmen. Das sind Technologiekonzerne, die Monopolien oder marktdominante Stellung in allen möglichen Bereichen haben.

632
00:54:47,000 --> 00:54:54,000
Und die öffentliche Diskussion, da geht es eigentlich immer fast nur um Social Media. Aber es ist ja völliger Quatsch, die ganze Infrastruktur dahinter,

633
00:54:54,000 --> 00:55:04,000
die Daten, die überall herkommen, die Datencenter, die Server, etc. Du merkst, ich bin so ein bisschen in Verlegenheit, weiß ich nicht, soll ich,

634
00:55:04,000 --> 00:55:09,000
es ist ja auch nicht mein, das ist was Kollektives. Wer bin ich, das ich jetzt sagen soll, wie das aussehen soll?

635
00:55:09,000 --> 00:55:16,000
Das Einzige, was ich sagen kann, ist, es gibt so ein paar Grundsätze und Prinzipien, die ich fände sinnvoll wären.

636
00:55:17,000 --> 00:55:27,000
Und ein Prinzip ist, es gibt eine Reihe von Fragen, die einfach, solange sie menschenrechtskonform sind, in unterschiedlichen Teilen der Welt eben anders beantwortet werden.

637
00:55:27,000 --> 00:55:40,000
Was finden wir akzeptabel, was empfinden wir als Hass, was sollte man nicht sagen dürfen, etc. Und das muss natürlich einer demokratischen Kontrolle unterliegen.

638
00:55:41,000 --> 00:55:54,000
Gut, ich lasse dich heraus. Aber am Ende, Frederike, stelle ich einer jeden und einem jeden immer noch die Frage, wenn du dir Zukunft vorstellst, was stimmt dich freudig?

639
00:55:54,000 --> 00:56:07,000
Ich muss wirklich sagen, diese Woche war ich, die ganzen 18 Monate waren wirklich bitter. Also die Pandemie war ganz schön hart, dann irgendwie auch noch politisch.

640
00:56:07,000 --> 00:56:18,000
Also wir leben schon in ziemlich harten Zeiten. Aber auf der anderen Seite merkt man dann halt auch, worum es geht und was wirklich wichtig ist.

641
00:56:18,000 --> 00:56:28,000
Ich bin einfach, das ist halt einfach nur meine Persönlichkeit, ich bin immer optimistisch, ich sehe immer die Möglichkeiten und was man noch ändern kann und was man machen kann.

642
00:56:28,000 --> 00:56:34,000
Sonst würde ich aber auch nicht zu dem Thema arbeiten, weil es wirklich zum Teil diffamierend ist.

643
00:56:34,000 --> 00:56:41,000
Also ich habe für eine Organisation gearbeitet, die Klagen eingereicht hat gegen Geheimdienste nach den Snowden-Enthüllungen.

644
00:56:41,000 --> 00:56:48,000
Und dann Jahre später haben sie Recht bekommen, aber währenddessen haben die Regierung einfach das Gesetz verändert.

645
00:56:48,000 --> 00:56:54,000
Also ja, es war illegal, aber jetzt ist es legal. Es ist schon so ein, ja, was gibt mir Hoffnung?

646
00:56:55,000 --> 00:57:02,000
Ich finde ganz aufregend die Fridays for Future Klimabewegung.

647
00:57:02,000 --> 00:57:07,000
Auch wenn es hier in unserem Gespräch geht es ja nicht um Klima, sondern um Technologien.

648
00:57:07,000 --> 00:57:15,000
Aber irgendwie habe ich das Gefühl, da kommt eine neue Generation, die viel fordernder ist, als ich das in dem Alter war.

649
00:57:15,000 --> 00:57:18,000
Und das gibt mir sehr viel Hoffnung.

650
00:57:19,000 --> 00:57:24,000
Und dann denke ich schon, dass sich in den letzten Jahren, es hat sich schon auch viel verbessert.

651
00:57:24,000 --> 00:57:29,000
Also es gibt viel mehr, wenn ich mit Politikern spreche, das Verständnis ist viel größer.

652
00:57:29,000 --> 00:57:32,000
Vor ein paar Jahren war das noch desaströs.

653
00:57:32,000 --> 00:57:36,000
Es gibt viel mehr Menschen in Entscheidungspositionen, die verstehen, worum es geht.

654
00:57:36,000 --> 00:57:43,000
Und es gibt auch ein größeres Verständnis dafür, dass der Status Quo so nicht richtig nachhaltig ist.

655
00:57:43,000 --> 00:57:47,000
Und das sind kleine Fortschritte und die geben mir Hoffnung.

656
00:57:47,000 --> 00:57:51,000
Wunderbar, dann sollten wir alle viel mehr fordern.

657
00:57:51,000 --> 00:57:54,000
Und ich danke dir, Friederike, für dieses Gespräch.

658
00:58:06,000 --> 00:58:09,000
Euch auch danke fürs Zuhören und Dasein.

659
00:58:09,000 --> 00:58:14,000
Das war Future Histories für heute.

660
00:58:14,000 --> 00:58:21,000
Vielen Dank fürs Zuhören, Show-Notizen und vieles mehr findet ihr auf www.futurehistories.today.

661
00:58:21,000 --> 00:58:27,000
Diskutiert mit auf Twitter unter dem Hashtag Future Histories oder im eigenen Subreddit.

662
00:58:27,000 --> 00:58:33,000
Ihr könnt Future Histories nicht nur auf allen großen Podcast-Plattformen hören und abonnieren,

663
00:58:33,000 --> 00:58:41,000
sondern auch auf YouTube, wo ihr neben den Episoden dann auch Kurzvideos zu Kernbegriffen einzelner Episoden findet.

664
00:58:41,000 --> 00:58:45,000
Schreibt mir gerne unter jan at futurehistories.today.

665
00:58:45,000 --> 00:58:50,000
Ich freue mich immer sehr über interessante Rückmeldungen und Hinweise.

666
00:58:50,000 --> 00:58:57,000
Wenn ihr Future Histories unterstützen wollt, dann könnt ihr das auf patreon.com-futurehistories

667
00:58:57,000 --> 00:59:00,000
oder auch via Spende auf unserer Homepage.

668
00:59:00,000 --> 00:59:06,000
Future Histories ist eine Produktion von MetaLapses, zu finden auf meta-lapses.net.

669
00:59:06,000 --> 00:59:09,000
Bis zum nächsten Mal. Ich freue mich.

