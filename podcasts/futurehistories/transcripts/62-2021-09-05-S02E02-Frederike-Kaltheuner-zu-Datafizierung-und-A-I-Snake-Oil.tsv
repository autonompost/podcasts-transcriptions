start	end	text
0	6000	Herzlich willkommen bei Future Histories, dem Podcast zur Erweiterung unserer Vorstellung von Zukunft.
6000	12000	Mein Name ist Jan Groß und ihr hört heute die zweite Future Histories Live-Episode,
12000	15000	die ich kürzlich in Hamburg aufgezeichnet habe.
15000	19000	In diesem Fall, da war ich beim Mind the Progress-Kongress eingeladen
19000	23000	und hatte die Freude, mit Friederike Kaltheuner als Gast zu sprechen.
23000	31000	Friederike, die forscht, schreibt und denkt zu Fragen der Datafizierung und ist Expertin im Bereich Tech Policy.
31000	37000	Sie hat unter anderem das Corporate Exploitation Program bei Privacy International geleitet,
37000	40000	war Tech Policy Fellow bei der Mozilla Foundation.
40000	46000	Gemeinsam mit Nele Obermüller hat sie auch ein Buch geschrieben mit dem Titel Datengerechtigkeit.
46000	50000	Vielen Dank an dieser Stelle hier an die OrganisatorInnen des Kongresses für die Einladung
50000	55000	und vor allem auch an Sophia für diese wirklich ausgezeichnete Organisation und Betreuung.
55000	60000	1.000 Dank. Bevor wir jetzt aber zum Gespräch mit Friederike kommen,
60000	63000	hatte ich ja in der vergangenen Episode angekündigt,
63000	68000	noch ein paar Worte zur gerade begonnenen zweiten Staffel von Future Histories zu sagen,
68000	72000	ob sich da was geändert hat im Vergleich zur Staffel 1 und wenn ja, was.
72000	78000	Falls euch diese Reflektionen über den Verlauf des Podcasts Future Histories nicht so sehr interessieren
78000	83000	und ihr eigentlich nur Friederike hören wollt, dann nehme ich euch das nicht im geringsten Übel,
83000	87000	sondern habe genau dafür einen Kapitelmarker gesetzt.
87000	92000	Wenn eure App das unterstützt, dann könnt ihr da also jetzt vorspringen.
92000	97000	Für alle, die es sehr wohl interessiert, willkommen bei diesem kurzen Einschub.
97000	100000	Also vielleicht zunächst mal für euch zur Einordnung.
100000	105000	Ich hatte damals, als ich Future Histories vor jetzt über zwei Jahren gestartet habe,
105000	109000	ja eher so eine vage Idee davon, wie ich das ungefähr machen will.
109000	113000	Und der Aufbau in Staffeln, der schien mir damals einfach sinnvoll,
113000	118000	um zum einen eben die Möglichkeit einzubauen, dass man auch mal eine Pause machen kann.
118000	123000	Und zum anderen wollte ich auch nicht, dass die Themen, mit denen ich damals gestartet bin,
123000	125000	auf ewig in Stein gemeißelt sind.
125000	131000	Eine neue Staffel, die bietet also auch die Möglichkeit, thematisch da Neuausrichtungen vorzunehmen.
131000	135000	Und das ist im Fall von Future Histories nicht als harter Bruch zu verstehen,
135000	142000	sondern eher als eine stetige Entwicklung der forschenden Suchbewegung dieses Podcasts hier.
142000	145000	Es wird also auch in der zweiten Staffel drei Überthemen geben,
145000	150000	aber zum einen werden sie auch weiterhin eher als grobe Orientierung dienen
150000	154000	und eine Offenheit zulassen, die ich als essentiell empfinde.
154000	159000	Und zum anderen leiten sie sich auch unmittelbar aus den unterschiedlichen Auseinandersetzungen,
159000	162000	den Pferden und den Entwicklungen der ersten Staffel ab.
162000	166000	Aus dem Thema Homo economicus, da hat sich zum Beispiel,
166000	168000	ich hatte das an anderer Stelle schon erwähnt,
168000	173000	über den Verlauf der ersten Staffel hinweg die Frage entwickelt,
173000	177000	wie alternative Systeme politischer Ökonomie denn aussehen könnten.
177000	182000	Politische Ökonomien der Zukunft wird also ein Überthema der zweiten Staffel sein.
182000	187000	Ich bin ja absolut der Meinung, dass es nicht reicht, in der Kritik zu verharren,
187000	192000	sondern dass es zwingend notwendig ist, konstruktive und produktive Vorschläge zu machen,
192000	199000	wie man es denn anders machen könnte, gerade auch auf einer Makroebene wie der der politischen Ökonomie.
199000	205000	Denn diese Makroebene, die es absolut unterversorgt, wenn es um Alternativen geht, finde ich,
205000	212000	das gilt es zu ändern und ich möchte dabei zum einen bestimmte Stränge der ersten Staffel weiterverfolgen,
212000	215000	die sich als fruchtbare Pferden herausgestellt haben.
215000	219000	Konkret ist, dass die Auseinandersetzung mit dem Themenkomplex,
219000	225000	den ich mittlerweile unter dem spielerisch provokanten Begriffspaar Freie Planwirtschaft zusammenfasse,
225000	230000	da ist noch vieles offen und ich habe sehr, sehr viele Fragen an diesen Themenkomplex,
230000	233000	die definitiv noch umgeklärt sind.
233000	240000	Und daran anschließend gibt es auch eine andere Fragestellung oder einen thematischen Ansatz
240000	246000	zu diesem Komplex zukünftiger politischer Ökonomien, einen anderen Ansatz,
246000	249000	den ich ebenfalls in Staffel 1 wiederholt auch aufgegriffen habe,
249000	254000	der aber auch absolut überhaupt nicht abschließend geklärt ist in irgendeiner Form.
254000	260000	Und das ist die Frage nach alternativen Regierungskünsten, nach alternativer Governamentalität,
260000	264000	wie es Frieda Vogelmann in Episode 11 nennt.
264000	268000	Das sind also so zwei Aspekte, die ich aus der ersten Staffel mitnehmen möchte,
268000	275000	aber ich will natürlich auch neue Pfade eröffnen, die für politische Ökonomien der Zukunft von Bedeutung sind.
275000	279000	Das Ganze wird also explorativ sein, wie man so schön sagt.
279000	281000	Und ich freue mich auch immer sehr über Hinweise.
281000	290000	Also solltet ihr in dieser Richtung irgendwie gute Ansätze kennen, Zugänge, Bücher, TheoretikerInnen und so weiter,
290000	295000	gerne immer mir schreiben unter jan-at-future-histories.today.
295000	300000	Ein wichtiger Bereich, den es für diesen Themenblock in jedem Fall zu erarbeiten gilt,
300000	309000	ist das Blicken über den Tellerrand, um nicht in so einer leider ja immer wieder sehr eurozentristischen Debatte eigentlich stecken zu bleiben.
309000	315000	Ein wichtiger Aspekt wird also sein zu fragen, wie wird denn die Frage nach alternativen politischen Ökonomien
315000	318000	in anderen Teilen der Welt gestellt und verhandelt.
319000	326000	Der zweite Themenblock, der ergibt sich im Grunde unmittelbar aus dem ersten und widmet sich der Frage der Transformation.
326000	330000	Ich finde es unglaublich wichtig, diese Ebenen miteinander zu verbinden,
330000	336000	dass man sich also auf der einen Seite wieder traut, auch über Alternativen auf der Makroebene nachzudenken,
336000	340000	dass aber gleichzeitig auch eine plausible Idee davon entwickelt wird,
340000	344000	wie diese alternativen politischen Ökonomien denn in die Welt gebracht werden könnten,
344000	348000	wie eine Transformation also ganz konkret aussehen könnte.
348000	355000	Nicht das Blaupause, also bitte nicht falsch verstehen, nicht als ein Prozess mit einem finalen Ziel oder sowas,
355000	361000	sondern als ein wechselseitiger Prozess zwischen gelebter alternativer Praxis im Hier und Jetzt
361000	368000	und auch Projekten, die quasi an die Substanz dessen gehen, was es neu zu verhandeln gilt
368000	372000	und im Entwickeln alternativer Zukunft auch jenseits der Nische.
372000	379000	Ich glaube, dass das sich überhaupt nicht irgendwie widerspricht, sondern im Gegenteil einfach ganz fantastisch gegenseitig befruchten kann.
379000	385000	Die Frage der Transformation ist ein sehr weites Feld und auch eine harte Nuss, finde ich.
385000	393000	Da wird es also einiges zu besprechen geben und ich freue mich schon auf viele, viele spannende Episoden zu diesem Thema.
393000	397000	Das dritte Themenfeld, das uns in der zweiten Staffel begleiten wird,
397000	405000	das greift den technopolitischen Strang von Future Histories auf und verdichtet ihn entlang eines Forschungsprojektes,
405000	408000	das ich gemeinsam mit Robert Seifert verfolge.
408000	413000	Das Thema lautet das Regieren der Algorithmen und das freut mich ganz besonders,
413000	418000	nicht nur, weil ich Robert Seifert sehr schätze und das Thema auch unglaublich gut in Future Histories passt,
418000	424000	sondern auch, weil es bedeutet, dass hier zwei Sphären noch enger zueinander finden,
424000	429000	die für mich persönlich einfach von großer Bedeutung sind, nämlich Future Histories als Projekt
429000	437000	und die Forschung im Rahmen meiner Dissertation zu sozio-technischen Imaginationen algorithmischer Regierungskunst.
437000	442000	Diese Dissertation, die ist nämlich Teil des Forschungsprojektes, das Regieren der Algorithmen.
442000	450000	Und weil ich ebenfalls daran arbeite, Podcasten als Teil der erweiterten Forschungspraxis zu etablieren und zu plausibilisieren,
450000	456000	passt das alles natürlich ganz großartig zusammen und freut mich wirklich sehr.
456000	461000	Worum wird es also im Themenstrang das Regieren der Algorithmen gehen?
461000	466000	Das Forschungsprojekt, das trägt den Untertitel eine Soziologie algorithmischer Regierungskunst
466000	475000	und steht somit in ausgezeichneter Korrespondenz zur in Themenblock 1 bereits erwähnten Frage nach alternativen Regierungskünsten.
475000	484000	Der Begriff des Regierens, der ist dabei angelehnt an Foucault weit gefasst und geht über eine rein politische Definition hinaus.
484000	490000	Regieren bezieht sich hier ganz allgemein auf soziale Felder, Technologien und individuelle Handlungsformen,
490000	495000	die der Selbst- und Fremdführung von Menschen dienen und Regierungskünste,
495000	500000	die stellen dabei dann das reflektierte Nachdenken über die beste Form des Regierens dar.
500000	509000	Algorithmische Regierungskünste tun dies wiederum unter Einbezug algorithmischer Verfahren als maßgebliche Elemente des Regierens.
509000	515000	Und wir führen im Namen des Forschungsprojektes das Regieren der Algorithmen ja eine Kecke Zweideutigkeit mit,
515000	523000	denn der Name das Regieren der Algorithmen, das kann sich ja beziehen sowohl auf das Regieren von Algorithmen,
523000	532000	also im Sinne des Regulierens von Algorithmen, als auch auf soziotechnische Imagination des Regierens mit algorithmischen Technologien.
532000	540000	Siehe mein Promotionsprojekt. Das heißt, wir kommen da aus verschiedensten Richtungen, was ich als sehr produktiv empfinde.
540000	547000	Mehr dazu gibt es dann in einer kommenden Episode mit Robert Seifert als Gast. Ich freue mich schon sehr.
548000	553000	Was mich auch sehr freut und das wollte ich auch schon längst ganz fröhlich verkünden,
553000	558000	ist, dass Future Histories eine Förderung der Wiener Medieninitiative erhalten hat,
558000	565000	um das Projekt etwas auszubauen und auch auf solidere Beine stellen zu können, sage ich mal.
565000	570000	Daher stammt zum Beispiel auch der Umstand, dass jetzt zumindest mal für die nächsten 20 Episoden
570000	575000	jede zweite Woche ein Future Histories Kurzvideo veröffentlicht wird auf YouTube.
575000	583000	Gerne auch übrigens den Kanal abonnieren. Das ist definitiv auch eine Hilfe im Ausbau dieses Medienstranges, sage ich mal.
583000	588000	Es gibt jetzt auch ein kleines Studio in einem Gemeindebau direkt beim Wiener Prater
588000	594000	und die Förderung ermöglicht auch, dass ich endlich nicht mehr gezwungen bin, alles alleine zu machen
594000	597000	und mich an den Rand der Überlastung zu treiben.
597000	601000	Denn ich werde jetzt hier bei Future Histories von Clara unterstützt.
601000	608000	Clara, vielen Dank für alles. Ich freue mich total, dass du mitmachst und dass du mich in all diesen Sachen unterstützt.
608000	610000	Ich freue mich auch. Hallo.
610000	615000	Die Förderung der Wiener Medieninitiative bewegt sich im Bereich dessen, was man gemeinhin
615000	619000	wahrscheinlich Kreativwirtschaft nennen würde und so ist es integraler Bestandteil,
619000	624000	dass ich mich im Zuge dessen mit einer Neugründung selbstständig gemacht habe.
624000	628000	Die Firma, die daraus entsprungen ist, die trägt den Namen MetaLapsis
628000	634000	und ihr findet sie unter www.metalapsis.net mit einer sehr gelungenen Homepage vertreten,
634000	639000	für deren Gestaltung und Umsetzung ich Thomas, Daniel und Leon danken möchte.
639000	645000	Und wo wir schon bei Webseiten sind, es gibt jetzt auch Infos zu meiner Person gebündelt unter
645000	652000	jan-groß.de, geschrieben Gustav Richard Otto Otto Siegfried, also mit zwei O und einem normalen S.
652000	658000	Auf eine Art fand ich es zwar auch ganz schick, muss ich sagen, dass es bisher nur so verstreute
658000	665000	Informationen über mich gab im Netz, aber nun hat dann doch der pragmatische Zugang gewonnen.
665000	672000	So, jetzt genug Eigenwerbung. Ich freue mich riesig auf die zweite Staffel, möchte ich noch mal sagen.
672000	678000	Und mir verbleibt noch Marianne, Andrea und Nausi K.A. ganz herzlich in der Gemeinschaft
678000	684000	der Patreon-UnterstützerInnen zu begrüßen. Und ich danke Fabian, Carmen, Rudolf und Wilfried
684000	690000	für ihre Spenden und wünsche euch jetzt viel Freude mit einer neuen Episode Future Histories Live,
690000	693000	diesmal mit Friederike Kalt-Heuner.
701000	706000	Ja, herzlich willkommen auch von meiner Seite. Ich freue mich sehr, dass wir heute hier sein dürfen.
706000	708000	Herzlich willkommen, Friederike.
708000	710000	Danke für die Einladung.
710000	716000	Ich freue mich sehr. Friederike, ich mag gerne mit einer ganz basic Frage einsteigen, die vielleicht
716000	722000	gar nicht so einfach ist, wie sie im ersten Moment scheint. Denn gerade im Zusammenhang mit Daten,
722000	727000	da scheint mir allzu oft zu einer gewissen Naturalisierung am Werke. Man sieht es an Framings,
727000	734000	wie Daten sind, das neue Öl oder auch der Begriff Raw Data, Rotdaten. Also zunächst mal vorab.
734000	737000	Was sind Daten und wie entstehen sie?
739000	744000	Vielleicht ist es am einfachsten, mit einer ganz einfachen Aussage anzufangen.
744000	752000	Wir leben in einer Welt, in der es noch nie so einfach war, so viel über jeden von uns zu wissen.
752000	758000	Und das hat natürlich auch was mit Daten zu tun. Also der Begriff Datafizierung heißt,
758000	768000	die Umwandlung von Räumen, Verhalten, Informationen in Daten. Wenn ich einen Vortrag zu dem Thema halte,
768000	773000	habe ich eine Lieblingsfolie, die ich eigentlich fast immer verwende. Und auf dem Bild sieht man eine,
773000	781000	das ist ein Foto von einer Stasi-Akte. Und die zeigt so ein bisschen, wie auf der einen Seite,
781000	787000	wie arbeitsaufwendig es früher war, Informationen herauszufinden und wie trivial die Informationen
787000	793000	zum Teil auch waren. Also da steht dann in der Akte, welches Buch liest jemand, wer sind so die Freunde,
793000	800000	wo verbringen die so ihren Nachmittag. Und was Datafizierung jetzt ganz grob runtergebrochen heißt,
800000	804000	ist, dass wir jetzt in einer Welt leben, in all diese Dinge, die früher sehr aufwendig
804000	810000	und kostenspielig waren, herauszufinden. Diese Dinge werden automatisch über uns aufgezeichnet
810000	817000	in Form von Daten. Und wenn wir über Daten sprechen, zum öffentlichen Diskurs, geht es eigentlich meistens
817000	821000	um die Daten, die wir mehr oder weniger bewusst teilen. Also die Bilder, die wir teilen,
821000	827000	was wir ins Internet stellen. Aber das ist eigentlich nur die Spitze des Eisbergs. Der viel größere Teil,
827000	831000	der den meisten Menschen, glaube ich, gar nicht so bewusst ist, sind die Daten, die automatisch
831000	838000	über uns aufgezeichnet werden. Unser Bewegungsmuster, nicht nur was wir kaufen, sondern wie sich unsere Maus bewegt,
838000	843000	wo wir vielleicht zögern, was uns vielleicht vorher noch interessiert hat. Und dann die dritte Kategorie,
843000	848000	die mich, ich glaube, das ist vielleicht eigentlich auch der Einstieg für mich persönlich zu dem Thema.
848000	855000	Die dritte Kategorie sind all die Rückschlüsse, die man aus diesen Mustern ableiten kann.
855000	861000	Also aus meinem Bewegungsmuster, aus meinem Netflix-Verhalten kann man sehr viel ablesen,
861000	867000	zum Beispiel, ob jemand depressiv ist, ob jemand jetzt das Haus nicht verlassen hat.
867000	872000	Man kann anhand meiner Telefondaten sehen, bin ich eher jemand, die sofort zurückruft?
872000	879000	Oder muss ich angerufen werden? Und wen rufe ich an? Und das ist eigentlich das, was mich interessiert.
879000	886000	Weil auf der einen Seite ist es so, wenn diese Vorhersagen stimmen, sind sie unheimlich genau
886000	892000	und können Dinge aussagen über uns, die wir vielleicht selber gar nicht wissen oder nicht wahrhaben wollen.
892000	897000	Gleichzeitig ist es aber auch so, dass diese Muster und Vorhersagen oft völliger Quatsch sind.
897000	901000	Und man sieht das ganz gut anhand zum Beispiel der Online-Werbung, die ich sehe.
901000	907000	Da gibt es manchmal Dinge, die sind unheimlich fast hellseherisch genau.
907000	911000	Und dann wiederum gibt es einfach Quatsch.
911000	916000	Also Facebook hat jetzt über 12, 13 Jahre Daten von mir und denkt immer noch, ich wäre ein Mann.
916000	919000	Und dieses Spannungsfeld ist das, was mich interessiert.
919000	922000	Also das Spannungsfeld zwischen Orwell und Kafka.
922000	925000	Auf der einen Seite sind wir sehr lesbar geworden.
925000	930000	Auf der anderen Seite ist das, was gelesen wird, auch oft völlig falsch.
930000	935000	Und manchmal ist es einfach nur irritierend, dass es falsch ist.
935000	938000	Manchmal geht es aber auch um etwas ganz anderes.
938000	940000	Manchmal werden wir in Kategorien gepackt.
940000	944000	Manchmal werden wir falsch einsortiert auf Arten und Weisen, die diskriminierend sind,
944000	948000	die bestehende diskriminierende Strukturen reproduzieren.
948000	956000	Und ich glaube, neben Klimawandel, Ungerechtigkeit, wachsender Ungleichheit,
956000	960000	ist eben einer der wichtigsten und dringendsten Themen unserer Zeit,
960000	963000	wie wir die Spielregeln dieser neuen Welt gestalten,
963000	968000	die in ihren vollen Konsequenzen immer noch nicht viele Leute wirklich verstehen.
968000	974000	Also wir leben einer radikal anderen Welt, als wir 2001 gelebt haben.
974000	976000	Und es liegt an uns, diese Welt zu gestalten.
976000	978000	Und deswegen finde ich das Thema Zukunft so spannend,
978000	981000	weil im Deutschen das Wort Digitalisierung suggeriert manchmal,
981000	985000	als wäre das ein Prozess, der uns passiert, der über uns kommt.
985000	988000	Dabei ist es eigentlich andersrum. Es liegt an uns, ihn zu gestalten.
988000	991000	Alles, was jetzt ist, hätte auch anders sein können.
991000	996000	Es ist viel freier, viel offener.
996000	1001000	Und was mich motiviert oder meine persönliche Motivation ist,
1001000	1006000	ich möchte eben mehr Menschen dazu bewegen und auch selber Teil dieser Gestaltung sein
1006000	1009000	und das nicht einfach nur mich überkommen lassen.
1009000	1013000	Da hast du jetzt verschiedene Sachen angesprochen,
1013000	1018000	unter anderem auch dieses Spannungsfeld zwischen Genauigkeit und danebenliegen.
1018000	1021000	Ich finde das auch unter einem anderen Aspekt interessant,
1021000	1026000	weil ja auch auf Seiten der Kritikerinnen und Kritiker dieser Technologien
1026000	1030000	das mitunter dann fast schon unbeabsichtigt scheint mir irgendwie passiert,
1030000	1035000	dass die an den Mythen dieser Technologien eigentlich mitarbeiten,
1035000	1041000	indem sie, ich glaube fast aus Versehen sozusagen,
1041000	1045000	diese Erzählungen, diese Narrative für bare Münze nehmen
1045000	1050000	und dann quasi im Aufschrei dagegen, wie genau man uns denn jetzt sehen könne,
1050000	1053000	wie genau man jetzt Zukunft voraussehen könne und so weiter,
1053000	1057000	eigentlich auf eine Art eben diese Mythologien auch mit verfestigen.
1057000	1062000	Wie versuchst du das zu umschiffen? Wie gehst du mit dieser Problematik um?
1064000	1069000	Kritik ist ja erstmal grundsätzlich positiv, aber es gibt natürlich Kritik und Kritik.
1069000	1075000	Man sieht das immer anhand dieses Beispiels oder zwei Beispiele finde ich hier besonders gut.
1075000	1079000	Das eine ist künstliche Intelligenz. Natürlich muss man das kritisch sehen,
1079000	1083000	aber Kritik heißt nicht, dass wir jetzt Angst vor Terminator haben müssen
1083000	1088000	oder dass KI jetzt bald alles, wie sagt man das auf Deutsch,
1088000	1092000	also dass KI jetzt plötzlich unser ganzes Leben entscheiden wird oder bestimmen wird.
1092000	1096000	Das ist die falsche Sorge, sondern die eigentliche Sorge ist,
1096000	1102000	dass KI oft nicht funktioniert, oft richtig schlampig gebaut wird,
1102000	1109000	Vorurteile, Diskriminierung reproduziert, also die Gefahr ist oft viel subtiler als man das denkt.
1110000	1113000	Und dann die eigentliche Gefahr oder wir müssen gar nicht über Gefahren reden,
1113000	1118000	aber ich glaube der Bereich, der am meisten kritisiert werden muss,
1118000	1123000	ist die Tatsache, dass es ganz viele Fragen gibt, die wir nicht beantworten können,
1123000	1129000	die nur eine ganz kleine Anzahl an Firmen sind überhaupt in der Lage diese Fragen zu beantworten.
1129000	1134000	Ein Beispiel ist sowas Radikalisierung im Netz von Empfehlungsalgorithmen.
1134000	1137000	Die Studien dazu sind extrem schwierig durchzuführen,
1137000	1141000	weil personalisierte Algorithmen eben für jeden anders aussehen.
1141000	1147000	Alleine schon ist es sehr schwierig, auch kollektiv zum Beispiel über Online-Werbung zu sprechen.
1147000	1151000	Meine Online-Werbung sieht anders aus als deine, sieht anders aus als ihre.
1151000	1157000	Und da gibt es oft Kritik in der Öffentlichkeit, sowas wie das Internet macht uns dumm,
1157000	1162000	oder das Internet macht, oder soziale Medien machen X.
1162000	1166000	Und ich finde diese Art von Kritik schwierig, weil die Frage ist ja,
1166000	1169000	was ist denn überhaupt Social Media und was sollte es sein?
1169000	1176000	Indem wir so monokausale Kritik üben, sehen wir die eigentliche Technik Social Media
1176000	1178000	als etwas viel starreres als es eigentlich ist.
1178000	1183000	Wir meinen damit eine Bandbreite an Produkten, die sich permanent verändern,
1183000	1186000	die selbst vor fünf Jahren anders aussahen als jetzt.
1186000	1189000	Das heißt, diese kausalen Aussagen sind schwierig.
1190000	1193000	Also im Kern geht es für mich um zwei Sachen.
1193000	1199000	Die eine Sache ist, es ist nicht nur, dass wir in einer Welt leben, in der man mehr über uns weiß,
1199000	1203000	sondern der öffentliche Raum, und das ist ja nicht nur Raum, in dem wir sprechen,
1203000	1207000	das ist auch der Raum, in dem sich soziale Bewegungen formen,
1207000	1210000	in dem wir über die Welt lernen, jetzt diese Woche Afghanistan.
1210000	1215000	Das liest man auf Twitter mit und das lesen auch Journalisten auf Twitter mit.
1215000	1222000	Und es ist schon sehr erstaunlich, dass diese Räume von einer Handvoll an Firmen kontrolliert und gesteuert werden.
1222000	1225000	Das ist historisch schon extrem einmalig.
1225000	1229000	Das ist eine Art von Macht, die diese Firmen haben, die wir, ich glaube,
1229000	1232000	die wir im vollen Ausmaß noch nicht richtig verstanden haben.
1232000	1239000	Und die andere Sache, die ich so interessant finde, ist die Informationsungleichheit.
1239000	1242000	Auf der einen Seite kann man sehr viel über uns wissen,
1242000	1248000	auf der anderen Seite ist es sehr schwer überhaupt zu verstehen, wie zum Beispiel ein Handy funktioniert.
1248000	1250000	Was passiert eigentlich auf dem Handy?
1250000	1255000	Oder sehr wenig Menschen wissen eigentlich, hört mein Handy mir zu, wie wird eigentlich Werbung personalisiert?
1255000	1260000	Und das kreiert eine Informationsasymmetrie, die natürlich auch ein Machtgefälle ist.
1260000	1264000	Also das sind für mich diese Fragen um Macht und Gerechtigkeit.
1264000	1268000	Das sind eigentlich die Kernfragen oder das ist da, wo die Kritik ansetzen muss.
1269000	1275000	Es gibt so eine Demarkationslinie, die ich noch nie so recht habe begreifen können.
1275000	1279000	Vielleicht kannst du mir da ein bisschen weiterhelfen und das ist die Wahlwerbung.
1279000	1285000	Also die Mechanismen, die Tools, die eingesetzt werden in dieser Wahlwerbung,
1285000	1291000	das sind Tools, die sind im normalen Online-Werbegeschäft total frei verfügbar für jeden.
1291000	1296000	Also du kannst quasi diese Lookalikes oder sowas, das ist ganz normal,
1296000	1300000	wenn du eine Facebook-Werbung schaltest, dass du das dann auswählen kannst.
1300000	1305000	Wenn das aber wiederum dann auf den politischen Bereich übertragen wird,
1305000	1308000	dann gibt es immer plötzlich so einen Aufschrei.
1308000	1313000	Dadurch können jetzt irgendwie in einer Art und Weise die öffentliche Meinung manipuliert werden,
1313000	1316000	wie das vorher noch nicht dagewesen sei.
1316000	1321000	Es gäbe jetzt Microtargeting und das würde quasi diesen demokratischen Willensbildungsprozess
1321000	1325000	in einer Art und Weise beeinflussen, die jetzt aber wirklich nicht mehr zulässig wäre.
1325000	1329000	Und ich frage mich immer, wenn ich das höre, ohne jetzt sagen zu wollen,
1329000	1332000	dass das eine komplett unberechtigte Kritik wäre oder sowas,
1332000	1335000	aber ich habe noch nie begriffen, wie man diese Linie zieht.
1335000	1340000	Also was ist quasi, was ist irgendwie okay?
1340000	1343000	Früher gab es auch immer schon Medienkonglomerate,
1343000	1348000	die hatten halt dann irgendwie eine Mehrheit der Tageszeitungen in Besitz
1348000	1354000	und hatten dadurch eine gewisse Medienherrschaft, wenn man so will.
1354000	1363000	Warum wird diese Demarkationslinie so gezogen in Bezug auf zum Beispiel Online-Targeting bei politischen Wahlkämpfen?
1363000	1366000	Erstmal danke für die Frage. Das Thema ist mir extrem wichtig.
1366000	1372000	Ich arbeite gerade für die EU-Kommission zu der Regulierung von politischer Online-Werbung.
1372000	1375000	Man muss unterschiedliche Sachen unterscheiden.
1375000	1380000	Es wurde ganz viel über Online-Werbung diskutiert im Kontext des Brexit-Referendums.
1380000	1382000	Und ich habe jetzt jahrelang in England gelebt.
1382000	1389000	Und es ist natürlich nicht so, dass Anti-EU, Anti-Einwanderer-Rechte, Populismus nur online stattfindet.
1389000	1391000	Man muss nur in einen Kiosk gehen.
1391000	1395000	Und die Tabloids sind voll von Missinformationen.
1395000	1400000	Missinformationen und Lügen. Also das ist kein inhärentes Online-Problem.
1400000	1404000	Was aber spannend ist oder was auch eine wirkliche Gefahr ist,
1404000	1409000	und das ist so ein Phänomen, was man oft auch in anderen Bereichen sieht, zum Beispiel in der Polizeiarbeit.
1409000	1413000	Die Regeln, die offline im Wahlkampf gelten, gelten nicht online.
1413000	1417000	Beispielsweise, wenn ich jetzt hier ein Wahlplakat aufhänge,
1417000	1421000	muss ich deklarieren, von wem das kommt und wer dafür bezahlt hat.
1421000	1426000	Wenn ich, ich kann nicht einfach als Milliardär, der ich nicht bin,
1426000	1431000	im deutschen Fernsehen jetzt einfach alle Werbeplätze kaufen
1431000	1434000	und nur noch Werbung für Friederike als Kanzlerin machen.
1434000	1436000	Das ist einfach nicht erlaubt.
1436000	1440000	Weil es da Kontingente gibt, es gibt Plätze, das ist alles sehr streng reguliert.
1440000	1444000	Oder man merkt ja auch jetzt im Bundeswahlkampf in Deutschland,
1444000	1446000	es hängen eigentlich erst jetzt Plakate.
1446000	1448000	Es waren nicht schon seit Monaten Plakate.
1448000	1453000	Der Unterschied ist, dass online die Regeln einfach nicht greifen.
1453000	1455000	Aus ganz unterschiedlichen Gründen.
1455000	1461000	Was dann dazu führt, dass eben Parteien, und das ist in den USA passiert,
1461000	1467000	dass zum Beispiel die Trump-Kampagne Millionen verschiedene Versionen
1467000	1469000	von Werbung ausgestrahlt hat.
1469000	1471000	Das ist noch ein weiterer Unterschied.
1471000	1475000	Also offline, was auch immer ich auf Wahlplakaten behaupte,
1475000	1478000	wen ich beleidige, wie radikal auch immer das ist,
1478000	1480000	es hängt in der Öffentlichkeit.
1480000	1484000	Und es kann damit gesehen und im öffentlichen Diskurs diskutiert werden.
1484000	1490000	Der Unterschied zur Online-Werbung ist, weil die Online-Werbung eben personalisiert ist,
1490000	1494000	ich kann eine Million Versionen derselben Werbung schalten
1494000	1497000	und im Endeffekt jedem was anderes erzählen.
1497000	1501000	Ich kann den einen Wählern das versprechen, den anderen Wählern das versprechen.
1501000	1505000	Ich kann Gruppen gegeneinander aufhetzen,
1505000	1509000	ohne dass man überhaupt etwas in der Öffentlichkeit davon mitkriegt,
1509000	1513000	weil es eben so vergänglich ist und so schnelllebig ist.
1513000	1516000	Die Werbung wird einen Tag geschaltet und dann wird sie nicht mehr geschaltet.
1516000	1519000	Und vielleicht noch der letzte Grund ist,
1519000	1522000	man kann durch, und das wurde auch in den USA gemacht,
1522000	1525000	man kann durch Online-Werbung ganz gezielt auch testen,
1525000	1529000	was so die Stimmung ist, worauf die Leute anspringen.
1529000	1533000	Und Werbung wird ja nicht nur geschaltet, sondern das wird dann optimiert
1533000	1537000	und das wird dann sehr genau gezielt an bestimmte Gruppen ausgeschaltet.
1537000	1541000	Was ich damit sagen will, in der öffentlichen Debatte
1541000	1545000	wurde das oft verkürzt auf politische Werbung ist eine Gefahr,
1545000	1549000	weil die Menschen manipuliert, die dann anders wählen als sie sonst wählen würden.
1549000	1553000	Das ist natürlich zu kurz gegriffen, das Problem ist viel komplexer
1553000	1556000	und das zeigt so ein bisschen auch, wenn man über Polizeiarbeit
1556000	1559000	oder vorausschauende Polizeiarbeit spricht, das Problem ist häufig,
1559000	1563000	dass dieselben Regeln, die offline gelten, nicht online gelten.
1563000	1567000	Und das quote-unquote online, das ist ja auch verschmolzen,
1567000	1570000	dass plötzlich Dinge möglich sind, die so nicht möglich sind.
1570000	1574000	Es ist offline nicht möglich, jedem einen anderen Wahlspruch zu zeigen.
1574000	1580000	Das heißt, es bedarf irgendwie Regeln, es bedarf Intervention
1580000	1584000	und so wie es momentan aussieht, machen das die Plattformen eben freiwillig
1584000	1588000	und sie machen das auch nicht konsequent und auch nicht immer sehr zuverlässig.
1588000	1591000	Und deswegen muss es halt Regeln geben, die das klar definieren.
1591000	1594000	Auch wenn zum Beispiel dann bei Facebook mal ein anderer Chef ist,
1594000	1597000	dass sich die Regeln dann nicht ändern.
1597000	1601000	Aber ich mag die Frage, um nochmal darauf zurückzugehen, was ich an der Frage mag,
1601000	1606000	ist, es stimmt, es gibt viel Digitalisierungskritik.
1606000	1610000	Allein die Frage, so Digitalisierung gut oder schlecht, ist die völlig falsche Frage,
1610000	1614000	sondern die Frage ist, wie, wie Digitalisierung, wie Technologien,
1614000	1618000	was wollen wir eigentlich kollektiv als Gesellschaft
1618000	1623000	und nicht das Empfinden als so eine unaufweichbare Bewegung.
1624000	1629000	Und bei dieser Online-Werbung, also zumindest, das würde mich vielleicht auch noch interessieren,
1629000	1636000	ich wüsste jetzt auch gar nicht, wie effektiv oder ob die, vielleicht eher so,
1636000	1641000	ob die Erzählung dessen, wie effektiv das angeblich gewesen sei,
1641000	1645000	wie sehr das mit der Realität übereinstimmt.
1645000	1649000	Und tendenziell neige ich dazu, dass ich immer so ein bisschen die Intuition habe,
1649000	1654000	dass es mir so scheint, als ob andere Aspekte, die eher sozioökonomische,
1654000	1659000	sozio-politische Aspekte sind, die zu verschiedenen Entwicklungen führen,
1659000	1662000	dass die unterschlagen werden, wenn man das so darstellt als,
1662000	1666000	eben als sei quasi Trump, also jetzt ich bin absichtlich polemisch,
1666000	1672000	Trump wäre gewählt worden, weil er über Social Media quasi die Leute gegeneinander aufgehetzt hat.
1672000	1676000	Also dafür muss es ja ein Fundament geben, das in dem Fall, glaube ich,
1676000	1679000	auch mit einer realen Not zum Beispiel zu tun hat,
1679000	1684000	weil man kann Leute in Not besser gegeneinander aufhetzen, zum Beispiel.
1684000	1688000	Die, sorry, jetzt habe ich meinen Faden verloren.
1688000	1694000	Ich habe immer noch was hinterhergeschoben, immer noch was hinterhergeschoben.
1694000	1700000	Es muss ja noch nicht passiert sein, das heißt aber nicht, dass es nicht doch auch noch passieren könnte.
1700000	1704000	Also wir wissen, was technisch möglich ist und das Beispiel, was ich immer gerne bringe, ist,
1704000	1708000	es ist belegt, es gab in den USA Abtreibungsgegner,
1708000	1715000	die abtreibungswillige Frauen im Klinikbesuch mit Antiabtreibungswerbung bombardiert haben.
1715000	1718000	Werden die deshalb jetzt nicht abtreiben?
1718000	1724000	Wahrscheinlich nicht, es ist aber eine ganz perfide Form von Belästigung.
1724000	1727000	Wahlen werden ja auch oft, es gibt immer mehr Wahlen,
1727000	1732000	die werden in bestimmten Wahlkreisen durch wenige tausend Stimmen getroffen.
1732000	1735000	Die Wahlerwerbung muss ja noch nicht mal sagen, wähl Trump,
1735000	1737000	sondern die Wahlerwerbung kann auch einfach sagen,
1737000	1740000	der ganze Politikbetrieb ist kaputt, geh doch gar nicht wählen.
1740000	1743000	Und das macht das halt alles so kompliziert.
1743000	1746000	Und ich glaube, das ist ein gutes Beispiel, weil das zeigt, wir wissen,
1746000	1752000	technisch ist es eben möglich, sehr spezifische Gruppen sehr spezifisch anzusprechen.
1752000	1755000	Keine Ahnung, wie effektiv das ist, aber das Problem ist,
1755000	1758000	niemand kann es messen, außer die Plattformen selber.
1758000	1761000	Aber hier geht es ja nicht um irgendwas, hier geht es um Demokratie.
1761000	1766000	Es ist eigentlich extrem beunruhigend, dass wir gar nicht mal richtig wissen,
1766000	1769000	welchen Einfluss Plattformen auf Wahlen hatten,
1769000	1772000	weil nur die Plattformen selber überhaupt die Daten dazu haben.
1772000	1774000	Das alleine für mich reicht schon aus.
1774000	1778000	Es muss nicht ausgenutzt werden, wir wissen, dass es ausgenutzt werden könnte.
1778000	1780000	Und das sollte nicht passieren.
1780000	1783000	Total, also da würde ich natürlich sofort zustimmen.
1783000	1787000	Wir sollten dafür sorgen, dass die Dinge eben in einer Art und Weise nachvollziehbar werden,
1787000	1789000	wie sie es bisher leider nicht sind.
1789000	1792000	Und das nährt ja am Ende dann eben auch diese Mythologien,
1792000	1796000	weil die Plattformen haben natürlich ein Eigeninteresse daran zu tun,
1796000	1800000	als wären ihre Werbung unglaublich effektiv und könnten extrem genau,
1800000	1803000	also detailliert targetten und so weiter und so fort.
1803000	1808000	Ich würde aber gerne jetzt dann noch zu einer anderen Demystifisierung übergehen,
1808000	1812000	nämlich zu etwas, was du in einem Block-Eintrag AI Snake Oil genannt hast.
1812000	1817000	Das ist ein Ausdruck, der mir sehr gut gefallen hat und sofort ins Auge gesprungen ist.
1817000	1825000	Und du sprichst da auch vom Jahr 2020 als dem Jahr der AI-Desillusionierung.
1825000	1828000	Vielleicht kannst du zum einen ein bisschen darauf eingehen, warum du meinst,
1828000	1833000	dass 2020 dieses Jahr der AI-Desillusionierung war
1833000	1840000	und vielleicht auch ein konkretes Beispiel geben für AI Snake Oil.
1840000	1846000	Ich glaube, 2020 war das Jahr, also alleine empirisch das Schöne ist,
1846000	1848000	wenn man schon so lange an diesem Themenkomplex arbeitet,
1848000	1851000	dass man sieht, dass die Dinge immer in Wellen kommen.
1851000	1854000	Also 2014 sprachen wir über Big Data.
1854000	1858000	Niemand redet mehr von Big Data plötzlich, aber die Phänomene sind ja nicht weg.
1858000	1866000	Dann kam eine Zeit, wo alles war AI, ohne auch kritische Äußerungen.
1866000	1870000	Und dann gab es die Welle, wo man gesagt hat, na ja, aber AI kann diskriminieren,
1870000	1873000	AI hat ethische Probleme etc.
1873000	1877000	Also es gibt immer diese Wellen. Und für mich war 2020 schon ein interessantes Jahr,
1877000	1883000	weil es wurde jahrelang behauptet, dass AI eigentlich jedes Problem lösen kann.
1883000	1889000	Und dann 2020 stand und steht die Welt vor einem wirklichen Problem.
1889000	1893000	Wir haben ja genug Probleme, aber so einer akuten Krise,
1893000	1898000	wie die Pandemie. Und der Witz war, dass AI relativ wenig dazu beigetragen hat,
1898000	1903000	die Pandemie zu lösen, weil nämlich Probleme hochkomplex sind.
1903000	1907000	Und die Pandemie, es liegt ja nicht daran, AI kann die Pandemie nicht lösen,
1907000	1912000	sondern es ist eine Kombination aus politischem Willen, auch Desinformation online.
1912000	1915000	Es sind hochkomplexe gesellschaftliche Probleme.
1915000	1917000	Es ist kein informationelles Problem.
1917000	1919000	Wir kriegen die Pandemie nicht in den Griff,
1919000	1923000	weil wir zu wenig wissen, sondern aus ganz vielen komplexen anderen Gründen.
1923000	1927000	Und das hat, glaube ich, schon ein bisschen dazu zu der allgemeinen Stimmung beigetragen.
1927000	1933000	Der Grund, warum mich das Thema interessiert, ich habe eben letztes Jahr ein Fellowship gemacht.
1933000	1939000	Und das Fellowship-Projekt ist ein Buch, was ich herausgebe zum Thema KI-Pseudo-Wissenschaften.
1939000	1944000	Und in der Einleitung, das kommt auf Englisch raus, und in der Einleitung des Buches steht,
1944000	1950000	Also dieses Buch ist eine Intervention, weil es ist ein bisschen mehr abgeflaucht jetzt,
1950000	1954000	aber es vergeht eigentlich kein Tag oder keine Woche,
1954000	1957000	in der ich nicht irgendeinen Nachrichtenartikel auf Twitter lese,
1957000	1962000	wo wieder behauptet, dass KI irgendetwas kann, was es auf keinen Fall kann.
1962000	1967000	Von KI kann erkennen, ob wir unsere sexuelle Orientierung erkennen,
1967000	1970000	KI kann erkennen, ob wir unsere sexuelle Orientierung erkennen,
1970000	1973000	KI kann erkennen, ob wir unsere sexuelle Orientierung erkennen,
1973000	1980000	KI kann etnie, da kommt man ganz schnell in sehr hochproblematische, rassistische Ecken.
1980000	1986000	Oder auch ganz am Anfang der Pandemie waren auch so KI Videokameras,
1986000	1989000	können entdecken, ob jemand Covid hat oder nicht.
1989000	1991000	Das ist ja völliger Quatsch.
1991000	1996000	Aber die Frage, die dem Buch oder die das Buch nachgeht,
1996000	2001000	und ich habe auch ganz bewusst, durch die Pandemie konnte ich auch dieses Fellowship nicht so verbringen,
2001000	2005000	wie ich es verbringen wollte und dachte, ich möchte dieses Projektgeld dann zumindest verteilen
2005000	2009000	und dann auch mit anderen Leuten teilen.
2009000	2013000	Ich habe bewusst unterschiedliche Autoren eingeladen, die eben dieser Frage nachgehen,
2013000	2016000	warum gibt es einfach so viel KI-Pseudowissenschaften?
2016000	2018000	Warum ist das so?
2018000	2024000	Und die Autorinnen haben jeweils eine ganz unterschiedliche Erklärung.
2024000	2028000	Und die erste Frage, mit der wir uns so ein bisschen auseinandersetzen in dem Buch ist,
2028000	2034000	was ist eigentlich der Unterschied zwischen Pseudowissenschaften, Schlangenöl und Hype?
2034000	2038000	Das sind ja ganz unterschiedliche Dinge, die unterschiedliche Kerne des Problems beschreiben.
2038000	2043000	Also Schlangenöl, das ist im Deutschen weniger gängig, als es so im Englischen ist.
2043000	2050000	Snake Oil, das beschreibt eigentlich unfaires Marketing, also Produkte.
2050000	2054000	Schlangenöl, Haut, sehr viel Kosmetik, Frauenkosmetik ist Schlangenöl.
2054000	2059000	Da steht, hier ist eine Wundercreme, die irgendetwas kann und die Creme kann eigentlich gar nichts.
2059000	2061000	Das ist Snake Oil runtergebrochen.
2061000	2064000	Hype ist natürlich nochmal was anderes.
2064000	2067000	Hype ist gar nicht immer schlecht.
2067000	2070000	Hype ist einfach, naja, auch Aufregung.
2070000	2075000	Wir leben ja in einer Welt, in der Aufmerksamkeit das teuerste Gut ist.
2075000	2078000	Irgendwie muss man ja auch gehört werden.
2078000	2081000	Und dann die dritte Kategorie ist einfach Pseudowissenschaften.
2081000	2086000	Also einfach Wissenschaft, die nach Wissenschaft aussieht, aber völliger Quatsch ist.
2086000	2089000	Und diese drei Kategorien schauen sich eben das Buch an.
2089000	2094000	Wir haben zum Beispiel einen Beitrag von einem Journalisten, der über KI schreibt für The Verge
2094000	2100000	und so ein bisschen erzählt, wie es ist, wenn er Produktpitches bekommt.
2100000	2103000	Zum Beispiel hier ist eine KI-gesteuerte Zahnbürste.
2103000	2107000	Und in dem Artikel reflektiert er auch darüber, wie kann ich darüber kritisch berichten,
2107000	2110000	ohne dem gleichzeitig noch mehr Aufmerksamkeit zu geben.
2110000	2112000	Das ist irgendwie ein Aspekt.
2112000	2122000	Da ist auch eine Autorin, die spricht, eine Ingenieurin, die sagt, ganz viel ist wirklich einfach verantwortungslos.
2122000	2126000	Manchmal werden einfach verantwortungslose Produkte gebaut, schlampig gebaut.
2126000	2130000	Und das fand ich ganz spannend, weil das ist natürlich was anderes, wenn ich das sage,
2130000	2133000	als wenn das eine Ingenieurin sagt, die in diesen Teams gearbeitet hat.
2133000	2135000	Also wir schauen uns unterschiedliche Aspekte an.
2135000	2143000	Aber der Begriff KI AI Snake Oil oder KI Schlangenöl stammt von einem Professor aus Princeton,
2143000	2145000	den wir auch in dem Buch interviewen.
2145000	2152000	Der hat diesen Begriff geprägt in einem Vortrag, in dem er irgendwie heißt How to recognize AI Snake Oil.
2152000	2156000	Und das Tolle an dem Vortrag ist, er nimmt so ein bisschen auseinander.
2156000	2158000	Was ist eigentlich KI?
2158000	2162000	Das Problem ist ja, dass das Wort hat eine ganz klare technische Definition,
2162000	2167000	wird im allgemeinen Sprachgebrauch aber benutzt, um alles Mögliche zu bezeichnen.
2167000	2173000	Es gab mal eine Studie, die hat gesagt, dass 60 Prozent, die ist glaube ich jetzt zwei Jahre alt,
2173000	2179000	60 Prozent aller europäischen KI Startups machen eigentlich gar nichts mit KI.
2179000	2182000	Es gibt halt einfach viel Geld, wenn man KI draufschreibt.
2182000	2189000	Und was er in diesem Vortrag gemacht hat, er unterscheidet, auf der einen Seite gab es wirklich in den letzten Jahren
2189000	2198000	in sehr klar und eng definierten Problembeschreibungen hat der Einsatz von KI immense Fortschritte gemacht.
2198000	2201000	Sowas wie ein Objekt zu erkennen.
2201000	2205000	Oder wenn man sich anschaut, wie gut sich zum Beispiel Google Translate verändert hat.
2205000	2211000	Ich war vor einem Jahr in China und man kann einfach in ein Gerät auf Englisch reinsprechen
2211000	2214000	und das Gerät übersetzt automatisch auf Chinesisch.
2214000	2218000	Das hätten wir noch vor zehn Jahren für unglaublich gehalten.
2218000	2224000	Sein Punkt ist aber, das sind sehr klar definierte, eng abgegrenzte Aufgaben.
2224000	2229000	Das sind keine offenen Probleme wie die Pandemie lösen.
2229000	2234000	Und er beschreibt so ein bisschen, er unterteilt die Aufgaben in unterschiedliche Kategorien
2234000	2239000	und sagt eben, es gibt eine Kategorie und das habe ich immer noch nicht raus, wie man das gut auf Deutsch übersetzt,
2239000	2242000	wo er sagt so Predicting Social Outcomes.
2242000	2252000	Also KI ist einfach grundsätzlich immer dubios, wenn es dazu angewendet wird, um die Zukunft vorauszusagen.
2252000	2255000	Sowohl die kollektive als auch die individuelle.
2255000	2258000	Weil das einfach nicht geht.
2258000	2263000	Und das heißt sowas wie vorausschauende Polizeiarbeit oder so sagen, wer du bist,
2263000	2266000	was deine sexuelle Orientierung ist, das ist einfach Quatsch.
2267000	2273000	Und das hat nichts und nur weil wir Fortschritt darin gemacht haben, Objekte zu erkennen, heißt das nicht,
2273000	2278000	dass wir automatisch auch in diesen anderen Fragestellen, in diesen inherent normativen
2278000	2281000	und inherent sozialen Fragestellungen denselben Fortschritt machen werden.
2281000	2287000	Das heißt, das Buch ist so ein bisschen eine Aufforderung, differenzierter zu betrachten,
2287000	2293000	was ist eigentlich KI, was ist eigentlich der Fortschritt und was ist auch einfach völliger Unsinn.
2294000	2302000	Und du hast recht, ich mag deine Frage, weil oft trägt die Kritik dann auch dazu bei, zum Hype, zur Überhöhung der Technik.
2302000	2307000	Weil man sagt, wir müssen jetzt Angst vor Algorithmen haben, wir müssen Angst vor KI haben, das ist ja Quatsch.
2307000	2310000	Es kommt nämlich einfach darauf an, worum es geht.
2310000	2315000	Ja und auch in der Darstellung, was sie eben können, liegt die Kritik manchmal halt dann falsch,
2315000	2319000	weil sie eben zu sehr quasi diesen Marketing-Sprech eigentlich kauft.
2319000	2326000	Und zur Frage der Zukunft ist natürlich dann noch das perfide, was da vielleicht noch hinzuzusagen wäre,
2326000	2332000	dass auf einer anderen Ebene es ja sehr wohl wieder funktioniert, nämlich dadurch, dass so getan wird,
2332000	2339000	als könne man quasi Zukunft bis zu einem gewissen Grad vorausschauen, dann ja auch Politiken betrieben werden,
2339000	2346000	die auf Basis dieser Pseudo-Vorhersagen handeln und dadurch dann wiederum de facto
2346000	2350000	möglichkeitskorridore eingeschränkt werden. Und das ist natürlich schrecklich.
2350000	2359000	Also es gibt dem der schönste Text, das heißt Cheap AI von einer Wissenschaftlerin, die heißt Abhiba Birhane,
2359000	2365000	die schreibt, man redet manchmal so in der Linguistik von Cheap Talk.
2365000	2371000	Also es ist sehr einfach, diese Produkte zu konzipieren und zu bauen, aber für diejenigen, die betroffen sind,
2371000	2379000	haben sie eben wirkliche Konsequenzen von, weiß ich nicht, trans Menschen, die gar nicht erkannt werden,
2379000	2385000	die nicht existieren in der Kategorisierung des Systems, bis hin zu wirklich übelst rassistischen Systemen.
2385000	2390000	Und diese, nur wenn man KI draufschreibt, das ist mehr als nur ein Marketing-Trick.
2390000	2396000	Das gibt dem den Anschein von Zukunft und von Wissenschaftlichkeit und von Neuheit.
2397000	2403000	Dabei gibt es bestimmte Anwendungen, wo wirklich einfach nur überholte Bilder repliziert werden.
2405000	2411000	Auf eine Art leitet das auch über, nochmal zur Frage der Daten.
2413000	2422000	Eine ein bisschen gewagte Hypothese, die ich mal in den Raum stellen will, ist nochmal zu fragen, ob wir es denn tatsächlich mit Big Data zu tun haben
2422000	2429000	oder ob es nicht vielleicht angebrachter wäre zu sagen, was wir vor uns sehen ist eher Small Data insofern,
2429000	2437000	als dass die Player, die in der Lage sind, diese Mengen an Daten zu produzieren, das ist ja nichts, was irgendwie frei da draußen rumliegt
2437000	2442000	und müsste nur eingesammelt werden, sondern es wird produziert, es wird erschaffen und dann ausgewertet.
2442000	2449000	Und die Player, die überhaupt die Ressourcen haben, um das effektiv zu tun, sind entweder Staaten oder eben Großkonzerne.
2449000	2456000	Und die haben ja ein spezifisch vorgefertigtes Eigeninteresse, das im Grunde eigentlich dann ja ein Gebiet erzeugt,
2456000	2463000	was relativ eng gefasst ist, also deswegen eigentlich Small Data und was dann auf dieses spezifische Gebiet hin
2463000	2470000	vielleicht immer detaillierter werden kann, das mag ich sehr wohl glauben, aber das eigentlich ein riesengroßes Feld,
2470000	2478000	das jenseits dieser spezifisch formierten Interessen liegt, dass das damit eigentlich unmöglich erfasst werden kann.
2478000	2484000	Und daraus schließt sich dann eigentlich die Frage an, brauchen wir andere Daten?
2484000	2490000	Und dann eigentlich natürlich auch bräuchten wir andere Player, die in der Lage wären, diese Daten zu produzieren.
2490000	2496000	Also ich würde sagen Big but shallow. Also es gibt wirklich, es gab noch nie so viele Daten.
2496000	2503000	Dieser Begriff Big Data kommt auch daher, das erfordert ganz andere Verarbeitungsmechanismen und Techniken.
2503000	2510000	Ich hatte mal ein ganz spannendes Gespräch mit einer Entwicklerin, die für die BBC arbeitet
2510000	2517000	und für die BBC Empfehlungsalgorithmen erarbeitet. Und die Fragestellung, ich weiß nicht, ob das nie jemals verwirklicht wurde,
2517000	2526000	aber die Fragestellung, die sie sich gestellt hat, was sind Personalisierungs- oder Empfehlungssysteme, die im öffentlichen Interesse sind?
2527000	2533000	Und dann die Frage, die aufkam, also weiß ich nicht, ich gehe auf BBC News und dann werden wir auch, wie bei Netflix oder bei Amazon,
2533000	2542000	empfohlen, was ich als nächstes sehen soll. Der Unterschied ist nur, dass die BBC anders als Netflix eine ganz andere Daseinsberechtigung und auch Mission haben.
2542000	2548000	Und die Frage, die dann aufkam, war, naja, all diese Firmen, die sammeln ja so viel Daten von uns,
2548000	2553000	wäre es nicht möglich, dass man seine Netflix-Daten an die BBC spenden könnte?
2554000	2563000	Und die Antwort darauf ist, es ist Quatsch, weil Netflix eben, weil es auf ein ganz anderes Ziel optimiert ist, ganz andere Arten von Daten sammelt,
2563000	2572000	die für ein Empfehlungssystem im öffentlichen Interesse oder im Gemeinwohlinteresse gar nicht richtig verwertbar sind.
2572000	2581000	Ich habe mal nach meinem Studium kurz gedacht, ich möchte Data Scientist werden und habe bei Zeit Online ein Praktikum gemacht,
2581000	2589000	wo es auch darum ging zu messen, ja, wie kann man Qualitätsjournalismus messen?
2589000	2598000	Also man will ja nicht nur auf Klicks optimieren, die Zeit ist nicht irgendein Blog, so ein Clickbait-Blog,
2598000	2603000	sondern eine Zeitung, die auch schwierige Themen behandeln möchte.
2603000	2612000	Was ich so interessant fand aus dieser Erfahrung heraus war, dass die ganzen Tools, die es damals vor zehn Jahren intern gab,
2612000	2617000	um überhaupt das Leseverhalten zu messen, alle aus dem E-Commerce-Marketing kamen.
2617000	2623000	Das heißt, die Zahlen waren dann optimiert für ganz andere Szenarien, also Szenarien,
2623000	2628000	wo man möchte, dass Leute so viel Zeit wie möglich auf der Seite verbringen, dass sie so oft wie möglich klicken.
2628000	2633000	Dabei ist das eigentlich keine Erfolgsmetrik für ein Medium wie Zeit Online.
2633000	2640000	Es ist bestimmt jetzt ganz anders. Aber jedenfalls, genau, es gibt sehr viele Daten, aber es gibt auch sehr viele Daten überhaupt nicht.
2640000	2648000	Also das ist halt dieses Paradox. Also was ich zum Beispiel spannend fand in der 2015 oder auch jetzt in der Pandemie,
2648000	2653000	ganz viele Gesundheitsämter haben keine kompatiblen Datensysteme und können die Daten nicht austeilen.
2654000	2657000	Also genau, zu wenig und zu viele gleichzeitig.
2658000	2665000	Das bringt mich dann jetzt zu einer anderen Frage, die eher auf diese systemische Ebene abzielt eigentlich,
2665000	2673000	weil sich ja auch die Frage stellt, wie kommen wir dahin, dass diese anderen Daten auch produziert werden,
2673000	2682000	dass aus einer anderen Logik heraus Daten produziert werden und dadurch dann eben auch andere technologische Infrastrukturen erzeugt werden können,
2682000	2689000	die andere Zielmaßgaben haben. Und da möchte ich einen kurzen Satz erwähnen, den Felix Stahlder,
2689000	2695000	ein Gast aus Future Histories, auch in seinem Buch Kultur der Digitalität geschrieben hat.
2695000	2701000	Der sagte nämlich, nicht der Algorithmus ist pervers, sondern die Situation, in der er lebt.
2701000	2707000	Und das lässt sich jetzt wiederum ein bisschen quer schließen zum Thema dieses Kongresses hier,
2707000	2711000	denn hier geht es ja auch um die Frage konsequent, inkonsequent.
2711000	2720000	Und was mich immer so ein bisschen gestört hat an diesem Framing ist, dass es für meinen Geschmack zu sehr in diese Richtung tendiert,
2720000	2728000	dem Individuum diese Frage zu beantworten, ob jetzt das richtig gemacht wird oder nicht.
2728000	2735000	Aber meiner Meinung nach sind diese Infrastrukturen derzeit ja absichtlich und extra einfach so aufgebaut,
2735000	2744000	dass wir möglichst inkonsequent im Sinne des zum Beispiel Datenschutzes usw. handeln, wenn wir uns in diesen digitalen Welten bewegen.
2744000	2752000	Das heißt, um quasi den letzten Schritt dann noch zu machen, sind wir nicht inkonsequent oder konsequent inkonsequent
2752000	2758000	dann insofern, als dass wir diese systemische Ebene in der Fragestellung eigentlich allzu oft ausblenden
2759000	2766000	und sollten wir nicht eigentlich auf dieser Ebene auch ansetzen, weil die jetzigen Logiken, die incentivieren das ja.
2766000	2772000	Die viel wichtigere Frage finde ich, wer ist wir? Und das wir ist nämlich kompliziert.
2772000	2777000	Also Risiken und sowohl Risiken als auch Chancen sind extrem ungleich verteilt.
2777000	2783000	Und das ist die Hauptlektion aus fast zehn Jahren Arbeit an dem Thema, ist genau das.
2783000	2796000	Beispiel, wenn man sagt, Gesichtserkennung für die allermeisten, für weiße Männer ist Gesichtserkennung sehr akkurat.
2796000	2804000	Für nicht weiße Frauen sieht die Genauigkeit ganz anders aus.
2804000	2809000	Das ist aber nicht nur da der Fall, das ist wirklich so ein Muster, was sich durch alles zieht.
2809000	2816000	Beispiel, wir werden ja auch, da geht es oft darum so, es gibt dann diese Daten von mir und dann können die gegen mich verwendet werden.
2816000	2818000	Das hängt natürlich auch von meiner Lebenssituation ab.
2818000	2827000	Wenn ich beispielsweise finanziell unabhängig bin, dann kann mir das sehr herzlich egal sein, ob ich damit bewerte, ob meine Kreditwürdigkeit dadurch bewertet wird.
2827000	2836000	Und besonders risikobehaftete Anwendungen werden auch in der Regel tendenziell immer auf bestimmte Bevölkerungsgruppen angewendet.
2836000	2840000	Deswegen dieses Wir ist kompliziert.
2840000	2846000	Und das, was für die einen vielleicht sehr angenehm ist, ist vielleicht auch für jemand ganz anderen lebensgefährlich.
2846000	2848000	Also das Wir ist kompliziert.
2848000	2851000	Und das andere ist, ich hasse dieses auf die Individualebene bringen.
2851000	2859000	Und versuche auch aus dem Grund nicht mehr so viel über Daten zu sprechen, sondern viel mehr über Macht, über Infrastruktur.
2860000	2866000	Weil sobald man über Daten redet, geht es dann schnell auf das Thema, ja, was soll ich denn mit meinen Daten machen?
2866000	2872000	Weil jeder fühlt sich schuldig, alle haben das Gefühl, dass sie zu viele Daten irgendwie, das ist so wie recyclen.
2872000	2876000	Ich sollte mehr recyceln, ich gebe mir sehr viel Mühe, aber manchmal mache ich es doch nicht.
2876000	2878000	Dabei ist das eigentlich der völlig falsche Ansatz.
2878000	2883000	Niemand hat Zeit, sich jeden Tag damit auseinanderzusetzen, was mit den eigenen Daten passiert.
2883000	2887000	Das ist meine Arbeit und ich habe keine Zeit und kein Interesse daran.
2887000	2892000	Und wir unterschätzen auch oft, dass die, genau, das sind systemische Fragen.
2892000	2899000	Es ist oft, sogar wird es einem unmöglich gemacht, überhaupt eine Entscheidung zu fällen.
2901000	2908000	Man sagt so, der Satz ist immer so in dem Security-Bereich, die NutzerInnen sind eigentlich das schwächste Glied in dem ganzen System.
2908000	2911000	Und wir können nicht die Verantwortung darauf abwälzen.
2912000	2924000	Und wenn man da jetzt in diese Richtung weitergeht, dann, finde ich, liegt da eigentlich auch noch ein Potential für ein eigentlich uneingelöstes Versprechen, wenn man so will.
2924000	2934000	Weil also diese Frage in Bezug auf was diese Technologien und Technologiebündel, was die leisten können oder nicht,
2934000	2939000	die ist natürlich auf der einen Seite wichtig zu verhandeln im Sinne des Demystifizierens.
2939000	2948000	Auf der anderen Seite ist es ja auch so, dass da gerade unglaublich viele Ressourcen, Kreativität, Gestaltungskraft und so weiter,
2948000	2957000	jetzt mal ein bisschen polemisch zugespitzt da hineinfließen, dass man Leute dazu bringt, auf irgendwelche idiotischen Online-Ads zu klicken.
2957000	2969000	Also im Grunde gäbe es da ja auch noch eine Potentialität innerhalb alternativer Technologien und alternativer technologischer Infrastrukturen,
2969000	2979000	die bis zu einem gewissen Grad innerhalb der jetzigen, auch politökonomischen Formatierung eigentlich nicht eingelöst werden kann.
2981000	2985000	Wenn du in diese Richtung denkst, was zeigt sich dir dann?
2987000	2995000	Das Wichtigste ist mir, dass ich nicht deterministisch bin, also auch auf Plattformen, die nach fürchterlichen Logiken funktionieren,
2995000	3001000	auf denen es Hass gibt, die radikalisieren, gibt es trotzdem wunderbare, interessante, spannende Inhalte.
3001000	3007000	Also das ist alles viel beweglicher, als es einem so scheint.
3007000	3019000	Und ich arbeite ja im Thema Regulierung, aber weil ich schon glaube, dass, oder meine Erfahrung ist so ein bisschen das Einzige, was so funktioniert,
3019000	3030000	wir haben es ja mit wirklich marktdominanten Firmen zu tun. Das ist ja keine normale Situation, in dem es hier gibt ein paar Optionen
3030000	3035000	und wir können uns dafür entscheiden, welche wir am besten finden, sondern wir haben eine Realität geschaffen,
3035000	3041000	in der, und das war wirklich diese Woche, ich habe so viel über Afghanistan nachgedacht, man muss sich wirklich fragen,
3041000	3046000	was ist denn die Außenpolitik der Plattform, die die Taliban jetzt als Regierung anerkennen?
3046000	3053000	Das zeigt nochmal, wie viel Macht diese Plattformen haben. Das ist ja absurd, dass wir überhaupt diese Frage stellen müssen,
3053000	3061000	weil das das Medium ist, über das die ganze Welt Dinge erfährt. Und selbst wenn man eine Nachrichtenagentur ist,
3061000	3067000	aber wenn man eine Nachrichtenseite hat, eine Zeitung, man ist abhängig vom Plattform zu den Lesern zu finden.
3067000	3074000	Das ist schon nur unglaublich, also das ist völlig absurd. Also wir befinden uns nicht in so einem Markt, wo man sagen kann,
3074000	3080000	das ist so, was könnten die Alternativen sein, sondern für mich ist schon der erste Punkt, es kann nicht sein,
3080000	3089000	dass so viel Macht in der Hand ungewählter Firmen liegt, auch wenn die gar nicht nur schlecht sind und auch oft,
3089000	3098000	das wird oft vergessen in Europa, dass es natürlich auch viele Länder gibt, in denen Social Media Firmen lange Zeit
3098000	3105000	und auch immer noch Alternativen zu staatlicher Zensur sind. Also das ist komplexer, das sind nicht nur die Bösen,
3105000	3112000	weil man will ja auch nicht, dass autoritäre Regime die Öffentlichkeit kontrollieren. Also es ist hochkomplex,
3112000	3118000	aber so ganz grundsätzlich, wenn man nur, wenn ich jetzt einen Schritt zurückgehe und mir darüber nachdenken würde,
3118000	3124000	in welcher Welt möchte ich leben? Ich möchte nicht in einer Welt leben, in der ich mich fragen muss, was Facebooks Außenpolitik ist
3124000	3134000	und wie Facebook zur Taliban steht. Das ist einfach absurd. Und um zumindest dieser Macht etwas entgegenzusetzen,
3134000	3146000	das Einzige, was funktioniert, sind börsennotierte Unternehmen, Regeln, Klagen, Regeln, Klagen und natürlich auch so ein bisschen
3146000	3156000	veränderte Stimmung in der Gesellschaft, verändertes Nutzungsverhalten, aber ich glaube schon an Regulierung zu einem gewissen Grad.
3156000	3163000	Das sind aber nicht, das sind nur die Spielregeln. Also Regulierung sind die Spielregeln der Digitalisierung.
3163000	3170000	Und deswegen ist es unglaublich wichtig. Aber das ist nicht das Endziel, das ist nur so der erste Schritt, um überhaupt irgendwie
3170000	3176000	die Möglichkeit für Alternativen zu schaffen. Weil wenn jedes Start-up davon träumt, von Google gekauft zu werden,
3177000	3180000	dann werden wir auch keine Alternativen kriegen.
3183000	3184000	Und von da aus?
3184000	3185000	Wie bitte?
3185000	3196000	Und von da aus? Also ich gehe mit, sozusagen kurzfristig, Regulierung sehr gerne und viel. Aber ich finde trotzdem noch die darüber hinausgehende Frage
3196000	3207000	auch interessant. Also was für digitale Infrastrukturen bräuchten wir, jenseits der Frage, ob man jetzt irgendwie eine Marktkonkurrenz
3207000	3215000	wieder herstellen kann, in Anführungsstrichen. Weil meine Vermutung wäre, dass auch das Herstellen einer mutmaßlichen Marktkonkurrenz
3215000	3224000	trotzdem natürlich ja dann wieder Player zurücklassen würde, die am Ende über monetäre Incentivierung funktionieren
3224000	3236000	und letztlich eigentlich auf einen Profitstreben hinaus ausgerichtet sind. Was ja wieder die Logik eigentlich gewaltsam sehr stark beschränkt oder begrenzt.
3236000	3248000	Wir leben halt im Kapitalismus, ne? Wir leben halt im Kapitalismus. Also, for better or worse, da geht es dann um ganz Grundsätzliche.
3248000	3256000	Grundsätzliche, klar. Sollte der Raum, und Raum ist ja schon schwierig. Was ist denn eigentlich, ich war so fasziniert von dem Vortrag, der eben war,
3256000	3265000	wo es darum ging, da hatte eine Zuhörerin gefragt, ist denn WhatsApp Social Media? Das fand ich großartig, weil oft einfach diese Begriffe verendet werden.
3265000	3275000	Was ist denn das überhaupt? Also was ist denn die Alternative, ich gehe so ein bisschen deiner Frage aus, aber über die Alternative von was sprechen wir denn?
3275000	3287000	Also so Amazon, Facebook, das sind ja nicht Social Media Firmen. Das sind Technologiekonzerne, die Monopolien oder marktdominante Stellung in allen möglichen Bereichen haben.
3287000	3294000	Und die öffentliche Diskussion, da geht es eigentlich immer fast nur um Social Media. Aber es ist ja völliger Quatsch, die ganze Infrastruktur dahinter,
3294000	3304000	die Daten, die überall herkommen, die Datencenter, die Server, etc. Du merkst, ich bin so ein bisschen in Verlegenheit, weiß ich nicht, soll ich,
3304000	3309000	es ist ja auch nicht mein, das ist was Kollektives. Wer bin ich, das ich jetzt sagen soll, wie das aussehen soll?
3309000	3316000	Das Einzige, was ich sagen kann, ist, es gibt so ein paar Grundsätze und Prinzipien, die ich fände sinnvoll wären.
3317000	3327000	Und ein Prinzip ist, es gibt eine Reihe von Fragen, die einfach, solange sie menschenrechtskonform sind, in unterschiedlichen Teilen der Welt eben anders beantwortet werden.
3327000	3340000	Was finden wir akzeptabel, was empfinden wir als Hass, was sollte man nicht sagen dürfen, etc. Und das muss natürlich einer demokratischen Kontrolle unterliegen.
3341000	3354000	Gut, ich lasse dich heraus. Aber am Ende, Frederike, stelle ich einer jeden und einem jeden immer noch die Frage, wenn du dir Zukunft vorstellst, was stimmt dich freudig?
3354000	3367000	Ich muss wirklich sagen, diese Woche war ich, die ganzen 18 Monate waren wirklich bitter. Also die Pandemie war ganz schön hart, dann irgendwie auch noch politisch.
3367000	3378000	Also wir leben schon in ziemlich harten Zeiten. Aber auf der anderen Seite merkt man dann halt auch, worum es geht und was wirklich wichtig ist.
3378000	3388000	Ich bin einfach, das ist halt einfach nur meine Persönlichkeit, ich bin immer optimistisch, ich sehe immer die Möglichkeiten und was man noch ändern kann und was man machen kann.
3388000	3394000	Sonst würde ich aber auch nicht zu dem Thema arbeiten, weil es wirklich zum Teil diffamierend ist.
3394000	3401000	Also ich habe für eine Organisation gearbeitet, die Klagen eingereicht hat gegen Geheimdienste nach den Snowden-Enthüllungen.
3401000	3408000	Und dann Jahre später haben sie Recht bekommen, aber währenddessen haben die Regierung einfach das Gesetz verändert.
3408000	3414000	Also ja, es war illegal, aber jetzt ist es legal. Es ist schon so ein, ja, was gibt mir Hoffnung?
3415000	3422000	Ich finde ganz aufregend die Fridays for Future Klimabewegung.
3422000	3427000	Auch wenn es hier in unserem Gespräch geht es ja nicht um Klima, sondern um Technologien.
3427000	3435000	Aber irgendwie habe ich das Gefühl, da kommt eine neue Generation, die viel fordernder ist, als ich das in dem Alter war.
3435000	3438000	Und das gibt mir sehr viel Hoffnung.
3439000	3444000	Und dann denke ich schon, dass sich in den letzten Jahren, es hat sich schon auch viel verbessert.
3444000	3449000	Also es gibt viel mehr, wenn ich mit Politikern spreche, das Verständnis ist viel größer.
3449000	3452000	Vor ein paar Jahren war das noch desaströs.
3452000	3456000	Es gibt viel mehr Menschen in Entscheidungspositionen, die verstehen, worum es geht.
3456000	3463000	Und es gibt auch ein größeres Verständnis dafür, dass der Status Quo so nicht richtig nachhaltig ist.
3463000	3467000	Und das sind kleine Fortschritte und die geben mir Hoffnung.
3467000	3471000	Wunderbar, dann sollten wir alle viel mehr fordern.
3471000	3474000	Und ich danke dir, Friederike, für dieses Gespräch.
3486000	3489000	Euch auch danke fürs Zuhören und Dasein.
3489000	3494000	Das war Future Histories für heute.
3494000	3501000	Vielen Dank fürs Zuhören, Show-Notizen und vieles mehr findet ihr auf www.futurehistories.today.
3501000	3507000	Diskutiert mit auf Twitter unter dem Hashtag Future Histories oder im eigenen Subreddit.
3507000	3513000	Ihr könnt Future Histories nicht nur auf allen großen Podcast-Plattformen hören und abonnieren,
3513000	3521000	sondern auch auf YouTube, wo ihr neben den Episoden dann auch Kurzvideos zu Kernbegriffen einzelner Episoden findet.
3521000	3525000	Schreibt mir gerne unter jan at futurehistories.today.
3525000	3530000	Ich freue mich immer sehr über interessante Rückmeldungen und Hinweise.
3530000	3537000	Wenn ihr Future Histories unterstützen wollt, dann könnt ihr das auf patreon.com-futurehistories
3537000	3540000	oder auch via Spende auf unserer Homepage.
3540000	3546000	Future Histories ist eine Produktion von MetaLapses, zu finden auf meta-lapses.net.
3546000	3549000	Bis zum nächsten Mal. Ich freue mich.
