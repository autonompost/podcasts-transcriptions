# FH Complete S01E04 - Felix Stalder zu Machtausübung durch Algorithmen

Welcome to Future Histories, the podcast to expand our idea of the future. My name is Jan Groß and I'm very happy to have you back. Today I'm talking with Felix Stahlder about forms of techno-political exercise of power. The attentive listener of this podcast will be wondering, Jan, what's going on? In episode 0, you explained to us that there is always an alternating interview and a think-with-me episode. Last episode, Amanda Vanessian was the guest, so it was an interview episode. That means there should be a think-with-me episode now. You're absolutely right, but I'm breaking out of this pattern at the first opportunity, because I've noticed that there are just too many interviews that I'd like to do, and on the other hand, it's perfectly sufficient if I do a larger Think With Me episode at irregular intervals. That is, there are more interviews and I will then also simply communicate before the respective interview episodes, updates, news, smaller horses and so on, such as the input from the Association for the Design of History pages, through which I came across a book entitled The People's Republic of Walmart, or the new article by Evgeny Morozov in the New Left Review, brought to my attention by user Ed Guy Philosopher, entitled Digital Socialism, both very much on topic to the second episode of Future Histories on the Socialist Calculation Debate. I'll put the links in the show notes, really very interesting stuff and I will definitely continue to follow the Socialist Calculation Debate in its contemporary manifestation here in Future Histories. I think that's a very central theme. In today's episode, in the interview with Felix Stahl, by the way, the topic is also touched upon. The interview was recorded sometime at the beginning of the year, I don't remember exactly, but I hadn't read Evgeny Morozov's article yet. So the topic will definitely stay with us. Enjoy the interview and if you want to learn more about the podcast, go to www.futurehistories.today, there's some additional material there. Let me know what you think about the whole thing on Twitter under hashtag Future Histories or on Reddit. For example, there has already been some very exciting input on the subject of cybernetics from a user, I think his name was Bjarne. Thanks for that Bjarne. So on Reddit, for example in our subreddit or quite classically by mail at future underscore histories at protonmail.com. And if you want to support the podcast beyond that, check out patreon.com slash Future Histories. Welcome to Future Histories, the podcast to expand our idea of the future. My name is Jan Groß and I'm very happy to welcome Felix Stahlder as a guest today. Felix is a cultural and media scientist and professor of digital culture and theories of networking at the Zurich University of the Arts. Welcome Felix. Hello Jan. Today's conversation is about the exercise of power through technology in general and in particular about the question of the exercise of power through algorithms. So first of all, the basic question, what is an algorithm and what are the different forms of algorithms? In its most general form, an algorithm is a series of instructions to transform an input into an output in order to solve a problem in a general sense. The essence of the algorithm in this very general definition is that these action steps are uniquely describable and that it has a finite number of action steps. In that sense, you could almost call the instruction manual that comes with an Ikea piece of furniture a kind of algorithm. It is a series of steps that are described as unambiguously as possible. That's why you take a formal language. In the case of the Ikea enclosure, these are pictograms that transform an input, the series of boards you buy, into an output, for example a table or a shelf. The executing machine, so to speak, is ourselves, following these steps. In order for a machine to actually execute these steps, they must be clearly described. Even a pictogram is never completely unambiguous, we know that. There are always problems in building it, because it's unclear what you have to do now. That's why most algorithms are written in mathematics, so that there is no more question of interpreting the steps. And then, if these steps are described without interpretation, a machine can execute them. In this sense, algorithms have actually been around as long as computers have been around. Basically, there is no difference between an algorithm and a program in a computer, which also converts an input into an output in a series of defined steps. What is new today, however, is that these algorithms, these programmable steps, can be applied to new classes of problems and thus things can be automated, i.e. problem solutions can be automated, which until very recently could not be automated. And that's why we're talking about algorithms today, because they're now emerging in a series of situations where they haven't appeared before, and so the boundary is being redefined, what can be done by a machine and what is a capability that is genuinely human. And what forms of algorithms are now emerging as a result of technological progress? So I'm also thinking of self-learning systems and the like? First of all, two preconditions or actually three preconditions had to come together in order to make this wave of algorithms possible, which have, so to speak, penetrated all areas of society and life in the last five or ten years. The first thing is computing power, so the computing power of computers has grown tremendously and at the same time the prices have become very cheap. That's a classic Moore's Law, every 18 months computing power doubles at the same price. The second thing is that there is an enormous amount of data now available over the Internet, millions and millions of images, lots and lots of text data, lots and lots of motion data that you can analyze, lots and lots of video and voice data and so on. That means we now have very large amounts of data that we can use to develop these algorithms. And the third thing is that there have been a series of conceptual breakthroughs, so new innovations in the field of algorithm development, or computer science, that now allow us to write very different and more complex algorithms. And that has led to the emergence of a new idea, so to speak, a new practice or idea has taken hold, let's say like how we construct algorithms. Quasi the first ideas of artificial intelligence in the 1960s and 1970s came from so-called expert systems. That is, an idea that you can formulate human knowledge, or at least in an expert domain, hence the name, to the point where you can transfer it to a machine. So for example you make categories, one tree is a plant, many trees are a forest, and so you can generate a logical system of relationships. That has worked in very narrow areas, that's why these expert systems, they've actually been around since the 1970s, but was very limited because it was just not possible to generate these ontologies, these category systems, without contradictions and without interpretation. And what has happened now is that another approach has prevailed, so to speak, which is no longer about category systems at all, but is purely about statistical conspicuities. It's no longer about understanding an object, it's just about determining correlations between different statistical features. For example, translation programs, which used to be attempted to be written by trying to find just some kind of a structure of the language and then translating that from one language to another, that has been completely abandoned. And now today it's all about statistics. So translation programs are written today, in that very, very much text, of which one knows, so to speak, that an adequate translation exists, for example from the entire publication area of the UN, the United Nations, which are very, very multilingual. The publications of the European community are also very, very important as a corpus of multilingualism. And these are now simply analyzed statistically. If the word x is written in German, then there is such and such a probability that the word y and z will follow. And in English it is then so and so and translation is made from that. And that is only possible with very large amounts of data and very strong computer computing power and specific models of how something like that can be calculated. And that is the reason why today we suddenly see algorithms, i.e. automatic decision-making systems in areas where until very, very recently we thought that a machine was not capable of doing that. Image recognition and similar things are perhaps the best known. And how is power being exercised now with these increased capabilities, also just through the strategy changes sometimes, how is power being exercised via algorithms? There are different ways in which algorithms are part of a structure of power and the exercise of power. Perhaps it is important to distinguish between different terms of what an algorithm is. An algorithm in computer science is really just the executable code, the series of steps that a computer can perform. It has no before and no after. In a comprehensive perhaps so sociological or political analysis of an algorithm, you have to see it as a part of a decision chain. It's never autonomous. There can't be such a thing as autonomous technology, but it's embedded in a series of decisions, some of which are made by humans and some of which are made by machines. But it's always embedded. That is, you can first ask yourself, what is the problem that an algorithm is designed to solve?
   What is a problem that I want to solve using technology? That is the first fundamentally political aspect, so to speak, which is always decided institutionally, so to speak. The algorithm itself never decides what it is to be developed for. So if the goal of my algorithm is to lower the limit or the number of loans that are not repaid, so to speak, then that is a very specific orientation. And then I design that accordingly. That's the first one. So what is the problem in the first place? Then the second is, how is this problem operationalized? Because rarely can a social problem or a goal that comes from a business plan be quantified directly, so to speak. Instead, you then have to start constructing a model. That says, for the question I want to answer, these and these points are relevant and everything else is not relevant. And with that, a model is constructed. It is often the case that what you actually want to measure cannot be measured directly because it is too diffuse, too complicated, too distributed. And then you make a so-called proxy. So you define a value that stands for something else. For example, if I want to measure how well a child learns, that's a very complex issue. What is that actually? What does learning mean? What is that relative to other things? I can't measure that. So I insert a proxy, a standardized test. And then I can say, okay, the child answered nine out of ten questions correctly. Then I have a number. And so a series of decisions are made about how I construct a problem, what I even decide is a solvable problem or a problem that's relevant to me. And with that, of course, there are always certain interests of those who then decide these problems or these questions are connected to the agents and the plans and the ideas that these agents have. And this is now inscribed in technology, which is then said to be autonomous. And with that, these ideas and these settlements that are made in quite a few places disappear. There are other places where these settlements are made. They disappear and at the end a supposedly objective result comes out. This child answered nine out of ten questions, the other answered only seven out of ten. And that tells us something about who is better and who is worse. So that's basically a starting point where you can say, okay, it's the way things are looked at that actually exercises power in the first instance, by setting the course, by making decisions about what the problem is and how it's going to be solved. In addition, there are also - and this is certainly a consequential problem, I would say in quotation marks - certain architectures that are also algorithmically determined, for example, and to which we are exposed on a daily basis. So the social media, in which a world is constituted for us by the algorithm or Google Maps or something like that, which recommends us a certain direction of travel or a timetable, and we will probably choose that in most cases, and so on and so forth. So that means that there is a shaping of our environment by algorithms in a very pragmatic way. What interests me is also, say, a question of shifting political sovereignty. It's actually the case that political sovereignty is tending to shift away from such constructs as the nation state or something like that towards other forms of governance, including algorithmic governance. And what is your assessment, who are the main actors, that is perhaps the easiest to answer, but what is perhaps the underlying idea of political governance itself, which is inherent in these forms? So the power that is being exercised, that is, as we talked, on the one hand, at the level of problem solving or problem definition and then modeling to solve it. There are quite a lot of sentences being made here. And on the other side, so to speak, on the user side, we then have the case that even in the best case, so it's not about whether the algorithm is right or not, let's assume it's right and it's also made with a higher intention, so to speak, we have the problem that then certain world views suddenly become very strong. So the part of the world that can be calculated algorithmically, so to speak, that is made accessible to us. And other kinds, other areas of the world that can't be captured that way, so to speak, then suddenly become more difficult to see. They are devalued when the other is moved to the center, so to speak. And that's also something that shapes a world view and makes the, so to speak, implicit, non-quantifiable knowledge more and more difficult or more and more precarious. So for example Google Maps, we have an idea of how we want to drive from A to B, but Google Maps tells us to drive it through somewhere else. Then of course we have the question, who do we trust now? And except in places where we know very, very well, very, very accurately and very, very promptly also what the road conditions are, we're likely to trust Google Maps because it can incorporate real-time data and so on and so forth. And often we are well advised with that, so it's not a criticism of that at all. But it just means that one form of knowledge is sort of being overwritten by another form of knowledge. And that's also where our view of the world begins, so to speak. Not only is the world made, but how we experience ourselves in the world, how we act in the world, is changed. And the actors are actually those institutions that have the prerequisites to be able to incorporate algorithms into their decision-making processes on a large scale in the first place. In other words, the ones who have large amounts of data at their disposal in order to be able to train algorithms and then apply them are the ones who have large computing capacities and are the ones who then also have an output that allows them to act on the basis of these decisions. For example, they can show us a map on Google Maps or display search results. So these three prerequisites describe the group of people who are capable of acting in this way. And this is actually a relatively clear description of who they are. They are large institutions, for individuals it is virtually impossible to act in this field. They are large institutions that have a great deal of data at their disposal and that have the corresponding opportunities to act in order to use the knowledge that is generated, so to speak, to actually change action in the world. And this asymmetry of resources is ultimately also something that cannot be overcome so easily, or so I would think. But that also means that all those who act in a similar way, who want to use the positive aspects of this development, but in a form that is, I don't know what you call it, less restrictive or less characterized by a libertarian paternalism. If people want to use that in a different way, so to speak, then they are actually per se always already at a disadvantage, aren't they? Is that first of all, is that true, because that would already be fatal actually? For the absolute majority of users, basically for all users, these systems represent a take-it-or-leave-it situation. If I want to use Facebook, then I use it with everything that's there. I can only say, yes, I'll just let myself be, or no, I won't just let myself be. In many situations, however, that's no longer a real choice, because certain things are required in order to participate in today's life, because that's the new normal, so to speak. Everybody does it. It's unusual not to participate in it, and if you don't have that, then you get problems with the action. In quite a few examples, that's the case. There are quite a few things we need to participate in life. For example, we need an address. And if you don't have an address, for example, if you are homeless, then you can't apply for an apartment, then you can't apply for a job, then you can't apply for a whole lot of things. That means we need all these things and also access to these systems and use of these systems in order to be able to participate in life at all in the present. But then, as I said, these are decisions that are all or nothing. Either I take Facebook as it is, or I stay out of it, which is not an option for most people. And it's not as if these services and this algorithmic support, so to speak, are simply negative. They have grown and developed because there was a problem, for example, with the amount of information that we have to deal with in a society that is becoming more and more information-intensive and complex. And we need new tools for this, and these algorithmic filters in this case, or these possibilities to look through very large quantities of images according to image content and not just according to image descriptions, that is something that is genuinely useful. That means that for most people, for most users, the direct added value is given by the use and that should not be denied. But with this all-or-nothing proposition, there are also things that are traded in, namely that one moves in an environment that one can hardly see through, where one cannot see at all what takes place behind the scenes, so to speak, and which is completely controlled by a single identity, by an actor, let's say in the case of Facebook or Google or the search engine for airplane tickets or whatever. That means that if they decide they'd rather show X instead of Y, then that's simply the way it is and as a user you have no influence on it, often you don't even notice it. And of course there is an extreme power imbalance between the one side, which can control the parameters, and the other side, the user side, which then has to find its way in this world, so to speak, in a positive or negative sense, but where there is hardly any possibility of feedback, hardly any possibility of influence, hardly any possibility for the users to decide how they would then have constructed these parameters. That is entirely in the hands of this relatively small number of actors who construct these new systems in which we ultimately live, and always construct them in such a way that the space they create, the problem they want to solve, so to speak, is one that benefits them. And what would alternatives look like that try to shape this differently, where the individual users and I see this as a bit of the core point, which you then also address, where the individual users have the opportunity to participate in decision-making processes, higher decision-making processes. So on the one hand, how would you have to approach that and is that even possible now, when such a large platform power has already been established that basically any competition is always faced with the problem that a broad part of the cake is already taken, so to speak?
   I think there are two or at least two different ways of approaching this. These are collective approaches in each case. I think the individual behavior, I can now align my behavior on Facebook a little bit this way or a little bit differently. Basically, I can't change anything about the structure. So the individual level of action, which is always put in the foreground, is very, very limited here. One level of action is to create transparency, to know what is happening, to know how selections are made, and there are various approaches to this. One approach, which is being pursued by the General Data Protection Regulation, for example, is that it is now being newly introduced into law to receive relevant information about how an algorithm works and how a decision was reached. This is not yet an algorithm in the sense of an audit, a code, but perhaps more like the ingredients on a food package, where you can see how much sugar something contains or other things, how many stabilizers are in a chocolate and the like. That's not yet the recipe after that food is made, but it gives me a good idea of what it is that I'm eating now. It gives me a start of an idea of that and that's what the General Data Protection Regulation is trying to introduce something like that, that you have the right to see how a decision has been made. The other way to create transparency, which actually seems to me almost the more important, is that you generate comparability because if I go on Facebook now or do a Google search, I have no way to see what I don't see. I only see a result and the result has a certain credibility, has a certain usefulness. I go on Google, search for something, and find something that's roughly what I think I'm going to find because, of course, it takes into account my expectation horizon in the evaluation. I go on Facebook and see a selection of messages from people I know, but I can't know what I don't see. One way to create transparency here would be to make different algorithms, different filters in that case, different search strategies comparable. If I search like this, this comes up, if I search like this, this comes up. If only I could choose, do I want to turn the personalization on or off, or perhaps I could also choose, do I want my own personalization or that of another person. Then I would suddenly see, aha, under that comes that too, under the other and I now suddenly see things that I would not otherwise see. And that is, I think, this comparability of the output is all the more important because these algorithms are completely intransparent. On the one hand, they are non-transparent for reasons of trade secrets, and on the other hand, they are non-transparent because they are constantly changing. They are sometimes readjusted in advance. Facebook says they now want less, so to speak, commercially produced news, but more, so to speak, from your friends, and then they change the algorithms because they change their business model or their orientation a bit. That's one thing. That is, we are faced with outputs from a machine that is constantly changing. In a stable machine, in a classic black box, I can guess over time how the inside works because I can see if I put x in, then y comes, and if I put z in, then a comes. That's okay, well, that probably does this or that. But these new algorithms, they're highly dynamic. That means if I put the same thing in twice, the same thing won't come out. That means I have even less ability to understand what's going on. I can only take it as it comes. But if I can compare it, then I still don't understand what's happening inside, but I can see that there are different ways to solve the problem and that maybe one is better than the other or one is more useful than the other. That is, the question of how I can create comparability here, so comparability between different algorithmic strategies seems to me to be very, very relevant to create transparency that just goes further than just a list of, yes, we take into account age, place of residence, and income, so to speak, in the filter that we construct. Transparency is one direction, both will only go, the data protection regulation has started there, we both only go with state regulation that prescribes something like that, because of course that is not in the interest of the provider. The provider's interest is, so to speak, to make their own decisions virtually invisible and thus to remove them from any criticism, from any collective control, from any political negotiation. That's one part. And the other part is certainly to introduce some regulation, some limits on what an algorithm is allowed to do, what it can do, must be able to do, and how accountability for decisions is constructed. So for example, if an algorithm makes a bad decision, then I can't hold anybody accountable, so to speak. Because who is it? Is it the entity that uses it? Is it the body that wrote it? Is it, so to speak, is it a wrong decision at all or is it just a personalized decision, so to speak, and so on. And so we create a series of very problematic incentives. For example, if my goal is to lower the rate of those who, for example, I'm an insurer, I provide auto insurance, and I want to not insure or insure more expensively those who are more likely to have an accident. That's classic actuarial science. But if I now say, okay, I now have a new algorithm that does this for me, so I can sort of, from some population I have 100 correct identifications, so people who actually have higher probability. But I also have 30 episodes in there. People who are, so to speak, now unfairly either not getting the service or getting it at a higher price, then that makes sense from a business calculation. I still improved by 70 cases. From the perspective of the user, he is then suddenly confronted with the system that he decides from his perspective, from their perspective completely arbitrarily. I want confronted with the wrong decision and I can not do anything about it, because so to speak for the provider it does not matter. And I don't even know exactly where it's coming from. I can't even say for sure that it's a wrong decision. And here I think it is important that the question of responsibility and that means in an economic sense also the compensation for damages is regulated in such a way that these incentives function differently. That it's just a bad algorithm that wrongly screens out 30 people but correctly identifies 100, that the calculation just doesn't work anymore. That would be sort of the second or the third point. The first is transparency, the second is sort of limiting what is allowed. And the third one is question about consequences, which have to be actionable and have to be linked to damages, because otherwise they are irrelevant. Now, there are a couple of follow-up questions that come to me. On the one hand, quite specifically with regard to the actually artificial limitation of algorithms to algorithms that actually ultimately remain completely comprehensible. Isn't it the case that this limit has already been exceeded? So there are not already algorithms that are actually no longer comprehensible by humans themselves, because actually algorithms train algorithms and then say at the end of one for humans basically already from cognitive abilities, so from the limitation of cognitive abilities, that it is then actually no longer comprehensible? And is it then, does it seem realistic to you? Again, that has to do with the question of what an algorithm is. If I say an algorithm is a series of coded steps and that sort of starts with the first line of code and ends with the last line of code, then it's actually the case that we're dealing with quite a lot of algorithms that we can't read anymore. And when we have finally read them and perhaps understood them, if that is still possible, then what we have understood is already no longer true, because the algorithm itself is always evolving. That is, at the level of the code, we actually have the situation that we no longer understand a lot of things. But if we take a somewhat broader concept of the algorithm, then an algorithm is always embedded in an institutional structure that defines the problem on the one hand and the solution on the other. So what is a good solution? What the algorithm then does is to find a way from the problem to the solution. But we can't train an algorithm if we don't have an idea of the solution. So we want to define a face recognition algorithm that recognizes a person who is on a video and matches that against a database of stored passport photos. Then we have to first have a series of videos where we know who's on it and say, look, this is like this, this is like this. And then we can start training the algorithm. That means even though we may not know anymore, can't read the code anymore, we still know how is the problem defined and what is the solution. That is, we can then still do an analysis of this algorithm at the level of the step before and after, now not so much as a technical artifact, but as an institutional artifact. What kind of decision chain is being made here and is it compatible with things that we hold dear in society, anti-discrimination, price transparency, so to speak, whatever it is afterwards and what it's about specifically. But we can counter this purely technical lack of transparency, so to speak, by making an institutional analysis that produces this algorithm in the first place, by running this algorithm, so to speak, and by constantly correcting it to see whether it is still on target and, if not, by making adaptations so that it is exactly on target.
   That is, as long as we know the problem and the goal, that is, the problem and the solution, we don't have to know at all exactly whether the steps that are taken to get from A to B are what they look like. Nor for what you had just mentioned, that is, for the question of, for example, legal accountability for decisions that have been made. That would be, I think, the first aspect that would come up for me. And the second question would be, a year or a year and a half ago, I think, there was this relatively publicity-heavy, press program where, I think, Deep Mind was trained to learn old-school arcade games without any goal definition. Because it hasn't even, I think, as far as I can remember when I read that, it hasn't even been told it's a game, nor has it been told learn this game, but, at least that's how I understood it, that it's just been set to each other and then based on that then the favorable intelligence has both learned the game and achieved a mastery in it that far surpasses any human doing. So games have a very clear solution in them. So score points, next level, be faster. And games are actually very good for learning them, because then you can say, okay, one algorithmic strategy scored 7350 points and the other algorithmic strategy scored 7480 points. So it is clear, two is better than one, we continue there. From that point of view, there is of course a solution inside. But you haven't even told the algorithm that it's a game. I think that was a bit of the point you wanted to make, because a game in itself can be learned well, so to speak, because it works with quantifiable parameters. That would not have been so surprising now. I think what was emphasized was really the aspect that before, that is, that this target was not set, I think. We would have to look at this in detail. So a training can't work without a target, because you always, especially these self-learning algorithms are iterative. They are sleeping in feedback, again and again tens of thousands, hundreds of thousands, millions of times. And what happens, so how that works is you start with very primitive solution strategies and then you look at how close are you to the goal. And the one that's closer, you take that one, you make quite a lot of variations, you look at which one is closer, then you take the one that's closest, you make quite a lot of variations of that one, and so on. This means that such a learning process cannot function without a target strategy. From that point of view, the question is, on which level of abstraction do you make the game? So you do these learning processes. Maybe in this case it is still said, learn the rules of the game yourself and then you will be good at it. The target is abstract. But now, so to speak, if the algorithm first has to decide, do I want to play the game at all, then that won't work, because then it doesn't know what it should learn. That is, algorithms are actually always, even still, very narrow in the sense of this solution finding on and before defined problem. General artificial intelligence that can, so to speak, solve this range of problems, as human intelligence is, is still very, very far away, according to everything I know. We can train very, very well to recognize faces in images. But that is fundamentally different from an algorithm that knows how to pick up a soft trauma in a robot without crushing it. These are fundamentally different algorithms that cannot be easily combined. From that point of view, I think this targeting is very, very crucial. And without that, as far as I know, there's virtually nothing or nothing that you could actually consider learning. The first part was whether the question of legal responsibility is also answered by saying that we still know about a beginning and about a goal. And by setting these parameters, we can also create an enforceability, if, for example, as you said with the insurance, the 30 people then wrongly pay a higher premium or something. The question is, what do they have to provide as evidence, or is it the insurance company's duty to provide evidence? That would be logical, for example. That would be the question. So, for example, if you take an anti-discrimination prohibition and then there is one, someone is stopped, checked in by the police and they enter after a discrimination complaint and say, I was only checked here because I have a dark skin color. Then the policeman would be asked now yes, afterwards around that to decide whether that was now actually a discriminating handler or not. He says, why did you control this person? And then he might say, yes, I had the order here to check every tenth person who passes this corner. That was my assignment, I carried it out, that was the tenth person, done, right? And then the question of the inner motivation of the police officer, so to speak, would actually also be relatively unimportant. That would be sort of the code that we don't quite understand either, why did he do that now. Would be relatively unimportant, because he was in a very clear rule-bound procedural system. And then, however, one could ask the next question, so to speak, why was this kind of control carried out at this corner? Is it not clear that at this corner only, so to speak, because this is an area, such and such a group of people passes. But then the responsibility, so to speak, whether this is now a racial profiling, would no longer be with the individual police officer, but so to speak one level higher with the head of operations said, go there. And in this respect, you can already define it on this institutional level, who defines the problem and who does both the problem and the solution. And then the question of whether this problem definition and this solution strategy were discriminatory must be decided on a case-by-case basis. But we can do that today in other areas as well, so to speak. And the question is not, oh God, we have a million cases that we suddenly have to decide on every day, but rather what is the price to pay if discrimination is found and is it worth it for the provider to take the risk of having to pay this price if discrimination is found. That's a business consideration. And this responsibility must be structured accordingly according to the possible claims for damages. That would be a proposed solution, as you just said, that we create a set of rules for cases of discrimination and how to proceed. But that could still mean that the insurer itself, i.e. the company itself, may not be able to understand this internally, right? Because, for example, the algorithm has trained itself and within the training it has come to the conclusion that certain ZIP codes, certain postal codes somehow lead to more favorable results for the insurance company. But without the algorithm knowing this now, these are then just zip codes in which the majority of people live, for example, with a migration background, and so on and so forth. So that could still happen, right? It could still happen, but then it's just classic compliance for a company to keep an eye on it. Basically, an algorithm is nothing more than the automated work of potentially very many people. With means that would not be available to humans in this case, right? Yes, it is an automation. And if you translate it back into a very large number of people, you can't say that the CEO of a company can't possibly know what all 100,000 employees are doing. How can he be responsible for what the company does? But he is. Yes, that's not what I wanted to say. So I didn't mean to say that because of that, the company is no longer.... No, no, I'm just saying that just because it's in a different materiality, in a different language, a language that is actually often difficult to understand and partly, so to speak, not understandable in real time for anyone, doesn't mean that these classic issues of responsibility, of, so to speak, legal security and I don't know what, no longer apply. It is simply an attempt to evade this responsibility by saying, yes, these are machines. Google says, well, that's always just an algorithm, we're not responsible for the search results. In fact, of course, they are responsible for the search results because they are responsible for the algorithm and actually design the search results so that it is right within their business model. Let's perhaps move on to the level of socio-political designs, because it is already the case that the tremendous efficiency that is created by the availability of these large volumes of data and the corresponding computing power, which is not available to every actor but only to a few, creates a convenience in everyday interaction with the respective products that is incredibly seductive. Google Maps is really incredibly practical, incredibly good, and simple in the vast majority of cases, and very few people succeed in resisting it. If you now apply this to a larger socio-political vision of the future, very roughly speaking, what designs do you see for the future that will succeed in resisting this offer of convenience that is being made? I wouldn't call it an offer of convenience, and that's why, so to speak, the question of resisting the candy, I think, is a wrong way to think about it. I think these are offerings that allow us to navigate a much, much more complex world. Let's take Google Maps. We're all traveling a lot more. We're all much more in places where we don't know our way around. And now, of course, Google Maps is very, very good for finding my way from A to B in those places where I don't know my way around well. And the fact that we often are in those situations is a function of a changed life. I want to know what kind of offer is there to fly from A to B. I want to know how to get there. That's where the complexity, so to speak, of the market is so great that I'm not able to orient myself at all without the help of platforms that aggregate these offers, so to speak, and make them comparable. For very, very many problems, how do I optimize traffic in the city?
   Do I need very big data to get a different view on that? How can we perhaps also think about energy cycles in a complex way and reorganize our economic system accordingly? Do we need very, very complex models that generate very, very large amounts of data? In this respect, these algorithmic services that allow us to see the world differently are necessary, because we need a different picture of the world. For many, many reasons. Not least for reasons of climate protection. But we also live in a world that is dynamic, that is very large, because we are moving very far, and accordingly we need tools to orient ourselves. So I wouldn't see this as something that is just a matter of convenience. As we have already discussed, this one way of finding one's way around, now with Google Maps for example, is increasingly replacing other ways of finding one's way around. That is, one then becomes dependent on this new normal, so to speak, to a certain extent. But that simply means we become dependent as a society on the tools with which society is built. That's actually always been the case. The problem now comes from the fact that we get these tools that allow us to see the world, so to speak, in a way that corresponds to our behavior in the world, we get them as part of a package. And that package says, we give you the tool, but you have no way at all of seeing how we can influence you through that tool. How we can control you, what you see and how you move. We don't know how Google Maps, that's still a relatively understandable example because it's actually in the city and I can move through it and see, ah, I could have gone differently. But we don't know how it comes about on the route. And you can just imagine how, for example, a state, Google says, but we don't want too much traffic to go by that corner because it's sensitive. And then we'll never get the route at that corner and we don't know why. And that's where the problem is. The problem that these tools, we need those, those are positive, those allow us to live in the present, so to speak. And very, very complex problems that we have, that were not simply made by these tools, so to speak, but they do exist. Maybe better to tackle than without these tools. But they are caught in a political-economic institution, so to speak, that does not want to accept any rules, so democratic versions and so to speak, that administers this power, which is connected with it, completely non-transparent. And there are no possibilities whatsoever, from the democratic, or weak possibilities so far only, from the democratically constituted expressions of will, to introduce certain rules, to introduce certain restrictions, to prefer certain lines of development and to cut off others, so to speak. Perhaps I expressed myself too imprecisely, because I didn't necessarily mean that with sugar, or actually not at all with sugar, that one should rely on these positive effects of making the world accessible and so on, which algorithms and technology in general can also achieve, that one should refrain from it, but I ask myself, how can a conception of the future look like, which ultimately also corresponds to this political-economic network, which is emerging and which is also gaining a certain dominance on a political-social level? How can visions of the future, or what kind of visions of the future are there that pursue a different, not more open or more democratically constituted structure? So are there alternatives and what do they look like? So what's being discussed fairly widely right now, as it starts to be discussed, is the notion of technological sovereignty. And what that means is that you bring the technologies, and that includes the data that those technologies need, into the realm where the democratic decision-making of those who are affected by those technologies takes place. The problem today, which exists even at the EU level, is that the big technology providers are hard to grab. They say, yes, we're American law, or if we're in Europe, they're kind of in Ireland. And that makes it, for example, a city like Vienna now. It's difficult to get data from Airbnb to enforce the laws here in terms of levies on rental apartments and so on. And here the attempt is to say, we actually want those who are now collecting our city with data, so to speak, and changing it with technology, to be located within the city and its political institutions. That is, it would be an attempt now, a policy that says we have to get away from these centralized solutions that are located somewhere abroad difficult to achieve, so to speak, through the political institutions that are democratically anchored. Instead, we want to see that they actually operate within our political framework, so to speak. And that's where the cities are currently the furthest along. Amsterdam, Berlin, Barcelona, New York, even in the U.S., where this is being discussed very strongly. The question is how can we strengthen the influence of the democratically legitimized institution and the democratically legitimized power in this area? And at the moment we have the problem that these two geographies, so to speak, are completely divergent. And the idea of technological sovereignty would be to bring these geographies back together. But not by saying, well, we need a global Internet government now, that was the idea of the 1990s and 2000s, so to speak. But rather by saying, no, we have to tie this back to an actual place where quasi-democratic politics takes place. And in this specific case, that was the city. That would mean that Airbnb can't just say, well, we don't really have anything to do with Vienna. We are based in Ireland, which is actually an American company. Instead, one would have to say, well, if they want to operate in Vienna, then they have to actually accept certain rules here and, so to speak, bow to this democratic decision-making process that takes place here, for example, when one says that private apartments can now be rented out, what are the taxes that go into it, etc. But this is a political decision, so to speak. But this is a political dispute, so to speak. On the one hand, there are certain democratic institutions; on the other hand, there are very powerful players, not just Internet players, but others as well, who are actually trying more and more to get out of this democratic control. And that is a classic political dispute. And the actors are actually then, so the main actors, I say, are actually again the two, so to speak. So the alternative is, or say, the control authority or the auxiliary authority that is then called for, in quotes, is then a state. Tendentially, is that the case? Ultimately, it's always a state one, because the state sets the legal framework. The legal framework can certainly be made in such a way that it also strengthens other actors. But the framework, so to speak, as long as it's based on laws, is always the state. For example, as part of this idea of technological sovereignty, there's also the idea that the data that accumulates, for example, in an intelligent transport system or that accumulates in a city, is generally no longer understood as private property, be that my own personal data or the company's, but actually says that's a communal one. That belongs to everybody in the city, so to speak. So data is a new form of Cummins. And then you could create rules about how that data can be used. I could say, for example, all actors located in the city, companies and NGOs or also citizens' initiatives have free access to this data, so always within the data protection framework, but have free access and can develop new offers on it, so to speak, in services. And actors from a national framework have access under certain conditions, for example they have to pay for it. And global players, who have access under other conditions, have to pay even more. You would have a graduated system where you can say that the goal of this regulation is to strengthen local actors. And such a framework must be created at least at the state level, so that an institution is empowered, so to speak, to create such a framework, a data protection authority or a data collection authority or a data collection network or whatever institutional form it takes. But it has to be charged with developing and enforcing such a framework. And are there broader ideas of the future where the solutions are not so particular? So you say, okay, there's an isolated solution, as it were, for an existing problem that functions according to principles that we find more worthy of support, but are you familiar with designs, that is, also designs of sociopolitical systems of order that are based neither on the state as an instance of order nor on the market as an instance of order, although one would actually have to think of the two as fixed together. Do you know any drafts? Not on that scale, but there has of course been a long and intensifying struggle between three groups of actors: state actors, market-oriented actors and civil society actors, i.e. NGOs in the broadest sense, which are increasingly able to take on public tasks and are doing so to some extent. And this sector is certainly one that can better organize itself through new technological possibilities of organization and communication and the dismantling of these knowledge monopolies that are sometimes extremely trapped in institutions. And there is a great debate about which area should be regulated in what way, and of course there are also individual examples of this, where people say, for example, that the energy supplier, the local energy supplier, should no longer be organized on a quasi private-sector basis, but should be organized on a kind of public-sector basis. And then there's this whole movement of remunicipalizing the energy networks, which was very successful in Hamburg, for example, and failed in Berlin, but was also very popular, but then failed because of the quorum in the vote. So there are already ideas about how these three areas can be linked together in a new way and how this growing organizational potential of civil society can be better integrated, so to speak, by expanding democratic decision-making opportunities and opportunities for co-determination. But I don't think it's an either-or situation. It's not the market or the state. So the libertarian utopia will always remain a utopia. It's clear, so to speak, that completely state-run planning is not the vision that you want, so to speak, but negotiating in the best sense how these three things can stand in relation to each other, I think there's very, very much movement in that. Nowadays, the dominant narrative of political economy is still based on the efficiency of markets, the alleged efficiency, which in the last instance also says, so to speak, that the highest possible allocation of resources can be achieved through price formation, because otherwise no other system would be able to gain such complete knowledge about the market, which is so complex, as markets can do, because they are organized in a decentralized way, and every single buyer and seller gives their information input, so to speak. Where do we end up when we ask ourselves whether this narrative, which is based on efficiency, will still be tenable in the future?
   And here I'm thinking on the one hand of algorithmic pricing, which also includes factors other than price alone, so to speak, in order to arrive at a basis for the exchange of goods in the end, thought in that direction on the one hand and also whether there is such a thing as a cybernetic planned economy in times of its technical feasibility, to put it a bit polemically. I think what we see is that the increased ability of non-market-oriented actors to organize themselves and generate outputs means that certain activities that were previously organized only in the market and were correspondingly price-oriented, so to speak, can be organized differently and thus a different calculus comes into play. So clearly, for example, it is conceivable and is also being experimented with to link various decentralized energy supply systems with each other in such a way that producers can exchange energy directly with each other in a neighborhood or in a region. Without prices being involved in the end? Prices are also involved, but the price is no longer the only child figure that is relevant, so to speak, but other qualities such as self-sufficiency, sustainability and so on and so forth also come into play. And the price is actually only a kind of calculation parameter to be able to verify exchange ratios, so to speak. I have now purchased this much electricity from you and you have purchased this much electricity from me. The result is that at the end of the month I still get this much money from you, but we leave it in the account, because next month it might be the other way around and so we might exchange things with each other in the long term, where hardly ever the money is actually transferred, so to speak. But it is of course an accounting level, is still aha, you have now received so much electricity from me, I have received so much electricity from you. So it's already conceivable to do something that happens according to other criteria, that happens more in civil society, that have a more complex, multidimensional idea of what they do and the price, it's in there, but it's no longer the only dominant element, which in the end is only about what's in the quarterly statement at the bottom of the total. And there are no plans that you know of for a quasi-revival of planned economy, state structures in the course of newly available technological means? No, I said it's difficult because the state is already planning all the time and allocating certain things, so to speak. Now we want to build a highway here and not there and so and so much. So you can well imagine that this has to be somehow decided in the context of a climate regime, who gets which so to speak pollution rights or how allocate, how the pollution that we can offer, that we can afford, so ecologically in society. And to do that simply via a market for CO2 emissions, that has failed grandiosely so far. But the state, somewhere, is saying, or a public body, somewhere, is saying, with our climate targets, only so and so much steel output or aluminum output or transport is conceivable. And now we have to organize it differently. That is already conceivable. But that's also how the state sees itself, seeing itself more and more as an entity that creates the framework conditions for markets. We are simply extremely far away from that. So I have the feeling that these more complex planning ideas actually come more from civil society than from the state. The state's task would then be to create the framework so that non-market-oriented actors can also act there. And here, too, we are still very, very far away from that. Perhaps as a final word. I like to end on a positive note. So the question, when you imagine the future, what makes you feel joyful? Yes, I believe that society's ability to deal with complexity and thus with diversity is increasing. Even if many actors have problems with it and work against it, so to speak, we live in a more diverse world. And we need means, which are often technological means, to be able to deal with this diversity. Be that a cultural diversity, but also a diversity of actors that it is people, animals, plants, weather systems and so on. And I think that's fundamentally a positive development. Wonderful. Felix, thank you very much for talking to us. You're welcome. That's Future Histories for today. Thank you very much for listening. You can find show notes and more at www.futurehistories.today. Join the discussion on Twitter under the hashtag Future Histories or on Reddit. Let me know what you think about it all and how you liked this episode here. Be sure to rate it well on any podcast platforms you use. For our Patreon supporters, there is a lot of additional material on www.patreon.com slash future histories. For example, every month I read in a text that fits the respective topic. So you can drop by there, too. See you next time. I'm looking forward to it.

Episode Keywords:

#FelixStalder, #JanGroos, #FutureHistories, #Zukunft, #Algorithmen, #KulturDerDigitalität, #Vernetzung, #Automatisierung, #Digitalisierung, #Data, #Daten, #CreativeCommons, #BigData