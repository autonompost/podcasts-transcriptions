# FH Complete S01E45 - Benjamin Bratton on Synthetic Catallaxies, Platforms of Platforms & Red Futurism (part 2/2) | Future Histories

Welcome to Future Histories. My name is Jan Gross, and what you are about to hear today is the second part of an interview I did with Benjamin Bratton in the context of a guest lecture that I was invited to hold at the Goldsmiths College in London. Before we start, I want to thank Carmen, Florian, and David for their kind donations. But now please enjoy the second part of the interview with Benjamin Bratton on Synthetic Catalaxy's Platforms of Platforms and Red Futurism. With more than one of my guests, I have hit a wall when asking how post-human political constellations could actually look like. Your writing and thinking, as we just experienced, is very illuminating in that regard, and it absolutely confirms my suspicion that the idea of the individual in the strict sense needs to be overcome in order to really get to fundamentally different sociopolitical arrangements. You advocate for the disindividuation of the user position. Could you explain what you mean by that? Sure. So this also goes back to the interface layer discussion that we had about the ways in which the interface layer can be, is a kind of map, operates as a kind of map of the way in which the system works. And I don't mean to, it sounds abstract, but I mean it, again, just to remind your listeners in this very simple sense of, there's lots of things you can do with your computer, but the graphical user interface presents you trillions of things you could do, build a bomb, do whatever. But the graphical user interface presents you with this mechanism of several dozen menu options that you sort of pick and choose. So it's, part of it is that in this, a little bit has to do with the history of interface design, there is a presumption that is built into the interface structure that we're talking of, that the way in which the system should present itself to human users is one human user at a time. That the fact that our culture of interface design is built around this sort of individuated, psychologized, single-serving homo sapien user, separated from other single-serving homo sapien users in and of itself is not, is not, we take it as a highly naturalized condition, but it's not necessarily, it's not necessarily historically, it's a highly contingent kind of arrangement. We have lots of interfaces that are meant for plural, a plurality of users, like traffic signals. A traffic signal is an interface system that doesn't make any sense for one user, I mean, for users at a time. The whole thing, like it's green light for me because it's red light for you and yellow, and something that we all recognize that the way in which the street interface is presented to me is conditional in relationship to other users. And we are, once you sort of, when it's green light for me, I am a green light user of the street and you are a red light user of the street. And therefore I know what you're going to do because you're a red light user. And we're all in a way interpolated through this interface as being red light, green light drivers in this sort of way. But also we understand and can negotiate the relationship with each other in this differential. If it's made clear. So the traffic again is a kind of simple example of what a plural interface would be. Now, in a stupid sense, Facebook attempted to make, in a way attempted in social network software, attempts to make interfaces that are meant for pluralities of users, but do so in a way that I think are actually, that actually concretizes and refortifies the fiction of the autonomous individual in relationship to those, in relation to the kind of thing. It produces a subjectivity. And in our society, as I mentioned, subjectivity and agency are utterly conflated. And within the kinds of interpolations of the user that you see in social media software, that this conflation is not only re-emphasized, but there's a construction of a social subjectivity, of a social subjectivity of this individuated user as this kind of encaps, self-encapsulated, sort of piloting creature of its own data and its own presentation of self-identity in relationship to this. One of the arguments that I sometimes make, find myself making arguments against the, making arguments against those whose primary concern with the social function of planetary's computation is in terms of surveillance. And the argument that I make is basically twofold. One is that, one is ultimately, I mean ultimately comes down to the term surveillance has been overinflated to describe so many different kinds of ways in which we would sense and model and construct images of the world, that it's beginning, that we have a kind of a surveillance, anti-surveillance bubble. That climate science and the way in which we would sense and model and structure the world is also a giant surveillance regime, if you really want to think about it this way. But the key point in relationship to what we're discussing is using planetary scale computation for the sensing, modeling and prediction of what individual homo sapiens are likely to wanna look at next and click on next in which kind of video or funny picture they're gonna wanna see next is incredibly tragic misuse of planetary scale computation as a whole. It goes deeper than the fact that these people are being surveilled, right? And so you think of the Shoshana Zuboff kind of critique. The problem is not just that you have these individual people are constructed and are being observed and tracked and surveilled and manipulated by these large scale platforms. This is a problem. This is a tragic misuse of the apparatus. The deeper problem is that society is, is it through this individuated interpolation of the user as the user as this figure is that society is being defined and constructed as the aggregation of these atomic individual subject, agent, actors in the first place. It's a misrecognition of what a society is. It's a misorganization of how it is that the relations between a social species such as humans is constructed. It is absolutely 100% predicated, I believe, upon the kind of self-congratulatory self-atomizing logics of liberal individualism and legal subjectivity, which of course is why someone like Shoshana Zuboff can't make this critique at that level because as someone from the world of the law, this formulation of society as an aggregation of self-transparent and self-sufficient atomic liberal individualisms is a sacred concept. And so the only thing that she's able to do is to say that these sovereign individuals have been manipulated into forming improper contractual relationships with these institutions, these manipulated contractual relationships, and that we need to resolve this jurisprudence to make these contractual relationships between these sovereign individuals and these platforms, in fact, more fair and more transparent. And I'm arguing is that, no, it's this hyper-individuation of society through that usually that's the problem and that's a much deeper problem. This is something I think that Ghani and I also agree on, I think, in terms of some of the things that he's written about Zuboff. I think it's that we probably have a common interest and common point about that as well. So part of the question then, to your question of this multiplication of the user layer here has to go with, is also an understanding of ways in which we can learn to think about our subjectivity and our agency in relationship to these systems that is not through this lens of this kind of figure. Now, the other thing I should say is two points on this, I think, examples that I think will kind of specify what I mean as possible alternatives here. One is, and I just finished this book on the pandemic, which I just wrote for Verso, which will be coming out in June. It's a book called The Revenge of the Real, post-pandemic politics. And one of the things I talk about in this book is what I call the epidemiological mode of society. That is one of the things that we learned from the pandemic was to see society the way epidemiologists see society. Epidemiologists don't see society as a bunch of atomic monads that happen to once in a while enter into contractual relationships with one another. They see society as a kind of landscape of organisms, each of which is a medium for the transmission towards one or the other. But they see it in terms of a kind of aggregate biological population scale that indeed is, I think, a good lesson for us to learn. It's a conceptualization of society that we should bring forward with us into a post-pandemic world, and I think is one that is more accurate, more social, and more viable than the kind of atomic individual version of this. The other point in terms of the plurality of interfaces, part of the longer term, what now, what should we do next question is, it's not enough to take back our data, because the data that we would be taking back is not the data we need. Because something like Facebook, and this is also where Zuboff and I would depart from each other, it's not enough that Facebook, because Facebook has mismodeled society so tragically, that its model of society as this aggregation of acquisitive atomic individuals is such an inaccurate model of society, that I could say, here you go, you got it all. Here is all of the information on all of the Facebook users, all four and a half billion of them or whatever, all the likes and all the cat videos and all the conspiracy memes and all the rest of this. Here it is, you have it. The government of whatever, EU, take it all. You've got it. Now use this to solve climate change. Go. You can't, it's the wrong data. Data about what it is about this model of society is not the data that we need. And this is kind of the other, so taking it back from Facebook is useless. What we need is to produce the right data. What we need is to actually understand what data would we need in order to actually enter in these transformations. And so simply think is that to simply, again, to reclaim the property of the data that Facebook has produced about us isn't enough because in essence it's the wrong data. So anyway, those are some thoughts on this question about the user position, the necessity for its plurality, the problems of its over-individuation and where my thinking sort of aligns and differentiates from some other people who have thought about it. Super interesting. And I will skip the question on the UBI, which would be a kind of riddle on how to actually apply it on. I can say something real quick about it if you like. So because it connects with the catalaxy idea. Part of, I think the more convincing argument about universal basic income also has to do with a kind of distortion. And I think it's a distortion in the value that society gets from its own participants, that there is a distortion in this. And I don't know if you call it a price distortion. I think that might be a bit awkward, but it is a distortion of value at the very least. When you have a comp lit PhD making coffee because that's the best job that she can get, even though the contributions that she could make to society are so much greater than the provision of cups of coffee, there's a distortion at work. Or when you have a genius, some genius kid doing other kinds of menial work because they need to support themselves, because the provision of basic food, shelter, healthcare or whatever is only available to them because they are trading most of their working hours for the provision of these basic things. Society as a whole is not getting from them what we should be getting from them. That there is an entire second economy worth of value that is being suppressed by the under participation of people doing things that are trivial compared to their real talents and capacities to contribute. And so I see this in some ways, you could see this in a way as a distortion of social value that is much greater than anything high equity side with the socialist pricing problem of how much the bananas should cost. This is trivial compared to what we're missing out from a lifetime of work of people doing something that quite easily could be done by machine if we were allowing them to do so. And I think when I think about the long-term value of something like UBI in terms of how to measure this, how to price it or kind of something like this, this is the sort of the bigger picture that I'm trying to keep in mind is not having to do with does it cost more or less than some other kind of welfare policy? Does it cost more or less in the relationship of this as well? But rather that in the long-term, what does the world look like when the true social catalaxy is possible? When it's actually possible for people to contribute to the production of society one with the other in ways in which half their time is not being produced on something idiotic and trivial and depressing. In order to get there and you touched up on it before already, we need different kinds of data. And so let's talk about data governance since this is actually essential to more or less all of the topics we discussed today. If I understand it correctly, you state that on a very fundamental level, there's basically always a kind of Potemkin ontology at work, which, and I quote you here, attempts to reduce reality to the level of complexity that the control system is capable of thinking with, end quote. You touched upon this already before quite at the beginning of our talk. I'm asking myself if this leaves us with a battle around the agency of different models that are used to interpret data and I'm also asking myself if and how one can successfully incorporate the reductiveness of one's model into the way the model is being applied. Sure. Yeah, good question. So there's a kind of a philosophical approach to this and more of a practical approach to this and maybe they can converge at some point, but just to sort of restate the question in a different way that, first of all, I should say that, I think part of the way in which approach this is to think of data not as something that's extracted, but as something that's produced. Data is produced about a phenomenon. Data doesn't exist out in the wild like strawberries where you can go and pick it and bring it back. It is produced in such a way that you can have 10 data scientists approach the same strawberry field and ask them to make a model of it and you get 20 different models of the strawberry field, each one of which might be statistically, scientifically accurate based on the questions and preconditions that it's asking and what it's looking for. And it's not necessarily one is more true than the other anyway more than like, you can say the word strawberry, you can say the phrase strawberry field in English and French and German and Dutch and Italian and they're all true. It's not that, obviously English is more true than all of the other ones. I'm joking, but none of the languages are more, the ontologies of the language are none of them are more true than the other. They are sort of producing a symbolic differentiation, a symbolic reference of this one. This is how I think about quantification as well in a certain sense, though that there are ways obviously then that the value of the model works not only in terms of its capacity for rhetorical rhetorically but also for its value in terms of realism and direct mimetic relation to the described. This is why we have things like astronomy and so forth. So what I mean by that is this, there's at least three different kinds of models that we want to be talking about here in terms of governance. There are descriptive models, predictive models and projective models. And they have different social functions, they have different epistemic functions and they have different political functions and scientific functions. A descriptive model basically is a model that's value is produced according to its correspondence to a mind external reality. In that heliocentrism is a reductive model of the solar system that it can be described by a series of styrofoam circles. It's a highly reductive model of the solar system but it's one that is more accurate in its representation of the solar system than a geocentric model of the solar system is in this way. Its correspondence to the real is the basis of its value. And that there's a politics of this in the sense to which it obviously has to do around the issues of climate science and our ability to, for example, is to produce a model at planetary scale of planetary material ecological processes that we believe to be accurate in the same way in which heliocentrism is accurate that would therefore give us confidence for acting recursively on its behalf because we have confidence in its representational capacity. But importantly, it's also one that, as I've mentioned before, that that model, just the way in which heliocentric astronomy could never have been produced without the technical abstraction of the telescope, the concept of climate change and planet earth science and that capacity for us to understand those processes with a realist model at any degree of sophistication is also dependent upon the technical abstraction of planetary scale computation. That's in fact, again, what planetary scale computation is for. There's also then a predictive model. So predictive models have to do with the value of the predictive model is not necessarily in how much they correspond to the real, but in how much they allow you to anticipate what is likely to happen. A simulation is to some extent, a kind of a blurring between the predictive and the realist model in this regard. It's particularly in terms of climate structures, but more stronger predictive model is more like what Wall Street does where they're trying to figure out what the likely price of Bitcoin is going to be next month or whether it's gonna make more sense to put money in Microsoft or Alibaba or something where the value of it has more to do with just the ability to predict what is likely to happen next. It's a kind of perspective heuristic. But then there's also then the projective model. The projective model is a normative model. It's a model that's not describing what is happening right now empirically. It's not a model that's describing what is most likely to happen. It's a model describing what should happen, what should happen, a drawing of what should happen. And so architecture, for example, most of its expertise is with the perspective model. It's you don't hire an architect to make a model of a building that already exists. You don't hire an architect to make a model of a building that's most likely to happen. You hire them to make a model of the building that should happen. And part of the role in I think both for politics, for design, even for political science is in the construction of projective models and prospective models based on descriptive models and grounded in descriptive models such that we have some ability not only to predict what is likely to happen in the case of climate catastrophe, but also to actually model what should happen. And so your question about where does governance and politics fit into this direction of data, I think there's at least three different answers to this question in relationship to the descriptive model, the projective model and the projective model that there's a logic for of how it is that we, for example, how it is that we might describe the planetarity that we want, describe what a viable planetarity is in a normative sense, in a projective sense, in an aspirational sense through the models, through its models that is also dependent upon the use of models to actually produce some kind of reliable description of the circumstance. And you can imagine, you can imagine, sort of skipping forward a little bit here, but you can imagine the ways in which this kind of, where algorithmic governance, which again would imply both how we govern algorithms and how we algorithms become part of the governing function more generally, how it is that algorithmic governance could become so ingrained in how it is that we function, how it is the city functions, how it is that states function, how it is the corporations function and so forth and so on that you can imagine something like a political party that is basically a software stack where you could imagine you've got four political parties all vying for being for the city government. And each one of these political parties is in essence running a simulation of what they think the city of the next few years is going to be based on the variables that they think is important based on what their political ideology is, running this in a sort of massive simulation of what this future would be. And then the voters decide, I'm going to like party number three wins the election. And once you win the election, your servers go live. And that in fact, and that becomes the basis of the city. I mean, this is one way of sort of thinking about that, thinking about that kind of dynamic. There's a bigger question though, again, around sort of the longer term arc of this of not just how it is that the data can be used within the existing political institutional structures that we already have, city governments, state governments, country governments, UN, whatever, but the ways in which that at a certain scale and at a certain point that it becomes a qualitatively, it constitutes and enables qualitatively different forms of governance per se that could not have existed otherwise. And that not only demand, but indeed already constitute other institutional forms in the first place. And it should be clear while I'm interested in the former, how would it, with my servers going live scenario, my longer term interest is in the latter. My longer term interest is in how planetary scale computation as a whole constitutes the basis by which it becomes possible, perhaps for the first time for an always already planetary scale human society to be able to produce reliable technical abstractions about its own processes at a scale commensurate with those processes itself, which is to say planetary scale, that would allow for a reorganization of some of the fundamentals of a economic, political and ecological circumstance. That is where I try to link this question of planetary scale compilation to some of the questions of planetarity per se that animates the work of people like Spivak or Mbembe and where they have an emphasis on literacy, which in my mind is a bit more numeracy. The bigger question of how it is that it's possible for collective reason to be made to manifest itself. How is it possible for society to compose itself? And I think one of the things we've seen in the pandemic in the West, the calamity of how the West has dealt with the pandemic is a dramatic example of how much the West is now unable to compose itself, unable for a society to organize itself in a kind of way. Certainly the United States and UK and Italy and Germany are some of the more tragic examples of this. But this is the real question of how can a society sense itself so that it knows what's going on? How can it model itself in different ways so that it can produce multiple and different conceptions of its own collective image? How can it simulate its possible futures so that it understands what its possible capacities are? And then how can these models of the future recursively act back upon that society so that it is able to manifest this capacity for self-composition and collective reason? That's what data governance is for. That's what data governance is, I think, to be in the long-term. Fantastic. Benjamin, I would like to leave some time for the students to ask questions, but there is one closing question I'd like to ask all of my guests. And that is, if you think about the future, what makes you joyful? I'll just give a simple answer. My son, my 12-year-old son, I have more the idea of thinking about just the process of watching him understand more and more of how the world works around him and seeing his interest and frustration in the rest of this. And so, probably the ability to think about the world that he will grow up in and the world that he will sort of go through and just sort of think about it on that arc, it kind of gives you, there's just a sense of, there's a sense of the stakes of what's at stake, what's at stake for the future and where, sort of, to mean. So, I mean, that's, I mean, it's a simple, stupid answer, but it's really, you know, like the most, that kind of like immediately honest one. You know, more philosophically, it kind of depends on what we mean by the future. You know, there's a, we have a problem of thinking about the future in terms of the scale of human lifespans, but, you know, the environments and ecologies in which we live operate on a different kind of scale. But, I don't know, there's more specific things, but I'll just sort of leave it, I'll kind of leave it at that, maybe. Fantastic. Benjamin, thank you so much for your time and thanks a lot for being part of Future Histories. My pleasure. Thank you for inviting me. Thank you so much for listening. This has been the interview with Benjamin Breton that I did in the context of my guest lecture at the Goldsmiths College in London. Thanks a lot to Mattia for inviting me and thanks a thousand times to Benjamin for taking part. I could not have imagined a better transition from 2020 into 21 than having Benjamin Breton as a guest in the last episode of 2020 and then the second part as a start for this year, so I absolutely love it. You will now hear the discussion that took place afterwards and since we didn't have any consent from the students in terms of releasing their questions as the original audio, I will simply restate them in order to avoid any problems in terms of privacy. The first question comes from Mattia, so this one I won't have to restate, but then afterwards I will do so. Please enjoy. Perhaps I can ask one question that made all together what has been said in such detail. You presented very poignantly the problem of not thinking technology, politics, and subjectivity in the classic. I absolutely agree it is paramount that we look at these issues. You did say something at some point that seemed to me to go against the grain of the rest of what you were saying and probably just the way I heard it, but I think it's important to expand on it. At some point you said that your best guess or your look forward was that the superstructure would prevail over the base structure in terms of where agency and gravity are located. No, no, no, no, no, no. The other way that I was arguing, just on the point of cause and effect, I was suggesting that if we want to ask the question in the totally over reductive way or if I want to answer the question in this very over reductive way of whether or not the culture causes infrastructure, infrastructure causes culture, and where we should invest our time, I would argue I'm on team base causes infrastructure, base causes superstructure, base causes superstructure, right, so that the geotechnology causes geoculture was more where I was trying to come down on that. The first student question is, since you were talking about pricing the future, I was interested in how that applies to Shojana Zuboff's surveillance capitalism approach and how pricing the future could impact individuals and the kind of ethical implications of that. Right, so you're right. I think it's certainly correct that this question of pricing the future is something that already exists. It's not something that would be, it's already part of the way in which our economies work. Our economies in many respects are, whether that's the stock market or pricing the future attention of an individual user or something like that, that pricing the future is not something that would need to be introduced ex nihilo or invented from tabula rasa. It's actually already part of the system. And then what I'm suggesting then is that the recognition of this as a strategy and the recognition of this as something that we have already built a degree of sophistication and capability with is something that should not be overlooked if the goal of the deployment of that capacity is for something other than predicting whether or not a user is, the purpose of this is for rectifying the pathologies identified as part of the capless pricing problem as opposed to predicting whether or not a user is likely to look at this particular cat video or that particular cat video, that the relevancy of this technical capacity is being suppressed by the stupidity of the way in which we are using planetary scale computation. To underscore the point, one of the differences I have with Zuboff on this point is, whereas I think for Zuboff, the individual, the privacy and sovereignty of the individual as the core unit of planetary scale computation is not only left unquestioned, it's in fact re-fortified as the basis of something that needs to be re-privatized, that needs to be re-individuated, that needs to be further individuated from the supervisory manipulative molestation from the platforms. I think deeper and probably more fundamental critique of the way planetary scale computation works is that this unit of the self-sovereign private and privatized individual as the space unit is itself a kind of original sin for how we have organized planetary scale computation. And there is no solution, no kind of social dilemma, restructuring our privacy, re-fortifying our anonymity in relationship to one another in relation to the system that will ultimately solve that problem. I think in many ways it's actually re-fortifying that problem by re-conquering this idea. So saying the same thing in a few different ways, but the more fundamental point of the predictive capacity, the kind of preemptive logic, the pricing of preemption, even within planetary scale computation is something that exists both in terms of something we might admire, as in climate science, where we are preemptively simulating the future through scenarios, and something we might not admire in terms of the preemptive simulation of what individual users might click on. But you're correct that this capacity for preemption already exists. The next student question is, picking up on your last point of predictive, prescriptive, and projective models of governance, I was curious how well you think humans are even able to determine how well computers or algorithms are modeling. So when we move into global computation, or we've already been in global computation, how well we are actually able to evaluate models going forward. Taking chess or Go as an example, which are comparatively extremely simple systems, but we already struggle to evaluate how well computers model them. They play a lot of moves that we think look wrong, and only turn out to be effective much later on, or much further down the road at the end of the game. So I'm wondering from our perspective, how well we are able to evaluate how good the models actually are. Yeah, it's a really important question, and one that is doesn't have a, one that gets more complicated and more interesting the more you continue to ask it with greater specificity. So thank you for that. I think it has a bit of a question around, if we keep this differentiation as I proposed between descriptive, predictive, and projective models, we might have different ways of answering that. If that is a descriptive model, the function of which has the ability, is supposed to be a kind of reductive, a reductive approximation of a mind-independent reality, like heliocentrism or something, at least we have the experimental method to try to test the model over and over again, and to try to find out, to try to test the descriptive capacity of the model through its predictive capacity. That is, if this model is true, then it should be able to predict what is going to happen under experimental conditions, X, Y, or Z. And if under those conditions, it's not able to predict it, then we realize that its description might be a bit wrong. And so, you know, science marches forward through that kind of testing capacity. The projective models are ones that are a little bit more difficult, but it's also interesting to speak, since we're talking about temporality of models, you know, one of the interesting things about climate models is the way in which the future is validated against the past. And what I mean by that is this, the ways in which, and I'm sure you're probably aware of that, but I'll just mention the point in the context of this question, the way in which we have some degree of confidence in why it is that we think that the world will look a certain way in 2040 or 2050, given whatever ecological variables we wish to emphasize, is because you test that model against past data. And so, for example, you'll take, here's the model, what we think is gonna happen between 2020 and 2050 in that 30-year arc based on these variables. Let's take that model and try to predict what would happen between 1820 and 1850 based on what that model should tell us what would likely happen. And then you go back and look at what did happen in 1850. And if the prediction turned out to be true, if the prediction of the future turns out to be able to predict the past, then you have a degree of confidence in the way in which it would work. Now, your question, though, about the kind of the patterns within this, and I think a lot of it has to do with the way in which models allow us to deduce and abstract patterns. And as I've said a few times, this capacity for technical abstraction of collective technical abstraction is one of the important epistemological functions of planetary scale computation. And I'll just emphasize the point here. There is, in many ways, the arguments I'm making have as much to do with the way in which the technologies function instrumentally. That is, what are we able to do with them? But equally importantly, if not more, is the epistemological function of the technologies. How is it that through using of these technologies, we're able to know things that we wouldn't have been able to know otherwise? And in many cases, it's the epistemological function of planetary scale computation that's most important, that's more important in the long run, I think. But your examples of AlphaGo and the chess and the rest of this bring up an important point, and that is, in theory, the function of something like an AI would be that it is able to see patterns in the world and patterns in the data that we would not be able to intuit on our own. Otherwise, why would we bother building them? That it's able to see something we can't see. But what happens when it seems to see something that seems so weird and unlikely that we're not sure whether or not it's stupid or ingenious? We don't know whether or not there's obviously a deep error in our presumptions and we've produced a machine that's incapable of producing nothing but stupid noise, or we've produced this genius machine that's able to see patterns that we never would have seen otherwise and cause and effect relationships between dynamics that we've overlooked for our entire evolutionary career. How would we know? How would we know whether a totally unlikely pattern is trivial or fantastically important? And someone needs to invent a name for this within AI, I think, where the AI finds a pattern that we don't recognize as being logical and yet we have to, in essence, take this, we have to decide in a way to decide to believe the pattern or decide not to believe it in a way. And this is again part of the epistemological complexity of these technologies. The example that you gave of AlphaGo is the key one here, is the one that everyone refers to in relationship to this phenomenon, is that, and for those that, the computer made this move that at the time, early in the game, move 33, I think, of the game against Lee Sedol, that everyone thought, oh my God, the machine, it's broken. The AI is broken. This is very embarrassing for Google. Right in the middle of game three, their computer went nuts and placed a stone way off in the middle of nowhere and, oh shit, this Google stock's gonna go down, everything's horrible. And then, lo and behold, over a period of the rest of the game, the whole game arcs towards this one stone in such a way that it turned out to be ingenious and as the Sedol, the Go player said, no human ever would have made that move. And it was also seen example of the ways in which it was something that we could take as optimistic, at least in this regard, that part of the problem of reinforcement learning-based AIs and, indeed, using big data models of the past in order to simulate the future is that you run the risk of, in essence, just replicating pre-existing patterns and presuming that pre-existing patterns will be continuous into the future in ways that, in essence, make it so. You're enforcing the possibility that pre-existing patterns would be the future patterns by, in essence, predicting it and then acting on those predictions. And so some of the people who do serious work around AI bias, there's a lot of un-serious work around AI bias too. People who do serious work around AI bias, a lot of the work is based on this question that you have previous patterns and that making predictions and policies and the previous patterns will enforce this, which is a real and serious general problem about the ways in which previous models replicate themselves. What it showed with the AlphaGo thing was at least the possibility that, no, novelty is possible, that there is some way in which, even through the reinforcement learning methods of the built AlphaGo, that in this production and its conceptualization and its prediction and its sort of stochastic perspective, prediction of what the right move would be. And in this case, the right move would be part of what we're calling a projective model, a normative model, not a descriptive predictive, but a normative model of what the right move would be, that it could come up with something that we would never have thought of itself. That it was an example that AI is capable of being a kind of epistemic machine that's capable of producing ideas, producing novelty, producing patterns we wouldn't have been able to see. And in that sense, I take the weird alienation we may have had from the logic of that move as very optimistic, as very, very good news in this kind of regard. But again, your question about how do we validate the model and how do we not only validate the model, whether it's come up with something stupid, but how do we ensure that even the validity of past models, how do we outsource size ourselves from this position where the previous models are simply replicating themselves, becoming recursive in the bad sense, becoming a kind of echo effect where we're using all of these systems as simply a way to enforce the condition by which the future is just an echo of the past. This is also a way in which our relationship to these models must be seriously interrogated. The other side of this is, so you could think of it in this way, one, how do you use these models in such a way that you can act upon their implications recursively, which is what we want to do with climate change models, but also how do you ensure that these models continue to accelerate the space-based search function by which they're able to identify and look for and produce novelty, produce innovation, produce something that hasn't happened before? And I think a lot of the philosophy of computation of the next several decades will be based upon that conflict between modulating recursion and modulating face-based search and novelty production. So the last student question was asked with a strong British accent, and I hope I managed to capture the essence. The question brings up Andrew Geim and how he used very off-the-charts methods and ideas in his so-called Friday experiments when discovering graphene. In relation to that, AI seems very methodical and logical, and so there's the question of how AI could incorporate these acts of randomness, and also when it does so, how it knows when it found anything of value. Yeah, I mean, it's the same thing with the Friday experiments, is that many of them, the value of them is uncertain, where it may be that it's not as though... It's not as you can expect that through some sort of randomization process that by definition, some kind of fitness, some sort of process will necessarily hold. It's the same kind of dynamic of mutation and evolution, I suppose, in that regard. Most mutations are useless. But that's probably the same for the AI, too, that you're expecting them to every time come up with this is probably inaccurate. But I should say that there's a whole other lecture to describe about the ways... I think that also going forward, this term AI will hopefully sort of fork into a much more specific terminology, that when we're talking about the kinds of... The things that different kinds of AI are good for, where reinforcement learning versus other kinds of techniques, Judea Pearl and other people, and Gary Marcus are trying to bring back a more symbolic modeling type of structures here as well. That AI is lots of different technologies, and AI is lots of different techniques, and AI is built upon its sensing capacity as much of its calculative capacity, that the question of how can we guarantee AI is heading in one direction in terms of that, I think it will be able to ask the question with a greater deal of specificity and context. So, and I don't know, I probably would not agree with the idea that in general, AI is just a kind of brute force rationalization as its only capacity and its core capacity. But at the same time, I might suggest that there's something quite amazing about brute force rationalization in its own sense, and that it may be that many of the most interesting forms of mind-bending, pattern-finding abstraction that we learn from AI is from its brute force rationality, that we shouldn't assume that rationality and creativity are somehow the opposite of one another. In many cases, they're quite at home, and that many of the ways in which we kind of lapse into, on the human side of the ledger, lapse into convention and cliché and stereotype and muddy thinking and sort of replicating the known is because of a reliance on sort of arbitrary, irrational processes of semiotic symbolization, that in many cases the fact that we're constantly repeating ourselves is more to do with culture than it has to do with math. It may be that rationality does provide a certain kind of escape route that we shouldn't overlook. There was a question about Ted Chiang. People are probably most familiar with Ted Chiang, he's the author of the movie Arrival was based upon. Anyone who enjoyed that movie, I would strongly suggest reading the short story, it's called The Story of Your Life, which is a really fascinating story. It's based around this idea of artificial linguistics, and linguistics. It's a short story that you kind of need to know a little bit about linguistics theory in order to make your way through it, or if you make your way through the story, you will have learned quite a bit about linguistics theory by reading the story. But in terms of things like, there's a lot of interest of late, of course, in things like GPT-3 and other ways in which the algorithmic production of different kinds of language as a predictive model, language as structuring model of here as well. I think if we want to go deeper on this question of where it is that the technologies of thought and the technologies of inscription, technologies of modeling, technologies of description, the basis of inscription and ontology in relationship to some of these kinds of dynamics, the relation between our own kind of cognitive prospection, the ways in which our models of the world, the Bayesian prediction functions of our models of the world are the basis of our own neuroscience, and how that gets inscribed into our capacities for signification and communication through language. This field of artificial linguistics is one that would be certainly a rich area for anyone to spend some time with. Thank you very much. It has been extremely interesting. Oh, thank you. Thanks for the invitation. Thanks a lot, Benjamin. That was our show for today. Thanks a lot for listening. If you want to support future histories, you can do so on Patreon, or you can just simply tell a friend that you heard of the show and that he or she might like it as well. Have a good time and hear you in two weeks. 

Episode Keywords:

#FutureHistories #Podcast #DataPolitics #BenjaminBratton #JanGroos #Interview #Society #SyntheticCatallaxy #Catallaxy #PlannedEconomy #Planwirtschaft #DesignAndGeopolitics #Terraforming #Strelka #CyberneticRevolutionaries #PlatformCapitalism #NickSrnicek #EdenMedina #Democracy #SurveillanceCapitalism #Überwachungskapitalismus #ShoshanaZuboff #ProjectCybersyn #StaffordBeer #BigData #EvgenyMorozov #DigitalSocialism #SocialistCyberneticPlanning #Cybernetics #Cybersyn #Kybernetik #SocialistCalculation #sozialePlanwirtschaft #kybernetischeGovernance #OpsRoom #digitalePlanwirtschaft #Hayek #User #TheStack #Platform #PlatformOfPlatforms #RedPlenty #FutureHistoriesInternational
