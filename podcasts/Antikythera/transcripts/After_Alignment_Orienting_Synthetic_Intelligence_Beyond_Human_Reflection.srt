1
00:00:30,000 --> 00:00:58,440
The

2
00:00:58,440 --> 00:01:04,120
history of AI and the history of the philosophy of AI are deeply intertwined, from Leibniz

3
00:01:04,120 --> 00:01:07,600
to Turing to Hubert Dreyfus to today.

4
00:01:07,600 --> 00:01:12,960
Thought experiments drive technologies, which in turn drive a shift in the understanding

5
00:01:12,960 --> 00:01:19,040
of what intelligence itself is and might become and back and forth.

6
00:01:19,040 --> 00:01:27,440
But for that philosophy to find its way today, for this phase of AI, which it needs to, that

7
00:01:27,760 --> 00:01:33,200
finding its way needs to include expanding from the European philosophical tradition

8
00:01:33,200 --> 00:01:38,120
of what AI even is.

9
00:01:38,120 --> 00:01:42,800
And the connotation of this, from the connotation of artificial intelligence drawn from the

10
00:01:42,800 --> 00:01:49,000
Deng era in China was as a kind of, in relation to a kind of industrial mass mobilization.

11
00:01:49,000 --> 00:01:54,320
The Eastern European includes what Stanislav Lem called existential technologies, just

12
00:01:54,320 --> 00:01:59,760
as the Soviet era, it meant something more like governance rationalization.

13
00:01:59,760 --> 00:02:06,600
All of these contrast with the Western individualized and singular anthropomorphic models that dominate

14
00:02:06,600 --> 00:02:10,020
contemporary debates still today.

15
00:02:10,020 --> 00:02:17,360
So to ponder seriously the planetary pasts and futures of AI, must not only, it must

16
00:02:17,360 --> 00:02:23,960
extend and alter our notions of artificiality as such, intelligence as such, and must not

17
00:02:23,960 --> 00:02:30,760
only draw from this range of traditions, but also to a certain extent, almost inevitably,

18
00:02:30,760 --> 00:02:33,960
also leave them behind.

19
00:02:33,960 --> 00:02:38,400
What Turing proposed in his famous test as a sufficient condition for intelligence, for

20
00:02:38,400 --> 00:02:44,520
example, has become instead, solipsistic demands and misrecognitions.

21
00:02:44,520 --> 00:02:51,920
To idealize what appears and performs as most quote human in AI, either as praise or as

22
00:02:51,960 --> 00:02:58,960
criticism, is to willfully constrain our understanding of what machine intelligence is as it is.

23
00:03:02,880 --> 00:03:06,360
And this includes language itself.

24
00:03:06,360 --> 00:03:11,320
Large language models and their eerily convincing text and prediction capabilities has been

25
00:03:11,320 --> 00:03:16,840
used to write novels and screenplays, to make images in movies, songs, voices, symphonies,

26
00:03:17,600 --> 00:03:22,480
and are even being used by biotech researchers to predict gene sequences for drug discovery.

27
00:03:22,480 --> 00:03:27,520
Here at least, the language of genetics really is a language.

28
00:03:27,520 --> 00:03:33,000
LLMs also form the basis of generalist models capable of mixing inputs and outputs from

29
00:03:33,000 --> 00:03:38,120
one modality to another, interpreting what an image sees, so instruct the movement of

30
00:03:38,120 --> 00:03:41,320
a robot arm and so forth.

31
00:03:41,320 --> 00:03:46,840
Such foundational models may become a new kind of public utility around which industrial

32
00:03:46,840 --> 00:03:52,800
sectors organized, what we call cognitive infrastructures.

33
00:03:52,800 --> 00:03:56,400
So with their speculative philosophy then?

34
00:03:56,400 --> 00:04:02,440
Well, I honestly don't think that society at present has the critical and conceptual

35
00:04:02,440 --> 00:04:06,720
terms to properly approach this reality head on.

36
00:04:06,720 --> 00:04:12,120
As a co-author and I wrote recently, quote, reality overstepping the boundaries of comfortable

37
00:04:12,120 --> 00:04:17,560
vocabulary is the start, not the end of the conversation.

38
00:04:17,560 --> 00:04:22,960
Instead of groundhog day debates about whether machines have souls or can think like people

39
00:04:22,960 --> 00:04:28,760
imagine themselves to think, the ongoing double helix relationship between AI and the philosophy

40
00:04:29,440 --> 00:04:35,440
to do less projection of its own maxims and instead construct newer, more nuanced vocabularies

41
00:04:38,320 --> 00:04:46,320
for analysis, critique and composition based on the weirdness right in front of us.

42
00:04:46,320 --> 00:04:52,320
And that is really the topic of my talk, the weirdness right in front of us, and the clumsiness

43
00:04:52,320 --> 00:04:55,440
of our language to engage it.

44
00:04:55,520 --> 00:05:01,760
Toward that, let me say that, again, that the impasses over whether machine intelligence

45
00:05:01,760 --> 00:05:07,280
as mind or sentience or consciousness are, in fact, impasses in our language and our

46
00:05:07,280 --> 00:05:14,160
imagination more than they are in what is actually happening has happened and will happen.

47
00:05:14,160 --> 00:05:21,280
The productive interest instead may have less to do with how AI adheres to precedent models

48
00:05:21,360 --> 00:05:27,760
than what it reveals and does about the limitations of those models.

49
00:05:27,760 --> 00:05:34,240
So I say reveals and does. Why the split? Well, instead of presuming that ideas are

50
00:05:34,240 --> 00:05:40,280
first formed and then tools are wielded to act upon them, we may observe instead that

51
00:05:40,280 --> 00:05:46,960
different tools, or as well, that different tools make different ideas possible. It's

52
00:05:46,960 --> 00:05:52,000
not just that they invite different dispositions towards the world, they literally make the

53
00:05:52,000 --> 00:05:59,000
world conceivable in ways otherwise impossible. Per Lem, Stanislaw Lem that is, and his notion

54
00:06:01,900 --> 00:06:08,900
of an epistemological and instrumental technology distinction, we might say that some kinds

55
00:06:10,000 --> 00:06:16,120
of technologies have the greatest social impact in what they do and enable in artificially

56
00:06:16,120 --> 00:06:22,720
transforming the world. These are instrumental. Others, however, have greater social impact

57
00:06:22,720 --> 00:06:29,720
in what they reveal about how the world works. These are epistemological technologies. Telescopes

58
00:06:30,160 --> 00:06:37,160
and microscopes are good examples. Yes, they allow perception of the very large and the

59
00:06:37,280 --> 00:06:44,280
very small, but more importantly, they enable Copernican shifts in self-comprehension,

60
00:06:44,960 --> 00:06:51,960
grasping of our very selves as part of planetary and indeed extraplanetary conditions. With

61
00:06:53,120 --> 00:06:59,040
such shifts, it was possible to orient not only where the planet is, but thereby where

62
00:06:59,040 --> 00:07:06,240
and when and what, quote, we are, and of course, also thereby putting into question that collective

63
00:07:06,240 --> 00:07:13,240
pronoun itself. Taken together, again, these may be called epistemological technologies.

64
00:07:14,280 --> 00:07:21,280
Again it is certain that computation is artificially transforming the world in the form of a accidental

65
00:07:21,640 --> 00:07:28,640
megastructure that shifts politics and economics and cultures in its own image. However, computation

66
00:07:29,440 --> 00:07:36,400
is also an epistemological technology that has and does and will reorient the course

67
00:07:36,400 --> 00:07:43,400
of what a viable planetary condition may be. In fact, we may say that the planetary as

68
00:07:43,880 --> 00:07:50,880
such is an image that emerges via computation, via, for example, climate science, which is

69
00:07:51,480 --> 00:07:56,800
based of course on planetary sensors and models and most of all super computing simulations

70
00:07:56,800 --> 00:08:03,800
of the planetary past, present and future. The very idea of climate change is, in other

71
00:08:03,880 --> 00:08:10,880
words, an epistemological accomplishment of planetary computation. And thus, so indirectly,

72
00:08:11,040 --> 00:08:18,040
is the notion of the Anthropocene and of humanity as a terraforming subject. And that is what

73
00:08:18,960 --> 00:08:25,960
is at stake. So, full disclosure then, in this regard, my own approach to philosophy

74
00:08:27,720 --> 00:08:32,440
for a philosophy of technology then can then be understood as, in a sense, an inversion

75
00:08:32,440 --> 00:08:39,440
of the malaise carefully lamented by Heidegger for whom technical reasons alienation of

76
00:08:39,440 --> 00:08:46,360
the intuitive givenness of the world is its and indeed our downfall. Whereas for me, I

77
00:08:46,360 --> 00:08:53,360
think he has it backwards. That alienation, that Copernican weirdness achieved through

78
00:08:54,240 --> 00:09:01,240
technological mediation of our cognition has been and will be a path to access anything

79
00:09:01,240 --> 00:09:08,240
called being. Once again, where we are, when we are, where we are and how we are. But clearly,

80
00:09:13,800 --> 00:09:20,800
AI is not only disclosing these, it is also forcing us to question them. We then experience

81
00:09:21,360 --> 00:09:28,360
different kinds of, if you like, AI overhangs and arguably underhangs. An AI application

82
00:09:28,680 --> 00:09:35,100
overhang means that the technology is capable of doing things, that a society has a hard

83
00:09:35,100 --> 00:09:42,100
time integrating, modeling, adopting for any number of good or bad reasons. What one of

84
00:09:42,840 --> 00:09:48,800
our, in Antikythera program, one of the meta-science projects by Darren Zhu, Will Freudenheim and

85
00:09:48,800 --> 00:09:55,800
Imran Sekolala calls an AI epistemic overhang, on the other hand, means that AI is capable

86
00:09:56,440 --> 00:10:03,440
of discovering things, knowing things, disclosing things. That human science has a hard time

87
00:10:05,200 --> 00:10:11,840
modeling and integrating and adopting. The latter, we would argue, is not just an issue

88
00:10:11,840 --> 00:10:18,840
for science. In its generality, it is many ways the focus of this talk. So first, about

89
00:10:18,840 --> 00:10:25,840
alignment. What does it mean to ask machine intelligence to align to human wishes and

90
00:10:38,920 --> 00:10:45,840
self-image? Is this a useful tactic for design or a dubious metaphysics that obfuscates how

91
00:10:45,840 --> 00:10:52,360
intelligence as a whole might evolve? Given that AI and philosophy of AI have evolved

92
00:10:52,360 --> 00:10:58,120
in a tight coupling, informing and delimiting one another, how should we rethink this framework

93
00:10:58,120 --> 00:11:04,520
in both theory and practice? Or let me put it somewhat differently. If the stack describes

94
00:11:04,520 --> 00:11:11,520
the topology of planetary scale computation, the question that it implies is what is planetary

95
00:11:12,040 --> 00:11:18,920
scale computation for? We might insist that the emergence of machine intelligence must

96
00:11:18,920 --> 00:11:26,080
be steered toward a kind of planetary sapience in the service of viable long-term futures.

97
00:11:26,080 --> 00:11:33,080
And for that, instead of strong alignment with human values and superficial anthropocentrism,

98
00:11:33,240 --> 00:11:40,240
this steerage of AI means treating these humanisms with some nuanced suspicion and recognizing

99
00:11:41,240 --> 00:11:48,240
instead a broader potential. At stake is not only what AI is, but what a society is and

100
00:11:50,880 --> 00:11:57,880
indeed what either one is for. And what should align with what? The term synthetic intelligence

101
00:12:00,080 --> 00:12:05,600
in our parlance refers to the wider field of artificially composed intelligent systems

102
00:12:05,600 --> 00:12:12,600
that both do and do not correspond with humanism's traditions. These systems, however, can of

103
00:12:15,520 --> 00:12:22,520
course complement and combine with human cognition and intuition and creativity and abstraction

104
00:12:22,640 --> 00:12:29,640
and discovery. But as such, both are forever altered by these amalgamations.

105
00:12:30,640 --> 00:12:37,640
Now, machine intelligence itself may or may not be, strictly speaking, artificial. Now,

106
00:12:40,160 --> 00:12:45,920
if we mean artificial as something that is composed deliberately by some kind of precedent

107
00:12:45,920 --> 00:12:52,920
intelligence, then AI is a form of machine intelligence that is so composed. But it is

108
00:12:53,600 --> 00:13:00,200
perhaps not so simple. We recognize every day that there are forms of machine intelligence

109
00:13:00,200 --> 00:13:07,200
that are genuine and yet evolved without deliberate design. Just look around the city. And second,

110
00:13:10,480 --> 00:13:17,040
we can zoom out and see that the intelligence that does any artificializing is itself evolved.

111
00:13:17,040 --> 00:13:24,040
And so, its artifacts are then also part of this evolutionary phylogeny. Now, the primary

112
00:13:24,560 --> 00:13:31,200
significance of this for the present talk is that we should not, repeat not, see AI

113
00:13:31,200 --> 00:13:38,200
simply as a direct reflection of human ideas, culture, and economics. Nor, vis-a-vis alignment

114
00:13:38,480 --> 00:13:45,480
should it be. Put directly, the extent that it directly reflects human culture is not

115
00:13:45,520 --> 00:13:52,960
a goal, nor is it a reality. The extent to which it departs from human culture is not

116
00:13:52,960 --> 00:13:59,960
what we might name a disaster, and nor is it a hypothetical. That departure is, in fact,

117
00:14:00,080 --> 00:14:07,080
our reality. That human intelligence must or should orient, which is our preferred term,

118
00:14:07,080 --> 00:14:14,080
AI toward viable planetary futures, is essential. But again, that viability does not arrive

119
00:14:18,440 --> 00:14:25,000
simply from making AI resemble us or admire us or be subservient to our wishes. To the

120
00:14:25,000 --> 00:14:32,000
contrary. Now, there is obviously a Venn diagram overlap between AI ethics on the one hand,

121
00:14:37,480 --> 00:14:44,040
and AI alignment on the other. But there is also, between them, a kind of impasse. Or

122
00:14:44,040 --> 00:14:51,040
at least in principle, a contradiction between their visions. AI ethics, particularly in

123
00:14:51,040 --> 00:14:58,040
guises associated with popular pundits and so forth, insists, when coaxed, that AI is

124
00:14:58,500 --> 00:15:05,400
simply the reflection of human societies. From unjust biases and unequal economic systems

125
00:15:05,400 --> 00:15:11,400
that produce it. It seeks to demystify AI as just us. Nothing more. Nothing less. As a

126
00:15:15,360 --> 00:15:21,360
theory, it asks us to identify in principle any technology, especially AI, as ontologically

127
00:15:23,760 --> 00:15:29,760
artifactual. AI alignment, on the other hand, and again, my abstraction of these correlates more to

128
00:15:29,760 --> 00:15:35,760
the popular and populist guises than any deeper serious research. Alignment may hold that the

129
00:15:40,800 --> 00:15:48,160
potential existential, or at least serious, risk of AI is based in the fact that it is now so

130
00:15:48,160 --> 00:15:55,160
deeply divergent from human cultural values and norms. And that securing a safe future means

131
00:15:55,160 --> 00:16:01,160
actually gluing it to those values and norms. So you see the theoretical problem. How can AI be

132
00:16:04,320 --> 00:16:09,840
both automatically reflective of human biases and values and dangerously unreflective of human

133
00:16:09,840 --> 00:16:15,840
biases and values? How can the positions that straddle my cartoon binary hold the apparently

134
00:16:15,840 --> 00:16:21,840
contradictory conclusion at once? How can we observe that, again, that AI is us, and this is

135
00:16:25,420 --> 00:16:31,420
bad, and simultaneously that it is not us, and this is also bad? Well, it can be both, but only

136
00:16:33,920 --> 00:16:40,800
if we would radically qualify and specify what we mean by alignment and allow for alignment such

137
00:16:40,840 --> 00:16:48,840
that AI not only bends to social norms, but also for which society evolves in the positive sense

138
00:16:48,840 --> 00:16:54,840
in relation to the epistemological and instrumental affordances of AI. So before I go into a little

139
00:16:57,920 --> 00:17:03,960
bit more detail about what I envision, let me further contrast and clarify what I don't

140
00:17:04,000 --> 00:17:10,000
envision, what I don't mean. The possibly very sensible perspective that AI and potential AGI

141
00:17:13,200 --> 00:17:20,320
pose an existential risk and so therefore should be the focus of planetary concern for

142
00:17:20,320 --> 00:17:27,720
geopolitical and geosocial debate has not always been well represented in the public sphere as it

143
00:17:27,720 --> 00:17:33,720
should be. AI moral panics overwhelm imaginative reason in what amounts to several simultaneous AI

144
00:17:38,600 --> 00:17:44,600
moral panics competing for attention, oxygen and hegemony. These range from rather predictable

145
00:17:45,880 --> 00:17:51,880
American culture war templates that focus less on what is said than who is saying it to a strange

146
00:17:52,040 --> 00:17:58,040
inversion where some of the most well-known for rapturous, millenarian visions of AI have rotated

147
00:18:01,080 --> 00:18:08,040
to apocalyptic, eschatological ideas without missing a beat. From the singularity to the

148
00:18:08,040 --> 00:18:14,040
Unibomber and back is the new horseshoe theory of AI politics. The hype doom binary imploding into

149
00:18:15,000 --> 00:18:21,000
itself. Elsewhere, but not too far away, many public intellectuals spent the spring of 2023 in a

150
00:18:24,440 --> 00:18:32,280
perhaps well-meaning piety game to see who could say that we are in fact more fucked. You say we

151
00:18:32,280 --> 00:18:41,320
are fucked because of AI. Well, I say we are more fucked than thou. Sometimes the game

152
00:18:41,320 --> 00:18:47,960
eventuated in public letters of concern of varying quality and intellectual legitimacy,

153
00:18:47,960 --> 00:18:52,840
I think, but almost all with signatories that included very good and smart people.

154
00:18:54,920 --> 00:19:02,840
The most dubious of these, however, called for those in charge to push the imaginary red pause

155
00:19:02,840 --> 00:19:10,520
button on AI, quote, until we can figure out what's going on, as the man said. Knowing full

156
00:19:10,520 --> 00:19:17,240
well that no such button exists. That bad actors will not play along and that they themselves

157
00:19:17,240 --> 00:19:21,160
perhaps stand to benefit down the line for having signaled their concern thusly.

158
00:19:23,240 --> 00:19:29,400
Most importantly, that unlike aliens in a sci-fi movie who land and say take me to your leader,

159
00:19:30,600 --> 00:19:38,040
that when it comes to the serious, serious risks identified, there is no them to petition.

160
00:19:38,040 --> 00:19:44,360
Who is in charge is the right question. And I would like to think that the posing of that

161
00:19:44,360 --> 00:19:54,040
question was the real point and hopefully impact of the letters. That said, it is clear that the

162
00:19:54,040 --> 00:20:01,000
present discourse around AI is ironically enough exemplary of the kind of discourse that the

163
00:20:01,000 --> 00:20:08,440
discourse around AI is warning us about. Tribal, hyperbolic, truthy, unnecessarily dominated by

164
00:20:08,440 --> 00:20:15,000
whoever talks loudest and says the most outrageous thing, all amplified by advertising machines.

165
00:20:17,000 --> 00:20:26,600
Some critics may even go so far as to insist that AI is neither artificial nor intelligent.

166
00:20:26,760 --> 00:20:32,840
They say that it is not artificial because it is made of physical materials by people for specific

167
00:20:32,840 --> 00:20:38,840
purposes, which is the very definition of the artificial. They say it is not intelligent

168
00:20:38,840 --> 00:20:44,120
because it is merely modeling and solving problems, which in many significant ways is a good

169
00:20:44,120 --> 00:20:49,640
shorthand definition for a general theory of intelligence that is inclusive of but not

170
00:20:49,640 --> 00:20:56,520
exclusive to what it feels like to be human. They say that AI is not artificial because it is made

171
00:20:56,520 --> 00:21:06,840
human. Others may go a step further and advance a position that we might call AI denialism. That is,

172
00:21:06,840 --> 00:21:13,480
AI doesn't really exist. Don't be fooled. It is just statistics. It is just gradient descent.

173
00:21:15,480 --> 00:21:20,360
This is a bit like saying a symphony doesn't exist, it is just sound waves. Or that food

174
00:21:20,360 --> 00:21:24,920
doesn't exist, it is just molecules. All of these are trivially true,

175
00:21:26,600 --> 00:21:33,240
but this AI denialism is not remotely helpful in addressing the concerns it purports to stand for

176
00:21:33,240 --> 00:21:41,720
by making this particular case. The thing is it is usually advanced as part of a political position

177
00:21:41,720 --> 00:21:49,160
in relation to the economics of AI, often a quite legitimate one, that then raises the stakes

178
00:21:49,160 --> 00:21:55,560
to an ontological claim. And so it is perhaps difficult to climb down from denialism

179
00:21:56,520 --> 00:21:59,320
because it seems to put the validity of the politics in question.

180
00:22:01,560 --> 00:22:09,080
Elsewhere, AI denialism dovetails with what we might call AI abolitionism. AI does not really

181
00:22:09,080 --> 00:22:17,160
exist but should nevertheless be abolished. Now, there is a lot to unpack here and to do

182
00:22:17,160 --> 00:22:22,200
these positions justice and to systematically critique the critique of AI in this way and would

183
00:22:22,200 --> 00:22:28,200
take at least a few other lectures, but let me then just offer the punch line of what those other

184
00:22:28,200 --> 00:22:35,240
lectures might be because it is also the punch line of this lecture or one of them. There are

185
00:22:35,240 --> 00:22:45,480
several. Sociomorphism, that AI is or should be the reflection of human society, is the logical

186
00:22:45,480 --> 00:22:51,000
extrapolation of anthropomorphism, that AI is or should be the reflection of a single human.

187
00:22:52,360 --> 00:23:00,600
Both of these, neither of these, is a real alternative to a California ideology's planetary

188
00:23:00,600 --> 00:23:07,800
hegemony. They are in fact the pinnacle of it. The strong anthropomorphic view of AI

189
00:23:08,440 --> 00:23:15,160
goes back at least to the Turing test where what he offered as a sufficient condition for

190
00:23:15,240 --> 00:23:23,000
identifying machine intelligence became a necessary condition. That is, unless AI could perform

191
00:23:23,000 --> 00:23:31,960
thinking the way that humans think that humans think, then it's disqualified. This idealization

192
00:23:31,960 --> 00:23:38,840
of what we could call reflectionism, it manifested as well in the psychologism of human-centered

193
00:23:38,840 --> 00:23:46,040
design, which proved a very mixed bag as it turned out, and is present in the now contested

194
00:23:46,040 --> 00:23:54,280
terrain for human-centered AI, humanistic AI and so on, which are perhaps posed to make many of

195
00:23:54,280 --> 00:24:03,080
the same errors as human-centered design. I will talk about what we call human-AI interaction

196
00:24:03,160 --> 00:24:09,080
design, or HAID, in a moment. We are, in antikythera, very invested in this idea,

197
00:24:09,960 --> 00:24:13,800
not despite of its weirdness and complexity, but because of it.

198
00:24:15,480 --> 00:24:23,560
We are interested in the weirdness as well that is ensured by attempts to eradicate the badness.

199
00:24:24,200 --> 00:24:32,200
The last decade of AI ethics surely prevented a lot of horrible things. Alignment researchers

200
00:24:36,520 --> 00:24:42,040
themselves contributed to many of the core technologies that we now make use of, scaling

201
00:24:42,040 --> 00:24:50,040
laws to ROHF in particular. And yet, also, at the same time, ethics ended up dovetailing

202
00:24:50,200 --> 00:24:57,640
accidentally with corporate brand concerns to give us LLMs that are lobotomized to never speak

203
00:24:57,640 --> 00:25:05,640
about sex and violence in any meaningful way. We have prudish AIs. We all foresee how bad it

204
00:25:05,640 --> 00:25:11,560
could get if the worst human preoccupations were directly aligned with the power of foundation

205
00:25:11,560 --> 00:25:18,600
models. But we also, at the same time, think about the critical role of sex and violence in

206
00:25:18,600 --> 00:25:26,040
the evolution of animal intelligence, including ours. And so recognize the weirdness of machine

207
00:25:26,040 --> 00:25:33,240
intelligence evolving with these topics as unspeakables. Now, again, to be clear, my

208
00:25:33,240 --> 00:25:39,880
positions on this are not intended to be posed at the expense of the research in AI ethics and

209
00:25:39,880 --> 00:25:46,840
alignment, but rather actually in concert with their conclusions that ethics and alignment as

210
00:25:46,840 --> 00:25:55,720
such are together necessary but insufficient frameworks for the long-term orientation of

211
00:25:55,720 --> 00:26:03,000
machine intelligence. That, in other words, alignment overfitting is real. My concern,

212
00:26:03,000 --> 00:26:13,000
however, is that exhaustion with tech solutionism gives way to self-congratulatory parades of

213
00:26:13,000 --> 00:26:20,280
political solutionism that is now overflowing op-ed columns, trade books, and yes, schools and

214
00:26:20,280 --> 00:26:28,520
universities. On the continent, the inability to grasp how planetary computation upends 18th

215
00:26:28,520 --> 00:26:37,080
century forms of Westphalian citizenship leads as well to regulatory solutionism, AI laws that

216
00:26:37,080 --> 00:26:42,360
address yesterday's problems, the EU forever running to where the ball no longer is.

217
00:26:43,640 --> 00:26:50,680
Trying to shoehorn planetary dynamics into citizen-scale policies. For example,

218
00:26:50,680 --> 00:26:56,440
its permanent focus on individual citizen data as the core locus of concern and governance

219
00:26:57,160 --> 00:27:07,800
is, in a post-pandemic world, probably the wrong lens. The way out of this is to cut the knot

220
00:27:08,440 --> 00:27:14,520
of the weakest forms of reflectionism, which we might define as the moral and practical

221
00:27:14,520 --> 00:27:23,640
axiom that AI does or should be ontologically anthropomorphic. Not only are technologies not

222
00:27:23,640 --> 00:27:30,200
exhausted by the projection of social relations upon them, they are capable of forcing new

223
00:27:30,200 --> 00:27:35,960
practical concepts that contravene those social relations in the first place.

224
00:27:38,520 --> 00:27:45,880
As such, the harm, to use the parlance of the day, is not only in what reflectionism directs

225
00:27:45,880 --> 00:27:53,160
our attention to, but equally, perhaps more so, what it directs our attention from.

226
00:27:55,160 --> 00:28:02,040
AI represents an existential risk and existential potential in both senses of that term.

227
00:28:02,520 --> 00:28:09,400
By this, I don't mean that it may or may not kill us, but that it may or may not disclose to humans

228
00:28:09,400 --> 00:28:15,000
and to animal intelligence, to planetary intelligence, fundamental truths about what we

229
00:28:15,000 --> 00:28:23,320
are, what their existential condition really is. This is the Copernican risk-reward calculus,

230
00:28:24,120 --> 00:28:30,120
one that is neither messianic nor apocalyptic. Now, you may hear my criticism, such as it is,

231
00:28:36,440 --> 00:28:43,320
of shallow AI anthropomorphism and say, yes, but as it turns out, AI actually does reflect

232
00:28:43,320 --> 00:28:49,960
human intelligence in some important ways. So, against a shallow anthropomorphism that

233
00:28:49,960 --> 00:28:57,400
insists that AI present itself as thinking how humans think that humans think, there is also

234
00:28:58,200 --> 00:29:05,400
a deep anthropomorphism, or better, a deep biomorphism, that may correspond to how humans

235
00:29:05,400 --> 00:29:12,200
and other animals do really think, even if they don't experience thinking in that way.

236
00:29:13,000 --> 00:29:20,520
This is not only true, it's profound, and this is, as I say, what shallow reflectionism

237
00:29:20,520 --> 00:29:30,440
models. For example, very recently, research in how AIs discern edges in images was, for example,

238
00:29:30,440 --> 00:29:36,280
directed researchers to unidentified, as yet identified, unspecified neuronal mechanisms in

239
00:29:36,280 --> 00:29:42,360
human brains that perform more or less the same thing. As I'll discuss in a moment in relation to

240
00:29:42,360 --> 00:29:49,960
one of Antikythera's projects, AIs are becoming a kind of experimental organism, like a lab rat,

241
00:29:51,000 --> 00:29:54,680
in which it's possible to test for human conditions and responses.

242
00:29:55,960 --> 00:30:04,360
This would not work if there was no fundamental correspondence. But what is and isn't the quality

243
00:30:04,360 --> 00:30:10,520
of that correspondence is of genuine philosophical and practical interest.

244
00:30:12,520 --> 00:30:18,360
We also see, at perhaps a higher level of abstraction, that iterative predictive dynamics

245
00:30:18,360 --> 00:30:23,800
of transformer models does correspond with the iterative predictive dynamics of biological

246
00:30:23,800 --> 00:30:32,760
neurons. This was not the plan, but there it is. So yes, actually, you are a stochastic parrot,

247
00:30:32,760 --> 00:30:38,520
after all. Always has been. But you should not take that as the insult

248
00:30:39,320 --> 00:30:42,760
that the authors of that infamous paper perhaps intended it to be.

249
00:30:45,000 --> 00:30:51,560
Iterative stochastic prediction and thinking through the recursions of mental simulation

250
00:30:51,560 --> 00:30:57,800
and embedded embodied perception is how humans made all the things that you hold most dear.

251
00:30:58,600 --> 00:31:05,800
Literature, music, science, and so on. Parrots, by the way, are actually very smart and creative,

252
00:31:05,800 --> 00:31:12,040
so they're kind of a lousy token species for mindless repetition, but that's another thing.

253
00:31:13,720 --> 00:31:20,760
The deeper correspondence between iterative stochastic prediction and artificial and

254
00:31:20,760 --> 00:31:26,280
natural systems is technically an anthropomorphism, but as said, probably better to call it a

255
00:31:26,280 --> 00:31:34,680
biomorphism. And it suggests then a different kind of AGI, an artificial generic intelligence.

256
00:31:37,480 --> 00:31:44,760
And this is really the point. Emphasis on the correspondence between AI and the manifest image

257
00:31:44,760 --> 00:31:53,000
of human thought and intelligence and culture comes at this terrible price, obscuring the real

258
00:31:53,000 --> 00:32:00,440
and profoundly significant correspondence between animal machine intelligence that do not already

259
00:32:00,440 --> 00:32:08,280
register in common cultural norms, but which could orient those norms to the underlying reality

260
00:32:09,000 --> 00:32:16,200
from which they emerge. A different bidirectional path of and for alignment.

261
00:32:16,200 --> 00:32:23,640
And notice I say from which they emerge, as opposed to the reality that emerges from those

262
00:32:23,640 --> 00:32:29,240
cultural norms. This is where perhaps there are some points of difference in our approach

263
00:32:29,960 --> 00:32:37,640
and some others on offer in the humanities. It comes down to something rather fundamental

264
00:32:37,640 --> 00:32:43,480
of what is inside of what. The planet makes worlds or worlds make planets.

265
00:32:43,480 --> 00:32:50,280
I say that we must avoid what obscures the deeper and more philosophically challenging ways

266
00:32:50,280 --> 00:32:58,680
that AI does think like brains, but simultaneously does not orient itself around humanist norms,

267
00:32:59,400 --> 00:33:06,440
which thus distances human brains from human values in uncomfortable ways.

268
00:33:06,920 --> 00:33:13,640
In uncomfortable ways. Is this the real point of contention and unease?

269
00:33:16,040 --> 00:33:22,680
In the most extreme versions of reflectionism, what is being defended, I sometimes wonder,

270
00:33:23,720 --> 00:33:30,600
seems like a kind of political theological conviction that there is nothing outside the

271
00:33:30,680 --> 00:33:37,240
text as they used to say. Nothing outside the sociological interpretation of technology.

272
00:33:37,240 --> 00:33:42,920
The political economy of science. The reality of culture as determinant of reality.

273
00:33:43,720 --> 00:33:50,680
Nothing causing culture, but culture itself. Culture causing culture, which is caused by more

274
00:33:50,680 --> 00:33:58,520
culture and thus anything, including AI, is intrinsically a reflection of that culture

275
00:33:59,240 --> 00:34:06,520
and nothing more. We might call this social reductionism and cultural determinism,

276
00:34:07,080 --> 00:34:15,960
which for all its lip service to post-humanism can be the most militant guise of humanism.

277
00:34:17,480 --> 00:34:26,120
Now, obviously AI as it exists is fortunately and unfortunately a reflection of the cultures

278
00:34:26,120 --> 00:34:32,440
that produced it, but and here is the perhaps critical fork in the road. It is not nor should

279
00:34:32,440 --> 00:34:42,680
it be only a reflection of culture. It is more. We are more. AI itself and more importantly,

280
00:34:42,680 --> 00:34:49,800
the qualities of reality that are certain to be revealed by AI, all the Copernican twists to come

281
00:34:50,760 --> 00:34:59,560
are things to which culture must and will align. Not only something that must and will align to

282
00:34:59,560 --> 00:35:05,240
culture. So, two conclusions.

283
00:35:05,240 --> 00:35:23,800
Just some. AI is an existential technology and as such must align in both directions.

284
00:35:24,440 --> 00:35:32,440
AI aligning to culture's wisdom. Culture to AI's disclosures. So, specifically,

285
00:35:33,320 --> 00:35:40,840
first, lowercase alignment should be seen as a tactic for making machine intelligence's

286
00:35:40,840 --> 00:35:49,000
instrumentality, making it cohere to agency and intention to make it work. But uppercase alignment

287
00:35:49,720 --> 00:35:55,880
and the attendant metaphysics is an inadequate grounding for the long-term orientation of machine

288
00:35:55,880 --> 00:36:05,720
intelligence by animal intelligence. Second, a two-way alignment is both possible and desirable.

289
00:36:06,360 --> 00:36:12,440
AI's epistemic overhangs, things that it knows and implies for us to know that we have a difficult

290
00:36:12,440 --> 00:36:20,200
time grasping and accommodating and incorporating are not pathologies. They are in fact the deeper

291
00:36:20,200 --> 00:36:29,720
point of AI. And so, between AI as generic intelligence, AI as an experimental super

292
00:36:29,720 --> 00:36:35,240
organism per one of our Antikythera projects called the End of Science by our studio researchers

293
00:36:35,240 --> 00:36:44,360
Darren Zhu and Connor Cook, AI becomes an uncannily productive sort of mirror. But it's not a mirror

294
00:36:44,360 --> 00:36:50,680
reflecting what we think we are because we can see it and feel it, but rather a mirror of what

295
00:36:50,680 --> 00:37:12,840
we are but cannot see and cannot feel. At least not yet. Now, I would like to then

296
00:37:13,720 --> 00:37:18,360
specify this, ground this a bit in some of the work that we've done to try to explore

297
00:37:19,240 --> 00:37:25,720
these ideas and many others in the Antikythera program and to tell you a little bit about the

298
00:37:25,720 --> 00:37:33,240
studio but more importantly about the work. All the projects that I'll show you in a kind of

299
00:37:33,240 --> 00:37:38,760
summary coming up were all really just completed at the end of last week where they were first

300
00:37:38,760 --> 00:37:43,720
privately shown in Los Angeles. And again, we'll be back in London in the fall to do a

301
00:37:43,720 --> 00:37:51,880
bigger showcase around these. Antikythera, as Stephanie signaled to you, is a research

302
00:37:51,880 --> 00:37:57,720
program, a think tank of sorts for the speculative philosophy of computation. It is supported and

303
00:37:57,720 --> 00:38:04,520
housed at the Berggruen Institute. We're pleased to be joined by Niels Gilman from the Institute

304
00:38:04,520 --> 00:38:12,200
here tonight. Stephanie is the associate director. Nikolai is also here, my longtime friend and

305
00:38:12,200 --> 00:38:18,760
collaborator. Shrelka is our studio director and we are also held afloat by Case Miller and Emily

306
00:38:18,760 --> 00:38:25,320
Knapp and growing quite, growing every day it seems. The program includes 70 plus affiliate

307
00:38:25,320 --> 00:38:31,000
researchers from around the world, various universities, and has just completed in our

308
00:38:31,000 --> 00:38:37,400
last phase 12 studio researchers who completed this studio cycle. It is, in essence, a program

309
00:38:37,400 --> 00:38:43,480
that not only seeks to map planetary computation but to ask and provide some provisional answers

310
00:38:43,480 --> 00:38:49,480
to what planetary scale computation is for. Now, as I've already said, philosophy and more

311
00:38:52,680 --> 00:38:58,680
generally, the project of developing viable concepts about how the world works and thus

312
00:38:58,680 --> 00:39:04,440
thinking about and thus thinking about how the world works has always developed in conjunction

313
00:39:04,440 --> 00:39:09,880
with what technology reveals and does and thus what it's possible to think. And so at least in

314
00:39:09,880 --> 00:39:14,920
that regard, I am something of a technological determinist but only if we expand the definition

315
00:39:14,920 --> 00:39:23,960
of technology to its properly expansive scope. Here's the thing. At this moment, technology

316
00:39:24,040 --> 00:39:32,120
and particularly planetary computation has outpaced our theory. The response, as I've hinted

317
00:39:32,120 --> 00:39:37,560
tonight, is to some extent to force comfortable and settled ideas about ethics and scale and

318
00:39:37,560 --> 00:39:43,640
polity and meaning onto a situation that not only calls for a different framework but is already

319
00:39:43,640 --> 00:39:48,200
generating that different framework. So instead of simply applying philosophy to the topic of

320
00:39:48,200 --> 00:39:54,200
computation, we start from the other direction and produce ideas and the speculative from the

321
00:39:58,040 --> 00:40:04,040
direct encounter, from making things. So that being said, the program, the Antikythera program's

322
00:40:06,500 --> 00:40:12,200
real interest is not so much in calculation, in formalization, quantification, interoperability

323
00:40:12,200 --> 00:40:18,200
as such, than it is about how computation provides orientation, navigation, cosmology, in

324
00:40:21,740 --> 00:40:27,740
essence, planetary. The inspiration for the name comes from the Antikythera mechanism first

325
00:40:30,780 --> 00:40:38,780
discovered in 1901 in a shipwreck off the Greek island of said name and dated to about 200 B.C.

326
00:40:38,860 --> 00:40:46,160
It is perhaps apocryphally the first computer. But it is certainly a primordial computer. But it

327
00:40:46,160 --> 00:40:52,160
was not simply a calculator. It was also an astronomical machine mapping and predicting the

328
00:40:52,160 --> 00:40:58,080
movements of stars and planets, marking annual events and orienting a naval culture upon the

329
00:40:58,080 --> 00:41:04,080
surface of the globe. So it not only calculated interlocking variables, it gave a comprehensive

330
00:41:04,080 --> 00:41:10,080
orientation of thought in relation to its astronomic predicament, enabling prescriptive

331
00:41:14,340 --> 00:41:20,340
thought to act in relation to this revealed circumstance. So beyond forms of computation that

332
00:41:22,340 --> 00:41:29,080
are already perceivable in natural systems, artificial computation such as this is a kind of

333
00:41:29,080 --> 00:41:35,080
world ordering, a foundation for what would become complex culture. And that is really the

334
00:41:38,500 --> 00:41:46,500
core of it. For our initiative, the name Antikythera refers to computational technology

335
00:41:46,500 --> 00:41:52,500
that discloses and accelerates the planetary condition of intelligence. So let me go a bit

336
00:41:52,580 --> 00:42:00,580
deeper into some of the themes and ideas of the program and to show you some of the work,

337
00:42:00,580 --> 00:42:06,500
which will be playing in the background as I explain a little bit where it came from and what

338
00:42:06,500 --> 00:42:12,000
it's up to. Several of the projects, not all of them, several of the projects speak directly

339
00:42:12,000 --> 00:42:17,880
to questions of AI and indeed to questions of AI alignment rather directly. The project

340
00:42:17,880 --> 00:42:23,880
HAI ID or HAID, Human Artificial Intelligence Interaction Design by Antikythera Studio

341
00:42:26,040 --> 00:42:33,040
Researchers William Morgan, Sarah Olympia Scott and Daniel Barkay is what you see here. It is

342
00:42:33,040 --> 00:42:39,040
an ever growing catalog of existing and almost existing modes, positions and syndromes of human

343
00:42:40,840 --> 00:42:47,840
AI interaction that allows us to map and generalize the space. It is a compendium of

344
00:42:47,880 --> 00:42:53,880
hundreds of operant models, syndromes, patterns and persistent folk ontologies. As such, it maps

345
00:42:56,420 --> 00:43:01,540
not one but several conceptual models for what human AI interaction design may be and might be

346
00:43:01,540 --> 00:43:07,540
based upon. It sees HAID as present as a kind of subset of HCI but one that will arguably

347
00:43:09,640 --> 00:43:16,180
overwhelm and redefine that field, especially as personal AIs are more generally deployed at

348
00:43:16,180 --> 00:43:21,600
platform scale. As I've already insisted, the history of AI and the history of philosophy of

349
00:43:21,600 --> 00:43:27,020
AI are deeply intertwined. One side of that ledger is populated by numerous thought

350
00:43:27,020 --> 00:43:33,640
experiments, both canonical and obscure, the imitation game, the Chinese room, the paper clip

351
00:43:33,640 --> 00:43:40,140
maximizer, the three blue banana problem, Samantha's infidelity, the driverless red

352
00:43:40,140 --> 00:43:46,140
trolley and so on. These thought experiments responded to extend AI and in turn framed and drove

353
00:43:48,720 --> 00:43:55,560
further development of the technology. But each was not only a metaphor for what AI is but also a

354
00:43:55,560 --> 00:44:01,560
scenario for human AI interaction and indeed one because the other. We try to figure out what AI

355
00:44:01,560 --> 00:44:06,260
is by figuring out the terms for interacting with it and to learn how to interact with it by

356
00:44:06,260 --> 00:44:12,260
learning what it is, a perfectly understandable approach. Lately, however, with the rise of LLM,

357
00:44:14,340 --> 00:44:20,020
there are many new entries to this list, including Sydney's nervous breakdown in which instead of

358
00:44:20,020 --> 00:44:25,760
falling in love with this articulate OS, as in her, a journalist coaxes a chat bot to perform

359
00:44:25,760 --> 00:44:33,260
disturbing feats of abnormal psychology. Among the most notorious new entries may simply be

360
00:44:33,260 --> 00:44:39,260
called the Blake-Lamoine scenario where the highly evolved tendency to ascribe intentionality to

361
00:44:41,020 --> 00:44:47,020
linguistically competent conversance can lead to some unnecessary conclusions. Blaze and I wrote a

362
00:44:49,600 --> 00:44:54,980
piece addressing this episode called the model is the message, suggesting that the intelligence

363
00:44:54,980 --> 00:45:00,980
there is not quite what Lamoine thought it was, but not quite not what he thought it was either.

364
00:45:00,980 --> 00:45:08,100
With many AI interfaces, it would seem that computers have mastered presenting themselves in

365
00:45:08,100 --> 00:45:13,900
ways that require almost no additional comprehension for users beyond ingrained social

366
00:45:13,900 --> 00:45:20,860
interaction cues. The history of HCI is in this way a story that shifts from humans having to

367
00:45:20,860 --> 00:45:26,360
understand how computers work in order to use them to computers figuring out how humans work in

368
00:45:26,360 --> 00:45:33,280
order to be used by them. Now, language in its most abstract forms, linguistic reasoning, not only

369
00:45:33,280 --> 00:45:41,280
talking and writing, accelerates the latter dramatically and flexibly, even disturbingly. And so

370
00:45:41,280 --> 00:45:49,280
draws the practical boundaries of Haid both deep and wide. As you will see, the capacity for AI to

371
00:45:50,120 --> 00:45:57,120
present itself through human social cues is remarkable and in fact becomes the interface in and of

372
00:45:57,120 --> 00:46:03,120
itself. The shift from HCI to Haid means a shift from designing click paths to designing synthetic

373
00:46:05,120 --> 00:46:13,120
personalities. Now, perhaps the most quantitatively pervasive form of Haid is one in which the user

374
00:46:13,200 --> 00:46:20,000
doesn't even know the AI is there. Things just work. They work and who cares how. However, the forms

375
00:46:20,000 --> 00:46:25,580
of Haid that this project focuses on are those that inspire and extend personal relationships with

376
00:46:25,580 --> 00:46:33,580
not only AI but AI persona. Now, what we call personal AIs are essential to this and represent a

377
00:46:33,580 --> 00:46:39,840
field of tremendous interest, but personal can simply mean AIs that are customized by your

378
00:46:39,840 --> 00:46:46,800
personal use of it that are not necessarily persona. But many of the AIs that you will use and

379
00:46:46,800 --> 00:46:52,800
which will use you will be as personalized as your search history, if not your fingerprint. The

380
00:46:55,300 --> 00:47:00,680
avenue of exploration for this project is then the shift from a form of HCI that is based largely

381
00:47:00,680 --> 00:47:07,840
in spatial references, inside versus outside, up and down, over, under, to one that is based on

382
00:47:07,840 --> 00:47:15,340
psychosocial metaphors. Now, this is obviously a tremendously powerful shift, but one that comes

383
00:47:15,340 --> 00:47:21,340
with all the risks and downsides of human psychology itself. And yet, in the most basic form,

384
00:47:23,680 --> 00:47:29,760
this isn't optional. Some projective comprehension in the form of a mental model of what's

385
00:47:29,760 --> 00:47:35,760
going on here. In this case, what the AI is, what its affordances are, what it is and isn't doing,

386
00:47:35,800 --> 00:47:43,800
what and where and why it is, is so on. Folk ontologies are not optional. Most AI hate, we

387
00:47:43,800 --> 00:47:48,800
might suppose, will be interaction with, again, in some ways, AIs are invisible and boring and

388
00:47:48,800 --> 00:47:54,260
unmemorable and yet critical to the reproduction of everyday life, but as said, personal AIs are

389
00:47:54,260 --> 00:48:02,020
another matter. They are, they mean a kind of AI that is being trained in how you think, like

390
00:48:02,020 --> 00:48:08,360
teaching a dog a new trick, but also being trained by your thinking, like carving a rock into

391
00:48:08,360 --> 00:48:14,360
an arrowhead. It is a personalization of an external mental model. And is this potentially an

392
00:48:17,600 --> 00:48:25,600
experience of the self in the third person? If so, how can we not be fascinated? Now, in a moment,

393
00:48:26,180 --> 00:48:31,480
I will talk about another project of ours that is with simulations. Simulations, then, but

394
00:48:31,480 --> 00:48:37,180
however, clearly already tie back to the discussion of personal AIs, which are, in a sense,

395
00:48:37,180 --> 00:48:44,680
simulations of us or perhaps we are the digital twin of the AI that is working on our behalf. For

396
00:48:44,680 --> 00:48:50,680
both, the sim to real problematics are real and, of course, weird. Perhaps you are the NPC.

397
00:48:53,760 --> 00:48:59,760
Perhaps your shadow is chasing you. Perhaps all personality is a placebo. Memory is a

398
00:49:01,940 --> 00:49:08,940
key. Maybe the key to any personal alignment worth the name. Perhaps, then, if the uncanny valley

399
00:49:08,940 --> 00:49:15,980
is when you are weirded out by something that is but is not quite human, the inverse uncanny valley

400
00:49:15,980 --> 00:49:20,860
is when you are much more deeply weirded out by seeing yourself through the eyes of the

401
00:49:20,860 --> 00:49:27,660
machinic other and don't quite recognize what you see, but you do recognize that what you see is

402
00:49:27,820 --> 00:49:33,660
you, but in a way not, and yet it is more real than the version of you that you experience as

403
00:49:33,660 --> 00:49:40,780
you. Perhaps you and that newly demystified you will engage in what security teams call

404
00:49:40,780 --> 00:49:46,780
coordinated inauthentic behavior. What is alignment then? The field of Hade, then, is obviously

405
00:49:51,040 --> 00:49:57,280
not brand new in reality. It's quite old, but it is new perhaps as a formal disciplinary field of

406
00:49:57,280 --> 00:50:03,860
research and design, one that begins as such as said as a subset of HCI and may in time come to

407
00:50:03,860 --> 00:50:10,580
encompass it. Then, if so, does it pretend to shift from cognitive psychology of HCI to a

408
00:50:10,580 --> 00:50:16,580
renewal of psychoanalysis for HAI ID? Time will tell. Okay. Next. As said, we are as clear, we

409
00:50:16,580 --> 00:50:22,580
are quite interested in LLM, but not just as chatbots. We are also deeply interested, but

410
00:50:30,160 --> 00:50:35,920
rather also as a form of what we call cognitive infrastructure. The embedding of linguistic

411
00:50:35,920 --> 00:50:42,040
competence and hence symbolic reasoning in the inanimate and utterly non-anthropomorphic

412
00:50:42,040 --> 00:50:48,580
materials and systems of the world for which here, mind is literally distributed. This

413
00:50:48,580 --> 00:50:54,540
project, Whole Earth Codec, posits AI not as a brain in a petri dish, but as a synthetic

414
00:50:54,540 --> 00:51:00,540
augmentation of the forms of intelligence that emerge in and as complex ecological niches.

415
00:51:00,540 --> 00:51:08,540
It's by, this project is by Antikythera Studio researchers, Christina Lu, a DeepMind alum,

416
00:51:08,660 --> 00:51:16,660
and Delana Tran and Connor Cook. The project takes AI as a landscape scale phenomenon, focusing

417
00:51:16,660 --> 00:51:24,660
less on how AI may align with you or me than how it may align with AI in the wild, may align

418
00:51:24,660 --> 00:51:34,660
with the wider ecosystem. AI as an inorganic participant in an organic increasingly self-modeling

419
00:51:34,820 --> 00:51:42,080
living world. Put differently, Whole Earth Codec rethinks the position and application of

420
00:51:42,080 --> 00:51:48,000
artificial intelligence as a form of planetary intelligence and considers potential and

421
00:51:48,000 --> 00:51:55,200
necessary conditions for their alignment. It started by responding to a brief about the

422
00:51:55,200 --> 00:52:00,620
quality of data used for foundation models and by quality, it was meant whether the data is

423
00:52:00,620 --> 00:52:07,620
any good, but also what kind of data it is. Training models that would have global influence

424
00:52:07,620 --> 00:52:15,620
on just whatever data happens to be left out in the open, all but guarantees some degree of

425
00:52:15,620 --> 00:52:22,740
suboptimal quality. If the most interesting data that could, in theory, contribute to broad

426
00:52:22,740 --> 00:52:29,620
base socially constructive purposes is both private and or privatized, then other approaches

427
00:52:29,620 --> 00:52:36,240
are needed. Techniques like federated learning would allow that data to contribute to the

428
00:52:36,240 --> 00:52:42,240
reweighting of common models without disclosing underlying values. We could, in principle,

429
00:52:42,240 --> 00:52:49,960
have our cake and keep it private too. But for this project, such a rotation also implies a

430
00:52:49,960 --> 00:52:57,500
shift in what kind of data should be produced. It ventures that aggregating data about

431
00:52:57,500 --> 00:53:04,080
individual human users is only a fraction of what is possible and necessary for planetary

432
00:53:04,080 --> 00:53:11,580
intelligence worth the name. It proposes a fundamental deindividuation of computational

433
00:53:11,580 --> 00:53:20,000
observation in a focus instead on impersonal, ecological and systemic data. That is, instead of

434
00:53:20,000 --> 00:53:25,620
centering on individual data, instead of centering on individual data with ecological data as a kind

435
00:53:25,620 --> 00:53:31,900
of exceptional subsidiary, it inverts this. It posits data about individuals as a specific

436
00:53:31,900 --> 00:53:39,000
subset of ecological data, which, as should be clear, traces a recurring theme in all of our

437
00:53:39,000 --> 00:53:47,280
work, culture framed as a function of the planetary rather than the inverse. So, the scenario

438
00:53:47,280 --> 00:53:53,960
it explores for planetary intelligence is one in which systems of sensing and modeling are

439
00:53:53,960 --> 00:54:03,700
global, but importantly, it is an observatory looking not outward but inward. The self-attention

440
00:54:03,700 --> 00:54:10,540
of the transformer model is posed as an alternative metaphor to the panopticon of Foucault. The

441
00:54:10,540 --> 00:54:16,020
positions of the observer and the observed are less supervisor and supervised than they are

442
00:54:16,020 --> 00:54:25,520
mutually recursive. The model is sensing itself and thereby the planet is sensing itself. The

443
00:54:25,520 --> 00:54:32,760
transformer's self-referentiality is the figure, the allegory for planetary models as well. They

444
00:54:32,760 --> 00:54:41,120
call this folding the gaze. The scenario also hinges on the figure and function of multimodality.

445
00:54:41,120 --> 00:54:48,160
If we see computation itself as a kind of generic syntax between qualitatively unlike things and

446
00:54:48,160 --> 00:54:53,960
actions, then this project locates this generic syntactical function in the sensing and modeling

447
00:54:53,960 --> 00:55:01,380
applications of, as I say, AIs and landscape scale technology. The sensing and modeling system

448
00:55:01,380 --> 00:55:07,880
is a system for the artificial transduction of planetary phenomena into integrated and

449
00:55:07,880 --> 00:55:15,680
recombinant data, hence whole earth codec. Multimodality operates both at the level of the

450
00:55:15,680 --> 00:55:21,560
kinds of phenomena that are incorporated and artificially mutualized and in the range of

451
00:55:21,560 --> 00:55:28,120
applications and functions to which the system as a whole might be directed, mixing and matching

452
00:55:28,120 --> 00:55:36,000
inputs and outputs. Multimodal phenomenon, transduced and filtered into a generic syntax,

453
00:55:36,040 --> 00:55:43,680
outputted as multimodal application technologies. In this scenario, planetary intelligence enables

454
00:55:43,680 --> 00:55:51,880
planetary ecologies, again inclusive of human systems, to recompose themselves because the

455
00:55:51,880 --> 00:55:59,040
composition of whole earth codec as a technology for planetary composition enables the emergence

456
00:55:59,520 --> 00:56:11,520
of that intelligence. Knowing enables making, but making makes knowing possible. Last project that

457
00:56:11,520 --> 00:56:24,520
I'll show tonight. The, which is called vivarium. Perhaps the philosophy of simulation begins with

458
00:56:24,560 --> 00:56:31,000
the beginnings of philosophy itself. In a cave in Greece where Plato and Socrates cultivated a

459
00:56:31,000 --> 00:56:38,560
long-standing paranoia, not just of simulations, but of mediated perception and its relation to

460
00:56:38,560 --> 00:56:46,920
thought. External and internal simulation in conflict or alignment. There in that cave, they

461
00:56:46,920 --> 00:56:52,640
set the foundation not only as what would become a topic for philosophy, but perhaps, as I say,

462
00:56:52,720 --> 00:56:59,760
the foundational paranoia from which Western philosophy was born. Now, the politics of

463
00:56:59,760 --> 00:57:05,960
simulation can also be very personal. As you pass through a security gateway, perhaps at an

464
00:57:05,960 --> 00:57:12,600
airport, what is under inspection is not only your physical person, but also trace digital

465
00:57:12,600 --> 00:57:19,760
personas linked to you, but which live in a near-distance shadow city called the cloud. If the

466
00:57:20,200 --> 00:57:26,160
uniform lets you pass, it's because a decision was made according to risk models on those silhouettes

467
00:57:26,160 --> 00:57:33,640
of which your physical person is a reflection. Your ears may burn as the infrastructure whispers

468
00:57:33,640 --> 00:57:42,080
about your doubles, but it's not just you that's in play. At home and at work, as AI and simulations

469
00:57:42,080 --> 00:57:47,880
convene, the designer versus player distinction will collapse from both directions because large

470
00:57:47,880 --> 00:57:53,120
AI models and large simulation models will themselves converge, the latter as the interface

471
00:57:53,120 --> 00:58:00,720
to the former. Elsewhere, scientific simulations have proposed a different kind of planetary

472
00:58:00,720 --> 00:58:06,640
politics around the frame of climate change that seeks to give political priority and agency to

473
00:58:06,640 --> 00:58:13,880
large-scale long-duration simulations of macrological processes. It doesn't articulate itself as such,

474
00:58:13,920 --> 00:58:20,440
but the core of this approach, the core of climate politics, I say, is an attempt to refocus

475
00:58:20,440 --> 00:58:26,440
governmental attention from the mediation of voice to the mediation of ecologies and to make

476
00:58:26,440 --> 00:58:33,840
scientifically significant simulations sovereign actors, to make simulations of the future in

477
00:58:33,840 --> 00:58:41,160
order to govern the present. Now, Viverium, a project by Antikythera Studio researchers

478
00:58:41,160 --> 00:58:47,000
Deliana Tran, Christina Lu, and Will Freudenheim deals with the question of sim to real rather

479
00:58:47,000 --> 00:58:53,760
directly. It poses a platform for collective intelligence that aggregates multiple toy worlds

480
00:58:53,760 --> 00:59:00,600
into a larger platform of worlds that can be used to train physicalized AI and to aggregate

481
00:59:00,600 --> 00:59:06,600
collective data and thus collective intelligence. It works for one-to-one, one human, one AI,

482
00:59:06,720 --> 00:59:13,920
between one human and many AIs, many humans in one AI, and perhaps most interestingly, for forms

483
00:59:13,920 --> 00:59:21,440
of collaborative embodiment, many humans in many AIs. In practice, as already posed, simulations

484
00:59:21,440 --> 00:59:26,560
are an epistemological technology. They are technologies to think with, which in principle

485
00:59:26,560 --> 00:59:33,160
makes a philosophy of simulation, a philosophy of things to think with, central to the purpose of

486
00:59:33,160 --> 00:59:40,480
any program such as ours. We recognize that simulations are pervasive. Our friends from

487
00:59:40,480 --> 00:59:46,480
neuroscience raise the point that simulations are not only a kind of external technology with

488
00:59:46,480 --> 00:59:51,520
which intelligence figures out the world, but simulations is how minds have intelligence at

489
00:59:51,520 --> 00:59:57,560
all. The cortical columns of animal brains are constantly predicting what will be next,

490
00:59:57,560 --> 01:00:02,360
running through little simulations of the world and the immediate future, resolving them with

491
01:00:02,360 --> 01:00:09,160
new inputs and even competing with each other to organize perception and action. For many

492
01:00:09,160 --> 01:00:14,680
computational simulations, their purpose is as a model that reflects reality such as one hopes

493
01:00:14,680 --> 01:00:21,220
for climate science or astrophysics. For others, the back and forth is not just mirroring. Some

494
01:00:21,220 --> 01:00:26,820
simulations not only model the world, but feed back upon what they model both directly and

495
01:00:26,820 --> 01:00:33,340
indirectly. These we call recursive simulations. Recursive simulations are those which not only

496
01:00:33,340 --> 01:00:39,700
model that reality, but allow us to intervene and interact with it as part of our embodied and

497
01:00:39,700 --> 01:00:47,140
intentional experience in a decisive feedback loop. So what are called digital twins, perhaps a

498
01:00:47,140 --> 01:00:55,460
form of personal AI, are one such dynamic. So as Vyvarium shows, many AIs, especially those

499
01:00:55,460 --> 01:01:00,780
embodied in the world, such as driverless cars, are already trained in toy world simulations where

500
01:01:00,780 --> 01:01:07,500
they can explore more freely, bumping into walls until they, like us, learn the best ways to

501
01:01:07,500 --> 01:01:14,740
perceive and model and predict the real world. For the recursive simulation between simulation and

502
01:01:14,740 --> 01:01:21,820
the real, in some ways the real is the baseline model for the simulations and simulations are

503
01:01:21,820 --> 01:01:29,420
sometimes the baseline model for the real. Toy worlds serve as a kind of bounded domain of

504
01:01:29,420 --> 01:01:34,660
constrained information exchange interaction between otherwise unlike and incompatible things

505
01:01:34,660 --> 01:01:39,900
and actions. They are where some AIs learn to navigate the real world by navigating these

506
01:01:39,900 --> 01:01:48,220
focused reduction simulations of their contours. The sim to real passage is not just in terms of

507
01:01:48,220 --> 01:01:54,140
the implications of specific learned experiences, but also the physical virtual hybridization as

508
01:01:54,140 --> 01:02:03,380
such. ML exists in the world. AI is on its way to become something like a generic solvent soaked

509
01:02:03,380 --> 01:02:09,660
into things and into how they behave. And so the back and forth learning between artificial

510
01:02:09,660 --> 01:02:17,020
intelligence and natural intelligence never really stops. For the project, there are, as said,

511
01:02:17,020 --> 01:02:23,660
multiple possible combinations of human users, AI as prostheses, AI as users, human or humans as

512
01:02:23,660 --> 01:02:30,620
prostheses. There are multiple combinations of embodiment, of agency, of action in and across

513
01:02:30,620 --> 01:02:37,700
the simulation, the real and the recursion. Not just one to one, one to many, but ultimately many

514
01:02:37,700 --> 01:02:45,300
to many. Keep in mind, however, for us the AI's world is a simulation of ours and one we can

515
01:02:45,300 --> 01:02:53,940
interact with. The AI, our world, is just one part of the omni-simulation that it simply calls

516
01:02:53,940 --> 01:03:10,780
reality. Let me conclude. And while I make some concluding remarks, I will show clips from a

517
01:03:10,780 --> 01:03:19,540
fourth project called Xenoplex, which is on AI and the philosophy of biology, assembly theory and

518
01:03:19,540 --> 01:03:31,340
empirical astrobiology. It's by Antikythera, Derenzu and Connor Cook. Back to where we began, back

519
01:03:31,340 --> 01:03:36,780
to the Stanislaw Lem inspired distinction between existential or epistemological and

520
01:03:36,780 --> 01:03:47,100
instrumental phases of technologies. So for AI, in terms of its instrumental impact, alignment

521
01:03:47,100 --> 01:03:54,420
overfitting is itself a kind of existential risk. As said, capital A alignment is an inadequate

522
01:03:54,420 --> 01:04:02,420
practical metaphysics for what AI orientation implies and demands. Now, I assume that most, if

523
01:04:02,420 --> 01:04:08,460
not all, serious alignment researchers would not disagree. Now, if only the journalists,

524
01:04:08,460 --> 01:04:15,740
influencers and charismatic mega critics would follow their lead. As for the AI epistemological

525
01:04:15,740 --> 01:04:21,260
impact, what will be the ultimate impact of AI? The ultimate impact of AI will be on what we,

526
01:04:21,260 --> 01:04:26,460
quote unquote, come to grasp about what we are, how we are, why we are and the contingencies of

527
01:04:26,460 --> 01:04:35,060
that pronoun. What that will be, we don't know and we can't know. We can't really anticipate

528
01:04:35,060 --> 01:04:41,500
Copernican shifts and traumas in advance. Just recall that we didn't confirm the existence of

529
01:04:41,500 --> 01:04:49,420
other galaxies until 1924 or scientifically confident precise age of the earth until 1953.

530
01:04:49,420 --> 01:05:00,740
We don't know, but we have to defend the space in which what we will learn will go. We may presume

531
01:05:00,740 --> 01:05:08,700
that with regards to AI as an experimental super organism, one of those likely areas is what we

532
01:05:08,700 --> 01:05:15,340
today call neuroscience and tomorrow may simply call philosophy and vice versa. So what are the

533
01:05:15,340 --> 01:05:22,020
kinds of questions to be asked that are likely to lead in the direction of epistemic disclosure?

534
01:05:22,020 --> 01:05:27,300
There's likely no wrong answers, but safe bets are that posing fundamental questions about what,

535
01:05:27,300 --> 01:05:34,100
quote, life is and what intelligence is and why the words that we use may be inadequate

536
01:05:34,100 --> 01:05:42,220
signifiers for the range of phenomenon that they hope to describe. Even the boundary position

537
01:05:42,220 --> 01:05:47,620
between these is unclear as the boundary between life as is the boundary between life and technology

538
01:05:47,620 --> 01:05:53,820
and intelligence and technology are already, not just for advanced computers but for anything.

539
01:05:53,820 --> 01:06:02,300
Perhaps life is fundamentally something like evolutionary autopoiesis through niche

540
01:06:02,300 --> 01:06:11,820
technologization or perhaps intelligence is or perhaps both. Biotic systems make use of

541
01:06:11,820 --> 01:06:18,540
abiotic systems to replicate themselves as organisms and across generations. They can't

542
01:06:18,540 --> 01:06:25,220
exist without this fundamental technologization of the world. If part of the definition of

543
01:06:25,220 --> 01:06:33,100
intelligence is means agnostic problem solving, then this niche technologization is at least

544
01:06:33,100 --> 01:06:40,300
partially intelligent. This cycle happens not just once or twice here and there, but

545
01:06:40,300 --> 01:06:47,900
everywhere and constantly over and over for billions of years. It doesn't just happen on

546
01:06:48,260 --> 01:06:56,260
the earth. It happens by the earth. Life slash intelligence slash technologization is something

547
01:06:56,260 --> 01:07:02,860
that the earth does. And it not only does it, the earth makes and remakes itself through this

548
01:07:02,860 --> 01:07:09,780
process, building scaffolds for the next slightly more complex scaffold which are incorporated

549
01:07:09,780 --> 01:07:16,260
into the next scaffold and so on. It's why we have an atmosphere and why we have artificial

550
01:07:16,260 --> 01:07:24,900
intelligence. Some planets, at least one, fold themselves over time into forms of matter capable

551
01:07:24,900 --> 01:07:30,180
of not only participating in the cycle but of making abstractions about their own participation

552
01:07:30,180 --> 01:07:37,980
in them and thereby recalibrate them. Human brains are one such form. But they're not

553
01:07:37,980 --> 01:07:43,780
necessarily the only form capable of such abstractions. And nor are those brains

554
01:07:43,940 --> 01:07:48,500
independent of the technical systems of machine sensing and modeling and simulation and

555
01:07:48,500 --> 01:07:54,500
prediction that make those abstractions possible. Sapience itself is technological. More

556
01:07:58,260 --> 01:08:02,900
specifically, it's clear that simulation is, as said, not only part of how animal brains

557
01:08:02,900 --> 01:08:07,860
work, scientific inquiry works, how prediction works, it's ultimately how the recursion

558
01:08:07,860 --> 01:08:15,100
necessary for direct composition works. Simulation, as we know it, is an advanced coupling of

559
01:08:15,100 --> 01:08:21,100
biotic and abiotic systems. It is part of the scaffolding. That is, biospheres make

560
01:08:23,700 --> 01:08:31,700
technospheres that create biospheres that use technospheres to comprehend the whole dynamic. I

561
01:08:31,700 --> 01:08:37,340
think you see where I'm heading with this. What's called artificial intelligence is the

562
01:08:37,340 --> 01:08:43,420
name for a form of technologization that can occupy more than one position in this cycle.

563
01:08:43,420 --> 01:08:49,460
It can be part of the means of technical modeling that humans use to grasp planetary process.

564
01:08:49,460 --> 01:08:56,500
But it can also be the form of intelligence that's doing the grasping. It can be not only a

565
01:08:56,500 --> 01:09:02,500
means of planetary sapience but also co-constitutive of that sapience as such. That's one way in

566
01:09:02,660 --> 01:09:08,660
which the epistemic implications of AI get really interesting and really weird. It's then

567
01:09:12,240 --> 01:09:17,740
possible to locate AI not just in the reflective shadow of human intelligence but in the longer

568
01:09:17,740 --> 01:09:23,720
arc of intelligence as a planetary phenomenon and in the emergence of planetary intelligence as

569
01:09:23,720 --> 01:09:30,720
such. I suggested the very definition of these terms is, of course, put up for grabs not just by

570
01:09:31,240 --> 01:09:37,240
philosophy but by what clearly intelligent machine systems are already doing and by the need to

571
01:09:39,080 --> 01:09:45,080
shift the words to the reality. I repeat, to shift the words to the reality. I will end with

572
01:09:47,260 --> 01:09:53,260
this. There is at present a dangerous disconnect between cosmology and cosmology. By this I mean

573
01:09:54,260 --> 01:10:01,260
that if I go ask my friends in the astrophysics department, they will presume I want to know

574
01:10:01,260 --> 01:10:08,260
more about black holes and big bangs and curvature of space time and that kind of stuff. But if I

575
01:10:08,260 --> 01:10:12,220
go ask my friends in anthropology, they will presume I want to know about how different

576
01:10:12,220 --> 01:10:19,100
cultures imagine eschatology, kinship, how they think the universe began accordingly in

577
01:10:19,100 --> 01:10:25,100
relationship to those. Now, in the humanities there is, I am sorry to report, significant noise

578
01:10:27,400 --> 01:10:34,060
generated around this disconnect. The conclusion drawn adamantly by some is that the

579
01:10:34,060 --> 01:10:41,560
abstractions of scientific cosmology must be brought, quote, down to earth. Made to heal to the

580
01:10:41,560 --> 01:10:47,560
sovereignty of human cultures. I, however, wonder what are the cultures, plural, of the

581
01:10:49,180 --> 01:10:55,180
world that can be composed, not just inherited and lived and lived through that align their

582
01:10:56,640 --> 01:11:02,640
ways with the disclosures of planetary processes with that sapience that make them possible and

583
01:11:03,980 --> 01:11:09,980
which are graspable by them. That project is to collectively compose cosmologies adequate to the

584
01:11:10,860 --> 01:11:16,860
challenge of long-term planetary viability. Not necessarily the reconciliation of that with the

585
01:11:20,060 --> 01:11:26,060
diversity of cultural traditions by its subordination. That is, the disenchantment thesis

586
01:11:28,180 --> 01:11:34,180
that modern secularization of the cosmos removed cosmological grounding from culture is wrong.

587
01:11:35,180 --> 01:11:41,180
Instead, it made a real cosmology finally possible. Cultural cosmology emerges from the material

588
01:11:43,720 --> 01:11:49,140
possibility of thought. And that material possibility of thought emerges from the physical

589
01:11:49,140 --> 01:11:55,140
realities that are, in the long run, continuous among humans. Even if they exceed the

590
01:11:55,760 --> 01:12:03,760
uncertain boundaries of whatever humans are. So, instead of reifying cultural traditions and

591
01:12:03,760 --> 01:12:10,100
projecting them onto the universe, the better cosmopolitical project for the future is to

592
01:12:10,100 --> 01:12:16,100
grasp what is both convergent, because evolutionary, and what is divergent, because human, in the

593
01:12:17,760 --> 01:12:23,760
planetarization of civilizations and to derive abstraction and meaning accordingly. I hope

594
01:12:27,060 --> 01:12:33,060
that the implications for AI alignment, for shifting the words to the reality, are clear.

595
01:12:33,760 --> 01:12:40,180
What's at stake for that shift via AI is basically everything. Thank you.

