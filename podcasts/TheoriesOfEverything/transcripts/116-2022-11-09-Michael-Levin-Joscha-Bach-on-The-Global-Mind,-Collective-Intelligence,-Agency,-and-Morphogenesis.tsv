start	end	text
0	4400	Michael Levin's work on regulating intractable pattern formation in living systems has made
4400	8960	him one of the most compelling biologists of our time. In translation, this means that his team is
8960	13680	sussing out how to develop limbs, regenerate limbs, how to generate minds, and even life extension
13680	18000	by manipulating electric signals rather than genetics or epigenetics. His work is something
18000	21360	that I consider to be worthy of a Nobel Prize, and I don't think I've said that about anyone
21360	25840	on the podcast. Michael Levin's previous podcast, On Toe, is in the description. That's a solo
25840	30080	episode with him where we go into a two-hour deep dive, as well as there's a theolo-cution,
30080	34240	so that is him and another guest, just like today, except between Carl Fursten and Chris
34240	39120	Fields on consciousness. Josje Bach is widely considered to be the pinnacle of an AI researcher
39120	44080	dealing with emotion, modeling, and multi-agent systems. A large focus of Bach's is to build a
44080	49040	model of the mind from strong AI. Speaking of minds, Bach is one of the most inventive minds
49040	53520	in the field of computer science and has appeared several times on Toe prior. Again, there's a solo
53520	57920	episode. There's also a theolo-cution between Josje Bach and Donald Hoffman on consciousness,
57920	62160	and Josje Bach and John Vervecky also on consciousness and reality. Biology has much to
62160	67120	teach us about artificial intelligence and vice versa. This discussion between two brilliant
67120	73120	researchers is something that I'm extremely lucky, blessed, fortunate to be a part of, as well as us
73120	77440	as collective as an audience that are fortunate enough to witness. Thank you and enjoy this
77440	82880	theolo-cution between Josje Bach and Michael Levin. Welcome both Professor Michael Levin
82880	87200	and Josje Bach. It's an honor to have you on the Toe podcast. Again, both of you and then together
87200	92800	right now. Thank you. It's great to be here. Likewise. I enjoy very much being here and look
92800	97520	forward to this conversation. I look forward to it as well. So we'll start off with the question of
98160	102560	what is it that you, Michael, find most interesting about Josje's work? And then
102560	109440	Josje will go for you toward Michael. Yeah, I really enjoy the breadth. So I've been looking,
109440	114080	I think I've probably read almost everything on your website, the short kind of blog pieces
114080	122000	and everything. And yeah, I'm a big fan of the breadth of tackling a lot of the different issues
122000	128720	that you do with respect to computation and cognition and AI and ethics and everything.
128800	136640	I really like that aspect of it. And Josje? Yeah, my apologies. My blog is not up to date. I haven't
139440	145920	done any updates for a few years now on it, I think. So, of course, I'm still
146640	153600	in the process of progressing and having new ideas. And the ideas that I had in recent years,
153600	158320	I have a great overlap with a lot of the things that you are working on.
158320	165440	And when I listened to your Lex podcast last night, there were many thoughts that you had
165440	171760	that I had stumbled on that I've never heard from anybody else. And so I found this very
171760	177280	fascinating and thought, maybe let's look at some of these thoughts first and then go from there
177360	185920	and expand beyond those ideas. What I, for instance, found after thinking about how cells work
186800	191040	kind of obvious but missed by most people in neuroscience or in science in general,
191040	197280	is that every cell has the ability to send multiple message types and receive multiple
197280	201360	message types and do this conditionally and learn under which conditions to do that and to
201360	207200	modulate this. Also, every cell is an individual reinforcement learning agent. Single-celled animal
207200	211840	that tries to survive by cooperating with its environment gets most of its rewards from its
211840	218880	environment. And as a result, this means that every cell can in principle function like a neuron.
218880	224640	It can fulfill the same learning and information processing tasks as a neuron. The only difference
224640	230560	that exists with respect to neurons or the main difference is that they cannot do this over very
230560	236960	long distances because they are mostly connected only to cells that are directly adjacent. Of
236960	241760	course, neurons also only communicate to adjacent cells but the adjacency of neurons is such that
241760	246960	they have excellent parts of the cell that reach very far through the organism. So in some sense,
246960	253040	a neuron is a telegraph cell that uses very specific messages that are encoded in a way
253760	259280	like Morse signals in extremely short high-energy bursts that allow to send messages over very long
259280	265920	distances very quickly to move the muscles of an animal at the limit of what physics allows.
266640	271520	So it can compete with other animals in search for food. And in order to make that happen,
271520	276240	it also needs to have a model of the world that gets updated at this higher rate. So there is
276240	282080	going to be an information processing system that is duplicating basically this cellular brain that
282080	287120	is made from all the other cells in the body of the organism. And at some point, these two systems
287120	292720	get decoupled. They have their own codes, their own language, so to speak. But it still makes
292720	298400	sense, I guess, to see the brain as a telegraphic extension of the community of cells in the body.
298960	304880	And for me, this insight that I stumbled on just because means and motive that evolution would
304880	311120	equip cells with doing that information processing if the organism lives long enough and if the cells
311120	315120	share a common genetic destiny so they can get attuned to each other in an organism,
315840	322560	that basically every organism has the potential to become intelligent. And if it gets old enough to
322560	330640	possess enough data to get to a very high degree of understanding of its environment in principle.
330640	336560	So of course, a normal house plant is not going to get very old compared to us because its
336560	341360	information processing is so much slower. So they're not going to be very smart. But at the
341360	346720	level of ecosystems, it's conceivable that there is quite considerable intelligence. And then
346720	353360	I stumbled on this notion that our ancestors thought that one day in fairyland equals seven
353360	360800	years in human land, which is told in the old myth. And also, at some point, I revised my
360800	366800	notion of what the spirit is. For instance, a spirit is an old word for the operating system
366800	373680	for an autonomous robot. And when this word was invented, the only autonomous robots that were
373680	379600	known were people and plants and animals and nation states and ecosystems. There were no
379600	387440	robots built by people yet. But there was this pattern of control in it that people could observe
387440	393040	that was not directly tied to the hardware that was realized by the hardware but disembodied in
393040	398880	a way. And this notion of spirit is something that we lost after the Enlightenment when we tried to
398880	405760	deal with the wrong Christian metaphysics and superstition that came with it and threw out a
405760	410320	lot of babies with the bathwater. And suddenly, we basically lost a lot of concepts, especially
410320	417360	this concept of software that existed before in a way, this software being a control pattern or a
417360	422400	pattern of causal structure that exists at a certain level of coarse graining as some type of
422400	428480	very, very specific physical law that exists by looking at reality from a certain angle.
429120	436800	And what I liked about your work is that you systematically have focused on this
436800	442000	direction of what a cell can do, that a cell is an agent, and that levels of agency emerge
442000	449120	in the interaction between cells. And you use a very clear language and clear concepts. And you
449120	454080	obviously are driven by questions that you want to answer, which is unusual in science, I found.
454080	460400	Like most of our contemporaries in science get broken, if it doesn't happen earlier during the
460400	467440	PhD, into people who apply methods in teams instead of people who join academia because
467440	471680	they think it's the most valuable thing they can do with their lives to pursue questions that they
471680	477520	are interested in, want to make progress on. All right, Michael, there's plenty to respond to.
477520	485920	Yeah, yeah. Lots of ideas. Yeah, I think your point is very interesting about what really
485920	489920	fundamentally is the difference between neurons and other cells. Of course, evolutionarily,
489920	494960	they're reusing machinery that has been around for a very long time, since the time of bacteria,
494960	501520	basically, right? So our unicellular ancestors had a lot of the same machinery. And even, I mean,
501520	508160	of course, axons can be very long, but there are sort of intermediate structures, right? There are
508160	514240	tunneling nanotubes and things that allow cells to connect to maybe five or 10 cell diameters away,
514240	518480	right? So not terribly long, but also not immediate neighbors necessarily. So that kind
518480	526240	of architecture has been around for a while. And people like Gaurav Sohel look at very brain-like
526240	533840	electrical signaling in bacterial colonies. So I think evolution began to reuse this toolkit
533840	541440	specifically of using this kind of communication to scale up the computational and other kinds of
543440	550240	tricks a really long time ago. And I like to imagine that if somebody had come to
550240	555280	the people who were inventing connectionism and the first sort of perceptrons and neural networks
555280	560240	and so on, if somebody had come to them and said, oh, by the way, sorry, we're the biologists,
560240	564640	we got it wrong. Thinking isn't in the brain, it's in the liver. And so then the question is,
564640	568880	what would they do, right? Would they have changed anything about what they're doing? And then we
568880	572880	said, ah, now we have to rethink our model, or would they have said, fine, who cares? This is
572880	578480	exactly the same model. Everything works just as well. So I often think about that question,
578480	585600	what exactly do we mean by neurons? And isn't it interesting that we are able to steal most of
585600	592880	the tools, the concepts, the frameworks, the math from neuroscience and apply it to problems in
592880	597600	other spaces. So not movement in three-dimensional space with muscles, but for example, movement
597600	601760	through a morphous space, right? Anatomical morphous space. The techniques can't tell the
601760	606800	difference. We use all the same stuff, optogenetics, neurotransmitter signaling, we model active
606800	614560	inference, and we see perceptual by stability, you name it. We take concepts from neuroscience,
614560	620400	and we apply it elsewhere in the body. And generally speaking, everything works exactly
620400	625200	the same. And that shows us, I think, what you were saying, that there's this really interesting
628080	633280	symmetry between these, that a lot of the distinctions that we've been making are in terms
633280	636560	of having different departments and different PhD programs and other things that say,
636560	640640	no, this is neuroscience, this is developmental biology. A lot of these things are just not
642560	649360	as firm distinctions as we used to think. And I suspect that people who insist on strong
649360	656000	disciplinary boundaries do this out of a protective impulse. And what I noticed by
656000	662720	studying many disciplines when I was young, that the different methodologies are so incompatible
662720	669360	across fields, that when I was studying philosophy or psychology, I felt that computer scientists
669360	674320	would be laughing about the methods that each of these fields are using to justify what they're
674320	682240	doing. And this, I think, is indicative of a defect. Because if you take science into the
682240	688240	current regime of regulating it entirely by peer review, there is no external authority. Even the
688240	693840	grand authorities are mostly fields of people who have been trained in the sciences, in existing
693840	698560	paradigms, and then are finding the continuation of those paradigms from the outside. This
700480	706720	meta-paradigmatic thinking does not really exist that much in a peer-reviewed paradigm. And
706720	711040	ultimately, when you do peer review for a couple generations, it also means that if your peers
711040	717200	deteriorate, there is nothing who pulls your science back. And what I missed specifically in
717200	721280	a lot of the way in which neuroscience is done is what you call the engineering stance.
722400	727120	And this engineering sense is very powerful, and you get it automatically when you're a computer
727120	731280	scientist, because you don't really care what language is it written in. What you care in is
731280	737680	what causal pattern is realized. And how can this be realized? And how could I do it? How would I
737680	743280	do it? How can evolution do it? What it means is disposal, and this determines the search space for
743280	747920	the things that I'm looking for. But this requires that I think in causal systems.
747920	754640	And this thinking in causal systems is impossible not to do for a computer scientist,
755360	760480	but it is unusual outside of computer science. And once you realize that, it's very weird.
761120	766400	Suddenly, you have notions that try to replace causal structure with, say, evidence. And then
766400	773200	you notice that, for instance, evidence-based medicine is not about probabilities of how
773200	778080	something is realized and must work. You see people on the cruise ship getting infected over
778080	782400	distances, and you think, oh, this must be airborne. But no, there is no peer-controlled
782400	787040	study, so there is no evidence that it's airborne. And when you look at its disciplines from the
787040	793120	outside, like in this case, the medical profession or the medical messaging and decision-making,
793120	800160	or I get terrified because it directly affects us. And in terms of neuroscience, of course,
800160	805440	there's more theoretical for the most part, but there must be a reason why it's for the most part
805440	813120	a theoretical, why there is no causal model that clinicians can use to explain what is happening
813120	819600	in certain syndromes that people are exhibiting. And I notice this when I go to a doctor and
820240	825840	even at a reputable institution like Stanford, that most of the neuroscientists at some level
825840	832080	there, or most of the neurologists that I'm talking to, at some level dualists, that they
833200	839200	don't have a causal model of the way in which the brain is realizing things. And a lot of studies
839200	845600	which discover that very simple mechanisms like the ability of human beings to use grammatical
845600	849760	structure are actually reflected in the brain. This is so amazing. Who would have thought?
853440	858240	But the developments that existed in computer science have led us on a completely different
858240	865920	track. The perceptron is vaguely inspired by what the brain might be doing, but I think it's really
865920	871120	a toy model or a caricature of what cells are doing. Not in the sense that it's inferior,
871120	877440	it's amazing what you can brute force with the modern perceptron variations. The current
877440	882560	machine learning systems are mind blowing in what they can do, but they don't do it like
882560	888960	biological organisms at all. It's very different. The cells do not form change in which they weight
889760	896640	sums of real numbers. There is something going on that is roughly similar to it, but there's
896640	901040	a self-organizing system that designs itself from the inside out, not by a machine learning
901040	905360	principle that applies to the outside and updates weights after reading and comparing them
905360	911200	and computing gradients to the system. So this perspective of local self-organization by
911200	917200	reinforcement agents that try to trade rewards with each other, that is a perspective that I
917200	923760	find totally fascinating. And I wish this would have come from neuroscience into computer science,
923760	928000	but it hasn't. There are some people which have thought about these ideas to some degree,
928000	933600	but there's been very little cross pollination. And I think all this talk of neuroscience
933600	936480	influencing computer science is mostly visual thinking.
938880	944640	Yeah. It's also, I find this, you know, what you were saying about the different disciplines,
944640	952000	it's kind of amazing how, well, when I give a talk, I can always tell which department I'm in by
952000	956400	which part of the talk makes people uncomfortable and upset. And it's always different depending on
956400	960000	which department it is, right? So there are things you can say in one department that are
960000	964080	completely obvious. And you say this in another group of people and they throw tomatoes. I think
964080	972400	this is just craziness. For instance, I could say in a neuroscience department, I could say
973600	978640	information can be processed without changes in gene expression. You don't need changes in
978640	983920	gene expression to process information because the processing inside a neural network runs on
983920	989600	the physics of action potentials, right? So you can do all kinds of interesting information
989600	994240	processing and you don't need transcriptional or genetic change for that. If I say the same thing
994240	998640	in a molecular genetics department that say, hey, these cells could be processing tons of information
998640	1004320	long before the transcriptome ever finds out about it, this is considered just completely wild
1004320	1010560	because it's thought that most of the hard work or in fact, all of the hard work is done in gene
1010560	1014560	regulatory circuits and things like that, right? There are other examples. If I say,
1017600	1022640	here's a collection of cells that communicate electrically to remember a particular spatial
1022640	1028720	pattern, again, molecular cell biology, what do you mean? How can a collection of cells
1028720	1032720	remember a spatial pattern? But again, in neuroscience or in an engineering department,
1032720	1037520	yeah, of course. Of course, they have electrical circuits that remember patterns and can do pattern
1037520	1044960	completion and things like that. So views of causality, views of just lots of things like
1044960	1052960	that that are very obvious to one group of people is completely taboo elsewhere. So that distinction,
1052960	1060400	and yeah, and as Joshua just said, it impacts everything. It impacts education, it impacts
1060480	1066480	grants, grant reviews, because when these kind of interdisciplinary grants come up,
1067200	1071280	the study sections have a really hard time finding people that can actually review them
1071280	1077760	because what often happens is you'll get some kind of computational biology grant and you put a
1077760	1082480	proposal and you'll have some people on the panel who are biologists and some people who are the
1082480	1087600	computational folks. And it's very hard to get people that actually can appreciate both sides of
1087600	1092080	it and understand what's happening together. So they will sort of each critique a certain part
1092080	1097760	of it. And the other part, they say, I don't know what this is. And as a result, grants like that
1097760	1102960	don't tend to not have a champion, one person who can say, no, I get the whole thing and I think
1102960	1111520	it's really good or not. So yeah, even to the point where I'm often asked when people want to
1111520	1118400	you list me somewhere, they'll say, so what are you? What's your field? And I never know
1118400	1121440	how to answer that question. To this day, it's been 30 years. I still don't know how to answer
1121440	1127280	that question. I just can't boil it down to one. It just wouldn't make any sense to say any of the
1128880	1133520	traditional fields. So what do you say, Joshua, when someone asks you what field you're in?
1134480	1142080	And it depends on who's asking. So for instance, I found it quite useful to sometimes say,
1143840	1153200	sorry, I'm not a philosopher, but this or I'm not that interested in machine learning. And I
1153200	1158960	did publish papers in philosophy and in machine learning, but it's not my specialty in the sense
1158960	1165440	that I need to identify with it. And in some sense, I guess that these categories are important
1165440	1171920	when you try to write a grant proposal or when you try to find a job in a particular institution
1171920	1177520	and they need to fill a position. But for me, it's more, what questions am I interested in?
1177520	1181200	What is the thing that I want to make progress on or what is the thing that I want to build right
1181200	1188080	now? And I guess that in terms of the intersection, I'm a cognitive scientist.
1189920	1195360	So I was asking Michael, prior to you joining Yoshi, why is it Michael that you were doing
1195360	1199280	podcasts? And if I understand correctly, part of the reason was because you think out loud,
1199280	1202720	and you'd like to hear the other person's thoughts and take notes and espers your own.
1202720	1206400	And firstly, like Michael, you can correct me if that's incorrect. And then secondly,
1206400	1211280	Yoshi, I'm curious for an answer for this, the same question, what is it that you get out of
1211280	1216160	doing podcasts other than, say, some marketing for if you were promoting something, which I
1216240	1220720	don't imagine you are currently. No, I'm not marketing anything.
1221840	1229760	What I like about podcasts is the ability to publish something in a format that is engaging
1229760	1237600	to interesting to people who actually care about it. I like this informal way of holding on to some
1237600	1243920	ideas and also like conversations as a medium to develop thought is this space in which we can
1244880	1250160	reflect on each other, look into each other's minds, interact with the ideas of others in real
1250160	1256320	time. The production format of a podcast creates a certain focus of the conversation that can be
1256320	1264160	useful. And it's a pleasant kind of tension that focuses you to stay on task. And I also found that
1265280	1271840	it's generally useful to some people. The feedback that I get is that people tell me,
1271840	1278000	I had this really important question and I found this allowed me to make progress on it.
1278000	1284240	And I feel much better now about these questions. I just clarified something for me that has plagued
1284240	1290320	me for years and put me on track to solving it, or this has inspired the following work.
1290320	1296560	So it's a form of publishing ideas and getting them into circulation in our global hive minds
1297280	1305200	that is very informal in a way, but it's not useless. And also it relieves me in this instance,
1305200	1310320	at least of the work of cutting, editing, and so on. But anyways, I'm very grateful that you
1310320	1316160	provide the service of curating our conversation and putting it in a form that is useful to other
1316160	1324160	people. Yeah, there's something, well, two things I was thinking of. One is that I have conversations
1324160	1327760	with people all day long about these issues, right? So people in my lab, collaborators,
1327760	1332240	whatever. And most, of course, the vast majority of those conversations are not recorded and they
1332240	1336560	just sort of disappear into the ether. And then I take something away from it and the other person
1336560	1341600	takes something away from it. But I've often thought that, isn't it a shame that all of this
1342880	1347440	just kind of disappears and it would be amazing to have a record of it? And of course, not every
1347440	1354080	conversation is gold, but a lot of them are useful and interesting. And there are plenty of
1354080	1359920	people that could be interested and could benefit from it. So I really like this aspect
1359920	1365920	that we can have conversations and then they're sort of canned and they're out there for people
1365920	1371040	who are interested. The other kind of aspect of it, which I don't really understand, but it's kind
1371040	1379600	of neat, is that when somebody asks me to pre-record a talk, it takes a crazy amount of time
1379600	1383120	because I keep stopping and realizing, ah, I could have said that better. Let me start from
1383120	1388080	the beginning. And it's just, it's an incredible ordeal. Whereas something like this that's real
1388080	1393920	time, I'm sure has as many mistakes and things that I would have rather fixed later, but you
1393920	1397200	can't do that, right? So you just sort of go with it and that's it. And then it's done and you can
1397200	1403200	move on. So I like that real time aspect of it because it just helps you to get the ideas out
1403200	1409360	without getting hung up and trying to redo things 50 times. Yeah, it's a format that allows
1409360	1416960	tentativity. If we have published, we have a culture in sciences that requires us to
1417600	1423120	publish the things that we can hope to prove and make the best proof that we can. But when we have
1423120	1428480	anything complicated, especially when we take our engineering stance, we often cannot prove how
1428480	1434320	things work. Instead, our answers are in the realm of the possible and we need to discuss the
1434320	1440800	possibilities. And there is value in understanding these possibilities to direct our future
1440800	1446560	experiments and the practical work that we do to see what's actually the case. And we don't really
1446560	1452880	have a publication format for that. We don't get neuroscientists to publish their ideas on how the
1452880	1457280	mind works because nobody has a theory that they can prove. And as a result, there is basically a
1457280	1462640	vacuum where theories should be. And the theory building happens informally in conversations
1462640	1468320	that basically requires personal contact, which is a big issue once conferences been virtual
1468320	1473920	because that contact diminished. And you get a lot of important ideas by reading the publications
1473920	1479920	and so on. But this what could be or connecting the dots or possibilities or ideas that might be
1479920	1484880	proven wrong later that we just exchange as in the status of ideas. That is something that has
1484880	1492000	a good place in a podcast. Is this podcast, not this TOE podcast, but podcast in general something
1492000	1496880	new? So for instance, I was thinking about this and I well podcasts go back a while and Brogan
1496880	1503120	invented this long form format or popularized it. However, on television, there are interviews,
1503120	1507760	so there's Oprah and those are long one hour, there's 60 minutes. And then back in the 90s,
1507760	1512800	there was a three and a half hour, it's essentially a podcast, it's like Charlie Rose, three and a
1512800	1518480	half hour conversation. It's like a Theolocution with Freeman Dyson, Daniel Dennett, Stephen J.
1518560	1526640	Gould, like the Rupert Sheldrake, all of those on the same one format, it's essentially a podcast
1526640	1531040	talking about metaphysics. Like, man, oh, man, I can't believe that got published. And then also,
1531040	1535200	I think about it, well, did Plato have the first podcast? Because he's just publishing these
1535200	1538720	dialogues and you read them, but it's not as if they're maybe he would have published it in video.
1538720	1544960	I think Plato was the first podcaster. So is there something new about this format of podcasting
1544960	1549840	that wasn't there before? Or what's new about it? I think it's like it's like blogging. Blogging
1549840	1557920	is also not new, right? Being able to write text that you publish, and people can follow what you
1557920	1563200	are writing and so on, did exist in some sense before, but the internet made it possible to
1563200	1568480	publish this for everyone. You don't need a publisher anymore. And you don't need a TV
1568480	1573120	studio anymore. You don't need a broadcast station that is recording your talk show and sends it to
1573120	1578720	an audience. There is no competition with all the other talk shows because there is no limitations
1578720	1585840	on how many people can broadcast at the same time. And this allows an enormous diversity of thoughts
1585840	1592560	and small productions that are done at a very low cost, lowering the threshold for putting
1592560	1598080	something out there and seeing what happens. So in this sense, it's the ecosystem that emerged is new
1598080	1602080	because a variable change that changed the cost of producing a talk show.
1603440	1605760	Right. Michael, you agree?
1606640	1614560	Yeah. Yeah. I mean, yes, that and all of that. And also just the fact that, as you just said,
1614560	1619040	these kind of like long form things were fairly rare. So most of the time, if you're going to be
1619040	1624720	in one of the traditional media, they tell you, okay, you've got three minutes. We're going to
1624720	1628640	cut all this stuff and we're going to boil it down to three minutes. And this is often incredibly
1628640	1633760	frustrating. And I understand. I mean, we're drowned in information. And so there is obviously
1633760	1638880	a place for very short statement on things, but the kind of stuff that we're talking about cannot
1638880	1645120	be boiled down to TV sound bites or anything. It's just not. And so the ability to have these
1645120	1650080	long form things so that anybody who wants to really dig in can hear what the actual thought
1650080	1657520	is as opposed to something that's been just boiled into a very, very, very short statement,
1657520	1661120	I think is invaluable. Just being able to have it out there for people to find.
1661680	1666880	What's some stance of yours, some belief that has changed most drastically in the past few years,
1666880	1671360	let's say three, and it could be anywhere from something abstruse and academic to more colloquial,
1671360	1675520	like I didn't realize the value of children or overvalued children. Now I'm stuck with them.
1675520	1680080	Like, geez, that was a mistake. Yeah. So something where I changed
1680080	1687600	my mind was RNA-based memory transfer. And I think it's a super interesting idea in this context
1687600	1693040	because it's close to stuff that Michael has been working on and is interested in.
1693840	1700320	There have been some experiments in the Soviet Union, I think in the 70s, where scientists took
1700320	1707200	planaria, trained them to learn something. I think they learned how to be afraid of electric
1707200	1713040	shocks and things like that. And then they put their brains into a blender, extracted the RNA,
1713040	1716400	injected other planaria with it, and these other planaria had learned it.
1717280	1723440	And I learned about this as a kid when I, in the 1980s, read Soviet science fiction literature.
1723440	1731680	I grew up in Eastern Germany. And the evil scientist harvested the brains of geniuses
1731680	1738240	and injected himself with RNA extracted from these brains and thereby acquired the skills.
1738240	1743760	And even though I'm pretty sure this probably doesn't work if you do it at this level,
1745920	1751600	this was inspired by this original research. And I later heard nothing about this anymore.
1751600	1759600	And so I dismissed it as similar things as I read in Sputnik and other Russian publications,
1759600	1766240	which create their own mythological universe about ball lightning that is agentic and possibly
1766240	1771280	sentient and so on. And dismissed this all as basically another universe of another reader's
1771280	1776880	digest culture that is producing its own ideas that then later on get dissolved once science
1776880	1781360	advances. Because everybody knows it's synapses, it's connections between neurons that matter.
1781360	1786000	The RNA is not that important for the information processing. It might change some state, but you
1786000	1790880	cannot learn something by extracting RNA and re-injecting it into the next organism. Because
1790880	1796640	how would that work if it's done in the synapses? And then recently there were some papers which
1796640	1803360	replicated the original research and has been replicated from time to time in different types
1803360	1812160	of organisms. But to my knowledge, not in, of course, macaques or not even mice. So it's not
1812160	1817360	clear if their brains work according to the same principles as planaria. But planaria are not
1819360	1823440	extremely simple organisms, only a handful of neurons. They are something intermediate.
1823440	1828560	And so their main architecture is different from ours. And the functioning principles of
1828560	1832800	their neurons might be slightly different, but it's worth following this idea and going down
1832800	1838560	that rabbit hole. And then I looked from my computer science engineering perspective and
1838560	1845280	I realized that there are always things about the synaptic story that I find confusing because
1845280	1852000	they're very difficult to implement. For instance, weight sharing. As a computer scientist, I require
1852000	1857040	weight sharing. I don't know how to get around this. If I want to entrain myself as computational
1857040	1861680	primitives in the local area of my brain, for instance, the ability to rotate something,
1862640	1869680	rotation is some operator that I apply on a pattern that allows this pattern to be represented in a
1869680	1875120	slightly different way to have this object rotated a few degrees. But an object doesn't
1875120	1880800	consist of a single point. It consists of many features that all need to get the same rotation
1880800	1886320	applied to them using the same mathematical primitives. So how do you implement the same
1886320	1892800	operator across an entire brain area? Do you make many, many copies of the same pattern?
1892800	1896640	And so computer scientists solved that with so-called convolutional neural networks,
1896640	1902240	which basically use the same weights again and again in different areas, only
1902960	1906800	training them once and making them available everywhere. And that would be very difficult
1906800	1913520	to implement in synapses. Maybe there are ways, but it's not straightforward. Another thing is
1913520	1918880	if we see how training works in babies, they learn something and then they get rid of the
1918880	1925120	surplus synapses. Initially, they have much more connectivity than they need. And after they've
1925120	1930800	trained, they optimize the way in which the wiring works by discarding the things they don't need to
1930800	1937920	compute what they want to compute. So it's like calling the synapses. It does not freeze or edge
1937920	1942960	the learning into the brain, but it optimizes the energy usage of the brain. Another issue is that
1943760	1947920	patterns of activation are not completely stable in the brain. In the cortex, if you look,
1948480	1953200	you find that they might be moving the next day or even rotate a little bit, which is also difficult
1953200	1958080	to do with synapses. You cannot read out the weights and copy them somewhere else in an easy,
1958080	1963520	straightforward fashion. And another issue is defragmentation. If you learn, for instance,
1963520	1968960	your body map into a brain area and then somebody changes your body map because you have an accident
1968960	1973440	and lose a finger or somebody gives you an artificial limb and you start to integrate
1973440	1978560	this into your body map, how do you shift all the representations around? How do you make space for
1978560	1983600	something else and move it? Or also initially, when you set up your maps via happy and learning,
1983600	1988320	how do you make sure that the neighborhoods are always correct and you don't need to realign
1988320	1993280	anything? And I guess you need some kind of realignment. And all these things seem to be
1993280	2000560	possible when you switch to a different paradigm. And so if you take this RNA base series seriously,
2000560	2007840	go down this rabbit hole, what you get is the neurons are not learning a local function over
2007840	2012800	its neighbors, but they are learning how to respond to the shape of an incoming activation front,
2013600	2017120	like the spatial temporal pattern in their neighborhood. And they are densely enough
2017120	2023840	connected so the neighborhood is just a space around them. And in this space, they basically
2023840	2029680	interpret this according to a certain topology to say this is maybe a convolution that gives me
2029680	2034480	two and a half D or it gives me two D or one D or whatever the type of function is that they want
2034480	2043520	to compute and they learn how to fire in response to those patterns and thereby modulate the patterns
2043520	2047760	when they're passed on. So the neurons act something like self-modulating ether,
2047760	2054880	so which wavefronts propagate that perform the computations. And they store the responses to
2054880	2062400	the distributions of incoming signals, possibly in RNA. So you have little mixtapes, little tape
2062400	2067680	fragments that they store in a summa and that it can make more of very cheaply and easily. If
2067680	2071920	they are successful mixtapes and they're useful computational primitives that they discovered,
2071920	2077520	they can distribute this to other neurons through the entire cortex. So neurons of the same type
2077520	2083680	will gain the knowledge to apply the same computational primitives. And that is something
2083680	2087440	I don't know if the brain is doing that and if the human brain is using these principles
2087440	2092000	or if it's using them a lot and how important this is and how many other mechanisms exist.
2092000	2096320	But it's a mechanism that we haven't, to my knowledge, tried very much in AI and computer
2096400	2103040	science. And it would work. There is something that is a very close analog, that is a neural
2103040	2109760	cellular automaton. So basically instead of learning weight shifts or weight changes between
2109760	2116720	adjacent neurons, what you learn is global functions that tell neurons on how to respond
2116720	2122960	to patterns in the neighborhood. And these functions are the same for every point in your
2122960	2129520	matrix. And you can learn arbitrary functions in this way. And what's nice about this is that you
2129520	2134960	only need to learn computational primitives once. Our current neural networks need to learn the same
2135600	2140000	linear algebra over and over again in many different corners of the neural network
2140000	2144000	because you need vector algebra for many kinds of operations that we perform.
2144640	2149120	For instance, operations in space where we shift things around or rotate them.
2149760	2155760	And if they could exchange these useful operations with each other and just apply
2155760	2160800	an operator whenever the environment dictates that this would be a good idea to try to apply
2160800	2165360	this operator right now in this context, that could speed up learning. That could make training
2165360	2170320	much more sample efficient. So something super interesting to try. And this is one of the rabbit
2170320	2176720	holes I recently fell down over. I changed my thinking based on some experiment from
2176720	2182080	neuroscience that doesn't have very big impact for the mainstream of neuroscience,
2182080	2185840	but that I found reflected in Michael's work with planaria.
2187600	2192880	Yeah, that's super interesting stuff. I can sprinkle a few details onto this.
2195520	2201840	So the original finding in planaria was a guy named James McConnell at Michigan, actually,
2201840	2206320	in the US. And then that was in the 60s, the early 60s. And then there were some really
2206320	2211680	interesting Russian work that picked it up after that. We reproduced some of it recently
2211680	2218480	in using modern quantitative automation and things like this. But one of the really cool
2218480	2223360	aspects of this, and there's a whole community, by the way, with people like Randy Gallistil
2223360	2229280	and Sam Gershman and, of course, Glantzman, David Glantzman, and people who are... That story of
2229280	2235200	memory in the precise details of the synapses, that story is really starting to crack actually
2235200	2239280	for a number of reasons. But one of the cool things that was done in the Russian work,
2239280	2246240	and it was also done later on by Doug Blackiston, who's in my lab now as a staff scientist and other
2246240	2252560	people, is this. Certain animals that go through larval stages. So you can take... So the Russians
2252560	2260160	were using beetle larvae, and Doug and other people used moths and butterflies. So what happens is
2260880	2266560	you train the larvae. So here you've got a caterpillar. So this caterpillar lives in a
2266560	2270800	two-dimensional world. It's a soft-bodied robot. It lives in a two-dimensional world. It eats leaves
2270800	2275920	and so on. And so you train this thing for a particular task. Well, during metamorphosis,
2275920	2280480	it needs to become a moth or butterfly, which it lives in a three-dimensional world. Plus,
2280480	2285680	it's a hard-bodied creature. So the controller is completely different for running a caterpillar
2285680	2290720	versus a butterfly. So during that process, what happens is the brain is basically dissolved.
2290720	2295600	So most of the connections are broken. Most of the cells are gone. They die. You put together
2295600	2300160	a brand new brain that self-assembles, and you can ask all sorts of interesting philosophical
2300160	2304000	questions of what it's like to be a creature whose brain is undergoing this massive change.
2304640	2311520	But the information remains. And so one can ask, okay, certainly for computer science, it's amazing
2311520	2318400	to have a memory medium that can survive this radical remodeling and reconstruction.
2318400	2325840	And there's the RNA story, but also you had mentioned, does this work for mammals?
2325840	2330960	So there was a guy in the 70s and 80s, there was a guy named George Ungar who did tons of,
2330960	2337920	he's got tons of papers. He reproduced it in rats. So his was Fear of the Dark. And he actually,
2338640	2346400	by establishing this assay and then fractionating their brains and extracting this activity,
2346400	2352320	now he thought it was a peptide, not RNA. So he ended up with a thing called scotophobin,
2352320	2355760	which turns out to be, I think, an eight mer peptide or something.
2355760	2360080	And the claim was that you can transfer this scotophobin, you can synthesize it
2360080	2366320	and then transfer it from brain to brain. And that's what he thought it was. And then I think
2366320	2372560	David Glansman favors RNA again. But yeah, I agree with you. I think that's a super important
2372560	2380640	story of how it is that this kind of information can survive just massive remodeling of the
2380640	2386480	cognitive substrate. In planaria, what we did, and planaria, they have a true centralized brain.
2386480	2390720	They have all the same neurotransmitters that we have. They're not a simple organism.
2391920	2396080	What we did was McConnell's first experiments, which is to train them on something. And we
2396080	2401440	train them to recognize a laser etched kind of bumpy pattern on the bottom of the dish and to
2401440	2405440	recognize that that's where their food was going to be found. So they made this association between
2405440	2410800	this pattern and getting food. And then we cut their heads off and we took the tails and the
2410800	2415040	tails sit there for 10 days doing nothing. And then eventually they grow a new brain.
2415040	2419440	And what happens is that information is then imprinted onto the new brain and then you can
2419440	2425840	recover behavioral evidence that they remember the information. So that's pretty cool too,
2425840	2431360	because it suggests that, well, we don't know if the information is everywhere or if it's in other
2431360	2436560	places in the peripheral nervous system or in the nerve core that we don't know where it is yet.
2436560	2441280	But it's clear that it can move around, that the information can move around in the body because
2441280	2446000	it can be in the posterior half and then imprinted onto the brain, which actually drives all the
2446000	2451360	behaviors. So thinking about that, I totally agree with you that this is a really important
2451360	2456800	rabbit hole for asking, but there's an interesting puzzle here, which is this.
2457440	2465680	It's one thing to remember things that are evolutionarily adaptive, like fear of the dark
2465680	2470160	and things like this, but imagine, and this hasn't really been done well, but imagine for a moment
2470720	2475840	if we could train them to something that is completely novel. Let's say we train them,
2476880	2481040	three yellow life flashes means take a step to your left, otherwise you get shocked, something
2481040	2485200	like that. And let's say they learn to do it. We haven't done this yet, but let's say this could
2485200	2490720	work. One of the big puzzles is going to be when you extract whatever it is that you extract,
2490720	2496080	let's say it's RNA or protein, whatever it is, you stick it into the brain of a recipient host.
2496720	2500240	And in order for that memory to transfer, one of the things that the host has to be able to do is
2500240	2505040	has to be able to decode it. And in order to decode it, it's one thing if we share the same
2505040	2509200	codebook and by evolution, we could have the same codebook for things that come up all the time,
2509200	2516560	like fear of the dark, things like that. But how would the recipient look at a weird
2516560	2521840	sort of some kind of crazy hairpin RNA structure and analyze and be like, oh yes,
2521840	2527440	that's three light flashes and then a step to the left, I see. So you would need to be able to
2527440	2532800	interpret somehow this structure and convert it back to the behavior. And for behaviors that are
2532800	2538160	truly arbitrary, that might be, I don't know actually how that would work. And so I think
2538160	2545920	the frontier of this field is going to be to have a really convincing demonstration of a transfer
2545920	2551680	of a memory that doesn't have a plausible pre-existing shared evolutionary decoding,
2551680	2556400	because otherwise you have a real puzzle as to how the decoding is going to work.
2556400	2561760	So this idea, and then even without the transfer, you can also think of it a different way.
2561760	2566960	Every memory is like a message, is like basically a transplanted message from your past self to
2566960	2570960	your future self, meaning that you still have to decode your memories. Whatever your memories are,
2570960	2574640	in an important sense, you have to, those N-grams, you have to decode them somehow.
2574640	2581440	So that whole issue of encoding and decoding, whatever the substrate of memory is, is maybe
2581440	2587760	one of the most important questions there are. One of the ways we can think about these N-grams,
2587760	2594800	I think that there are priors that condition what kinds of features are being spawned in
2594800	2600560	which context. For instance, when we see a new scene, the way that perception seems to be working
2600560	2607680	is that we spawn lots of feature controllers that then organize into objects that are controlled at
2607680	2613840	the level of the scene. And this is basically a game engine that is forming in our brain,
2613840	2620640	that is creating a population of interacting objects that are tuned to track our perceptual
2620640	2626160	data at the lowest level. So all the patterns that we get from our retina and so on are samples,
2626160	2631920	noisy samples that are difficult to interpret, but we are matching them into these hierarchies
2631920	2638800	of features that are translated into objects that assign every feature to exactly one object and
2638800	2645040	every pixel, so to speak, to exactly one, except in the case of transparency, and use this to
2645040	2649840	interpret the scene that is happening in front of us. And when we are in the dark, what happens is
2649840	2655120	that we spawn lots of object controllers without being able to disprove them, because there is no
2655120	2661440	data that forces us to reject them. And if you have a vivid imagination, especially as a child,
2661440	2666000	you will fill this darkness automatically with lots of objects, many of which will be scary.
2667120	2672960	And so I think that lots of the fear of the dark doesn't need a lot of encoding in our brain. It
2672960	2678160	is just an artifact of the fact that there are scary things in the world which we learn to
2678160	2682720	represent at an early age, and that we cannot disprove them, that they will just spawn.
2684000	2690880	I remember this vividly as a child, that whenever I had to go into the dark basement to get some
2690880	2697360	food in our house in the countryside, that this darkness automatically filled with all sorts of
2697360	2704720	shapes and things and possibilities. And it took me later to learn that you need to be much more
2704720	2710640	afraid of the ghosts that can hide in the light. So what would be the implications of if you were
2710640	2716160	able to transfer memory for something that's not trivial, so nothing that's like an archetype of
2716160	2723840	fear of the dark between a mammal like rats? And when I say transfer memory, I mean, in this way
2723840	2728320	that you blend up the brain or you, and also, can you explain what's meant by, I think I understand
2728320	2731760	what it means to blend the brain of a planaria, but I don't think that's the same process that's
2731760	2737680	going on in rats. Maybe it is. Well, Ungar did exactly the same thing. He would train rats
2737680	2742480	for particular tasks. He would extract the brain, literally liquefy it to extract the chemical
2742480	2747840	contents. He would then either inject the whole extract or a filtered extract where you would
2747840	2752800	divide it up. You'd set fractionate it. So here's the RNAs, here's the proteins, here are other
2752800	2759440	things. And then he would inject that liquid directly into the brains of recipient rats.
2760400	2766320	When you do that, you lose spatial structure on the input because you just blended your brain.
2766320	2770720	Whatever spatial structure there was, you just destroyed it. Also on the recipient,
2772320	2776560	you just inject it. You're not finding that particular place where you're going to stick
2776560	2780720	it. You just inject this thing right in the middle of the brain. Who knows where it goes,
2780720	2786720	where the fluid goes. There's no spatial specificity there whatsoever. So if that works,
2786960	2792800	what you're counting on is the ability of the brain to take up information via a completely
2792800	2797680	novel route. So it's not information that's, for example, visual, right? Visual information
2797680	2803040	that comes in exactly the same place all the time, right? There are optic nerves that connect to
2803040	2808320	the same place in the brain, and that's where that information arrives. If you bathe the brain in
2808320	2814400	some sort of informational extract, you're basically asking the cells to take it up almost
2814400	2818560	as a primitive animal would with taste or touch you, right? That's kind of distributed all over
2818560	2822320	the body, and you can sort of pick it up anywhere, and then you have to process this information.
2822320	2826800	So you've got those issues right off the bat, right? That you've destroyed the incoming spatial
2826800	2833120	structure. You can't really count on where it's going to land in the brain. And then the third
2833120	2837920	thing, as you just mentioned, is the idea that, especially if we start with information that
2838160	2846880	especially if we start with information that isn't any, that is so kind of specific and
2851600	2855920	invented, that three light flashes means move to your left. I mean, there's never been an
2855920	2860000	evolutionary reason to have that encoded. Like as you just said, having a fear of the dark is
2860000	2864720	absolutely a natural kind of thing that you can expect. And then there are many other things like
2864720	2870880	that. But something as contrived as three light flashes, and then you move to your left,
2870880	2876320	there's no reason to think that we have a built-in way to recognize that. So when you as a recipient
2876320	2881680	brain are handed this weird molecule with a particular structure or a set of molecules,
2882240	2887920	being able to analyze that, having the cells in your brain or other parts of the body actually,
2887920	2892880	that could analyze that and recover that original information would be extremely puzzling. I
2892880	2897680	actually don't know how that would work. And I'm a big fan of unlikely sounding experiments
2897680	2903360	that have implications if they would work. So this is something that I think should absolutely
2903360	2909360	be done. And at some point we'll do it, but we haven't done it yet. So how far did the
2909360	2915200	research in my school, what is the complexity of things that could be transmitted via this route?
2915920	2925120	I don't remember everything that he did. The vast majority of, he did not go
2926400	2930400	far to test all the complexities. What he tried to do was, because as you can imagine,
2930400	2935840	he faced incredible opposition, right? So everybody sort of wanted to critique this thing.
2935840	2941120	So he spent all of his time on, he picked one simple assay, which was this fear of the dark
2941120	2948640	thing. And then he just bashed it for 20 years to just finally try to kind of crack that into the
2948640	2954560	paradigm. He did not, as far as I know, do lots of different assays to try and make it more complex.
2954560	2961600	I think it's very ripe for investigation. Did anyone else build upon his work?
2962880	2967280	Not that I know. I mean, David Glansman is the best modern person who works on this, right? So
2967280	2976240	he does a plesia and he does RNA. So he favors RNA. There's a little bit of work from Oded Rahavi
2976240	2982720	in Israel with C. elegans. He's kind of looking into that. There's related work that has to do
2982720	2992000	with cryogenics, which is this idea that if memories are a particular kind of dynamic electrical
2992000	2997760	state, then some sort of cryogenic freezing is probably going to disrupt that. Whereas if
2997760	3003360	it's a stable molecule, then it should survive. So again, I think there are people interested
3003360	3007360	in that aspect of it, but I'm not sure. I'm not sure they've done anything with it.
3008000	3016000	There's also Gaurav Venkataraman. I think he's at Berkeley. He told me that
3016880	3020720	he has been working on this for several years, but he said it's sociologically tricky.
3021520	3025840	And that's to me fascinating that we should care about that.
3026560	3027680	What does he mean by that?
3028480	3034320	What do you care about? What stupid people think? If this possibility exists that this works,
3034320	3040320	the upside is so big that it's criminal to not research this. I think it's a disaster
3040320	3045040	that you can read introductory textbooks on neuroscience and never ever hear about any of
3045120	3051200	these experiments. Everybody who gets the introductory stuff on neuroscience only knows
3051200	3057920	about information stored in the conic tome. And this leads to, for instance, the Blue Brain
3057920	3063440	project. If RNA-based memory transfer is a thing, then this entire project is doomed,
3064160	3070080	because you cannot get the story out of just recording the conic tome. Most of the research
3070080	3075520	right now is focused on reconstructing the conic tome as it was circuitry and hoping that
3075520	3081280	we can get the functionality of information processing and deduce the specificity of the
3081920	3085440	particular brain, what it has learned from the connections between neurons.
3085440	3092000	But what if it turns out this doesn't matter? You just need connections that are dense enough,
3092000	3096800	and so basically stochastic lattice that is somewhat randomly wired. What matters is what
3096800	3100160	the neurons are doing with the information that they're getting through this ether,
3100160	3104560	through this lattice. It just changes the entire way in which we need to look at things.
3104560	3108160	And if this possibility exists, and if this possibility is just 1%,
3108960	3115440	but there are some experimental points in this direction, it is ridiculous to not pursue this
3115440	3121360	with high pressure and focus on it and support research that goes in this direction. Basically,
3121360	3126240	what's useful is not so much answering questions in science, it's discovering questions,
3126240	3131280	it's discovering new uncertainty. Reducing the uncertainty is much easier than discovering new
3131280	3137920	areas of where you thought that you were certain, but that allow you to get new insights. And it
3137920	3143120	seems to me that a lot of neuroscience is stuck, that it does not produce results that seem to
3143120	3150000	accumulate in an obvious way towards a theory on how the brain processes information. So the
3150000	3156400	neuroscientists don't deliver input to the researchers, and the transformer is not the
3156400	3162720	result of reading a lot of neuroscience. It's really mostly the result of people's thinking
3162720	3170640	about statistics of data processing. And it would be great if we would focus on ideas that
3170640	3174800	are promising and new and that have the power to shake existing paradigms.
3175440	3181760	Yeah. This is so important, and it's not just neuroscience. In developmental biology,
3181760	3186720	we have exactly the same thing. And I'll just give you two very simple examples of it where,
3186720	3191200	and I tell the students, when I give talks to students, I say, isn't it amazing that
3192240	3196480	in your whole course of biology and your developmental biology textbook, there's not
3196480	3202960	a mention of any of this because it completely just undermines a lot of the basic assumptions.
3202960	3208640	So here's a couple of examples. One example is that as of trophic memory in deer,
3208640	3213680	so there are species of deer that every year they regenerate. So they make this antler
3213680	3219040	rack on their heads, the whole thing falls off, and then it regrows the next year. So these two
3219040	3224960	guys, Bobenak, which are a father and son team that did these experiments for 40 years, and I
3224960	3230640	actually have all these antlers in my lab now because when the younger one retired, he sent me
3230640	3235360	all these things, all these antlers. The idea is this, what you can do is you take a knife
3235360	3241360	and somewhere in this branch structure, you make a wound and the bone will heal and you get a
3241360	3246640	little callus and that's it for that year. Then the whole thing drops off. And then next year,
3247840	3252320	it starts to grow and it will make an ectopic tine, an ectopic branch at the point where you
3252320	3258320	injured it last year. And this goes on for five or six years, and then eventually it goes away and
3258320	3268160	you get a normal rack again. And so the amazing thing about it is that the standard models for
3268160	3276400	patterning for morphogenesis are these gene regulatory networks and genetic biochemical
3276400	3284080	gradients and so on. If you try to come up with a model for this, so for encoding an arbitrary point
3284080	3289200	within a branch structure that your cells at the scalp have to remember for months after the whole
3289200	3294000	thing is dropped off, and then not only remember it, but then implement it so that when the bone
3294000	3300160	starts to grow, something says, oh yes, that's the start another tine growing to your left exactly
3300160	3307200	here. Trying to make a model of this using the standard tools of the field is just incredibly
3307200	3312960	difficult. And there are other examples of this, but this kind of non-genetic memory that's just
3312960	3317440	very difficult to explain with standard models. The other thing, which is I think an even bigger
3317440	3324400	scandal, is the whole situation with planaria. Some species of planaria, the way they reproduce
3324400	3328400	is they tear themselves in half, each half regenerates the missing piece, and now you've
3328400	3333040	got two. That's how they reproduce. So if you're going to do that, what you end up avoiding is
3333040	3337360	Weissman's barrier, this idea that when we get mutations in our body, our children don't inherit
3337360	3342800	those mutations. So this means that any mutation that doesn't kill the stem cell in the body gets
3342800	3348960	amplified as that cell contributes to regrowing the worm. So as a result of this, for 400 million
3348960	3354160	years, these planaria have accumulated mutations. Their genomes are an incredible mess. Their cells
3354160	3358320	are basically mixoploid, meaning they're like a tumor. Every cell has a different number of
3358320	3365600	chromosomes potentially. It just looks horrible. As an end result, you've got an animal that is
3365600	3372240	immortal, incredibly good at regenerating with 100% fidelity and very resistant to cancer.
3372240	3379280	Now, all of this is the exact opposite of the message you get from a typical course through
3379280	3384000	biology, which says that, what is the genome for? The genome is for setting your body structure.
3384000	3388720	If you mess with the genome, that information goes away. You get aging, you get cancer.
3389360	3396320	Why does the animal with the worst genome have the best anatomical fidelity? I think we actually,
3397280	3401120	a few months ago, we actually, I think, have some insight into this, but it's been bugging
3401120	3406400	me for years. And this is the kind of thing that nobody ever talks about because it goes
3406400	3412160	against the general assumption of what genomes actually do and what they're for. And this complete
3412160	3417280	lack of correlation between the genome, in fact, an anti-correlation between the genome quality
3417280	3422640	and the incredible ability of this animal to have a healthy anatomy.
3422640	3427200	Yeah. What is that insight that you mentioned you acquired a few months ago, preliminary?
3427760	3435200	Okay. In the name of throwing out kind of new unproven ideas, right? So this is just my
3435200	3440720	conjecture. We've done some computational modeling of it, which I initially, this was a
3442320	3447120	very clever student that I work with named Laxwin, who did some models with me. And
3448720	3453440	I initially thought it was a bug. And then I realized that, no, actually, this is the feature.
3453440	3459280	The idea is this, imagine... So we've been working for a long time on a concept of
3460800	3464800	competency among embryonic parts. And what this means is basically the idea that
3466480	3473920	there are homeostatic feedback loops among various cells and tissues and organs that
3473920	3479920	attempt to reach specific outcomes in anatomical morphous space, despite various perturbations.
3479920	3485200	So the idea is that if you have a tadpole and you do something to it, whether by a mutation
3485200	3489760	or by a drug or something, you do something to it where the eye is a little off kilter,
3489760	3494240	or the mouth is a little off. All of these organs pretty much know where they're supposed to be.
3494240	3499200	They will try to minimize distance from other landmarks and they will remodel. And eventually
3499200	3507040	you get a normal frog so that they will recover the correct anatomy, despite starting off in the
3507040	3511040	wrong position, or even things like changes in the number of cells or the size of cells.
3511040	3516640	They're really good at getting their job done despite various changes. So they have these
3516640	3522160	competencies to optimize specific things like their position and the structure and things like
3522160	3529760	that. So that's competency. Now, here's the interesting thing. Imagine that you have a
3529760	3536560	species that has some degree of that competency. And so you've got an individual, if that species
3536560	3542000	comes up for selection, fitness is high, looks pretty good. But here's the problem.
3542000	3547120	Selection doesn't know whether the fitness is high because his genome was amazing,
3547120	3551920	or the fitness is high because the genome was actually so-so, but the competency made up for it
3551920	3557440	and now everything got back to where it needs to go. So what the competency apparently does
3557440	3563440	is shield information from evolution about the actual genome. It makes it harder to pick the
3563440	3569120	best genomes because your individuals that perform well don't necessarily have the best genomes.
3569120	3577120	What they do have is competency. So what happens in our simulations is that if you start off with
3577120	3583680	even a little bit of that competency, evolution loses some power in selecting the best genomes,
3583680	3588880	but where all the work tends to happen is increasing the competency. So then the competency
3588880	3593360	goes up. So the cells are even better and the tissues are even better at getting the job done
3593360	3600960	despite the bad genome. That makes it even worse. That makes it even harder for evolution to see
3600960	3606160	the best genomes, which relieves some of the pressure on having a good genome, but it basically
3606160	3613920	puts all the pressure on being really competent. So basically what happens is that the genetic
3613920	3619440	fitness basically levels out at a really suboptimal level. And in fact, the pressure
3619440	3624880	is off of it. So it's tolerant to all kinds of craziness, but the competency and the mechanisms
3624880	3631520	of competency get pushed up really high. So in many animals, and there are other factors that
3631520	3635840	push against this ratchet, but it becomes a positive feedback loop. It becomes a ratchet
3635840	3643760	for optimal performance despite a suboptimal genome. And so in some animals, this evens out
3643760	3648960	at a particular point, but I think what happened in planaria is that this whole process ran away
3648960	3656000	to its ultimate conclusion. The ultimate conclusion is the competency algorithm became so good that
3656000	3661520	basically whatever the genome is, it's really good at creating and maintaining a proper worm
3661520	3666960	because it is already being evolved in the presence of a genome whose quality we cannot control.
3666960	3672720	So in computer science speak, it's kind of like, and Steve Frank put me onto this analogy,
3672720	3677440	it's kind of like what happens in rate arrays. When you have a nice rate array where the software
3677440	3682240	makes sure that you don't lose any data, the pressure is off to have really high quality
3682240	3689520	media. And so now you can tolerate media with lots of mistakes because the software takes care of it
3689520	3694960	in the rate and the architecture takes care of it. So basically what happens is you've got this
3694960	3703120	animal where that runaway feedback loop went so far that the algorithm is amazing and it's been
3705520	3710720	evolved specifically for the ability to do what it needs to do even though the hardware is kind
3710720	3716400	of crap. And it's incredibly tolerant. So this has a number of implications that to my knowledge
3716400	3723040	have never been explained before. For example, in every other kind of animal, you can call a stock
3723040	3729280	center and you can get mutants. So you can get mice with kinky kind of kink tails. You can get
3729280	3735200	flies with red eyes and you can get chickens without toes and you can get humans come with
3736640	3744720	albinos and things. There's always mutants that you can get. Planaria, there are no abnormal lines
3744720	3750000	of planaria anywhere except for the only exception is our two-headed line and that one's not genetic.
3750480	3759040	That one's bioelectric. So isn't it amazing that nobody has been able, despite 120 years of
3759040	3765280	experiments with planaria, nobody has isolated a line of planaria that is anything other than
3765280	3770080	a perfect planaria. And I think this is why. I think it's because they have been actually selected
3770080	3775200	for being able to do what they need to do despite the fact that the hardware is just very junky.
3776160	3786080	So that's my current take on it. And really it puts more emphasis on the algorithm and the
3786080	3791760	decision making among that cellular collective of what are we going to build and what's the
3791760	3795120	algorithm for making sure that we're all working to build the correct thing.
3796000	3801600	So if you translate this idea into computer science, a way to look at it is imagine that you
3801600	3809360	find some computers that have hard disks that are very, very noisy and where the hard disk
3809360	3814240	basically makes lots and lots of mistakes in encoding things and bits often flip and so on.
3814240	3818880	And you will find that these computers still work and they work in pretty much the same way
3818880	3824640	as the other computers that you have. And there is an orthodox sect of computer scientists that
3824640	3831600	thinks it is necessary that every bit on the hard disk is completely reliable or reliable to
3831600	3837760	such a degree that you only have a mistake once every 100 trillion copies. And you can
3837760	3841600	have an error correction code running on the hard disk at the low level that corrects this.
3841600	3845760	And after some point it doesn't become efficient anymore. So you need to have reliable hard disks
3845760	3851200	to be able to have computers that work like this. But how would these other computers work?
3851200	3856720	And it basically means that you create a virtual structure on top of the noisy structure
3856720	3862160	that is correcting for whatever degree of uncertainty you have or the degree of
3862160	3869200	randomness that gets injected into your substrate. Dave Eggley has a very nice metaphor for this.
3869200	3871520	Do you know him, maybe? Yeah, I know him.
3871520	3879120	Yeah, he's a beautiful artist who explores complexity by tinkering with computational models
3879120	3883920	and really finds his work very inspiring. And he has this idea of best effort computing. So
3883920	3888960	in his view, our own nervous system is a best effort computer. It's one that does not rely
3888960	3895520	on the other neurons around you working perfectly, but make an effort to be better than random.
3896240	3903440	And then you stack the improbabilities empirically by having a system that evolves to measure in
3903440	3910160	effect the unreliability of its components. And then stack the probabilities until you get
3910160	3917920	the system to be deterministic enough to do what you're doing with it. If you have a system that
3917920	3924080	is, as in the planaria, inherently very noisy, where the genome is an unreliable witness of
3924080	3930000	what should be done in the body, you just need to interpret it in a way that stacks the probabilities,
3930000	3936080	that is evaluating things with much more error tolerance. And maybe this is always the case.
3936080	3941200	Maybe there is a continuum, maybe not. It's also possible that there is some kind of phase shift
3941200	3946640	where you switch from organisms with reliable genomes to organisms with noisy genomes. And you
3946640	3950960	basically use a completely different way to construct the organism as a result. But it's
3950960	3956960	a very interesting hypothesis then to see if this is a radical thing or a gradual thing that happens
3956960	3962560	in all organisms to some degree. What I also like about this description that you give about
3962560	3970000	how the organism emerges, it maps in some sense also in how perception works in our own mind.
3970960	3977920	At the moment, machine learning is mostly focused on recognizing images or individual frames. And
3977920	3982400	you feed in information frame by frame and the information is totally disconnected.
3983200	3988080	A system like Dali2 is trained by giving it several hundreds of millions of images.
3988800	3992720	And they are disconnected. They are not adjacent images in the space of images. And
3993280	3998480	maybe you could not probably learn from giving 600 million images in a dark room and only looking
3998480	4004000	at this introduced the structure of the world from this. Whereas Dali can, which gives testament to
4004000	4009200	the power of our statistical methods and hardware that we have. That far surpasses, I think, the
4009200	4013680	combined power and reliability of brains, which probably would not be able to integrate so much
4013680	4019440	information over such a big distance. For us, the world is learnable because its adjacent frames
4019440	4024800	are correlated. Basically, information gets preserved in the world through time. And we
4024800	4029680	only need to learn the way in which the information gets transmogrified. And these
4029680	4034240	transmogrification of information means that we have a dynamic world in which the static image
4034240	4038800	is an exception. The identity function is a special case of how the universe changes. And
4038800	4045120	we mostly learn change. I just got visited by my cat. And my cat has difficulty to recognize
4045120	4049920	static objects compared to moving objects, where it's much, much easier to see a moving ball than
4049920	4054640	a ball that is lying still. And it's because it's much easier to segment it out the environment
4054640	4059680	when it moves. So the task of learning on a moving environment, a dynamic environment,
4059680	4065280	is much easier because it imposes constraints on the world. And so how do we represent a moving
4066240	4072240	world compared to a static world? The semantics of features changes. And an object is basically
4072240	4077920	composed of features that can be objects themselves. And the scene is a decomposition
4077920	4083440	of all the features that we see into a complete set of objects that explain the entirety of the
4083440	4087280	scene. And the interaction between them and causality is the interaction between objects.
4088400	4092720	And in a static image, these objects don't do anything. They don't interact with each other.
4092720	4096880	They just stand in some kind of relationship that you need to infer, which is super difficult
4096880	4102800	because you only have this static snapshot. And so the features are classifiers that tell you
4102800	4109680	how to, whether a feature is a hand or a foot or a pen or a sun or a flashlight or whatever,
4109680	4114240	and how they relate to the larger scene, in which, again, you have a static relationship
4114240	4118640	in which you need to classify the object based on the features that contribute to them.
4118640	4122400	And you need to find some kind of description where you interpret features, which are usually
4122400	4126160	ambiguous and could be many different things, depending on the context in which you interpret
4126160	4131760	them, into one optimal global configuration, right? But if the scene is moving, this changes
4131760	4136240	a little bit. What happens now is that the features become operators. They're no longer
4136240	4141200	classifiers that tell you how your internal state needs to change, how your world needs to change,
4141200	4145760	how your simulation of the universe in your mind needs to change to track the sensory patterns.
4146400	4153440	Right, so a feature now is a change operator, a transformation. And the feature is in some sense
4153440	4158480	a controller that tells you how the bits are moving in your local model of the universe.
4158480	4163760	And they're organized in a hierarchy of controllers. And these controllers need to
4163760	4167760	be turned on and off at the level of the scene. And they have a lot of flexibility once you have
4167760	4171200	them. They can move around in the scene. They're basically now self-organizing,
4171200	4176320	self-stabilizing entities. In the same way as the mouse is moving around in your organism,
4176320	4180800	a feature can move around in the organism and shift itself around to communicate with other
4180800	4184480	features until they negotiate a valid interpretation of reality.
4185840	4191840	That's incredibly interesting because as soon as you started saying that,
4192720	4198400	I was starting to think that the virtualization that enables, right, so the earlier part of which
4198400	4207600	we're saying the virtualization of the information that allows you to deal with unreliable
4207600	4213440	hardware and everything, the bioelectric circuits that we deal with are a great candidate for that
4213440	4218720	because actually we see exactly that. We see a bioelectric pattern that is very resistant to
4218720	4223120	changes in the details and make sure that everybody does the right thing under a wide range of
4224080	4228560	different defects and so on. But even more than that, the other thing that you were just
4229200	4233120	emphasizing this, the fact that we learn the delta and that we're looking for change,
4233840	4239280	very interesting. If you pivot the whole thing from the temporal domain to the spatial domain,
4239280	4246080	so in development, when we look at these bioelectric patterns, now these patterns
4246080	4251040	are across space, not across time. So unlike in neuroscience where everything is in the temporal
4251040	4257040	domain for neurons, these are static voltage patterns across tissue, right, across the whole
4257040	4263600	thing. So for the longest time, we asked this question, how are these read out? How do cells
4263600	4269920	actually read these? Because one possibility early, this was a very early hypothesis 20 years ago,
4269920	4274720	was that maybe the local voltage tells every cell what to be. So it's like a paint by numbers kind
4274720	4282800	of thing. And each voltage value corresponds to some kind of outcome. That turned out to be false.
4282800	4287200	What we did find is that, and we have computational models of how this works now,
4289520	4294880	what is read out is the delta, the difference between regions. It doesn't care, nobody cares
4294880	4300800	about what the absolute voltage is, what is read out in terms of outcomes for downstream cell
4300800	4304720	behavior, gene expression, all that. What is actually read out is the voltage difference
4304720	4309440	between two adjacent domains. So that is exactly actually what it's doing just in the spatial
4309440	4317920	domain. It only keys off of the delta. And what is learned from that is exactly as you were saying,
4317920	4322880	it modifies the controller for what's downstream of that. And there may be multiple ones that are
4322880	4328880	sort of moving around and co-inhabiting. I mean, it's a very compelling picture actually and way
4328880	4335200	to look at some of the simulations that we've been doing about how the bioelectric data are
4335200	4339280	interpreted by the rest of the cells. It's very interesting.
4339280	4343600	So Professor Levin used the word competence earlier, and I'd like you to define that.
4344640	4355120	Yeah. In order to define it, I want to put out two concepts to this. One idea is that, to me,
4355120	4359760	and this goes back to what we were talking about before as the engineering stance on things,
4360400	4368800	I think that useful cognitive claims such as something, when you say this system has whatever
4368800	4373680	or it can whatever, as far as various types of cognitive capacities, I think those kinds of
4373680	4379280	claims are really engineering claims. That is, when you tell me that something is competent
4379280	4386400	at a particular level, so you can think about Wiener and Rosenbluth scale of cognition that
4386400	4392960	goes from simple passive materials and then reflexes and then all the way up to second
4392960	4396320	order metacognition and all that. When you tell me that something is on that
4396880	4402320	ladder and where it is, what you're really telling me is, if I want to predict its behavior or I
4402320	4407920	want to use it in an engineering context or I want to interact with it or relate to it in some way,
4407920	4412000	this is what I can expect. That's what you're really telling me. All of these terms,
4412800	4416880	what they really are, are engineering protocols. If you tell me that something
4416880	4423680	has the capacity to do associative learning or whatever, what you're telling me is that,
4423680	4427520	hey, you can do something more with this than you could with a mechanical clock.
4427520	4433040	You can provide certain types of stimuli or experiences and you can expect it to do this
4433040	4439600	or that afterwards. Or if you tell me that something is a homeostat, that means that,
4439600	4446080	hey, I can count on it to keep some variable at a particular range without having to be myself
4446080	4450080	to control it all the way. It has a certain autonomy now. If you tell me that something
4450080	4454640	is really intelligent and it can do XYZ, then I know that, okay, you're telling me that it
4454640	4459520	has even more autonomous behavior in certain contexts. All of these terms, to me, what they
4459520	4463360	really are, they're not... And that has an important implication. The implication is that
4463360	4470320	they're observer dependent, that you've picked some kind of problem space, you've picked some
4470320	4474960	kind of perspective. And from that problem space and that perspective, you're telling me that
4476800	4482000	given certain goal states, this system has that much competency to pursue those goal states.
4482000	4485840	And different observers can have different views on this for any given system. So for example,
4486400	4490720	somebody might look at a brain, let's say a human brain and say, well, I'm pretty sure the only
4490720	4495760	thing, this is a paperweight. So it's really pretty much just competent in going down gravitational
4495760	4499360	gradients. So all it can do is hold down paper, that's it. And somebody else will look at it and
4499360	4503520	say, you missed the whole point. This thing has competencies in behavioral space and
4503520	4510560	linguistic space. So these are all empirically testable engineering claims about what you can
4510640	4516880	expect the system to do. So when I say competency, what I mean is we specify a space, a problem
4516880	4521040	space. And at the time when we were talking about this, the problem space that I was talking about
4521040	4526480	was the anatomical morphous space. That was the space we were talking about. So the space of
4526480	4532400	possible anatomical configurations and specifically navigating that morphous space. So you start off
4532400	4538320	as an egg or you start off as a damaged limb or whatever, and you navigate that morphous space
4538320	4544240	into the correct structure. So when I say competency, I mean, you have the ability to deploy
4544240	4551600	certain kinds of tricks to navigate that morphous space with some level of performance that I can
4551600	4555840	count on. And so the competency might be really low or it might be really high. And I would have to
4555840	4560560	make specific claims about what I mean. Here's an example of a common, and there are many,
4560560	4564320	if you just think about the behavioral science of navigation, there are many competencies you
4564320	4569600	can think about. Does it know ahead of time where it's going? Does it have a memory of where it's
4569600	4577440	been? Or is it a very simple sort of reflex arc is all it has? Or here's one example of a pretty
4577440	4585600	cool competency that a lot of biological systems have. If we take some cells that are in the tail
4585680	4598800	of a tadpole and we modify their ion channels such that they now acquire the goal of navigating to
4598800	4603280	an eye fate in this morphous space, meaning that they're going to make an eye. These things,
4603280	4607280	in fact, will create an eye and they'll make an eye in the tail, on the gut, wherever you want.
4607280	4612640	But one of the cool, and so that's already pretty cool, but one of the amazing aspects is
4612640	4618480	if I only modify a few cells, not enough to make an actual eye, just a handful of cells,
4618480	4624080	and we've done this and you can see this work. One of the competencies they have is to recruit
4624080	4629440	local neighbors that were themselves not in any way manipulated to help them achieve that goal.
4629440	4634080	It's a little bit like in an ant colony. This idea of recruitment in ants and termites is an
4634080	4638880	idea of recruitment where individuals can recruit others and talk about a flexible collective
4638960	4644960	intelligence. This is it. You've re-specified the goal for that set of cells, but one of the
4644960	4649680	things that they do without us telling them how to do it or having to micromanage it,
4649680	4654640	they already have the competency to recruit as many cells as they need to get the job done.
4656400	4660800	For an engineer, that's a very nice competency because it means that I don't need to worry
4660800	4666240	about taking care of getting exactly the right number of cells. If I'm a little bit over,
4666240	4670400	that's fine. If I'm way under, also fine. The system has that competency of recruiting
4671040	4676960	other cells to get the job done. That's what I meant. To make any kind of a cognitive claim,
4676960	4682640	you have to specify the problem space. You have to specify the goal towards which it's expressing
4682640	4689200	competencies. Then you can make a claim about, well, how competent is it to get to that goal?
4689920	4693760	I wish I could remember who it was, but somebody made this really nice analogy about the ends of
4693760	4699840	that spectrum. They said two magnets try to get together and Romeo and Juliet try to get together,
4699840	4703760	but the degree of flexible problem solving that you can expect out of those two systems is
4703760	4709280	incredibly different. Within that range, there are all kinds of in-between systems that may be
4709280	4714320	better or worse and may deploy different kinds of strategies. Can they avoid local optima? Can they
4714320	4718480	have a memory of where they've been? Can they look further than their local environment? A
4718480	4723360	million different things. That's what I meant by competency. It's a claim about
4724240	4729680	what an engineer can expect the system to do given a particular problem space and a particular
4729680	4735280	goal that you think it's trying to reach. The way in which you use the word competency
4736080	4740960	could be treated as the capacity of a system for adaptive control.
4745440	4751280	One issue that I have with the notion of goals and goal directedness is that sometimes you only
4751280	4758000	have a tendency in a system to go in a certain direction. It's directed, but the goal is something
4758000	4762560	that can be emergent. Sometimes it's not. Sometimes there is an explicit representation
4762560	4766960	in the system of a discrete event that is associated or a class of events with fulfilling
4766960	4771120	a certain condition that the system has committed itself to. If you don't have that, you don't have
4771120	4778640	a proper goal. In real systems, it's difficult to say. When do we pursue goals? Sometimes we just
4778640	4784160	vaguely hungry or moving towards the kitchen because we hope that something will opportunistically
4784160	4789680	emerge that will deal with this vague tendency in our behavior. We could also say we have the
4789680	4795040	goal of finding food, but that is a rationalization that is maybe stretching things sometimes.
4796720	4801920	Sometimes a better distinction for me is going from a simple controller to an agent.
4804080	4808400	We are very good at discovering agency in the world. What does it actually mean when we discover
4808400	4815440	agency and when we discover our own agency and start to amplify it by making models of who we
4815440	4820480	are and how we deal with the world and with others and so on? The minimal definition of agent that I
4820480	4826160	found is a controller for future states. The thermostat doesn't have a goal by itself. It
4826160	4832960	just has a target value and a sensor that tells its deviation from the target value and when that
4832960	4838960	exceeds a certain threshold, the heating is turned on. If it goes below a certain threshold,
4838960	4844320	the heating is turned off again and this is it. The thermostat is not an agent. It only reacts
4844320	4850720	to the present frame. It's only a reactive system. Whereas an agent is proactive, which means that
4850720	4857360	it's trying to not just minimize the current deviation from the target value, but the integral
4858000	4865680	over the time span, the future deviation. It builds an expectation about how an action is
4865680	4872160	going to change this trajectory of the universe. Over that trajectory, it tries to figure out some
4872160	4878880	measure of how big the compound target deviation is going to be. As a result, you get a branching
4878880	4883840	universe. The branches in this universe, some of these branches depend on actions that are
4883840	4889920	available to you and that translate into decisions that you can make that move you
4889920	4893600	into more or less preferable wealth states. Suddenly, you have a system with emergent
4893600	4900240	beliefs, desires, and intentions. To make that happen, to move from a controller to agency,
4901040	4907600	agent just being a controller with an integrated set point generator and the ability to control
4907600	4914880	future states, that requires that you can make models that are counterfactual because the future
4914880	4920640	universe doesn't exist right now. You need to create a counterfactual model of the future
4920640	4925280	universe, maybe even a model of the past universe that allows you to reason about possible future
4925280	4931360	universes and so on. To make these counterfactual causal models of the universe, you need to have a
4931360	4937680	Turing machine. Without a computer, without something that is Turing complete, that insulates
4937680	4943600	you from the causal structure of your substrate, that allows you to build representations regardless
4943600	4948560	of what the universe says right now around you, you need to have that machine. The simplest
4950000	4956000	system in nature that has a Turing machine integrated is the cell. It's very difficult to
4956000	4962800	find a system in nature that is an agent, that is not made from cells as a result. Maybe there
4962800	4969120	are systems in nature that are able to compute things and make models, but I'm not aware of any.
4969840	4976320	The simplest one that I know that can do this reliably is the cells or arrangement of cells
4977280	4984000	that can possess agency, which is an interesting thing that explains this coincidence that living
4984000	4989200	things are agents and vice versa, that the agents that we discover are mostly living things,
4989200	4996560	or there are robots that have computers built into them, or virtual robots that rely on
4996560	5001600	computation. The ability to make models of the future is the prerequisite for agency.
5002400	5010560	To make arbitrary models, which means structures that embody causal simulations of some sort,
5010560	5017280	that requires computation. Yeah, yeah. I'm on board with that
5017280	5026160	ladder, that taxonomy of goals and so on. One interesting thing about goals, and as you say,
5026160	5032720	some are emergent and some are not, there's an interesting planarian version of this,
5032720	5039680	which is this. We made this hypothesis about, so within planaria, you chop it up into pieces
5039680	5045440	and every piece regenerates exactly the right rest of the work. If you chop it into pieces,
5045440	5052480	each piece will have one head, one tail. Then, of course, what happens is it stops when it reaches
5052480	5059440	a correct planarian, then it stops. We started to think that there are a couple of possibilities.
5059440	5065200	One possibility is that this is a purely emergent process and that the goal of rebuilding a head is
5065200	5069680	an emergent thing that comes about as a consequence of other things. Or could there be
5070560	5075200	an actual explicit representation of what a correct planarian is that serves as a
5075200	5079120	set point, as an explicitly encoded set point for these cells to follow?
5079920	5084400	Because it's a cellular collective, we were communicating electrically. We thought, well,
5084400	5091600	maybe what it's doing is basically storing a memory of what, like you would in a neural
5091600	5096000	circuit, storing a memory of what it should be. We started looking for this and this is what we
5096000	5105600	found. This is, I think, one important type of goal in a goal-seeking system is a goal that
5105600	5110800	you can rewrite without changing the hardware and the system will now pursue that goal instead of
5110800	5115920	something else. In a purely emergent system, that doesn't work. If you have a cellular automaton or
5115920	5119840	a fractal or something that does some kind of complex thing, if you want to change what that
5119840	5124400	complex thing is, you have to figure out how to change the local rules. That's very hard in most
5124400	5130640	cases. But what we found in planaria is that we can literally, using a voltage reporter die,
5130640	5136400	we can look at the worm and we can see now the pattern, and it's a distributed pattern,
5136400	5140400	but we can see the pattern that tells this animal how many heads it's supposed to have.
5140400	5148240	And what you can do is you can go in and using a brief transient manipulation of the ion channels
5148240	5152720	with drugs, with ion channel drugs, and we have a computational model that tells you what those
5152720	5158800	drugs should be, that briefly changes the electrical state of the circuit, but the circuit is amazing.
5159520	5164960	Once you've changed that state, it holds. So by default, in a standard planaria, it always says
5164960	5170320	one head, but it's kind of like a flip-flop in that when you temporarily shift it, it holds and
5170320	5176000	you can push it to a state that says two heads. So now something very interesting happens. Two
5176000	5180000	interesting things. One is that if you take those worms and you cut those into pieces,
5180720	5185440	you get two headed worms, even though the hardware is all wild type. There's nothing wrong with the
5185440	5189280	hardware. All the proteins are the same. All the genetics is the same, but the electric circuit
5189280	5195520	now says make two heads instead of one. And so this is in an interesting way. It is an explicit
5195520	5199760	goal because you can rewrite it because much like with your thermostat, there's an interface for
5199760	5202480	changing what the goal state is, and then you don't even need to know how the rest of the
5202560	5207200	thermostat works. As long as you know how to modify that interface, the system takes care of
5207200	5211680	the rest. The other interesting thing is, and I love what you said about the counterfactuals,
5212480	5219680	what you can do is you can change that electrical pattern in an intact worm and not cut it for a
5219680	5224400	long time. And if you do that, when you look at that pattern, that is a counterfactual pattern
5224400	5230320	because that two headed pattern is not a readout of the current state. It says two heads, but the
5230320	5236080	animal only has one head. It's a normal planarian. So that pattern memory is not a readout of what
5236080	5241920	the animal is doing right now. It is a representation of what the animal will do in the future if it
5241920	5248320	happens to get injured. And you may never cut it or you may cut it, but if you do, then the cells
5248320	5253280	consult the pattern and build a two headed worm, and then it becomes the current state. But until
5253280	5258960	then, it's this weird like primitive, it's a primitive counterfactual system because it's
5258960	5266480	able to, a body of a planarian is able to store at least two different representations of what a,
5266480	5270560	probably many more, but we've found two so far, what a correct planarian should look like. It
5270560	5276160	can have a memory of a one headed planarian or a memory of a two headed planarian. And both of
5276160	5281760	those can live in exactly the same hardware and exactly the same body. The other kind of cool
5281760	5288080	thing about this, and I'll just mention this even though this is disclaimer, this is not published
5288080	5295120	yet. So take all this with a grain of salt, but the latest thing you can do is you can actually
5295120	5301120	treat it with some of the same compounds that are used in neuroscience in humans and in rats as
5301120	5307200	memory blockers. So things that block recall or memory consolidation. And when you do that,
5307200	5310880	you can make the animal forget how many heads it's supposed to have. And then they basically
5310880	5316000	turn into a featureless circle when you can just wipe the pattern memory completely.
5316000	5321280	Were they using exactly the same techniques you would use in a rat or a human? They just forget
5321280	5325200	what to do when they turn into, they fail to break symmetry and they just become a circle.
5325760	5333520	So yeah, I think what you were saying is right on with this ability to store counterfactual
5333520	5339280	states that are not true now, but may represent aspects of the future. I think that's a very
5339280	5345840	important capacity. Another important notion is a constraint and constraint satisfaction. A
5345840	5351360	constraint is a rule that tells you whether two things are compatible or not. And the constraint
5351360	5355440	is satisfied if they're compatible. So you basically have a number of conditions that you
5355440	5361920	establish by measuring that somehow, for instance, whether you have a head or multiple heads,
5361920	5367680	and you try to find a solution where you can end up with exactly one head. And if you end up with
5367680	5372800	exactly one head based on the starting state, then you have managed to find a way to satisfy
5372800	5381040	your constraints. And so in a sense, what you call a competency is the ability of a system to take
5381040	5388080	a region of the states of the space of the universe, basically some local region of possible
5388080	5395920	state that the universe can be in, and move that region to a smaller region that is acceptable.
5395920	5400960	So there is a region on the universe state space where you have only one head. And there's a larger
5400960	5405600	region where you don't have any head at all, but the starting state of your organism. And then you
5405600	5411280	try to get from A to B. So you get from this larger region to the one in which you want to be. Of
5411280	5415440	course, if you have one head, you want to stay in the region in which you have one head, which,
5415440	5421840	of course, is usually much easier. But the ability basically to condense the space, to bridge over
5421840	5428240	many regions into the target region is what comes down to what this competency is. The system
5428240	5433120	basically has an emergent wanting to go in this region, and it's trying to move there. And so
5433120	5438880	there are constraints at the level of the substrate that are battling with the functional constraints
5438880	5444880	that the organism wants to realize to fulfill its function. And sometimes you cannot satisfy this,
5444880	5449760	and you end up with two heads because you don't know which one you get rid of, or how to digest
5449760	5457360	one of the heads and so on. And you end up with some Siamese twin. And so this is an interesting
5457360	5462320	constraint that you have to solve for when you are dealing with reality and how you battle with
5462320	5469840	the substrate until you get to the functional solution that you evolved for. Yeah, that's
5469840	5476240	interesting. I mean, we've also found that there are... So we look at exactly this
5477200	5480960	the navigation, this kind of navigation and morphous space, how you get from here to there
5480960	5485440	and what paths are possible to get from here to there and so on. One of the things that we found
5485440	5492720	is that there are regions of that space that belong to other species. And you can push a
5492720	5498560	planarian with a standard wild type genome into the goal state of a completely different species.
5498560	5502400	So we can get them to grow a head. So there's a species that normally has a triangular head.
5502400	5507520	You can make it grow a round head like a different species or a flat head or whatever.
5509280	5515040	So those are about 100 to 150 million years of evolutionary distance. And you can do it
5516320	5522080	within a few days just by perturbing that electrical circuit so that it lands in the
5522080	5528400	wrong space. And then outside of that, there are regions that don't belong to planaria at all.
5528400	5534240	So planaria are normally nice and flat. We've made planarians that look like they are a cylinder,
5534240	5540160	like a ski cap. They become like a hemisphere or really weird ones that are spiky. They're
5540160	5545680	like a ball with spikes on it. There are all kinds of other regions in that space that you can push
5545680	5551520	them to. And so- Those are new. Those are not species that they diverge from. Those are new.
5551520	5558160	No one's ever... To my knowledge, yes, there are no such species. It's easier to... And we've
5558160	5563680	done this in frog too. You can push tadpoles to make it to look like those of other species
5564240	5570080	or you can make... That's a whole interesting thing for evolution anyway. One species birth
5570080	5576640	defect is a perfectly reasonable different species. So we can make tadpoles with a rounded tail,
5576640	5582160	which for a Xenopus tadpole is a terrible tail, but for a zebrafish, that's exactly the right tail.
5582160	5591040	So you can imagine evolution manipulating the different information processing by electrical
5591040	5600720	circuits or other machinery that help the system explore that more of a space and start to move
5600720	5606400	away from whatever that speciation is moving away from your standard attractor that you usually
5606480	5612800	land on. How does this relate to intelligence? Well, intelligence is the ability to make models
5612800	5617840	and usually in the service of control, at least that's the way I would explain intelligence.
5619360	5622720	There are other definitions, but it's the simplest one that I've found.
5622720	5626480	It also accounts for the fact that many intelligent people are not very good at getting
5626480	5632320	things done. Basically, intelligence and goal rationality are somewhat orthogonal.
5633280	5638320	And excessive intelligence is often a prosthesis for bad regulation.
5638320	5641520	Have you read the intelligence trap? No.
5642400	5646880	Okay. The author makes a similar case and he's coming on shortly, essentially saying that there
5646880	5652160	are certain traps that people with high IQs have that are not beneficial for them as biological
5652160	5656800	beings. They're mainly cognitive biases. So for instance, it's extremely interesting. So let's
5656800	5661760	just give one of the biases to say you're either liberal biased or you're conservative biased,
5661760	5665680	and then you were to give a test where there's some data that says that on the surface, it shows
5665680	5671120	that the data shows that gun control prevents gun violence. Well, the liberals are more likely to
5671120	5675520	say, yes, this data does show that. But if you're conservative, you're more likely to find, oh,
5675520	5680800	actually the subtleties in the data show that gun control increases gun violence. And then they
5680800	5685040	thought, okay, well, let's just switch this to make it such that the superficial data suggests that
5685040	5689520	gun control increases violence. You need to look at the data carefully to show that it actually
5689520	5693200	prevents violence. Well, the conservatives in that case would be more quickly to say, oh, look,
5693200	5698400	the gun control increases violence, and the liberals would find the the loophole. Well,
5698400	5703120	that's one of the reasons why I don't mind interviewing people who are biased. Because to
5703120	5709520	me, they're more able to find a justification for something that may be true. But I or and others
5709520	5714000	are so well, we all have our own biases. We're so inclined in some other direction that we just were
5714000	5718560	blind to it. But anyway, the point is to affirm what you're saying, Yoshi. Okay, so I know Michael
5718560	5724640	has a hard cut off at 2pm. So I want to ask the question for a GI that is artificial general
5724640	5730480	intelligence. It seems as though we're far away or that our current methods of machine learning
5730480	5735920	and what we learn in neuroscience or or what we learn in computer science is something that we're
5735920	5740080	missing some paradigm shift or missing some new techniques. Is there something from Michael's
5740080	5744480	work? Yoshi, I'm asking you this and then Michael, please respond. Is there something from Michael's
5744480	5750000	work that you think can be applied to the development of a GI if such a creature mind
5750000	5754000	can exist? Because there are some arguments against it. First of all, I don't know how far
5754000	5758080	we are for a GI. It could be that the existing paradigms are sufficient to brute force it.
5758960	5762640	But we don't know that yet. It's a we're going to find out in the next few months.
5763360	5768320	But it could also be that we need to revive the stack to build systems that work in real time
5768320	5773520	that are entangled with the environment that can build shared representations with the environment.
5774000	5779120	And that we need to rewrite the stack. And there are actually a number of questions that I'd like
5779120	5787040	to ask Michael. What I noticed that Michael is wisely reluctant to use certain words like
5787040	5791040	consciousness a lot. And it's because a lot of people are very opinionated about what these
5791040	5795600	concepts mean. And you first have to deal with these opinions before you come down to saying,
5795600	5799520	oh, here I have the following proposal for implementing reflexive attention
5799520	5806000	as a tool to form coherence in a representation. And this leads to the same phenomena as what you
5806000	5811520	call consciousness. So that is a detailed discussion. Maybe you don't want to have
5811520	5817440	that discussion in every forum. And then having this discussion, you may be looking at how to
5817440	5822480	create coherence using a reflexive attention process that makes a real time model of what
5822480	5827200	it's attending to and the fact that it's attending to it so it remains coherent but for itself.
5827760	5834080	So this is a concrete thing. But I wonder how to implement this in a self-organized fashion
5834080	5839280	if the substrate that you have are individual agents. And there is a similarity here between
5839280	5848640	societies and brains and social networks. That is, if you have self-interested agents, in a way,
5848640	5854240	that try to survive and that get their rewards from other agents that are similar to them
5854240	5862960	structurally. And they have the capacity to learn to some degree. And that capacity is
5863520	5869600	sufficient so they can, in the aggregate, learn arbitrary programs, arbitrary computable functions.
5871840	5875840	And it's sufficient enough so they can converge on the functions that they need to
5876880	5881200	as a group reap rewards that apply to the whole group because they have a shared
5881200	5885360	destiny like the poor little cells that are locked in the same skull and they're all going
5885360	5891520	to die together if they fuck up. So they have to get along, they have to form an organization
5891520	5896400	that is distributing rewards among each other. And this gives us a search space for possible
5896400	5903200	systems that can exist. And the search space is mostly given, I think, by the minimal agent
5903760	5910160	that is able to learn how to distribute rewards efficiently while doing something useful. Using
5910160	5915520	these rewards to change how you do something useful. So you have an emergent form of governance
5915520	5920080	in these systems. There's not some centralized control that is imposed on the system from the
5920080	5925920	outside as an existing machine learning approaches and AI approaches. But this only is an emergent
5925920	5930960	pattern in the interactions between the individual small units, small reinforcement learning agents.
5931600	5936960	And this control architecture leads to hierarchical government. It's not fully decentralized in any
5936960	5941920	way. There are centralized structures that distribute rewards for instance via the dopaminergic
5941920	5946880	system in a very centralized top-down manner. And that's because every regulation has an optimal
5946880	5951440	layer where it needs to take place. Some stuff needs to be decided very high up, some stuff needs
5951440	5956640	to be optimally regulated very low down depending on the incentives. Game theoretically, a government
5956640	5964000	is an agent that imposes an offset on your payoff metrics to make your Nash equilibrium compatible
5964000	5971200	with the globally best outcome. To do this you need to have agents that are sensitive to rewards.
5971200	5976880	It's super interesting to think about these reward infrastructures. Elon Musk has bought Twitter I
5976880	5981680	think because he has realized that Twitter is the network among all the social networks that is
5981680	5987520	closest to a global brain. It's totally mind-blowing to realize that he basically trades a bunch of
5987520	5994960	wealthy stock for the opportunity to become pope. Pope of a religion that has more active participants
5994960	6000640	than Catholicism even, right? Daily practicing people who enter this church and think together.
6000640	6004320	And it's a thing that is completely incoherent at this point, almost completely incoherent.
6004320	6008320	There are bubbles of sentience but for the most part this thing is just screeching at itself.
6009040	6013120	And now there is the question, can we fix the incentives of Twitter to turn it into a global
6013120	6019360	brain? And Elon Musk is global brain-pilled. He believes that this is the case and that's the
6019360	6023520	experiment that he's trying to do which makes me super excited, right? This might fail, there's a
6023520	6028560	very big chance that it fails but there is also the chance that we get the global brain, that we get
6028560	6033600	emerging collective intelligence that is working in real time using the internet in a way that
6033600	6039280	didn't exist before. So super fascinating thing that might happen here. And it's fascinating that
6039280	6046160	very few people are seeing that Elon Musk is crazy enough to spend 44 billion dollars on that
6046160	6050560	experiment just because he can and has nothing else to do and thinks it's meaningful to do it,
6050560	6057520	more meaningful than having so much money in the bank, right? So this makes me interested in
6057520	6062480	this test bed for rules and this is something that translates into the way in which society
6062480	6066480	is organized because social media is not different from society, not separate from it.
6066480	6071280	Problem of governing social media is exactly the same thing as governing a society. You need a
6071280	6076480	right form of government, you need a legal system, ultimately you need representation and all these
6076480	6081840	issues, right? It's not just the moderation team and the same thing is also true for the brain.
6081840	6087760	What is the government of the brain that emerges in what Gary Edelman calls neural Darwinism among
6087760	6092720	different forms of organization in the mind until you have a model of a self-organizing agent that
6092720	6097200	discovers that what it's computing is driving the behavior of an agent in the real world and
6097200	6102160	it covers a first-person perspective and so on. How does that work? How can we get a system that
6102160	6108320	is looking for the right incentive architecture? And that is basically the main topic where I
6108320	6114480	think that Michael's research is pointing from my perspective that is super interesting. We have
6114480	6122480	this overlap between looking at cells and looking at the world of humans and animals and stuff
6122480	6134400	in general. Yeah, super interesting. Chris Fields and I are working on a framework to understand
6137280	6143840	where collective agents first come from, right? How do they organize themselves?
6143840	6152320	And we've got a model already about this idea of rewards and rewarding other cells with
6152720	6157280	neurotransmitters and things like this to keep copies of themselves nearby because they're the
6157280	6161840	most predictable. So this idea of reducing surprise, well, what's the least surprising thing?
6161840	6167200	It's a copy of yourself. And so you can sort of, Chris calls it the imperial model of multicellularity.
6167200	6174480	But one thing to really think about here is imagine an embryo. This is an amniote embryo,
6174480	6179520	let's say a human or a bird or something like that. And what you have there is you have a flat disc
6179520	6187200	of 10,000, 50,000 cells. And when people look at it, you say, what is that? They say it's an embryo,
6187200	6192080	one embryo. Well, the reason it's one embryo is that under normal conditions, what's going to
6192080	6198880	happen is that in this disc, one cell is symmetry breaking. One cell is going to decide that it's
6198880	6203280	the organizer. It's going to do local activation, long range inhibition. It's going to tell all the
6203280	6208000	other cells, you're not the organizer, I'm the organizer. And as a result, you get one special
6208000	6216400	point that begins a process that's going to walk through this memorphous space and create a particular
6216400	6219680	large scale structure with two eyes and four legs and whatever else it's going to have.
6220240	6226560	But here's the interesting thing. Those cells, that's not really one embryo. That's a weird kind
6226560	6231520	of Freudian ocean of potentiality. What I mean by that is if you take, and I did this as a grad
6231520	6236160	student, you can take a needle and you can put a little scratch through that blastoderm, put a
6236160	6241120	little scratch through it. What will happen is the cells on either side of that scratch don't
6241120	6245120	feel each other. They don't hear each other's signals. So that symmetry breaking process will
6245120	6250240	happen twice, once on each end. And then when it heals together, what you end up with is two
6250240	6255760	conjoined twins because each side organized an embryo and now you've got two conjoined twins.
6256320	6263120	Now, many interesting things happen there. One is that every cell is some other cell's
6263360	6268560	external environment. So in order to make an embryo, you have to self-organize a system that
6269360	6273760	puts an arbitrary boundary between itself and the outside world. You have to decide where do I end
6273760	6280160	and the world begins. And it's not given to you somehow from outside for a biological system.
6280160	6284480	Every biological system has to figure this out for itself, unlike modern robotics or whatever,
6284480	6287280	where it's very clear. Here's where you are. Here's where the world is. These are your
6287280	6291120	effectors. These are your sensors. Here's the boundary of the outside world. Living things
6291120	6294240	don't have any of that. They have to figure all of this out from scratch.
6294240	6299520	The benefit to being able to figure it out from scratch, having to figure it out from scratch,
6299520	6304160	is that you are then compatible with all kinds of weird initial conditions. For example,
6304160	6310000	if I separate you in half, you can make twins. You don't have a total failure because now
6310000	6312960	you have half the number of cells. You can make twins. You can make triplets,
6313840	6318400	probably many more than that. So if you ask the question, you look at that
6318400	6323120	blastoderm and you ask how many individuals are there, you actually don't know. It could be zero.
6323120	6328960	It could be one. It could be some small number of individuals. That process of autopolices has to
6328960	6336560	happen. And here are a number of things that are uniquely biological that I think relate to
6336560	6343360	the kind of flexibility plasticity that you need for AGI in whatever space. It doesn't have to be
6343360	6349120	the same space that we work in, but your boundaries are not set for you by an outside creator.
6349120	6352720	You have to figure out where your boundaries are. Where is the outside world? So you make
6352720	6357440	hypotheses about where you end and where the world begins. You don't actually know what your
6357440	6361840	structure is. Kind of like Vanguard's robots from 2006 where they didn't know their structure and
6361840	6366000	they had to make hypotheses about, well, do I have wheels? Do I have legs? What do I have? And then
6366720	6372640	make a model based on basically babbling, right? Like the way that babies babble. So you have to
6373520	6378000	make a model of where the boundary is. You have to make a model of what your structure is.
6378000	6384000	You are energy limited, which most AI and robotics nowadays are not. When you're energy and time
6384000	6389680	limited, it means that you cannot pay attention to everything. You are forced to coarse grain in
6389680	6394800	some way and lose a lot of information and compress it down. So you have to choose a lens,
6394800	6398720	a coarse graining lens on the world and figure out how you're going to represent things.
6398720	6405520	And all of this has to, and there are many more things that we could talk about, but all of these
6405520	6413920	things are self-constructions from the very beginning. And then you start to act in various
6413920	6418240	spaces, which again are not predefined for you. You have to solve problems that are metabolic,
6418240	6426080	physiological, anatomical, maybe behavioral if you have muscles, but nobody's defining the space
6426080	6430880	for you. For example, if you're a bacterium and Chris Fields points this out, if you're a bacterium
6430880	6436320	and you're in some sort of chemical gradient, you want to increase the amount of sugar in your
6436320	6441440	environment, you could act in three-dimensional space by physically swimming up the gradient,
6441440	6445840	or you can act in transcriptional space by turning on other genes that are better at
6445840	6450320	converting whatever sugar happens to be around and that solves your metabolic problem instead of,
6450320	6456000	right? So you have these hybrid problem spaces. So all of this, I think what contributes in
6456000	6460240	a strong sense to all the things that we were just talking about is the fact that everything
6460240	6464560	is in biology is self-constructed from the beginning. You can't rely on, you don't know
6464560	6469520	ahead of time when you're a new creature born into the world. And we have many examples of
6469520	6473360	this kind of stuff. You don't know how many cells you have, how big your cells are. You can't count
6473360	6480080	on any of the priors. So you have this weird thing that evolution makes these machines that
6480080	6484240	don't take the past history too seriously. It doesn't over train on them. It makes
6484240	6488400	problem-solving machines that use whatever hardware you have. This is why we can make
6488400	6495440	weird chimeras and cyborgs. And you can mix things and mix and match biology in every way
6496320	6500960	with other living things or with non-living things because all of this is interoperable,
6500960	6505280	because it does not make assumptions about what you have to have. It tries to solve whatever
6505280	6512640	problem is given. It plays the hands that it's dealt. And that results in that assumption that
6512640	6518400	you cannot trust what you come into the world with. You cannot assume that the hardware is
6518400	6523520	what it is. It gives rise to a lot of that intelligence, I think, and a lot of that plasticity.
6523520	6529120	So if you translate this into necessary and sufficient conditions, what seems to be necessary
6529120	6537440	for the emergence of general intelligence in a bunch of cells or units is that basically each
6537440	6544160	of them is a small agent, which means it's able to behave with an expectation of minimizing future
6544160	6549360	target value deviations. It learns that their configuration is environment that signal anticipated
6549360	6555200	reward. Next thing, these units need to be not just agents, they need to be connected to each other.
6556080	6560800	And they need to get their rewards or proxy rewards, something that allows them to anticipate
6560800	6565920	whether the organism is going to feed them in the future from other units that also adaptive.
6566880	6571680	So you need multiple message types and the ability to recognize and send them with a certain degree
6571680	6580160	of reliability. What else do you need? You need enough of them, of course. What's not clear to me
6580160	6585440	is how deterministic do the units need to be? How much memory do they need to be? How much state can
6585440	6593680	they store? How deep in time does their recollection need to go? And how much forward in time do they
6593680	6600720	need to be able to form expectations? So we see how large is this activation front that they can
6600720	6606560	with this shape of the distribution that they can learn and have to learn to make this whole thing
6606560	6613760	happen. And so basically conditions that are necessary are relatively simple. If you just
6613760	6619120	wait for long enough and get such a system to percolate, I imagine that the compound agency
6619120	6627040	will at some level emerge on the system, just in a competition of possibilities in the same way as
6627040	6632800	emerging agency has emerged on Twitter in a way, with devoked religion in a way that people
6632800	6638080	were starting to shift around their behavior to maximize likes and retweets. And there was no
6638080	6643840	external reward that was given on Twitter. So as a result, a local structure emerged a local agency
6643840	6648560	that was shifting the rewards by itself and emerging causal structure that was in some sense
6648560	6656000	in downward causation going to organize groups of people into behavioral things. It's really
6656000	6662960	as interesting to look at Twitter as something like a mind at some level, right? It's working slower,
6662960	6667280	but it would probably be possible to make a simulation of these dynamics in a more abstract
6667280	6674000	way and to use this for arbitrary problem solving. And so what would an experiment look like in which
6674000	6678400	we start with these necessary conditions and narrow down the sufficient conditions?
6680080	6686560	Yeah, right on. And yeah, we're doing some of that stuff, some of that kind of modeling.
6686560	6692320	I apologize. I've got to run here. Thank you both for coming out for this. I appreciate it.
6692320	6695680	Thank you so much. And thank you for bringing us together. So a great conversation. I really
6695680	6701040	enjoyed it. Likewise. I enjoyed it very much. Thank you, Kurt. Thank you so much, Kurt.
6701040	6705920	Thanks, Joshua. The podcast is now concluded. Thank you for watching. If you haven't subscribed
6705920	6711440	or clicked on that like button now would be a great time to do so as each subscribe and like
6711440	6716400	helps YouTube push this content to more people. Also, I recently found out that external links
6716400	6721680	count plenty toward the algorithm, which means that when you share on Twitter, on Facebook,
6721680	6726880	on Reddit, et cetera, it shows YouTube that people are talking about this outside of YouTube,
6726880	6730080	which in turn greatly aids the distribution on YouTube as well.
6730080	6734160	If you'd like to support more conversations like this, then do consider visiting theories
6734160	6739600	of everything.org. Again, it's support from the sponsors and you that allow me to work on
6739600	6745360	toe full time. You get early access to ad free audio episodes there as well. Every dollar helps
6745360	6750000	far more than you may think. Either way, your viewership is generosity enough. Thank you.
