1
00:00:00,000 --> 00:00:04,400
Michael Levin's work on regulating intractable pattern formation in living systems has made

2
00:00:04,400 --> 00:00:08,960
him one of the most compelling biologists of our time. In translation, this means that his team is

3
00:00:08,960 --> 00:00:13,680
sussing out how to develop limbs, regenerate limbs, how to generate minds, and even life extension

4
00:00:13,680 --> 00:00:18,000
by manipulating electric signals rather than genetics or epigenetics. His work is something

5
00:00:18,000 --> 00:00:21,360
that I consider to be worthy of a Nobel Prize, and I don't think I've said that about anyone

6
00:00:21,360 --> 00:00:25,840
on the podcast. Michael Levin's previous podcast, On Toe, is in the description. That's a solo

7
00:00:25,840 --> 00:00:30,080
episode with him where we go into a two-hour deep dive, as well as there's a theolo-cution,

8
00:00:30,080 --> 00:00:34,240
so that is him and another guest, just like today, except between Carl Fursten and Chris

9
00:00:34,240 --> 00:00:39,120
Fields on consciousness. Josje Bach is widely considered to be the pinnacle of an AI researcher

10
00:00:39,120 --> 00:00:44,080
dealing with emotion, modeling, and multi-agent systems. A large focus of Bach's is to build a

11
00:00:44,080 --> 00:00:49,040
model of the mind from strong AI. Speaking of minds, Bach is one of the most inventive minds

12
00:00:49,040 --> 00:00:53,520
in the field of computer science and has appeared several times on Toe prior. Again, there's a solo

13
00:00:53,520 --> 00:00:57,920
episode. There's also a theolo-cution between Josje Bach and Donald Hoffman on consciousness,

14
00:00:57,920 --> 00:01:02,160
and Josje Bach and John Vervecky also on consciousness and reality. Biology has much to

15
00:01:02,160 --> 00:01:07,120
teach us about artificial intelligence and vice versa. This discussion between two brilliant

16
00:01:07,120 --> 00:01:13,120
researchers is something that I'm extremely lucky, blessed, fortunate to be a part of, as well as us

17
00:01:13,120 --> 00:01:17,440
as collective as an audience that are fortunate enough to witness. Thank you and enjoy this

18
00:01:17,440 --> 00:01:22,880
theolo-cution between Josje Bach and Michael Levin. Welcome both Professor Michael Levin

19
00:01:22,880 --> 00:01:27,200
and Josje Bach. It's an honor to have you on the Toe podcast. Again, both of you and then together

20
00:01:27,200 --> 00:01:32,800
right now. Thank you. It's great to be here. Likewise. I enjoy very much being here and look

21
00:01:32,800 --> 00:01:37,520
forward to this conversation. I look forward to it as well. So we'll start off with the question of

22
00:01:38,160 --> 00:01:42,560
what is it that you, Michael, find most interesting about Josje's work? And then

23
00:01:42,560 --> 00:01:49,440
Josje will go for you toward Michael. Yeah, I really enjoy the breadth. So I've been looking,

24
00:01:49,440 --> 00:01:54,080
I think I've probably read almost everything on your website, the short kind of blog pieces

25
00:01:54,080 --> 00:02:02,000
and everything. And yeah, I'm a big fan of the breadth of tackling a lot of the different issues

26
00:02:02,000 --> 00:02:08,720
that you do with respect to computation and cognition and AI and ethics and everything.

27
00:02:08,800 --> 00:02:16,640
I really like that aspect of it. And Josje? Yeah, my apologies. My blog is not up to date. I haven't

28
00:02:19,440 --> 00:02:25,920
done any updates for a few years now on it, I think. So, of course, I'm still

29
00:02:26,640 --> 00:02:33,600
in the process of progressing and having new ideas. And the ideas that I had in recent years,

30
00:02:33,600 --> 00:02:38,320
I have a great overlap with a lot of the things that you are working on.

31
00:02:38,320 --> 00:02:45,440
And when I listened to your Lex podcast last night, there were many thoughts that you had

32
00:02:45,440 --> 00:02:51,760
that I had stumbled on that I've never heard from anybody else. And so I found this very

33
00:02:51,760 --> 00:02:57,280
fascinating and thought, maybe let's look at some of these thoughts first and then go from there

34
00:02:57,360 --> 00:03:05,920
and expand beyond those ideas. What I, for instance, found after thinking about how cells work

35
00:03:06,800 --> 00:03:11,040
kind of obvious but missed by most people in neuroscience or in science in general,

36
00:03:11,040 --> 00:03:17,280
is that every cell has the ability to send multiple message types and receive multiple

37
00:03:17,280 --> 00:03:21,360
message types and do this conditionally and learn under which conditions to do that and to

38
00:03:21,360 --> 00:03:27,200
modulate this. Also, every cell is an individual reinforcement learning agent. Single-celled animal

39
00:03:27,200 --> 00:03:31,840
that tries to survive by cooperating with its environment gets most of its rewards from its

40
00:03:31,840 --> 00:03:38,880
environment. And as a result, this means that every cell can in principle function like a neuron.

41
00:03:38,880 --> 00:03:44,640
It can fulfill the same learning and information processing tasks as a neuron. The only difference

42
00:03:44,640 --> 00:03:50,560
that exists with respect to neurons or the main difference is that they cannot do this over very

43
00:03:50,560 --> 00:03:56,960
long distances because they are mostly connected only to cells that are directly adjacent. Of

44
00:03:56,960 --> 00:04:01,760
course, neurons also only communicate to adjacent cells but the adjacency of neurons is such that

45
00:04:01,760 --> 00:04:06,960
they have excellent parts of the cell that reach very far through the organism. So in some sense,

46
00:04:06,960 --> 00:04:13,040
a neuron is a telegraph cell that uses very specific messages that are encoded in a way

47
00:04:13,760 --> 00:04:19,280
like Morse signals in extremely short high-energy bursts that allow to send messages over very long

48
00:04:19,280 --> 00:04:25,920
distances very quickly to move the muscles of an animal at the limit of what physics allows.

49
00:04:26,640 --> 00:04:31,520
So it can compete with other animals in search for food. And in order to make that happen,

50
00:04:31,520 --> 00:04:36,240
it also needs to have a model of the world that gets updated at this higher rate. So there is

51
00:04:36,240 --> 00:04:42,080
going to be an information processing system that is duplicating basically this cellular brain that

52
00:04:42,080 --> 00:04:47,120
is made from all the other cells in the body of the organism. And at some point, these two systems

53
00:04:47,120 --> 00:04:52,720
get decoupled. They have their own codes, their own language, so to speak. But it still makes

54
00:04:52,720 --> 00:04:58,400
sense, I guess, to see the brain as a telegraphic extension of the community of cells in the body.

55
00:04:58,960 --> 00:05:04,880
And for me, this insight that I stumbled on just because means and motive that evolution would

56
00:05:04,880 --> 00:05:11,120
equip cells with doing that information processing if the organism lives long enough and if the cells

57
00:05:11,120 --> 00:05:15,120
share a common genetic destiny so they can get attuned to each other in an organism,

58
00:05:15,840 --> 00:05:22,560
that basically every organism has the potential to become intelligent. And if it gets old enough to

59
00:05:22,560 --> 00:05:30,640
possess enough data to get to a very high degree of understanding of its environment in principle.

60
00:05:30,640 --> 00:05:36,560
So of course, a normal house plant is not going to get very old compared to us because its

61
00:05:36,560 --> 00:05:41,360
information processing is so much slower. So they're not going to be very smart. But at the

62
00:05:41,360 --> 00:05:46,720
level of ecosystems, it's conceivable that there is quite considerable intelligence. And then

63
00:05:46,720 --> 00:05:53,360
I stumbled on this notion that our ancestors thought that one day in fairyland equals seven

64
00:05:53,360 --> 00:06:00,800
years in human land, which is told in the old myth. And also, at some point, I revised my

65
00:06:00,800 --> 00:06:06,800
notion of what the spirit is. For instance, a spirit is an old word for the operating system

66
00:06:06,800 --> 00:06:13,680
for an autonomous robot. And when this word was invented, the only autonomous robots that were

67
00:06:13,680 --> 00:06:19,600
known were people and plants and animals and nation states and ecosystems. There were no

68
00:06:19,600 --> 00:06:27,440
robots built by people yet. But there was this pattern of control in it that people could observe

69
00:06:27,440 --> 00:06:33,040
that was not directly tied to the hardware that was realized by the hardware but disembodied in

70
00:06:33,040 --> 00:06:38,880
a way. And this notion of spirit is something that we lost after the Enlightenment when we tried to

71
00:06:38,880 --> 00:06:45,760
deal with the wrong Christian metaphysics and superstition that came with it and threw out a

72
00:06:45,760 --> 00:06:50,320
lot of babies with the bathwater. And suddenly, we basically lost a lot of concepts, especially

73
00:06:50,320 --> 00:06:57,360
this concept of software that existed before in a way, this software being a control pattern or a

74
00:06:57,360 --> 00:07:02,400
pattern of causal structure that exists at a certain level of coarse graining as some type of

75
00:07:02,400 --> 00:07:08,480
very, very specific physical law that exists by looking at reality from a certain angle.

76
00:07:09,120 --> 00:07:16,800
And what I liked about your work is that you systematically have focused on this

77
00:07:16,800 --> 00:07:22,000
direction of what a cell can do, that a cell is an agent, and that levels of agency emerge

78
00:07:22,000 --> 00:07:29,120
in the interaction between cells. And you use a very clear language and clear concepts. And you

79
00:07:29,120 --> 00:07:34,080
obviously are driven by questions that you want to answer, which is unusual in science, I found.

80
00:07:34,080 --> 00:07:40,400
Like most of our contemporaries in science get broken, if it doesn't happen earlier during the

81
00:07:40,400 --> 00:07:47,440
PhD, into people who apply methods in teams instead of people who join academia because

82
00:07:47,440 --> 00:07:51,680
they think it's the most valuable thing they can do with their lives to pursue questions that they

83
00:07:51,680 --> 00:07:57,520
are interested in, want to make progress on. All right, Michael, there's plenty to respond to.

84
00:07:57,520 --> 00:08:05,920
Yeah, yeah. Lots of ideas. Yeah, I think your point is very interesting about what really

85
00:08:05,920 --> 00:08:09,920
fundamentally is the difference between neurons and other cells. Of course, evolutionarily,

86
00:08:09,920 --> 00:08:14,960
they're reusing machinery that has been around for a very long time, since the time of bacteria,

87
00:08:14,960 --> 00:08:21,520
basically, right? So our unicellular ancestors had a lot of the same machinery. And even, I mean,

88
00:08:21,520 --> 00:08:28,160
of course, axons can be very long, but there are sort of intermediate structures, right? There are

89
00:08:28,160 --> 00:08:34,240
tunneling nanotubes and things that allow cells to connect to maybe five or 10 cell diameters away,

90
00:08:34,240 --> 00:08:38,480
right? So not terribly long, but also not immediate neighbors necessarily. So that kind

91
00:08:38,480 --> 00:08:46,240
of architecture has been around for a while. And people like Gaurav Sohel look at very brain-like

92
00:08:46,240 --> 00:08:53,840
electrical signaling in bacterial colonies. So I think evolution began to reuse this toolkit

93
00:08:53,840 --> 00:09:01,440
specifically of using this kind of communication to scale up the computational and other kinds of

94
00:09:03,440 --> 00:09:10,240
tricks a really long time ago. And I like to imagine that if somebody had come to

95
00:09:10,240 --> 00:09:15,280
the people who were inventing connectionism and the first sort of perceptrons and neural networks

96
00:09:15,280 --> 00:09:20,240
and so on, if somebody had come to them and said, oh, by the way, sorry, we're the biologists,

97
00:09:20,240 --> 00:09:24,640
we got it wrong. Thinking isn't in the brain, it's in the liver. And so then the question is,

98
00:09:24,640 --> 00:09:28,880
what would they do, right? Would they have changed anything about what they're doing? And then we

99
00:09:28,880 --> 00:09:32,880
said, ah, now we have to rethink our model, or would they have said, fine, who cares? This is

100
00:09:32,880 --> 00:09:38,480
exactly the same model. Everything works just as well. So I often think about that question,

101
00:09:38,480 --> 00:09:45,600
what exactly do we mean by neurons? And isn't it interesting that we are able to steal most of

102
00:09:45,600 --> 00:09:52,880
the tools, the concepts, the frameworks, the math from neuroscience and apply it to problems in

103
00:09:52,880 --> 00:09:57,600
other spaces. So not movement in three-dimensional space with muscles, but for example, movement

104
00:09:57,600 --> 00:10:01,760
through a morphous space, right? Anatomical morphous space. The techniques can't tell the

105
00:10:01,760 --> 00:10:06,800
difference. We use all the same stuff, optogenetics, neurotransmitter signaling, we model active

106
00:10:06,800 --> 00:10:14,560
inference, and we see perceptual by stability, you name it. We take concepts from neuroscience,

107
00:10:14,560 --> 00:10:20,400
and we apply it elsewhere in the body. And generally speaking, everything works exactly

108
00:10:20,400 --> 00:10:25,200
the same. And that shows us, I think, what you were saying, that there's this really interesting

109
00:10:28,080 --> 00:10:33,280
symmetry between these, that a lot of the distinctions that we've been making are in terms

110
00:10:33,280 --> 00:10:36,560
of having different departments and different PhD programs and other things that say,

111
00:10:36,560 --> 00:10:40,640
no, this is neuroscience, this is developmental biology. A lot of these things are just not

112
00:10:42,560 --> 00:10:49,360
as firm distinctions as we used to think. And I suspect that people who insist on strong

113
00:10:49,360 --> 00:10:56,000
disciplinary boundaries do this out of a protective impulse. And what I noticed by

114
00:10:56,000 --> 00:11:02,720
studying many disciplines when I was young, that the different methodologies are so incompatible

115
00:11:02,720 --> 00:11:09,360
across fields, that when I was studying philosophy or psychology, I felt that computer scientists

116
00:11:09,360 --> 00:11:14,320
would be laughing about the methods that each of these fields are using to justify what they're

117
00:11:14,320 --> 00:11:22,240
doing. And this, I think, is indicative of a defect. Because if you take science into the

118
00:11:22,240 --> 00:11:28,240
current regime of regulating it entirely by peer review, there is no external authority. Even the

119
00:11:28,240 --> 00:11:33,840
grand authorities are mostly fields of people who have been trained in the sciences, in existing

120
00:11:33,840 --> 00:11:38,560
paradigms, and then are finding the continuation of those paradigms from the outside. This

121
00:11:40,480 --> 00:11:46,720
meta-paradigmatic thinking does not really exist that much in a peer-reviewed paradigm. And

122
00:11:46,720 --> 00:11:51,040
ultimately, when you do peer review for a couple generations, it also means that if your peers

123
00:11:51,040 --> 00:11:57,200
deteriorate, there is nothing who pulls your science back. And what I missed specifically in

124
00:11:57,200 --> 00:12:01,280
a lot of the way in which neuroscience is done is what you call the engineering stance.

125
00:12:02,400 --> 00:12:07,120
And this engineering sense is very powerful, and you get it automatically when you're a computer

126
00:12:07,120 --> 00:12:11,280
scientist, because you don't really care what language is it written in. What you care in is

127
00:12:11,280 --> 00:12:17,680
what causal pattern is realized. And how can this be realized? And how could I do it? How would I

128
00:12:17,680 --> 00:12:23,280
do it? How can evolution do it? What it means is disposal, and this determines the search space for

129
00:12:23,280 --> 00:12:27,920
the things that I'm looking for. But this requires that I think in causal systems.

130
00:12:27,920 --> 00:12:34,640
And this thinking in causal systems is impossible not to do for a computer scientist,

131
00:12:35,360 --> 00:12:40,480
but it is unusual outside of computer science. And once you realize that, it's very weird.

132
00:12:41,120 --> 00:12:46,400
Suddenly, you have notions that try to replace causal structure with, say, evidence. And then

133
00:12:46,400 --> 00:12:53,200
you notice that, for instance, evidence-based medicine is not about probabilities of how

134
00:12:53,200 --> 00:12:58,080
something is realized and must work. You see people on the cruise ship getting infected over

135
00:12:58,080 --> 00:13:02,400
distances, and you think, oh, this must be airborne. But no, there is no peer-controlled

136
00:13:02,400 --> 00:13:07,040
study, so there is no evidence that it's airborne. And when you look at its disciplines from the

137
00:13:07,040 --> 00:13:13,120
outside, like in this case, the medical profession or the medical messaging and decision-making,

138
00:13:13,120 --> 00:13:20,160
or I get terrified because it directly affects us. And in terms of neuroscience, of course,

139
00:13:20,160 --> 00:13:25,440
there's more theoretical for the most part, but there must be a reason why it's for the most part

140
00:13:25,440 --> 00:13:33,120
a theoretical, why there is no causal model that clinicians can use to explain what is happening

141
00:13:33,120 --> 00:13:39,600
in certain syndromes that people are exhibiting. And I notice this when I go to a doctor and

142
00:13:40,240 --> 00:13:45,840
even at a reputable institution like Stanford, that most of the neuroscientists at some level

143
00:13:45,840 --> 00:13:52,080
there, or most of the neurologists that I'm talking to, at some level dualists, that they

144
00:13:53,200 --> 00:13:59,200
don't have a causal model of the way in which the brain is realizing things. And a lot of studies

145
00:13:59,200 --> 00:14:05,600
which discover that very simple mechanisms like the ability of human beings to use grammatical

146
00:14:05,600 --> 00:14:09,760
structure are actually reflected in the brain. This is so amazing. Who would have thought?

147
00:14:13,440 --> 00:14:18,240
But the developments that existed in computer science have led us on a completely different

148
00:14:18,240 --> 00:14:25,920
track. The perceptron is vaguely inspired by what the brain might be doing, but I think it's really

149
00:14:25,920 --> 00:14:31,120
a toy model or a caricature of what cells are doing. Not in the sense that it's inferior,

150
00:14:31,120 --> 00:14:37,440
it's amazing what you can brute force with the modern perceptron variations. The current

151
00:14:37,440 --> 00:14:42,560
machine learning systems are mind blowing in what they can do, but they don't do it like

152
00:14:42,560 --> 00:14:48,960
biological organisms at all. It's very different. The cells do not form change in which they weight

153
00:14:49,760 --> 00:14:56,640
sums of real numbers. There is something going on that is roughly similar to it, but there's

154
00:14:56,640 --> 00:15:01,040
a self-organizing system that designs itself from the inside out, not by a machine learning

155
00:15:01,040 --> 00:15:05,360
principle that applies to the outside and updates weights after reading and comparing them

156
00:15:05,360 --> 00:15:11,200
and computing gradients to the system. So this perspective of local self-organization by

157
00:15:11,200 --> 00:15:17,200
reinforcement agents that try to trade rewards with each other, that is a perspective that I

158
00:15:17,200 --> 00:15:23,760
find totally fascinating. And I wish this would have come from neuroscience into computer science,

159
00:15:23,760 --> 00:15:28,000
but it hasn't. There are some people which have thought about these ideas to some degree,

160
00:15:28,000 --> 00:15:33,600
but there's been very little cross pollination. And I think all this talk of neuroscience

161
00:15:33,600 --> 00:15:36,480
influencing computer science is mostly visual thinking.

162
00:15:38,880 --> 00:15:44,640
Yeah. It's also, I find this, you know, what you were saying about the different disciplines,

163
00:15:44,640 --> 00:15:52,000
it's kind of amazing how, well, when I give a talk, I can always tell which department I'm in by

164
00:15:52,000 --> 00:15:56,400
which part of the talk makes people uncomfortable and upset. And it's always different depending on

165
00:15:56,400 --> 00:16:00,000
which department it is, right? So there are things you can say in one department that are

166
00:16:00,000 --> 00:16:04,080
completely obvious. And you say this in another group of people and they throw tomatoes. I think

167
00:16:04,080 --> 00:16:12,400
this is just craziness. For instance, I could say in a neuroscience department, I could say

168
00:16:13,600 --> 00:16:18,640
information can be processed without changes in gene expression. You don't need changes in

169
00:16:18,640 --> 00:16:23,920
gene expression to process information because the processing inside a neural network runs on

170
00:16:23,920 --> 00:16:29,600
the physics of action potentials, right? So you can do all kinds of interesting information

171
00:16:29,600 --> 00:16:34,240
processing and you don't need transcriptional or genetic change for that. If I say the same thing

172
00:16:34,240 --> 00:16:38,640
in a molecular genetics department that say, hey, these cells could be processing tons of information

173
00:16:38,640 --> 00:16:44,320
long before the transcriptome ever finds out about it, this is considered just completely wild

174
00:16:44,320 --> 00:16:50,560
because it's thought that most of the hard work or in fact, all of the hard work is done in gene

175
00:16:50,560 --> 00:16:54,560
regulatory circuits and things like that, right? There are other examples. If I say,

176
00:16:57,600 --> 00:17:02,640
here's a collection of cells that communicate electrically to remember a particular spatial

177
00:17:02,640 --> 00:17:08,720
pattern, again, molecular cell biology, what do you mean? How can a collection of cells

178
00:17:08,720 --> 00:17:12,720
remember a spatial pattern? But again, in neuroscience or in an engineering department,

179
00:17:12,720 --> 00:17:17,520
yeah, of course. Of course, they have electrical circuits that remember patterns and can do pattern

180
00:17:17,520 --> 00:17:24,960
completion and things like that. So views of causality, views of just lots of things like

181
00:17:24,960 --> 00:17:32,960
that that are very obvious to one group of people is completely taboo elsewhere. So that distinction,

182
00:17:32,960 --> 00:17:40,400
and yeah, and as Joshua just said, it impacts everything. It impacts education, it impacts

183
00:17:40,480 --> 00:17:46,480
grants, grant reviews, because when these kind of interdisciplinary grants come up,

184
00:17:47,200 --> 00:17:51,280
the study sections have a really hard time finding people that can actually review them

185
00:17:51,280 --> 00:17:57,760
because what often happens is you'll get some kind of computational biology grant and you put a

186
00:17:57,760 --> 00:18:02,480
proposal and you'll have some people on the panel who are biologists and some people who are the

187
00:18:02,480 --> 00:18:07,600
computational folks. And it's very hard to get people that actually can appreciate both sides of

188
00:18:07,600 --> 00:18:12,080
it and understand what's happening together. So they will sort of each critique a certain part

189
00:18:12,080 --> 00:18:17,760
of it. And the other part, they say, I don't know what this is. And as a result, grants like that

190
00:18:17,760 --> 00:18:22,960
don't tend to not have a champion, one person who can say, no, I get the whole thing and I think

191
00:18:22,960 --> 00:18:31,520
it's really good or not. So yeah, even to the point where I'm often asked when people want to

192
00:18:31,520 --> 00:18:38,400
you list me somewhere, they'll say, so what are you? What's your field? And I never know

193
00:18:38,400 --> 00:18:41,440
how to answer that question. To this day, it's been 30 years. I still don't know how to answer

194
00:18:41,440 --> 00:18:47,280
that question. I just can't boil it down to one. It just wouldn't make any sense to say any of the

195
00:18:48,880 --> 00:18:53,520
traditional fields. So what do you say, Joshua, when someone asks you what field you're in?

196
00:18:54,480 --> 00:19:02,080
And it depends on who's asking. So for instance, I found it quite useful to sometimes say,

197
00:19:03,840 --> 00:19:13,200
sorry, I'm not a philosopher, but this or I'm not that interested in machine learning. And I

198
00:19:13,200 --> 00:19:18,960
did publish papers in philosophy and in machine learning, but it's not my specialty in the sense

199
00:19:18,960 --> 00:19:25,440
that I need to identify with it. And in some sense, I guess that these categories are important

200
00:19:25,440 --> 00:19:31,920
when you try to write a grant proposal or when you try to find a job in a particular institution

201
00:19:31,920 --> 00:19:37,520
and they need to fill a position. But for me, it's more, what questions am I interested in?

202
00:19:37,520 --> 00:19:41,200
What is the thing that I want to make progress on or what is the thing that I want to build right

203
00:19:41,200 --> 00:19:48,080
now? And I guess that in terms of the intersection, I'm a cognitive scientist.

204
00:19:49,920 --> 00:19:55,360
So I was asking Michael, prior to you joining Yoshi, why is it Michael that you were doing

205
00:19:55,360 --> 00:19:59,280
podcasts? And if I understand correctly, part of the reason was because you think out loud,

206
00:19:59,280 --> 00:20:02,720
and you'd like to hear the other person's thoughts and take notes and espers your own.

207
00:20:02,720 --> 00:20:06,400
And firstly, like Michael, you can correct me if that's incorrect. And then secondly,

208
00:20:06,400 --> 00:20:11,280
Yoshi, I'm curious for an answer for this, the same question, what is it that you get out of

209
00:20:11,280 --> 00:20:16,160
doing podcasts other than, say, some marketing for if you were promoting something, which I

210
00:20:16,240 --> 00:20:20,720
don't imagine you are currently. No, I'm not marketing anything.

211
00:20:21,840 --> 00:20:29,760
What I like about podcasts is the ability to publish something in a format that is engaging

212
00:20:29,760 --> 00:20:37,600
to interesting to people who actually care about it. I like this informal way of holding on to some

213
00:20:37,600 --> 00:20:43,920
ideas and also like conversations as a medium to develop thought is this space in which we can

214
00:20:44,880 --> 00:20:50,160
reflect on each other, look into each other's minds, interact with the ideas of others in real

215
00:20:50,160 --> 00:20:56,320
time. The production format of a podcast creates a certain focus of the conversation that can be

216
00:20:56,320 --> 00:21:04,160
useful. And it's a pleasant kind of tension that focuses you to stay on task. And I also found that

217
00:21:05,280 --> 00:21:11,840
it's generally useful to some people. The feedback that I get is that people tell me,

218
00:21:11,840 --> 00:21:18,000
I had this really important question and I found this allowed me to make progress on it.

219
00:21:18,000 --> 00:21:24,240
And I feel much better now about these questions. I just clarified something for me that has plagued

220
00:21:24,240 --> 00:21:30,320
me for years and put me on track to solving it, or this has inspired the following work.

221
00:21:30,320 --> 00:21:36,560
So it's a form of publishing ideas and getting them into circulation in our global hive minds

222
00:21:37,280 --> 00:21:45,200
that is very informal in a way, but it's not useless. And also it relieves me in this instance,

223
00:21:45,200 --> 00:21:50,320
at least of the work of cutting, editing, and so on. But anyways, I'm very grateful that you

224
00:21:50,320 --> 00:21:56,160
provide the service of curating our conversation and putting it in a form that is useful to other

225
00:21:56,160 --> 00:22:04,160
people. Yeah, there's something, well, two things I was thinking of. One is that I have conversations

226
00:22:04,160 --> 00:22:07,760
with people all day long about these issues, right? So people in my lab, collaborators,

227
00:22:07,760 --> 00:22:12,240
whatever. And most, of course, the vast majority of those conversations are not recorded and they

228
00:22:12,240 --> 00:22:16,560
just sort of disappear into the ether. And then I take something away from it and the other person

229
00:22:16,560 --> 00:22:21,600
takes something away from it. But I've often thought that, isn't it a shame that all of this

230
00:22:22,880 --> 00:22:27,440
just kind of disappears and it would be amazing to have a record of it? And of course, not every

231
00:22:27,440 --> 00:22:34,080
conversation is gold, but a lot of them are useful and interesting. And there are plenty of

232
00:22:34,080 --> 00:22:39,920
people that could be interested and could benefit from it. So I really like this aspect

233
00:22:39,920 --> 00:22:45,920
that we can have conversations and then they're sort of canned and they're out there for people

234
00:22:45,920 --> 00:22:51,040
who are interested. The other kind of aspect of it, which I don't really understand, but it's kind

235
00:22:51,040 --> 00:22:59,600
of neat, is that when somebody asks me to pre-record a talk, it takes a crazy amount of time

236
00:22:59,600 --> 00:23:03,120
because I keep stopping and realizing, ah, I could have said that better. Let me start from

237
00:23:03,120 --> 00:23:08,080
the beginning. And it's just, it's an incredible ordeal. Whereas something like this that's real

238
00:23:08,080 --> 00:23:13,920
time, I'm sure has as many mistakes and things that I would have rather fixed later, but you

239
00:23:13,920 --> 00:23:17,200
can't do that, right? So you just sort of go with it and that's it. And then it's done and you can

240
00:23:17,200 --> 00:23:23,200
move on. So I like that real time aspect of it because it just helps you to get the ideas out

241
00:23:23,200 --> 00:23:29,360
without getting hung up and trying to redo things 50 times. Yeah, it's a format that allows

242
00:23:29,360 --> 00:23:36,960
tentativity. If we have published, we have a culture in sciences that requires us to

243
00:23:37,600 --> 00:23:43,120
publish the things that we can hope to prove and make the best proof that we can. But when we have

244
00:23:43,120 --> 00:23:48,480
anything complicated, especially when we take our engineering stance, we often cannot prove how

245
00:23:48,480 --> 00:23:54,320
things work. Instead, our answers are in the realm of the possible and we need to discuss the

246
00:23:54,320 --> 00:24:00,800
possibilities. And there is value in understanding these possibilities to direct our future

247
00:24:00,800 --> 00:24:06,560
experiments and the practical work that we do to see what's actually the case. And we don't really

248
00:24:06,560 --> 00:24:12,880
have a publication format for that. We don't get neuroscientists to publish their ideas on how the

249
00:24:12,880 --> 00:24:17,280
mind works because nobody has a theory that they can prove. And as a result, there is basically a

250
00:24:17,280 --> 00:24:22,640
vacuum where theories should be. And the theory building happens informally in conversations

251
00:24:22,640 --> 00:24:28,320
that basically requires personal contact, which is a big issue once conferences been virtual

252
00:24:28,320 --> 00:24:33,920
because that contact diminished. And you get a lot of important ideas by reading the publications

253
00:24:33,920 --> 00:24:39,920
and so on. But this what could be or connecting the dots or possibilities or ideas that might be

254
00:24:39,920 --> 00:24:44,880
proven wrong later that we just exchange as in the status of ideas. That is something that has

255
00:24:44,880 --> 00:24:52,000
a good place in a podcast. Is this podcast, not this TOE podcast, but podcast in general something

256
00:24:52,000 --> 00:24:56,880
new? So for instance, I was thinking about this and I well podcasts go back a while and Brogan

257
00:24:56,880 --> 00:25:03,120
invented this long form format or popularized it. However, on television, there are interviews,

258
00:25:03,120 --> 00:25:07,760
so there's Oprah and those are long one hour, there's 60 minutes. And then back in the 90s,

259
00:25:07,760 --> 00:25:12,800
there was a three and a half hour, it's essentially a podcast, it's like Charlie Rose, three and a

260
00:25:12,800 --> 00:25:18,480
half hour conversation. It's like a Theolocution with Freeman Dyson, Daniel Dennett, Stephen J.

261
00:25:18,560 --> 00:25:26,640
Gould, like the Rupert Sheldrake, all of those on the same one format, it's essentially a podcast

262
00:25:26,640 --> 00:25:31,040
talking about metaphysics. Like, man, oh, man, I can't believe that got published. And then also,

263
00:25:31,040 --> 00:25:35,200
I think about it, well, did Plato have the first podcast? Because he's just publishing these

264
00:25:35,200 --> 00:25:38,720
dialogues and you read them, but it's not as if they're maybe he would have published it in video.

265
00:25:38,720 --> 00:25:44,960
I think Plato was the first podcaster. So is there something new about this format of podcasting

266
00:25:44,960 --> 00:25:49,840
that wasn't there before? Or what's new about it? I think it's like it's like blogging. Blogging

267
00:25:49,840 --> 00:25:57,920
is also not new, right? Being able to write text that you publish, and people can follow what you

268
00:25:57,920 --> 00:26:03,200
are writing and so on, did exist in some sense before, but the internet made it possible to

269
00:26:03,200 --> 00:26:08,480
publish this for everyone. You don't need a publisher anymore. And you don't need a TV

270
00:26:08,480 --> 00:26:13,120
studio anymore. You don't need a broadcast station that is recording your talk show and sends it to

271
00:26:13,120 --> 00:26:18,720
an audience. There is no competition with all the other talk shows because there is no limitations

272
00:26:18,720 --> 00:26:25,840
on how many people can broadcast at the same time. And this allows an enormous diversity of thoughts

273
00:26:25,840 --> 00:26:32,560
and small productions that are done at a very low cost, lowering the threshold for putting

274
00:26:32,560 --> 00:26:38,080
something out there and seeing what happens. So in this sense, it's the ecosystem that emerged is new

275
00:26:38,080 --> 00:26:42,080
because a variable change that changed the cost of producing a talk show.

276
00:26:43,440 --> 00:26:45,760
Right. Michael, you agree?

277
00:26:46,640 --> 00:26:54,560
Yeah. Yeah. I mean, yes, that and all of that. And also just the fact that, as you just said,

278
00:26:54,560 --> 00:26:59,040
these kind of like long form things were fairly rare. So most of the time, if you're going to be

279
00:26:59,040 --> 00:27:04,720
in one of the traditional media, they tell you, okay, you've got three minutes. We're going to

280
00:27:04,720 --> 00:27:08,640
cut all this stuff and we're going to boil it down to three minutes. And this is often incredibly

281
00:27:08,640 --> 00:27:13,760
frustrating. And I understand. I mean, we're drowned in information. And so there is obviously

282
00:27:13,760 --> 00:27:18,880
a place for very short statement on things, but the kind of stuff that we're talking about cannot

283
00:27:18,880 --> 00:27:25,120
be boiled down to TV sound bites or anything. It's just not. And so the ability to have these

284
00:27:25,120 --> 00:27:30,080
long form things so that anybody who wants to really dig in can hear what the actual thought

285
00:27:30,080 --> 00:27:37,520
is as opposed to something that's been just boiled into a very, very, very short statement,

286
00:27:37,520 --> 00:27:41,120
I think is invaluable. Just being able to have it out there for people to find.

287
00:27:41,680 --> 00:27:46,880
What's some stance of yours, some belief that has changed most drastically in the past few years,

288
00:27:46,880 --> 00:27:51,360
let's say three, and it could be anywhere from something abstruse and academic to more colloquial,

289
00:27:51,360 --> 00:27:55,520
like I didn't realize the value of children or overvalued children. Now I'm stuck with them.

290
00:27:55,520 --> 00:28:00,080
Like, geez, that was a mistake. Yeah. So something where I changed

291
00:28:00,080 --> 00:28:07,600
my mind was RNA-based memory transfer. And I think it's a super interesting idea in this context

292
00:28:07,600 --> 00:28:13,040
because it's close to stuff that Michael has been working on and is interested in.

293
00:28:13,840 --> 00:28:20,320
There have been some experiments in the Soviet Union, I think in the 70s, where scientists took

294
00:28:20,320 --> 00:28:27,200
planaria, trained them to learn something. I think they learned how to be afraid of electric

295
00:28:27,200 --> 00:28:33,040
shocks and things like that. And then they put their brains into a blender, extracted the RNA,

296
00:28:33,040 --> 00:28:36,400
injected other planaria with it, and these other planaria had learned it.

297
00:28:37,280 --> 00:28:43,440
And I learned about this as a kid when I, in the 1980s, read Soviet science fiction literature.

298
00:28:43,440 --> 00:28:51,680
I grew up in Eastern Germany. And the evil scientist harvested the brains of geniuses

299
00:28:51,680 --> 00:28:58,240
and injected himself with RNA extracted from these brains and thereby acquired the skills.

300
00:28:58,240 --> 00:29:03,760
And even though I'm pretty sure this probably doesn't work if you do it at this level,

301
00:29:05,920 --> 00:29:11,600
this was inspired by this original research. And I later heard nothing about this anymore.

302
00:29:11,600 --> 00:29:19,600
And so I dismissed it as similar things as I read in Sputnik and other Russian publications,

303
00:29:19,600 --> 00:29:26,240
which create their own mythological universe about ball lightning that is agentic and possibly

304
00:29:26,240 --> 00:29:31,280
sentient and so on. And dismissed this all as basically another universe of another reader's

305
00:29:31,280 --> 00:29:36,880
digest culture that is producing its own ideas that then later on get dissolved once science

306
00:29:36,880 --> 00:29:41,360
advances. Because everybody knows it's synapses, it's connections between neurons that matter.

307
00:29:41,360 --> 00:29:46,000
The RNA is not that important for the information processing. It might change some state, but you

308
00:29:46,000 --> 00:29:50,880
cannot learn something by extracting RNA and re-injecting it into the next organism. Because

309
00:29:50,880 --> 00:29:56,640
how would that work if it's done in the synapses? And then recently there were some papers which

310
00:29:56,640 --> 00:30:03,360
replicated the original research and has been replicated from time to time in different types

311
00:30:03,360 --> 00:30:12,160
of organisms. But to my knowledge, not in, of course, macaques or not even mice. So it's not

312
00:30:12,160 --> 00:30:17,360
clear if their brains work according to the same principles as planaria. But planaria are not

313
00:30:19,360 --> 00:30:23,440
extremely simple organisms, only a handful of neurons. They are something intermediate.

314
00:30:23,440 --> 00:30:28,560
And so their main architecture is different from ours. And the functioning principles of

315
00:30:28,560 --> 00:30:32,800
their neurons might be slightly different, but it's worth following this idea and going down

316
00:30:32,800 --> 00:30:38,560
that rabbit hole. And then I looked from my computer science engineering perspective and

317
00:30:38,560 --> 00:30:45,280
I realized that there are always things about the synaptic story that I find confusing because

318
00:30:45,280 --> 00:30:52,000
they're very difficult to implement. For instance, weight sharing. As a computer scientist, I require

319
00:30:52,000 --> 00:30:57,040
weight sharing. I don't know how to get around this. If I want to entrain myself as computational

320
00:30:57,040 --> 00:31:01,680
primitives in the local area of my brain, for instance, the ability to rotate something,

321
00:31:02,640 --> 00:31:09,680
rotation is some operator that I apply on a pattern that allows this pattern to be represented in a

322
00:31:09,680 --> 00:31:15,120
slightly different way to have this object rotated a few degrees. But an object doesn't

323
00:31:15,120 --> 00:31:20,800
consist of a single point. It consists of many features that all need to get the same rotation

324
00:31:20,800 --> 00:31:26,320
applied to them using the same mathematical primitives. So how do you implement the same

325
00:31:26,320 --> 00:31:32,800
operator across an entire brain area? Do you make many, many copies of the same pattern?

326
00:31:32,800 --> 00:31:36,640
And so computer scientists solved that with so-called convolutional neural networks,

327
00:31:36,640 --> 00:31:42,240
which basically use the same weights again and again in different areas, only

328
00:31:42,960 --> 00:31:46,800
training them once and making them available everywhere. And that would be very difficult

329
00:31:46,800 --> 00:31:53,520
to implement in synapses. Maybe there are ways, but it's not straightforward. Another thing is

330
00:31:53,520 --> 00:31:58,880
if we see how training works in babies, they learn something and then they get rid of the

331
00:31:58,880 --> 00:32:05,120
surplus synapses. Initially, they have much more connectivity than they need. And after they've

332
00:32:05,120 --> 00:32:10,800
trained, they optimize the way in which the wiring works by discarding the things they don't need to

333
00:32:10,800 --> 00:32:17,920
compute what they want to compute. So it's like calling the synapses. It does not freeze or edge

334
00:32:17,920 --> 00:32:22,960
the learning into the brain, but it optimizes the energy usage of the brain. Another issue is that

335
00:32:23,760 --> 00:32:27,920
patterns of activation are not completely stable in the brain. In the cortex, if you look,

336
00:32:28,480 --> 00:32:33,200
you find that they might be moving the next day or even rotate a little bit, which is also difficult

337
00:32:33,200 --> 00:32:38,080
to do with synapses. You cannot read out the weights and copy them somewhere else in an easy,

338
00:32:38,080 --> 00:32:43,520
straightforward fashion. And another issue is defragmentation. If you learn, for instance,

339
00:32:43,520 --> 00:32:48,960
your body map into a brain area and then somebody changes your body map because you have an accident

340
00:32:48,960 --> 00:32:53,440
and lose a finger or somebody gives you an artificial limb and you start to integrate

341
00:32:53,440 --> 00:32:58,560
this into your body map, how do you shift all the representations around? How do you make space for

342
00:32:58,560 --> 00:33:03,600
something else and move it? Or also initially, when you set up your maps via happy and learning,

343
00:33:03,600 --> 00:33:08,320
how do you make sure that the neighborhoods are always correct and you don't need to realign

344
00:33:08,320 --> 00:33:13,280
anything? And I guess you need some kind of realignment. And all these things seem to be

345
00:33:13,280 --> 00:33:20,560
possible when you switch to a different paradigm. And so if you take this RNA base series seriously,

346
00:33:20,560 --> 00:33:27,840
go down this rabbit hole, what you get is the neurons are not learning a local function over

347
00:33:27,840 --> 00:33:32,800
its neighbors, but they are learning how to respond to the shape of an incoming activation front,

348
00:33:33,600 --> 00:33:37,120
like the spatial temporal pattern in their neighborhood. And they are densely enough

349
00:33:37,120 --> 00:33:43,840
connected so the neighborhood is just a space around them. And in this space, they basically

350
00:33:43,840 --> 00:33:49,680
interpret this according to a certain topology to say this is maybe a convolution that gives me

351
00:33:49,680 --> 00:33:54,480
two and a half D or it gives me two D or one D or whatever the type of function is that they want

352
00:33:54,480 --> 00:34:03,520
to compute and they learn how to fire in response to those patterns and thereby modulate the patterns

353
00:34:03,520 --> 00:34:07,760
when they're passed on. So the neurons act something like self-modulating ether,

354
00:34:07,760 --> 00:34:14,880
so which wavefronts propagate that perform the computations. And they store the responses to

355
00:34:14,880 --> 00:34:22,400
the distributions of incoming signals, possibly in RNA. So you have little mixtapes, little tape

356
00:34:22,400 --> 00:34:27,680
fragments that they store in a summa and that it can make more of very cheaply and easily. If

357
00:34:27,680 --> 00:34:31,920
they are successful mixtapes and they're useful computational primitives that they discovered,

358
00:34:31,920 --> 00:34:37,520
they can distribute this to other neurons through the entire cortex. So neurons of the same type

359
00:34:37,520 --> 00:34:43,680
will gain the knowledge to apply the same computational primitives. And that is something

360
00:34:43,680 --> 00:34:47,440
I don't know if the brain is doing that and if the human brain is using these principles

361
00:34:47,440 --> 00:34:52,000
or if it's using them a lot and how important this is and how many other mechanisms exist.

362
00:34:52,000 --> 00:34:56,320
But it's a mechanism that we haven't, to my knowledge, tried very much in AI and computer

363
00:34:56,400 --> 00:35:03,040
science. And it would work. There is something that is a very close analog, that is a neural

364
00:35:03,040 --> 00:35:09,760
cellular automaton. So basically instead of learning weight shifts or weight changes between

365
00:35:09,760 --> 00:35:16,720
adjacent neurons, what you learn is global functions that tell neurons on how to respond

366
00:35:16,720 --> 00:35:22,960
to patterns in the neighborhood. And these functions are the same for every point in your

367
00:35:22,960 --> 00:35:29,520
matrix. And you can learn arbitrary functions in this way. And what's nice about this is that you

368
00:35:29,520 --> 00:35:34,960
only need to learn computational primitives once. Our current neural networks need to learn the same

369
00:35:35,600 --> 00:35:40,000
linear algebra over and over again in many different corners of the neural network

370
00:35:40,000 --> 00:35:44,000
because you need vector algebra for many kinds of operations that we perform.

371
00:35:44,640 --> 00:35:49,120
For instance, operations in space where we shift things around or rotate them.

372
00:35:49,760 --> 00:35:55,760
And if they could exchange these useful operations with each other and just apply

373
00:35:55,760 --> 00:36:00,800
an operator whenever the environment dictates that this would be a good idea to try to apply

374
00:36:00,800 --> 00:36:05,360
this operator right now in this context, that could speed up learning. That could make training

375
00:36:05,360 --> 00:36:10,320
much more sample efficient. So something super interesting to try. And this is one of the rabbit

376
00:36:10,320 --> 00:36:16,720
holes I recently fell down over. I changed my thinking based on some experiment from

377
00:36:16,720 --> 00:36:22,080
neuroscience that doesn't have very big impact for the mainstream of neuroscience,

378
00:36:22,080 --> 00:36:25,840
but that I found reflected in Michael's work with planaria.

379
00:36:27,600 --> 00:36:32,880
Yeah, that's super interesting stuff. I can sprinkle a few details onto this.

380
00:36:35,520 --> 00:36:41,840
So the original finding in planaria was a guy named James McConnell at Michigan, actually,

381
00:36:41,840 --> 00:36:46,320
in the US. And then that was in the 60s, the early 60s. And then there were some really

382
00:36:46,320 --> 00:36:51,680
interesting Russian work that picked it up after that. We reproduced some of it recently

383
00:36:51,680 --> 00:36:58,480
in using modern quantitative automation and things like this. But one of the really cool

384
00:36:58,480 --> 00:37:03,360
aspects of this, and there's a whole community, by the way, with people like Randy Gallistil

385
00:37:03,360 --> 00:37:09,280
and Sam Gershman and, of course, Glantzman, David Glantzman, and people who are... That story of

386
00:37:09,280 --> 00:37:15,200
memory in the precise details of the synapses, that story is really starting to crack actually

387
00:37:15,200 --> 00:37:19,280
for a number of reasons. But one of the cool things that was done in the Russian work,

388
00:37:19,280 --> 00:37:26,240
and it was also done later on by Doug Blackiston, who's in my lab now as a staff scientist and other

389
00:37:26,240 --> 00:37:32,560
people, is this. Certain animals that go through larval stages. So you can take... So the Russians

390
00:37:32,560 --> 00:37:40,160
were using beetle larvae, and Doug and other people used moths and butterflies. So what happens is

391
00:37:40,880 --> 00:37:46,560
you train the larvae. So here you've got a caterpillar. So this caterpillar lives in a

392
00:37:46,560 --> 00:37:50,800
two-dimensional world. It's a soft-bodied robot. It lives in a two-dimensional world. It eats leaves

393
00:37:50,800 --> 00:37:55,920
and so on. And so you train this thing for a particular task. Well, during metamorphosis,

394
00:37:55,920 --> 00:38:00,480
it needs to become a moth or butterfly, which it lives in a three-dimensional world. Plus,

395
00:38:00,480 --> 00:38:05,680
it's a hard-bodied creature. So the controller is completely different for running a caterpillar

396
00:38:05,680 --> 00:38:10,720
versus a butterfly. So during that process, what happens is the brain is basically dissolved.

397
00:38:10,720 --> 00:38:15,600
So most of the connections are broken. Most of the cells are gone. They die. You put together

398
00:38:15,600 --> 00:38:20,160
a brand new brain that self-assembles, and you can ask all sorts of interesting philosophical

399
00:38:20,160 --> 00:38:24,000
questions of what it's like to be a creature whose brain is undergoing this massive change.

400
00:38:24,640 --> 00:38:31,520
But the information remains. And so one can ask, okay, certainly for computer science, it's amazing

401
00:38:31,520 --> 00:38:38,400
to have a memory medium that can survive this radical remodeling and reconstruction.

402
00:38:38,400 --> 00:38:45,840
And there's the RNA story, but also you had mentioned, does this work for mammals?

403
00:38:45,840 --> 00:38:50,960
So there was a guy in the 70s and 80s, there was a guy named George Ungar who did tons of,

404
00:38:50,960 --> 00:38:57,920
he's got tons of papers. He reproduced it in rats. So his was Fear of the Dark. And he actually,

405
00:38:58,640 --> 00:39:06,400
by establishing this assay and then fractionating their brains and extracting this activity,

406
00:39:06,400 --> 00:39:12,320
now he thought it was a peptide, not RNA. So he ended up with a thing called scotophobin,

407
00:39:12,320 --> 00:39:15,760
which turns out to be, I think, an eight mer peptide or something.

408
00:39:15,760 --> 00:39:20,080
And the claim was that you can transfer this scotophobin, you can synthesize it

409
00:39:20,080 --> 00:39:26,320
and then transfer it from brain to brain. And that's what he thought it was. And then I think

410
00:39:26,320 --> 00:39:32,560
David Glansman favors RNA again. But yeah, I agree with you. I think that's a super important

411
00:39:32,560 --> 00:39:40,640
story of how it is that this kind of information can survive just massive remodeling of the

412
00:39:40,640 --> 00:39:46,480
cognitive substrate. In planaria, what we did, and planaria, they have a true centralized brain.

413
00:39:46,480 --> 00:39:50,720
They have all the same neurotransmitters that we have. They're not a simple organism.

414
00:39:51,920 --> 00:39:56,080
What we did was McConnell's first experiments, which is to train them on something. And we

415
00:39:56,080 --> 00:40:01,440
train them to recognize a laser etched kind of bumpy pattern on the bottom of the dish and to

416
00:40:01,440 --> 00:40:05,440
recognize that that's where their food was going to be found. So they made this association between

417
00:40:05,440 --> 00:40:10,800
this pattern and getting food. And then we cut their heads off and we took the tails and the

418
00:40:10,800 --> 00:40:15,040
tails sit there for 10 days doing nothing. And then eventually they grow a new brain.

419
00:40:15,040 --> 00:40:19,440
And what happens is that information is then imprinted onto the new brain and then you can

420
00:40:19,440 --> 00:40:25,840
recover behavioral evidence that they remember the information. So that's pretty cool too,

421
00:40:25,840 --> 00:40:31,360
because it suggests that, well, we don't know if the information is everywhere or if it's in other

422
00:40:31,360 --> 00:40:36,560
places in the peripheral nervous system or in the nerve core that we don't know where it is yet.

423
00:40:36,560 --> 00:40:41,280
But it's clear that it can move around, that the information can move around in the body because

424
00:40:41,280 --> 00:40:46,000
it can be in the posterior half and then imprinted onto the brain, which actually drives all the

425
00:40:46,000 --> 00:40:51,360
behaviors. So thinking about that, I totally agree with you that this is a really important

426
00:40:51,360 --> 00:40:56,800
rabbit hole for asking, but there's an interesting puzzle here, which is this.

427
00:40:57,440 --> 00:41:05,680
It's one thing to remember things that are evolutionarily adaptive, like fear of the dark

428
00:41:05,680 --> 00:41:10,160
and things like this, but imagine, and this hasn't really been done well, but imagine for a moment

429
00:41:10,720 --> 00:41:15,840
if we could train them to something that is completely novel. Let's say we train them,

430
00:41:16,880 --> 00:41:21,040
three yellow life flashes means take a step to your left, otherwise you get shocked, something

431
00:41:21,040 --> 00:41:25,200
like that. And let's say they learn to do it. We haven't done this yet, but let's say this could

432
00:41:25,200 --> 00:41:30,720
work. One of the big puzzles is going to be when you extract whatever it is that you extract,

433
00:41:30,720 --> 00:41:36,080
let's say it's RNA or protein, whatever it is, you stick it into the brain of a recipient host.

434
00:41:36,720 --> 00:41:40,240
And in order for that memory to transfer, one of the things that the host has to be able to do is

435
00:41:40,240 --> 00:41:45,040
has to be able to decode it. And in order to decode it, it's one thing if we share the same

436
00:41:45,040 --> 00:41:49,200
codebook and by evolution, we could have the same codebook for things that come up all the time,

437
00:41:49,200 --> 00:41:56,560
like fear of the dark, things like that. But how would the recipient look at a weird

438
00:41:56,560 --> 00:42:01,840
sort of some kind of crazy hairpin RNA structure and analyze and be like, oh yes,

439
00:42:01,840 --> 00:42:07,440
that's three light flashes and then a step to the left, I see. So you would need to be able to

440
00:42:07,440 --> 00:42:12,800
interpret somehow this structure and convert it back to the behavior. And for behaviors that are

441
00:42:12,800 --> 00:42:18,160
truly arbitrary, that might be, I don't know actually how that would work. And so I think

442
00:42:18,160 --> 00:42:25,920
the frontier of this field is going to be to have a really convincing demonstration of a transfer

443
00:42:25,920 --> 00:42:31,680
of a memory that doesn't have a plausible pre-existing shared evolutionary decoding,

444
00:42:31,680 --> 00:42:36,400
because otherwise you have a real puzzle as to how the decoding is going to work.

445
00:42:36,400 --> 00:42:41,760
So this idea, and then even without the transfer, you can also think of it a different way.

446
00:42:41,760 --> 00:42:46,960
Every memory is like a message, is like basically a transplanted message from your past self to

447
00:42:46,960 --> 00:42:50,960
your future self, meaning that you still have to decode your memories. Whatever your memories are,

448
00:42:50,960 --> 00:42:54,640
in an important sense, you have to, those N-grams, you have to decode them somehow.

449
00:42:54,640 --> 00:43:01,440
So that whole issue of encoding and decoding, whatever the substrate of memory is, is maybe

450
00:43:01,440 --> 00:43:07,760
one of the most important questions there are. One of the ways we can think about these N-grams,

451
00:43:07,760 --> 00:43:14,800
I think that there are priors that condition what kinds of features are being spawned in

452
00:43:14,800 --> 00:43:20,560
which context. For instance, when we see a new scene, the way that perception seems to be working

453
00:43:20,560 --> 00:43:27,680
is that we spawn lots of feature controllers that then organize into objects that are controlled at

454
00:43:27,680 --> 00:43:33,840
the level of the scene. And this is basically a game engine that is forming in our brain,

455
00:43:33,840 --> 00:43:40,640
that is creating a population of interacting objects that are tuned to track our perceptual

456
00:43:40,640 --> 00:43:46,160
data at the lowest level. So all the patterns that we get from our retina and so on are samples,

457
00:43:46,160 --> 00:43:51,920
noisy samples that are difficult to interpret, but we are matching them into these hierarchies

458
00:43:51,920 --> 00:43:58,800
of features that are translated into objects that assign every feature to exactly one object and

459
00:43:58,800 --> 00:44:05,040
every pixel, so to speak, to exactly one, except in the case of transparency, and use this to

460
00:44:05,040 --> 00:44:09,840
interpret the scene that is happening in front of us. And when we are in the dark, what happens is

461
00:44:09,840 --> 00:44:15,120
that we spawn lots of object controllers without being able to disprove them, because there is no

462
00:44:15,120 --> 00:44:21,440
data that forces us to reject them. And if you have a vivid imagination, especially as a child,

463
00:44:21,440 --> 00:44:26,000
you will fill this darkness automatically with lots of objects, many of which will be scary.

464
00:44:27,120 --> 00:44:32,960
And so I think that lots of the fear of the dark doesn't need a lot of encoding in our brain. It

465
00:44:32,960 --> 00:44:38,160
is just an artifact of the fact that there are scary things in the world which we learn to

466
00:44:38,160 --> 00:44:42,720
represent at an early age, and that we cannot disprove them, that they will just spawn.

467
00:44:44,000 --> 00:44:50,880
I remember this vividly as a child, that whenever I had to go into the dark basement to get some

468
00:44:50,880 --> 00:44:57,360
food in our house in the countryside, that this darkness automatically filled with all sorts of

469
00:44:57,360 --> 00:45:04,720
shapes and things and possibilities. And it took me later to learn that you need to be much more

470
00:45:04,720 --> 00:45:10,640
afraid of the ghosts that can hide in the light. So what would be the implications of if you were

471
00:45:10,640 --> 00:45:16,160
able to transfer memory for something that's not trivial, so nothing that's like an archetype of

472
00:45:16,160 --> 00:45:23,840
fear of the dark between a mammal like rats? And when I say transfer memory, I mean, in this way

473
00:45:23,840 --> 00:45:28,320
that you blend up the brain or you, and also, can you explain what's meant by, I think I understand

474
00:45:28,320 --> 00:45:31,760
what it means to blend the brain of a planaria, but I don't think that's the same process that's

475
00:45:31,760 --> 00:45:37,680
going on in rats. Maybe it is. Well, Ungar did exactly the same thing. He would train rats

476
00:45:37,680 --> 00:45:42,480
for particular tasks. He would extract the brain, literally liquefy it to extract the chemical

477
00:45:42,480 --> 00:45:47,840
contents. He would then either inject the whole extract or a filtered extract where you would

478
00:45:47,840 --> 00:45:52,800
divide it up. You'd set fractionate it. So here's the RNAs, here's the proteins, here are other

479
00:45:52,800 --> 00:45:59,440
things. And then he would inject that liquid directly into the brains of recipient rats.

480
00:46:00,400 --> 00:46:06,320
When you do that, you lose spatial structure on the input because you just blended your brain.

481
00:46:06,320 --> 00:46:10,720
Whatever spatial structure there was, you just destroyed it. Also on the recipient,

482
00:46:12,320 --> 00:46:16,560
you just inject it. You're not finding that particular place where you're going to stick

483
00:46:16,560 --> 00:46:20,720
it. You just inject this thing right in the middle of the brain. Who knows where it goes,

484
00:46:20,720 --> 00:46:26,720
where the fluid goes. There's no spatial specificity there whatsoever. So if that works,

485
00:46:26,960 --> 00:46:32,800
what you're counting on is the ability of the brain to take up information via a completely

486
00:46:32,800 --> 00:46:37,680
novel route. So it's not information that's, for example, visual, right? Visual information

487
00:46:37,680 --> 00:46:43,040
that comes in exactly the same place all the time, right? There are optic nerves that connect to

488
00:46:43,040 --> 00:46:48,320
the same place in the brain, and that's where that information arrives. If you bathe the brain in

489
00:46:48,320 --> 00:46:54,400
some sort of informational extract, you're basically asking the cells to take it up almost

490
00:46:54,400 --> 00:46:58,560
as a primitive animal would with taste or touch you, right? That's kind of distributed all over

491
00:46:58,560 --> 00:47:02,320
the body, and you can sort of pick it up anywhere, and then you have to process this information.

492
00:47:02,320 --> 00:47:06,800
So you've got those issues right off the bat, right? That you've destroyed the incoming spatial

493
00:47:06,800 --> 00:47:13,120
structure. You can't really count on where it's going to land in the brain. And then the third

494
00:47:13,120 --> 00:47:17,920
thing, as you just mentioned, is the idea that, especially if we start with information that

495
00:47:18,160 --> 00:47:26,880
especially if we start with information that isn't any, that is so kind of specific and

496
00:47:31,600 --> 00:47:35,920
invented, that three light flashes means move to your left. I mean, there's never been an

497
00:47:35,920 --> 00:47:40,000
evolutionary reason to have that encoded. Like as you just said, having a fear of the dark is

498
00:47:40,000 --> 00:47:44,720
absolutely a natural kind of thing that you can expect. And then there are many other things like

499
00:47:44,720 --> 00:47:50,880
that. But something as contrived as three light flashes, and then you move to your left,

500
00:47:50,880 --> 00:47:56,320
there's no reason to think that we have a built-in way to recognize that. So when you as a recipient

501
00:47:56,320 --> 00:48:01,680
brain are handed this weird molecule with a particular structure or a set of molecules,

502
00:48:02,240 --> 00:48:07,920
being able to analyze that, having the cells in your brain or other parts of the body actually,

503
00:48:07,920 --> 00:48:12,880
that could analyze that and recover that original information would be extremely puzzling. I

504
00:48:12,880 --> 00:48:17,680
actually don't know how that would work. And I'm a big fan of unlikely sounding experiments

505
00:48:17,680 --> 00:48:23,360
that have implications if they would work. So this is something that I think should absolutely

506
00:48:23,360 --> 00:48:29,360
be done. And at some point we'll do it, but we haven't done it yet. So how far did the

507
00:48:29,360 --> 00:48:35,200
research in my school, what is the complexity of things that could be transmitted via this route?

508
00:48:35,920 --> 00:48:45,120
I don't remember everything that he did. The vast majority of, he did not go

509
00:48:46,400 --> 00:48:50,400
far to test all the complexities. What he tried to do was, because as you can imagine,

510
00:48:50,400 --> 00:48:55,840
he faced incredible opposition, right? So everybody sort of wanted to critique this thing.

511
00:48:55,840 --> 00:49:01,120
So he spent all of his time on, he picked one simple assay, which was this fear of the dark

512
00:49:01,120 --> 00:49:08,640
thing. And then he just bashed it for 20 years to just finally try to kind of crack that into the

513
00:49:08,640 --> 00:49:14,560
paradigm. He did not, as far as I know, do lots of different assays to try and make it more complex.

514
00:49:14,560 --> 00:49:21,600
I think it's very ripe for investigation. Did anyone else build upon his work?

515
00:49:22,880 --> 00:49:27,280
Not that I know. I mean, David Glansman is the best modern person who works on this, right? So

516
00:49:27,280 --> 00:49:36,240
he does a plesia and he does RNA. So he favors RNA. There's a little bit of work from Oded Rahavi

517
00:49:36,240 --> 00:49:42,720
in Israel with C. elegans. He's kind of looking into that. There's related work that has to do

518
00:49:42,720 --> 00:49:52,000
with cryogenics, which is this idea that if memories are a particular kind of dynamic electrical

519
00:49:52,000 --> 00:49:57,760
state, then some sort of cryogenic freezing is probably going to disrupt that. Whereas if

520
00:49:57,760 --> 00:50:03,360
it's a stable molecule, then it should survive. So again, I think there are people interested

521
00:50:03,360 --> 00:50:07,360
in that aspect of it, but I'm not sure. I'm not sure they've done anything with it.

522
00:50:08,000 --> 00:50:16,000
There's also Gaurav Venkataraman. I think he's at Berkeley. He told me that

523
00:50:16,880 --> 00:50:20,720
he has been working on this for several years, but he said it's sociologically tricky.

524
00:50:21,520 --> 00:50:25,840
And that's to me fascinating that we should care about that.

525
00:50:26,560 --> 00:50:27,680
What does he mean by that?

526
00:50:28,480 --> 00:50:34,320
What do you care about? What stupid people think? If this possibility exists that this works,

527
00:50:34,320 --> 00:50:40,320
the upside is so big that it's criminal to not research this. I think it's a disaster

528
00:50:40,320 --> 00:50:45,040
that you can read introductory textbooks on neuroscience and never ever hear about any of

529
00:50:45,120 --> 00:50:51,200
these experiments. Everybody who gets the introductory stuff on neuroscience only knows

530
00:50:51,200 --> 00:50:57,920
about information stored in the conic tome. And this leads to, for instance, the Blue Brain

531
00:50:57,920 --> 00:51:03,440
project. If RNA-based memory transfer is a thing, then this entire project is doomed,

532
00:51:04,160 --> 00:51:10,080
because you cannot get the story out of just recording the conic tome. Most of the research

533
00:51:10,080 --> 00:51:15,520
right now is focused on reconstructing the conic tome as it was circuitry and hoping that

534
00:51:15,520 --> 00:51:21,280
we can get the functionality of information processing and deduce the specificity of the

535
00:51:21,920 --> 00:51:25,440
particular brain, what it has learned from the connections between neurons.

536
00:51:25,440 --> 00:51:32,000
But what if it turns out this doesn't matter? You just need connections that are dense enough,

537
00:51:32,000 --> 00:51:36,800
and so basically stochastic lattice that is somewhat randomly wired. What matters is what

538
00:51:36,800 --> 00:51:40,160
the neurons are doing with the information that they're getting through this ether,

539
00:51:40,160 --> 00:51:44,560
through this lattice. It just changes the entire way in which we need to look at things.

540
00:51:44,560 --> 00:51:48,160
And if this possibility exists, and if this possibility is just 1%,

541
00:51:48,960 --> 00:51:55,440
but there are some experimental points in this direction, it is ridiculous to not pursue this

542
00:51:55,440 --> 00:52:01,360
with high pressure and focus on it and support research that goes in this direction. Basically,

543
00:52:01,360 --> 00:52:06,240
what's useful is not so much answering questions in science, it's discovering questions,

544
00:52:06,240 --> 00:52:11,280
it's discovering new uncertainty. Reducing the uncertainty is much easier than discovering new

545
00:52:11,280 --> 00:52:17,920
areas of where you thought that you were certain, but that allow you to get new insights. And it

546
00:52:17,920 --> 00:52:23,120
seems to me that a lot of neuroscience is stuck, that it does not produce results that seem to

547
00:52:23,120 --> 00:52:30,000
accumulate in an obvious way towards a theory on how the brain processes information. So the

548
00:52:30,000 --> 00:52:36,400
neuroscientists don't deliver input to the researchers, and the transformer is not the

549
00:52:36,400 --> 00:52:42,720
result of reading a lot of neuroscience. It's really mostly the result of people's thinking

550
00:52:42,720 --> 00:52:50,640
about statistics of data processing. And it would be great if we would focus on ideas that

551
00:52:50,640 --> 00:52:54,800
are promising and new and that have the power to shake existing paradigms.

552
00:52:55,440 --> 00:53:01,760
Yeah. This is so important, and it's not just neuroscience. In developmental biology,

553
00:53:01,760 --> 00:53:06,720
we have exactly the same thing. And I'll just give you two very simple examples of it where,

554
00:53:06,720 --> 00:53:11,200
and I tell the students, when I give talks to students, I say, isn't it amazing that

555
00:53:12,240 --> 00:53:16,480
in your whole course of biology and your developmental biology textbook, there's not

556
00:53:16,480 --> 00:53:22,960
a mention of any of this because it completely just undermines a lot of the basic assumptions.

557
00:53:22,960 --> 00:53:28,640
So here's a couple of examples. One example is that as of trophic memory in deer,

558
00:53:28,640 --> 00:53:33,680
so there are species of deer that every year they regenerate. So they make this antler

559
00:53:33,680 --> 00:53:39,040
rack on their heads, the whole thing falls off, and then it regrows the next year. So these two

560
00:53:39,040 --> 00:53:44,960
guys, Bobenak, which are a father and son team that did these experiments for 40 years, and I

561
00:53:44,960 --> 00:53:50,640
actually have all these antlers in my lab now because when the younger one retired, he sent me

562
00:53:50,640 --> 00:53:55,360
all these things, all these antlers. The idea is this, what you can do is you take a knife

563
00:53:55,360 --> 00:54:01,360
and somewhere in this branch structure, you make a wound and the bone will heal and you get a

564
00:54:01,360 --> 00:54:06,640
little callus and that's it for that year. Then the whole thing drops off. And then next year,

565
00:54:07,840 --> 00:54:12,320
it starts to grow and it will make an ectopic tine, an ectopic branch at the point where you

566
00:54:12,320 --> 00:54:18,320
injured it last year. And this goes on for five or six years, and then eventually it goes away and

567
00:54:18,320 --> 00:54:28,160
you get a normal rack again. And so the amazing thing about it is that the standard models for

568
00:54:28,160 --> 00:54:36,400
patterning for morphogenesis are these gene regulatory networks and genetic biochemical

569
00:54:36,400 --> 00:54:44,080
gradients and so on. If you try to come up with a model for this, so for encoding an arbitrary point

570
00:54:44,080 --> 00:54:49,200
within a branch structure that your cells at the scalp have to remember for months after the whole

571
00:54:49,200 --> 00:54:54,000
thing is dropped off, and then not only remember it, but then implement it so that when the bone

572
00:54:54,000 --> 00:55:00,160
starts to grow, something says, oh yes, that's the start another tine growing to your left exactly

573
00:55:00,160 --> 00:55:07,200
here. Trying to make a model of this using the standard tools of the field is just incredibly

574
00:55:07,200 --> 00:55:12,960
difficult. And there are other examples of this, but this kind of non-genetic memory that's just

575
00:55:12,960 --> 00:55:17,440
very difficult to explain with standard models. The other thing, which is I think an even bigger

576
00:55:17,440 --> 00:55:24,400
scandal, is the whole situation with planaria. Some species of planaria, the way they reproduce

577
00:55:24,400 --> 00:55:28,400
is they tear themselves in half, each half regenerates the missing piece, and now you've

578
00:55:28,400 --> 00:55:33,040
got two. That's how they reproduce. So if you're going to do that, what you end up avoiding is

579
00:55:33,040 --> 00:55:37,360
Weissman's barrier, this idea that when we get mutations in our body, our children don't inherit

580
00:55:37,360 --> 00:55:42,800
those mutations. So this means that any mutation that doesn't kill the stem cell in the body gets

581
00:55:42,800 --> 00:55:48,960
amplified as that cell contributes to regrowing the worm. So as a result of this, for 400 million

582
00:55:48,960 --> 00:55:54,160
years, these planaria have accumulated mutations. Their genomes are an incredible mess. Their cells

583
00:55:54,160 --> 00:55:58,320
are basically mixoploid, meaning they're like a tumor. Every cell has a different number of

584
00:55:58,320 --> 00:56:05,600
chromosomes potentially. It just looks horrible. As an end result, you've got an animal that is

585
00:56:05,600 --> 00:56:12,240
immortal, incredibly good at regenerating with 100% fidelity and very resistant to cancer.

586
00:56:12,240 --> 00:56:19,280
Now, all of this is the exact opposite of the message you get from a typical course through

587
00:56:19,280 --> 00:56:24,000
biology, which says that, what is the genome for? The genome is for setting your body structure.

588
00:56:24,000 --> 00:56:28,720
If you mess with the genome, that information goes away. You get aging, you get cancer.

589
00:56:29,360 --> 00:56:36,320
Why does the animal with the worst genome have the best anatomical fidelity? I think we actually,

590
00:56:37,280 --> 00:56:41,120
a few months ago, we actually, I think, have some insight into this, but it's been bugging

591
00:56:41,120 --> 00:56:46,400
me for years. And this is the kind of thing that nobody ever talks about because it goes

592
00:56:46,400 --> 00:56:52,160
against the general assumption of what genomes actually do and what they're for. And this complete

593
00:56:52,160 --> 00:56:57,280
lack of correlation between the genome, in fact, an anti-correlation between the genome quality

594
00:56:57,280 --> 00:57:02,640
and the incredible ability of this animal to have a healthy anatomy.

595
00:57:02,640 --> 00:57:07,200
Yeah. What is that insight that you mentioned you acquired a few months ago, preliminary?

596
00:57:07,760 --> 00:57:15,200
Okay. In the name of throwing out kind of new unproven ideas, right? So this is just my

597
00:57:15,200 --> 00:57:20,720
conjecture. We've done some computational modeling of it, which I initially, this was a

598
00:57:22,320 --> 00:57:27,120
very clever student that I work with named Laxwin, who did some models with me. And

599
00:57:28,720 --> 00:57:33,440
I initially thought it was a bug. And then I realized that, no, actually, this is the feature.

600
00:57:33,440 --> 00:57:39,280
The idea is this, imagine... So we've been working for a long time on a concept of

601
00:57:40,800 --> 00:57:44,800
competency among embryonic parts. And what this means is basically the idea that

602
00:57:46,480 --> 00:57:53,920
there are homeostatic feedback loops among various cells and tissues and organs that

603
00:57:53,920 --> 00:57:59,920
attempt to reach specific outcomes in anatomical morphous space, despite various perturbations.

604
00:57:59,920 --> 00:58:05,200
So the idea is that if you have a tadpole and you do something to it, whether by a mutation

605
00:58:05,200 --> 00:58:09,760
or by a drug or something, you do something to it where the eye is a little off kilter,

606
00:58:09,760 --> 00:58:14,240
or the mouth is a little off. All of these organs pretty much know where they're supposed to be.

607
00:58:14,240 --> 00:58:19,200
They will try to minimize distance from other landmarks and they will remodel. And eventually

608
00:58:19,200 --> 00:58:27,040
you get a normal frog so that they will recover the correct anatomy, despite starting off in the

609
00:58:27,040 --> 00:58:31,040
wrong position, or even things like changes in the number of cells or the size of cells.

610
00:58:31,040 --> 00:58:36,640
They're really good at getting their job done despite various changes. So they have these

611
00:58:36,640 --> 00:58:42,160
competencies to optimize specific things like their position and the structure and things like

612
00:58:42,160 --> 00:58:49,760
that. So that's competency. Now, here's the interesting thing. Imagine that you have a

613
00:58:49,760 --> 00:58:56,560
species that has some degree of that competency. And so you've got an individual, if that species

614
00:58:56,560 --> 00:59:02,000
comes up for selection, fitness is high, looks pretty good. But here's the problem.

615
00:59:02,000 --> 00:59:07,120
Selection doesn't know whether the fitness is high because his genome was amazing,

616
00:59:07,120 --> 00:59:11,920
or the fitness is high because the genome was actually so-so, but the competency made up for it

617
00:59:11,920 --> 00:59:17,440
and now everything got back to where it needs to go. So what the competency apparently does

618
00:59:17,440 --> 00:59:23,440
is shield information from evolution about the actual genome. It makes it harder to pick the

619
00:59:23,440 --> 00:59:29,120
best genomes because your individuals that perform well don't necessarily have the best genomes.

620
00:59:29,120 --> 00:59:37,120
What they do have is competency. So what happens in our simulations is that if you start off with

621
00:59:37,120 --> 00:59:43,680
even a little bit of that competency, evolution loses some power in selecting the best genomes,

622
00:59:43,680 --> 00:59:48,880
but where all the work tends to happen is increasing the competency. So then the competency

623
00:59:48,880 --> 00:59:53,360
goes up. So the cells are even better and the tissues are even better at getting the job done

624
00:59:53,360 --> 01:00:00,960
despite the bad genome. That makes it even worse. That makes it even harder for evolution to see

625
01:00:00,960 --> 01:00:06,160
the best genomes, which relieves some of the pressure on having a good genome, but it basically

626
01:00:06,160 --> 01:00:13,920
puts all the pressure on being really competent. So basically what happens is that the genetic

627
01:00:13,920 --> 01:00:19,440
fitness basically levels out at a really suboptimal level. And in fact, the pressure

628
01:00:19,440 --> 01:00:24,880
is off of it. So it's tolerant to all kinds of craziness, but the competency and the mechanisms

629
01:00:24,880 --> 01:00:31,520
of competency get pushed up really high. So in many animals, and there are other factors that

630
01:00:31,520 --> 01:00:35,840
push against this ratchet, but it becomes a positive feedback loop. It becomes a ratchet

631
01:00:35,840 --> 01:00:43,760
for optimal performance despite a suboptimal genome. And so in some animals, this evens out

632
01:00:43,760 --> 01:00:48,960
at a particular point, but I think what happened in planaria is that this whole process ran away

633
01:00:48,960 --> 01:00:56,000
to its ultimate conclusion. The ultimate conclusion is the competency algorithm became so good that

634
01:00:56,000 --> 01:01:01,520
basically whatever the genome is, it's really good at creating and maintaining a proper worm

635
01:01:01,520 --> 01:01:06,960
because it is already being evolved in the presence of a genome whose quality we cannot control.

636
01:01:06,960 --> 01:01:12,720
So in computer science speak, it's kind of like, and Steve Frank put me onto this analogy,

637
01:01:12,720 --> 01:01:17,440
it's kind of like what happens in rate arrays. When you have a nice rate array where the software

638
01:01:17,440 --> 01:01:22,240
makes sure that you don't lose any data, the pressure is off to have really high quality

639
01:01:22,240 --> 01:01:29,520
media. And so now you can tolerate media with lots of mistakes because the software takes care of it

640
01:01:29,520 --> 01:01:34,960
in the rate and the architecture takes care of it. So basically what happens is you've got this

641
01:01:34,960 --> 01:01:43,120
animal where that runaway feedback loop went so far that the algorithm is amazing and it's been

642
01:01:45,520 --> 01:01:50,720
evolved specifically for the ability to do what it needs to do even though the hardware is kind

643
01:01:50,720 --> 01:01:56,400
of crap. And it's incredibly tolerant. So this has a number of implications that to my knowledge

644
01:01:56,400 --> 01:02:03,040
have never been explained before. For example, in every other kind of animal, you can call a stock

645
01:02:03,040 --> 01:02:09,280
center and you can get mutants. So you can get mice with kinky kind of kink tails. You can get

646
01:02:09,280 --> 01:02:15,200
flies with red eyes and you can get chickens without toes and you can get humans come with

647
01:02:16,640 --> 01:02:24,720
albinos and things. There's always mutants that you can get. Planaria, there are no abnormal lines

648
01:02:24,720 --> 01:02:30,000
of planaria anywhere except for the only exception is our two-headed line and that one's not genetic.

649
01:02:30,480 --> 01:02:39,040
That one's bioelectric. So isn't it amazing that nobody has been able, despite 120 years of

650
01:02:39,040 --> 01:02:45,280
experiments with planaria, nobody has isolated a line of planaria that is anything other than

651
01:02:45,280 --> 01:02:50,080
a perfect planaria. And I think this is why. I think it's because they have been actually selected

652
01:02:50,080 --> 01:02:55,200
for being able to do what they need to do despite the fact that the hardware is just very junky.

653
01:02:56,160 --> 01:03:06,080
So that's my current take on it. And really it puts more emphasis on the algorithm and the

654
01:03:06,080 --> 01:03:11,760
decision making among that cellular collective of what are we going to build and what's the

655
01:03:11,760 --> 01:03:15,120
algorithm for making sure that we're all working to build the correct thing.

656
01:03:16,000 --> 01:03:21,600
So if you translate this idea into computer science, a way to look at it is imagine that you

657
01:03:21,600 --> 01:03:29,360
find some computers that have hard disks that are very, very noisy and where the hard disk

658
01:03:29,360 --> 01:03:34,240
basically makes lots and lots of mistakes in encoding things and bits often flip and so on.

659
01:03:34,240 --> 01:03:38,880
And you will find that these computers still work and they work in pretty much the same way

660
01:03:38,880 --> 01:03:44,640
as the other computers that you have. And there is an orthodox sect of computer scientists that

661
01:03:44,640 --> 01:03:51,600
thinks it is necessary that every bit on the hard disk is completely reliable or reliable to

662
01:03:51,600 --> 01:03:57,760
such a degree that you only have a mistake once every 100 trillion copies. And you can

663
01:03:57,760 --> 01:04:01,600
have an error correction code running on the hard disk at the low level that corrects this.

664
01:04:01,600 --> 01:04:05,760
And after some point it doesn't become efficient anymore. So you need to have reliable hard disks

665
01:04:05,760 --> 01:04:11,200
to be able to have computers that work like this. But how would these other computers work?

666
01:04:11,200 --> 01:04:16,720
And it basically means that you create a virtual structure on top of the noisy structure

667
01:04:16,720 --> 01:04:22,160
that is correcting for whatever degree of uncertainty you have or the degree of

668
01:04:22,160 --> 01:04:29,200
randomness that gets injected into your substrate. Dave Eggley has a very nice metaphor for this.

669
01:04:29,200 --> 01:04:31,520
Do you know him, maybe? Yeah, I know him.

670
01:04:31,520 --> 01:04:39,120
Yeah, he's a beautiful artist who explores complexity by tinkering with computational models

671
01:04:39,120 --> 01:04:43,920
and really finds his work very inspiring. And he has this idea of best effort computing. So

672
01:04:43,920 --> 01:04:48,960
in his view, our own nervous system is a best effort computer. It's one that does not rely

673
01:04:48,960 --> 01:04:55,520
on the other neurons around you working perfectly, but make an effort to be better than random.

674
01:04:56,240 --> 01:05:03,440
And then you stack the improbabilities empirically by having a system that evolves to measure in

675
01:05:03,440 --> 01:05:10,160
effect the unreliability of its components. And then stack the probabilities until you get

676
01:05:10,160 --> 01:05:17,920
the system to be deterministic enough to do what you're doing with it. If you have a system that

677
01:05:17,920 --> 01:05:24,080
is, as in the planaria, inherently very noisy, where the genome is an unreliable witness of

678
01:05:24,080 --> 01:05:30,000
what should be done in the body, you just need to interpret it in a way that stacks the probabilities,

679
01:05:30,000 --> 01:05:36,080
that is evaluating things with much more error tolerance. And maybe this is always the case.

680
01:05:36,080 --> 01:05:41,200
Maybe there is a continuum, maybe not. It's also possible that there is some kind of phase shift

681
01:05:41,200 --> 01:05:46,640
where you switch from organisms with reliable genomes to organisms with noisy genomes. And you

682
01:05:46,640 --> 01:05:50,960
basically use a completely different way to construct the organism as a result. But it's

683
01:05:50,960 --> 01:05:56,960
a very interesting hypothesis then to see if this is a radical thing or a gradual thing that happens

684
01:05:56,960 --> 01:06:02,560
in all organisms to some degree. What I also like about this description that you give about

685
01:06:02,560 --> 01:06:10,000
how the organism emerges, it maps in some sense also in how perception works in our own mind.

686
01:06:10,960 --> 01:06:17,920
At the moment, machine learning is mostly focused on recognizing images or individual frames. And

687
01:06:17,920 --> 01:06:22,400
you feed in information frame by frame and the information is totally disconnected.

688
01:06:23,200 --> 01:06:28,080
A system like Dali2 is trained by giving it several hundreds of millions of images.

689
01:06:28,800 --> 01:06:32,720
And they are disconnected. They are not adjacent images in the space of images. And

690
01:06:33,280 --> 01:06:38,480
maybe you could not probably learn from giving 600 million images in a dark room and only looking

691
01:06:38,480 --> 01:06:44,000
at this introduced the structure of the world from this. Whereas Dali can, which gives testament to

692
01:06:44,000 --> 01:06:49,200
the power of our statistical methods and hardware that we have. That far surpasses, I think, the

693
01:06:49,200 --> 01:06:53,680
combined power and reliability of brains, which probably would not be able to integrate so much

694
01:06:53,680 --> 01:06:59,440
information over such a big distance. For us, the world is learnable because its adjacent frames

695
01:06:59,440 --> 01:07:04,800
are correlated. Basically, information gets preserved in the world through time. And we

696
01:07:04,800 --> 01:07:09,680
only need to learn the way in which the information gets transmogrified. And these

697
01:07:09,680 --> 01:07:14,240
transmogrification of information means that we have a dynamic world in which the static image

698
01:07:14,240 --> 01:07:18,800
is an exception. The identity function is a special case of how the universe changes. And

699
01:07:18,800 --> 01:07:25,120
we mostly learn change. I just got visited by my cat. And my cat has difficulty to recognize

700
01:07:25,120 --> 01:07:29,920
static objects compared to moving objects, where it's much, much easier to see a moving ball than

701
01:07:29,920 --> 01:07:34,640
a ball that is lying still. And it's because it's much easier to segment it out the environment

702
01:07:34,640 --> 01:07:39,680
when it moves. So the task of learning on a moving environment, a dynamic environment,

703
01:07:39,680 --> 01:07:45,280
is much easier because it imposes constraints on the world. And so how do we represent a moving

704
01:07:46,240 --> 01:07:52,240
world compared to a static world? The semantics of features changes. And an object is basically

705
01:07:52,240 --> 01:07:57,920
composed of features that can be objects themselves. And the scene is a decomposition

706
01:07:57,920 --> 01:08:03,440
of all the features that we see into a complete set of objects that explain the entirety of the

707
01:08:03,440 --> 01:08:07,280
scene. And the interaction between them and causality is the interaction between objects.

708
01:08:08,400 --> 01:08:12,720
And in a static image, these objects don't do anything. They don't interact with each other.

709
01:08:12,720 --> 01:08:16,880
They just stand in some kind of relationship that you need to infer, which is super difficult

710
01:08:16,880 --> 01:08:22,800
because you only have this static snapshot. And so the features are classifiers that tell you

711
01:08:22,800 --> 01:08:29,680
how to, whether a feature is a hand or a foot or a pen or a sun or a flashlight or whatever,

712
01:08:29,680 --> 01:08:34,240
and how they relate to the larger scene, in which, again, you have a static relationship

713
01:08:34,240 --> 01:08:38,640
in which you need to classify the object based on the features that contribute to them.

714
01:08:38,640 --> 01:08:42,400
And you need to find some kind of description where you interpret features, which are usually

715
01:08:42,400 --> 01:08:46,160
ambiguous and could be many different things, depending on the context in which you interpret

716
01:08:46,160 --> 01:08:51,760
them, into one optimal global configuration, right? But if the scene is moving, this changes

717
01:08:51,760 --> 01:08:56,240
a little bit. What happens now is that the features become operators. They're no longer

718
01:08:56,240 --> 01:09:01,200
classifiers that tell you how your internal state needs to change, how your world needs to change,

719
01:09:01,200 --> 01:09:05,760
how your simulation of the universe in your mind needs to change to track the sensory patterns.

720
01:09:06,400 --> 01:09:13,440
Right, so a feature now is a change operator, a transformation. And the feature is in some sense

721
01:09:13,440 --> 01:09:18,480
a controller that tells you how the bits are moving in your local model of the universe.

722
01:09:18,480 --> 01:09:23,760
And they're organized in a hierarchy of controllers. And these controllers need to

723
01:09:23,760 --> 01:09:27,760
be turned on and off at the level of the scene. And they have a lot of flexibility once you have

724
01:09:27,760 --> 01:09:31,200
them. They can move around in the scene. They're basically now self-organizing,

725
01:09:31,200 --> 01:09:36,320
self-stabilizing entities. In the same way as the mouse is moving around in your organism,

726
01:09:36,320 --> 01:09:40,800
a feature can move around in the organism and shift itself around to communicate with other

727
01:09:40,800 --> 01:09:44,480
features until they negotiate a valid interpretation of reality.

728
01:09:45,840 --> 01:09:51,840
That's incredibly interesting because as soon as you started saying that,

729
01:09:52,720 --> 01:09:58,400
I was starting to think that the virtualization that enables, right, so the earlier part of which

730
01:09:58,400 --> 01:10:07,600
we're saying the virtualization of the information that allows you to deal with unreliable

731
01:10:07,600 --> 01:10:13,440
hardware and everything, the bioelectric circuits that we deal with are a great candidate for that

732
01:10:13,440 --> 01:10:18,720
because actually we see exactly that. We see a bioelectric pattern that is very resistant to

733
01:10:18,720 --> 01:10:23,120
changes in the details and make sure that everybody does the right thing under a wide range of

734
01:10:24,080 --> 01:10:28,560
different defects and so on. But even more than that, the other thing that you were just

735
01:10:29,200 --> 01:10:33,120
emphasizing this, the fact that we learn the delta and that we're looking for change,

736
01:10:33,840 --> 01:10:39,280
very interesting. If you pivot the whole thing from the temporal domain to the spatial domain,

737
01:10:39,280 --> 01:10:46,080
so in development, when we look at these bioelectric patterns, now these patterns

738
01:10:46,080 --> 01:10:51,040
are across space, not across time. So unlike in neuroscience where everything is in the temporal

739
01:10:51,040 --> 01:10:57,040
domain for neurons, these are static voltage patterns across tissue, right, across the whole

740
01:10:57,040 --> 01:11:03,600
thing. So for the longest time, we asked this question, how are these read out? How do cells

741
01:11:03,600 --> 01:11:09,920
actually read these? Because one possibility early, this was a very early hypothesis 20 years ago,

742
01:11:09,920 --> 01:11:14,720
was that maybe the local voltage tells every cell what to be. So it's like a paint by numbers kind

743
01:11:14,720 --> 01:11:22,800
of thing. And each voltage value corresponds to some kind of outcome. That turned out to be false.

744
01:11:22,800 --> 01:11:27,200
What we did find is that, and we have computational models of how this works now,

745
01:11:29,520 --> 01:11:34,880
what is read out is the delta, the difference between regions. It doesn't care, nobody cares

746
01:11:34,880 --> 01:11:40,800
about what the absolute voltage is, what is read out in terms of outcomes for downstream cell

747
01:11:40,800 --> 01:11:44,720
behavior, gene expression, all that. What is actually read out is the voltage difference

748
01:11:44,720 --> 01:11:49,440
between two adjacent domains. So that is exactly actually what it's doing just in the spatial

749
01:11:49,440 --> 01:11:57,920
domain. It only keys off of the delta. And what is learned from that is exactly as you were saying,

750
01:11:57,920 --> 01:12:02,880
it modifies the controller for what's downstream of that. And there may be multiple ones that are

751
01:12:02,880 --> 01:12:08,880
sort of moving around and co-inhabiting. I mean, it's a very compelling picture actually and way

752
01:12:08,880 --> 01:12:15,200
to look at some of the simulations that we've been doing about how the bioelectric data are

753
01:12:15,200 --> 01:12:19,280
interpreted by the rest of the cells. It's very interesting.

754
01:12:19,280 --> 01:12:23,600
So Professor Levin used the word competence earlier, and I'd like you to define that.

755
01:12:24,640 --> 01:12:35,120
Yeah. In order to define it, I want to put out two concepts to this. One idea is that, to me,

756
01:12:35,120 --> 01:12:39,760
and this goes back to what we were talking about before as the engineering stance on things,

757
01:12:40,400 --> 01:12:48,800
I think that useful cognitive claims such as something, when you say this system has whatever

758
01:12:48,800 --> 01:12:53,680
or it can whatever, as far as various types of cognitive capacities, I think those kinds of

759
01:12:53,680 --> 01:12:59,280
claims are really engineering claims. That is, when you tell me that something is competent

760
01:12:59,280 --> 01:13:06,400
at a particular level, so you can think about Wiener and Rosenbluth scale of cognition that

761
01:13:06,400 --> 01:13:12,960
goes from simple passive materials and then reflexes and then all the way up to second

762
01:13:12,960 --> 01:13:16,320
order metacognition and all that. When you tell me that something is on that

763
01:13:16,880 --> 01:13:22,320
ladder and where it is, what you're really telling me is, if I want to predict its behavior or I

764
01:13:22,320 --> 01:13:27,920
want to use it in an engineering context or I want to interact with it or relate to it in some way,

765
01:13:27,920 --> 01:13:32,000
this is what I can expect. That's what you're really telling me. All of these terms,

766
01:13:32,800 --> 01:13:36,880
what they really are, are engineering protocols. If you tell me that something

767
01:13:36,880 --> 01:13:43,680
has the capacity to do associative learning or whatever, what you're telling me is that,

768
01:13:43,680 --> 01:13:47,520
hey, you can do something more with this than you could with a mechanical clock.

769
01:13:47,520 --> 01:13:53,040
You can provide certain types of stimuli or experiences and you can expect it to do this

770
01:13:53,040 --> 01:13:59,600
or that afterwards. Or if you tell me that something is a homeostat, that means that,

771
01:13:59,600 --> 01:14:06,080
hey, I can count on it to keep some variable at a particular range without having to be myself

772
01:14:06,080 --> 01:14:10,080
to control it all the way. It has a certain autonomy now. If you tell me that something

773
01:14:10,080 --> 01:14:14,640
is really intelligent and it can do XYZ, then I know that, okay, you're telling me that it

774
01:14:14,640 --> 01:14:19,520
has even more autonomous behavior in certain contexts. All of these terms, to me, what they

775
01:14:19,520 --> 01:14:23,360
really are, they're not... And that has an important implication. The implication is that

776
01:14:23,360 --> 01:14:30,320
they're observer dependent, that you've picked some kind of problem space, you've picked some

777
01:14:30,320 --> 01:14:34,960
kind of perspective. And from that problem space and that perspective, you're telling me that

778
01:14:36,800 --> 01:14:42,000
given certain goal states, this system has that much competency to pursue those goal states.

779
01:14:42,000 --> 01:14:45,840
And different observers can have different views on this for any given system. So for example,

780
01:14:46,400 --> 01:14:50,720
somebody might look at a brain, let's say a human brain and say, well, I'm pretty sure the only

781
01:14:50,720 --> 01:14:55,760
thing, this is a paperweight. So it's really pretty much just competent in going down gravitational

782
01:14:55,760 --> 01:14:59,360
gradients. So all it can do is hold down paper, that's it. And somebody else will look at it and

783
01:14:59,360 --> 01:15:03,520
say, you missed the whole point. This thing has competencies in behavioral space and

784
01:15:03,520 --> 01:15:10,560
linguistic space. So these are all empirically testable engineering claims about what you can

785
01:15:10,640 --> 01:15:16,880
expect the system to do. So when I say competency, what I mean is we specify a space, a problem

786
01:15:16,880 --> 01:15:21,040
space. And at the time when we were talking about this, the problem space that I was talking about

787
01:15:21,040 --> 01:15:26,480
was the anatomical morphous space. That was the space we were talking about. So the space of

788
01:15:26,480 --> 01:15:32,400
possible anatomical configurations and specifically navigating that morphous space. So you start off

789
01:15:32,400 --> 01:15:38,320
as an egg or you start off as a damaged limb or whatever, and you navigate that morphous space

790
01:15:38,320 --> 01:15:44,240
into the correct structure. So when I say competency, I mean, you have the ability to deploy

791
01:15:44,240 --> 01:15:51,600
certain kinds of tricks to navigate that morphous space with some level of performance that I can

792
01:15:51,600 --> 01:15:55,840
count on. And so the competency might be really low or it might be really high. And I would have to

793
01:15:55,840 --> 01:16:00,560
make specific claims about what I mean. Here's an example of a common, and there are many,

794
01:16:00,560 --> 01:16:04,320
if you just think about the behavioral science of navigation, there are many competencies you

795
01:16:04,320 --> 01:16:09,600
can think about. Does it know ahead of time where it's going? Does it have a memory of where it's

796
01:16:09,600 --> 01:16:17,440
been? Or is it a very simple sort of reflex arc is all it has? Or here's one example of a pretty

797
01:16:17,440 --> 01:16:25,600
cool competency that a lot of biological systems have. If we take some cells that are in the tail

798
01:16:25,680 --> 01:16:38,800
of a tadpole and we modify their ion channels such that they now acquire the goal of navigating to

799
01:16:38,800 --> 01:16:43,280
an eye fate in this morphous space, meaning that they're going to make an eye. These things,

800
01:16:43,280 --> 01:16:47,280
in fact, will create an eye and they'll make an eye in the tail, on the gut, wherever you want.

801
01:16:47,280 --> 01:16:52,640
But one of the cool, and so that's already pretty cool, but one of the amazing aspects is

802
01:16:52,640 --> 01:16:58,480
if I only modify a few cells, not enough to make an actual eye, just a handful of cells,

803
01:16:58,480 --> 01:17:04,080
and we've done this and you can see this work. One of the competencies they have is to recruit

804
01:17:04,080 --> 01:17:09,440
local neighbors that were themselves not in any way manipulated to help them achieve that goal.

805
01:17:09,440 --> 01:17:14,080
It's a little bit like in an ant colony. This idea of recruitment in ants and termites is an

806
01:17:14,080 --> 01:17:18,880
idea of recruitment where individuals can recruit others and talk about a flexible collective

807
01:17:18,960 --> 01:17:24,960
intelligence. This is it. You've re-specified the goal for that set of cells, but one of the

808
01:17:24,960 --> 01:17:29,680
things that they do without us telling them how to do it or having to micromanage it,

809
01:17:29,680 --> 01:17:34,640
they already have the competency to recruit as many cells as they need to get the job done.

810
01:17:36,400 --> 01:17:40,800
For an engineer, that's a very nice competency because it means that I don't need to worry

811
01:17:40,800 --> 01:17:46,240
about taking care of getting exactly the right number of cells. If I'm a little bit over,

812
01:17:46,240 --> 01:17:50,400
that's fine. If I'm way under, also fine. The system has that competency of recruiting

813
01:17:51,040 --> 01:17:56,960
other cells to get the job done. That's what I meant. To make any kind of a cognitive claim,

814
01:17:56,960 --> 01:18:02,640
you have to specify the problem space. You have to specify the goal towards which it's expressing

815
01:18:02,640 --> 01:18:09,200
competencies. Then you can make a claim about, well, how competent is it to get to that goal?

816
01:18:09,920 --> 01:18:13,760
I wish I could remember who it was, but somebody made this really nice analogy about the ends of

817
01:18:13,760 --> 01:18:19,840
that spectrum. They said two magnets try to get together and Romeo and Juliet try to get together,

818
01:18:19,840 --> 01:18:23,760
but the degree of flexible problem solving that you can expect out of those two systems is

819
01:18:23,760 --> 01:18:29,280
incredibly different. Within that range, there are all kinds of in-between systems that may be

820
01:18:29,280 --> 01:18:34,320
better or worse and may deploy different kinds of strategies. Can they avoid local optima? Can they

821
01:18:34,320 --> 01:18:38,480
have a memory of where they've been? Can they look further than their local environment? A

822
01:18:38,480 --> 01:18:43,360
million different things. That's what I meant by competency. It's a claim about

823
01:18:44,240 --> 01:18:49,680
what an engineer can expect the system to do given a particular problem space and a particular

824
01:18:49,680 --> 01:18:55,280
goal that you think it's trying to reach. The way in which you use the word competency

825
01:18:56,080 --> 01:19:00,960
could be treated as the capacity of a system for adaptive control.

826
01:19:05,440 --> 01:19:11,280
One issue that I have with the notion of goals and goal directedness is that sometimes you only

827
01:19:11,280 --> 01:19:18,000
have a tendency in a system to go in a certain direction. It's directed, but the goal is something

828
01:19:18,000 --> 01:19:22,560
that can be emergent. Sometimes it's not. Sometimes there is an explicit representation

829
01:19:22,560 --> 01:19:26,960
in the system of a discrete event that is associated or a class of events with fulfilling

830
01:19:26,960 --> 01:19:31,120
a certain condition that the system has committed itself to. If you don't have that, you don't have

831
01:19:31,120 --> 01:19:38,640
a proper goal. In real systems, it's difficult to say. When do we pursue goals? Sometimes we just

832
01:19:38,640 --> 01:19:44,160
vaguely hungry or moving towards the kitchen because we hope that something will opportunistically

833
01:19:44,160 --> 01:19:49,680
emerge that will deal with this vague tendency in our behavior. We could also say we have the

834
01:19:49,680 --> 01:19:55,040
goal of finding food, but that is a rationalization that is maybe stretching things sometimes.

835
01:19:56,720 --> 01:20:01,920
Sometimes a better distinction for me is going from a simple controller to an agent.

836
01:20:04,080 --> 01:20:08,400
We are very good at discovering agency in the world. What does it actually mean when we discover

837
01:20:08,400 --> 01:20:15,440
agency and when we discover our own agency and start to amplify it by making models of who we

838
01:20:15,440 --> 01:20:20,480
are and how we deal with the world and with others and so on? The minimal definition of agent that I

839
01:20:20,480 --> 01:20:26,160
found is a controller for future states. The thermostat doesn't have a goal by itself. It

840
01:20:26,160 --> 01:20:32,960
just has a target value and a sensor that tells its deviation from the target value and when that

841
01:20:32,960 --> 01:20:38,960
exceeds a certain threshold, the heating is turned on. If it goes below a certain threshold,

842
01:20:38,960 --> 01:20:44,320
the heating is turned off again and this is it. The thermostat is not an agent. It only reacts

843
01:20:44,320 --> 01:20:50,720
to the present frame. It's only a reactive system. Whereas an agent is proactive, which means that

844
01:20:50,720 --> 01:20:57,360
it's trying to not just minimize the current deviation from the target value, but the integral

845
01:20:58,000 --> 01:21:05,680
over the time span, the future deviation. It builds an expectation about how an action is

846
01:21:05,680 --> 01:21:12,160
going to change this trajectory of the universe. Over that trajectory, it tries to figure out some

847
01:21:12,160 --> 01:21:18,880
measure of how big the compound target deviation is going to be. As a result, you get a branching

848
01:21:18,880 --> 01:21:23,840
universe. The branches in this universe, some of these branches depend on actions that are

849
01:21:23,840 --> 01:21:29,920
available to you and that translate into decisions that you can make that move you

850
01:21:29,920 --> 01:21:33,600
into more or less preferable wealth states. Suddenly, you have a system with emergent

851
01:21:33,600 --> 01:21:40,240
beliefs, desires, and intentions. To make that happen, to move from a controller to agency,

852
01:21:41,040 --> 01:21:47,600
agent just being a controller with an integrated set point generator and the ability to control

853
01:21:47,600 --> 01:21:54,880
future states, that requires that you can make models that are counterfactual because the future

854
01:21:54,880 --> 01:22:00,640
universe doesn't exist right now. You need to create a counterfactual model of the future

855
01:22:00,640 --> 01:22:05,280
universe, maybe even a model of the past universe that allows you to reason about possible future

856
01:22:05,280 --> 01:22:11,360
universes and so on. To make these counterfactual causal models of the universe, you need to have a

857
01:22:11,360 --> 01:22:17,680
Turing machine. Without a computer, without something that is Turing complete, that insulates

858
01:22:17,680 --> 01:22:23,600
you from the causal structure of your substrate, that allows you to build representations regardless

859
01:22:23,600 --> 01:22:28,560
of what the universe says right now around you, you need to have that machine. The simplest

860
01:22:30,000 --> 01:22:36,000
system in nature that has a Turing machine integrated is the cell. It's very difficult to

861
01:22:36,000 --> 01:22:42,800
find a system in nature that is an agent, that is not made from cells as a result. Maybe there

862
01:22:42,800 --> 01:22:49,120
are systems in nature that are able to compute things and make models, but I'm not aware of any.

863
01:22:49,840 --> 01:22:56,320
The simplest one that I know that can do this reliably is the cells or arrangement of cells

864
01:22:57,280 --> 01:23:04,000
that can possess agency, which is an interesting thing that explains this coincidence that living

865
01:23:04,000 --> 01:23:09,200
things are agents and vice versa, that the agents that we discover are mostly living things,

866
01:23:09,200 --> 01:23:16,560
or there are robots that have computers built into them, or virtual robots that rely on

867
01:23:16,560 --> 01:23:21,600
computation. The ability to make models of the future is the prerequisite for agency.

868
01:23:22,400 --> 01:23:30,560
To make arbitrary models, which means structures that embody causal simulations of some sort,

869
01:23:30,560 --> 01:23:37,280
that requires computation. Yeah, yeah. I'm on board with that

870
01:23:37,280 --> 01:23:46,160
ladder, that taxonomy of goals and so on. One interesting thing about goals, and as you say,

871
01:23:46,160 --> 01:23:52,720
some are emergent and some are not, there's an interesting planarian version of this,

872
01:23:52,720 --> 01:23:59,680
which is this. We made this hypothesis about, so within planaria, you chop it up into pieces

873
01:23:59,680 --> 01:24:05,440
and every piece regenerates exactly the right rest of the work. If you chop it into pieces,

874
01:24:05,440 --> 01:24:12,480
each piece will have one head, one tail. Then, of course, what happens is it stops when it reaches

875
01:24:12,480 --> 01:24:19,440
a correct planarian, then it stops. We started to think that there are a couple of possibilities.

876
01:24:19,440 --> 01:24:25,200
One possibility is that this is a purely emergent process and that the goal of rebuilding a head is

877
01:24:25,200 --> 01:24:29,680
an emergent thing that comes about as a consequence of other things. Or could there be

878
01:24:30,560 --> 01:24:35,200
an actual explicit representation of what a correct planarian is that serves as a

879
01:24:35,200 --> 01:24:39,120
set point, as an explicitly encoded set point for these cells to follow?

880
01:24:39,920 --> 01:24:44,400
Because it's a cellular collective, we were communicating electrically. We thought, well,

881
01:24:44,400 --> 01:24:51,600
maybe what it's doing is basically storing a memory of what, like you would in a neural

882
01:24:51,600 --> 01:24:56,000
circuit, storing a memory of what it should be. We started looking for this and this is what we

883
01:24:56,000 --> 01:25:05,600
found. This is, I think, one important type of goal in a goal-seeking system is a goal that

884
01:25:05,600 --> 01:25:10,800
you can rewrite without changing the hardware and the system will now pursue that goal instead of

885
01:25:10,800 --> 01:25:15,920
something else. In a purely emergent system, that doesn't work. If you have a cellular automaton or

886
01:25:15,920 --> 01:25:19,840
a fractal or something that does some kind of complex thing, if you want to change what that

887
01:25:19,840 --> 01:25:24,400
complex thing is, you have to figure out how to change the local rules. That's very hard in most

888
01:25:24,400 --> 01:25:30,640
cases. But what we found in planaria is that we can literally, using a voltage reporter die,

889
01:25:30,640 --> 01:25:36,400
we can look at the worm and we can see now the pattern, and it's a distributed pattern,

890
01:25:36,400 --> 01:25:40,400
but we can see the pattern that tells this animal how many heads it's supposed to have.

891
01:25:40,400 --> 01:25:48,240
And what you can do is you can go in and using a brief transient manipulation of the ion channels

892
01:25:48,240 --> 01:25:52,720
with drugs, with ion channel drugs, and we have a computational model that tells you what those

893
01:25:52,720 --> 01:25:58,800
drugs should be, that briefly changes the electrical state of the circuit, but the circuit is amazing.

894
01:25:59,520 --> 01:26:04,960
Once you've changed that state, it holds. So by default, in a standard planaria, it always says

895
01:26:04,960 --> 01:26:10,320
one head, but it's kind of like a flip-flop in that when you temporarily shift it, it holds and

896
01:26:10,320 --> 01:26:16,000
you can push it to a state that says two heads. So now something very interesting happens. Two

897
01:26:16,000 --> 01:26:20,000
interesting things. One is that if you take those worms and you cut those into pieces,

898
01:26:20,720 --> 01:26:25,440
you get two headed worms, even though the hardware is all wild type. There's nothing wrong with the

899
01:26:25,440 --> 01:26:29,280
hardware. All the proteins are the same. All the genetics is the same, but the electric circuit

900
01:26:29,280 --> 01:26:35,520
now says make two heads instead of one. And so this is in an interesting way. It is an explicit

901
01:26:35,520 --> 01:26:39,760
goal because you can rewrite it because much like with your thermostat, there's an interface for

902
01:26:39,760 --> 01:26:42,480
changing what the goal state is, and then you don't even need to know how the rest of the

903
01:26:42,560 --> 01:26:47,200
thermostat works. As long as you know how to modify that interface, the system takes care of

904
01:26:47,200 --> 01:26:51,680
the rest. The other interesting thing is, and I love what you said about the counterfactuals,

905
01:26:52,480 --> 01:26:59,680
what you can do is you can change that electrical pattern in an intact worm and not cut it for a

906
01:26:59,680 --> 01:27:04,400
long time. And if you do that, when you look at that pattern, that is a counterfactual pattern

907
01:27:04,400 --> 01:27:10,320
because that two headed pattern is not a readout of the current state. It says two heads, but the

908
01:27:10,320 --> 01:27:16,080
animal only has one head. It's a normal planarian. So that pattern memory is not a readout of what

909
01:27:16,080 --> 01:27:21,920
the animal is doing right now. It is a representation of what the animal will do in the future if it

910
01:27:21,920 --> 01:27:28,320
happens to get injured. And you may never cut it or you may cut it, but if you do, then the cells

911
01:27:28,320 --> 01:27:33,280
consult the pattern and build a two headed worm, and then it becomes the current state. But until

912
01:27:33,280 --> 01:27:38,960
then, it's this weird like primitive, it's a primitive counterfactual system because it's

913
01:27:38,960 --> 01:27:46,480
able to, a body of a planarian is able to store at least two different representations of what a,

914
01:27:46,480 --> 01:27:50,560
probably many more, but we've found two so far, what a correct planarian should look like. It

915
01:27:50,560 --> 01:27:56,160
can have a memory of a one headed planarian or a memory of a two headed planarian. And both of

916
01:27:56,160 --> 01:28:01,760
those can live in exactly the same hardware and exactly the same body. The other kind of cool

917
01:28:01,760 --> 01:28:08,080
thing about this, and I'll just mention this even though this is disclaimer, this is not published

918
01:28:08,080 --> 01:28:15,120
yet. So take all this with a grain of salt, but the latest thing you can do is you can actually

919
01:28:15,120 --> 01:28:21,120
treat it with some of the same compounds that are used in neuroscience in humans and in rats as

920
01:28:21,120 --> 01:28:27,200
memory blockers. So things that block recall or memory consolidation. And when you do that,

921
01:28:27,200 --> 01:28:30,880
you can make the animal forget how many heads it's supposed to have. And then they basically

922
01:28:30,880 --> 01:28:36,000
turn into a featureless circle when you can just wipe the pattern memory completely.

923
01:28:36,000 --> 01:28:41,280
Were they using exactly the same techniques you would use in a rat or a human? They just forget

924
01:28:41,280 --> 01:28:45,200
what to do when they turn into, they fail to break symmetry and they just become a circle.

925
01:28:45,760 --> 01:28:53,520
So yeah, I think what you were saying is right on with this ability to store counterfactual

926
01:28:53,520 --> 01:28:59,280
states that are not true now, but may represent aspects of the future. I think that's a very

927
01:28:59,280 --> 01:29:05,840
important capacity. Another important notion is a constraint and constraint satisfaction. A

928
01:29:05,840 --> 01:29:11,360
constraint is a rule that tells you whether two things are compatible or not. And the constraint

929
01:29:11,360 --> 01:29:15,440
is satisfied if they're compatible. So you basically have a number of conditions that you

930
01:29:15,440 --> 01:29:21,920
establish by measuring that somehow, for instance, whether you have a head or multiple heads,

931
01:29:21,920 --> 01:29:27,680
and you try to find a solution where you can end up with exactly one head. And if you end up with

932
01:29:27,680 --> 01:29:32,800
exactly one head based on the starting state, then you have managed to find a way to satisfy

933
01:29:32,800 --> 01:29:41,040
your constraints. And so in a sense, what you call a competency is the ability of a system to take

934
01:29:41,040 --> 01:29:48,080
a region of the states of the space of the universe, basically some local region of possible

935
01:29:48,080 --> 01:29:55,920
state that the universe can be in, and move that region to a smaller region that is acceptable.

936
01:29:55,920 --> 01:30:00,960
So there is a region on the universe state space where you have only one head. And there's a larger

937
01:30:00,960 --> 01:30:05,600
region where you don't have any head at all, but the starting state of your organism. And then you

938
01:30:05,600 --> 01:30:11,280
try to get from A to B. So you get from this larger region to the one in which you want to be. Of

939
01:30:11,280 --> 01:30:15,440
course, if you have one head, you want to stay in the region in which you have one head, which,

940
01:30:15,440 --> 01:30:21,840
of course, is usually much easier. But the ability basically to condense the space, to bridge over

941
01:30:21,840 --> 01:30:28,240
many regions into the target region is what comes down to what this competency is. The system

942
01:30:28,240 --> 01:30:33,120
basically has an emergent wanting to go in this region, and it's trying to move there. And so

943
01:30:33,120 --> 01:30:38,880
there are constraints at the level of the substrate that are battling with the functional constraints

944
01:30:38,880 --> 01:30:44,880
that the organism wants to realize to fulfill its function. And sometimes you cannot satisfy this,

945
01:30:44,880 --> 01:30:49,760
and you end up with two heads because you don't know which one you get rid of, or how to digest

946
01:30:49,760 --> 01:30:57,360
one of the heads and so on. And you end up with some Siamese twin. And so this is an interesting

947
01:30:57,360 --> 01:31:02,320
constraint that you have to solve for when you are dealing with reality and how you battle with

948
01:31:02,320 --> 01:31:09,840
the substrate until you get to the functional solution that you evolved for. Yeah, that's

949
01:31:09,840 --> 01:31:16,240
interesting. I mean, we've also found that there are... So we look at exactly this

950
01:31:17,200 --> 01:31:20,960
the navigation, this kind of navigation and morphous space, how you get from here to there

951
01:31:20,960 --> 01:31:25,440
and what paths are possible to get from here to there and so on. One of the things that we found

952
01:31:25,440 --> 01:31:32,720
is that there are regions of that space that belong to other species. And you can push a

953
01:31:32,720 --> 01:31:38,560
planarian with a standard wild type genome into the goal state of a completely different species.

954
01:31:38,560 --> 01:31:42,400
So we can get them to grow a head. So there's a species that normally has a triangular head.

955
01:31:42,400 --> 01:31:47,520
You can make it grow a round head like a different species or a flat head or whatever.

956
01:31:49,280 --> 01:31:55,040
So those are about 100 to 150 million years of evolutionary distance. And you can do it

957
01:31:56,320 --> 01:32:02,080
within a few days just by perturbing that electrical circuit so that it lands in the

958
01:32:02,080 --> 01:32:08,400
wrong space. And then outside of that, there are regions that don't belong to planaria at all.

959
01:32:08,400 --> 01:32:14,240
So planaria are normally nice and flat. We've made planarians that look like they are a cylinder,

960
01:32:14,240 --> 01:32:20,160
like a ski cap. They become like a hemisphere or really weird ones that are spiky. They're

961
01:32:20,160 --> 01:32:25,680
like a ball with spikes on it. There are all kinds of other regions in that space that you can push

962
01:32:25,680 --> 01:32:31,520
them to. And so- Those are new. Those are not species that they diverge from. Those are new.

963
01:32:31,520 --> 01:32:38,160
No one's ever... To my knowledge, yes, there are no such species. It's easier to... And we've

964
01:32:38,160 --> 01:32:43,680
done this in frog too. You can push tadpoles to make it to look like those of other species

965
01:32:44,240 --> 01:32:50,080
or you can make... That's a whole interesting thing for evolution anyway. One species birth

966
01:32:50,080 --> 01:32:56,640
defect is a perfectly reasonable different species. So we can make tadpoles with a rounded tail,

967
01:32:56,640 --> 01:33:02,160
which for a Xenopus tadpole is a terrible tail, but for a zebrafish, that's exactly the right tail.

968
01:33:02,160 --> 01:33:11,040
So you can imagine evolution manipulating the different information processing by electrical

969
01:33:11,040 --> 01:33:20,720
circuits or other machinery that help the system explore that more of a space and start to move

970
01:33:20,720 --> 01:33:26,400
away from whatever that speciation is moving away from your standard attractor that you usually

971
01:33:26,480 --> 01:33:32,800
land on. How does this relate to intelligence? Well, intelligence is the ability to make models

972
01:33:32,800 --> 01:33:37,840
and usually in the service of control, at least that's the way I would explain intelligence.

973
01:33:39,360 --> 01:33:42,720
There are other definitions, but it's the simplest one that I've found.

974
01:33:42,720 --> 01:33:46,480
It also accounts for the fact that many intelligent people are not very good at getting

975
01:33:46,480 --> 01:33:52,320
things done. Basically, intelligence and goal rationality are somewhat orthogonal.

976
01:33:53,280 --> 01:33:58,320
And excessive intelligence is often a prosthesis for bad regulation.

977
01:33:58,320 --> 01:34:01,520
Have you read the intelligence trap? No.

978
01:34:02,400 --> 01:34:06,880
Okay. The author makes a similar case and he's coming on shortly, essentially saying that there

979
01:34:06,880 --> 01:34:12,160
are certain traps that people with high IQs have that are not beneficial for them as biological

980
01:34:12,160 --> 01:34:16,800
beings. They're mainly cognitive biases. So for instance, it's extremely interesting. So let's

981
01:34:16,800 --> 01:34:21,760
just give one of the biases to say you're either liberal biased or you're conservative biased,

982
01:34:21,760 --> 01:34:25,680
and then you were to give a test where there's some data that says that on the surface, it shows

983
01:34:25,680 --> 01:34:31,120
that the data shows that gun control prevents gun violence. Well, the liberals are more likely to

984
01:34:31,120 --> 01:34:35,520
say, yes, this data does show that. But if you're conservative, you're more likely to find, oh,

985
01:34:35,520 --> 01:34:40,800
actually the subtleties in the data show that gun control increases gun violence. And then they

986
01:34:40,800 --> 01:34:45,040
thought, okay, well, let's just switch this to make it such that the superficial data suggests that

987
01:34:45,040 --> 01:34:49,520
gun control increases violence. You need to look at the data carefully to show that it actually

988
01:34:49,520 --> 01:34:53,200
prevents violence. Well, the conservatives in that case would be more quickly to say, oh, look,

989
01:34:53,200 --> 01:34:58,400
the gun control increases violence, and the liberals would find the the loophole. Well,

990
01:34:58,400 --> 01:35:03,120
that's one of the reasons why I don't mind interviewing people who are biased. Because to

991
01:35:03,120 --> 01:35:09,520
me, they're more able to find a justification for something that may be true. But I or and others

992
01:35:09,520 --> 01:35:14,000
are so well, we all have our own biases. We're so inclined in some other direction that we just were

993
01:35:14,000 --> 01:35:18,560
blind to it. But anyway, the point is to affirm what you're saying, Yoshi. Okay, so I know Michael

994
01:35:18,560 --> 01:35:24,640
has a hard cut off at 2pm. So I want to ask the question for a GI that is artificial general

995
01:35:24,640 --> 01:35:30,480
intelligence. It seems as though we're far away or that our current methods of machine learning

996
01:35:30,480 --> 01:35:35,920
and what we learn in neuroscience or or what we learn in computer science is something that we're

997
01:35:35,920 --> 01:35:40,080
missing some paradigm shift or missing some new techniques. Is there something from Michael's

998
01:35:40,080 --> 01:35:44,480
work? Yoshi, I'm asking you this and then Michael, please respond. Is there something from Michael's

999
01:35:44,480 --> 01:35:50,000
work that you think can be applied to the development of a GI if such a creature mind

1000
01:35:50,000 --> 01:35:54,000
can exist? Because there are some arguments against it. First of all, I don't know how far

1001
01:35:54,000 --> 01:35:58,080
we are for a GI. It could be that the existing paradigms are sufficient to brute force it.

1002
01:35:58,960 --> 01:36:02,640
But we don't know that yet. It's a we're going to find out in the next few months.

1003
01:36:03,360 --> 01:36:08,320
But it could also be that we need to revive the stack to build systems that work in real time

1004
01:36:08,320 --> 01:36:13,520
that are entangled with the environment that can build shared representations with the environment.

1005
01:36:14,000 --> 01:36:19,120
And that we need to rewrite the stack. And there are actually a number of questions that I'd like

1006
01:36:19,120 --> 01:36:27,040
to ask Michael. What I noticed that Michael is wisely reluctant to use certain words like

1007
01:36:27,040 --> 01:36:31,040
consciousness a lot. And it's because a lot of people are very opinionated about what these

1008
01:36:31,040 --> 01:36:35,600
concepts mean. And you first have to deal with these opinions before you come down to saying,

1009
01:36:35,600 --> 01:36:39,520
oh, here I have the following proposal for implementing reflexive attention

1010
01:36:39,520 --> 01:36:46,000
as a tool to form coherence in a representation. And this leads to the same phenomena as what you

1011
01:36:46,000 --> 01:36:51,520
call consciousness. So that is a detailed discussion. Maybe you don't want to have

1012
01:36:51,520 --> 01:36:57,440
that discussion in every forum. And then having this discussion, you may be looking at how to

1013
01:36:57,440 --> 01:37:02,480
create coherence using a reflexive attention process that makes a real time model of what

1014
01:37:02,480 --> 01:37:07,200
it's attending to and the fact that it's attending to it so it remains coherent but for itself.

1015
01:37:07,760 --> 01:37:14,080
So this is a concrete thing. But I wonder how to implement this in a self-organized fashion

1016
01:37:14,080 --> 01:37:19,280
if the substrate that you have are individual agents. And there is a similarity here between

1017
01:37:19,280 --> 01:37:28,640
societies and brains and social networks. That is, if you have self-interested agents, in a way,

1018
01:37:28,640 --> 01:37:34,240
that try to survive and that get their rewards from other agents that are similar to them

1019
01:37:34,240 --> 01:37:42,960
structurally. And they have the capacity to learn to some degree. And that capacity is

1020
01:37:43,520 --> 01:37:49,600
sufficient so they can, in the aggregate, learn arbitrary programs, arbitrary computable functions.

1021
01:37:51,840 --> 01:37:55,840
And it's sufficient enough so they can converge on the functions that they need to

1022
01:37:56,880 --> 01:38:01,200
as a group reap rewards that apply to the whole group because they have a shared

1023
01:38:01,200 --> 01:38:05,360
destiny like the poor little cells that are locked in the same skull and they're all going

1024
01:38:05,360 --> 01:38:11,520
to die together if they fuck up. So they have to get along, they have to form an organization

1025
01:38:11,520 --> 01:38:16,400
that is distributing rewards among each other. And this gives us a search space for possible

1026
01:38:16,400 --> 01:38:23,200
systems that can exist. And the search space is mostly given, I think, by the minimal agent

1027
01:38:23,760 --> 01:38:30,160
that is able to learn how to distribute rewards efficiently while doing something useful. Using

1028
01:38:30,160 --> 01:38:35,520
these rewards to change how you do something useful. So you have an emergent form of governance

1029
01:38:35,520 --> 01:38:40,080
in these systems. There's not some centralized control that is imposed on the system from the

1030
01:38:40,080 --> 01:38:45,920
outside as an existing machine learning approaches and AI approaches. But this only is an emergent

1031
01:38:45,920 --> 01:38:50,960
pattern in the interactions between the individual small units, small reinforcement learning agents.

1032
01:38:51,600 --> 01:38:56,960
And this control architecture leads to hierarchical government. It's not fully decentralized in any

1033
01:38:56,960 --> 01:39:01,920
way. There are centralized structures that distribute rewards for instance via the dopaminergic

1034
01:39:01,920 --> 01:39:06,880
system in a very centralized top-down manner. And that's because every regulation has an optimal

1035
01:39:06,880 --> 01:39:11,440
layer where it needs to take place. Some stuff needs to be decided very high up, some stuff needs

1036
01:39:11,440 --> 01:39:16,640
to be optimally regulated very low down depending on the incentives. Game theoretically, a government

1037
01:39:16,640 --> 01:39:24,000
is an agent that imposes an offset on your payoff metrics to make your Nash equilibrium compatible

1038
01:39:24,000 --> 01:39:31,200
with the globally best outcome. To do this you need to have agents that are sensitive to rewards.

1039
01:39:31,200 --> 01:39:36,880
It's super interesting to think about these reward infrastructures. Elon Musk has bought Twitter I

1040
01:39:36,880 --> 01:39:41,680
think because he has realized that Twitter is the network among all the social networks that is

1041
01:39:41,680 --> 01:39:47,520
closest to a global brain. It's totally mind-blowing to realize that he basically trades a bunch of

1042
01:39:47,520 --> 01:39:54,960
wealthy stock for the opportunity to become pope. Pope of a religion that has more active participants

1043
01:39:54,960 --> 01:40:00,640
than Catholicism even, right? Daily practicing people who enter this church and think together.

1044
01:40:00,640 --> 01:40:04,320
And it's a thing that is completely incoherent at this point, almost completely incoherent.

1045
01:40:04,320 --> 01:40:08,320
There are bubbles of sentience but for the most part this thing is just screeching at itself.

1046
01:40:09,040 --> 01:40:13,120
And now there is the question, can we fix the incentives of Twitter to turn it into a global

1047
01:40:13,120 --> 01:40:19,360
brain? And Elon Musk is global brain-pilled. He believes that this is the case and that's the

1048
01:40:19,360 --> 01:40:23,520
experiment that he's trying to do which makes me super excited, right? This might fail, there's a

1049
01:40:23,520 --> 01:40:28,560
very big chance that it fails but there is also the chance that we get the global brain, that we get

1050
01:40:28,560 --> 01:40:33,600
emerging collective intelligence that is working in real time using the internet in a way that

1051
01:40:33,600 --> 01:40:39,280
didn't exist before. So super fascinating thing that might happen here. And it's fascinating that

1052
01:40:39,280 --> 01:40:46,160
very few people are seeing that Elon Musk is crazy enough to spend 44 billion dollars on that

1053
01:40:46,160 --> 01:40:50,560
experiment just because he can and has nothing else to do and thinks it's meaningful to do it,

1054
01:40:50,560 --> 01:40:57,520
more meaningful than having so much money in the bank, right? So this makes me interested in

1055
01:40:57,520 --> 01:41:02,480
this test bed for rules and this is something that translates into the way in which society

1056
01:41:02,480 --> 01:41:06,480
is organized because social media is not different from society, not separate from it.

1057
01:41:06,480 --> 01:41:11,280
Problem of governing social media is exactly the same thing as governing a society. You need a

1058
01:41:11,280 --> 01:41:16,480
right form of government, you need a legal system, ultimately you need representation and all these

1059
01:41:16,480 --> 01:41:21,840
issues, right? It's not just the moderation team and the same thing is also true for the brain.

1060
01:41:21,840 --> 01:41:27,760
What is the government of the brain that emerges in what Gary Edelman calls neural Darwinism among

1061
01:41:27,760 --> 01:41:32,720
different forms of organization in the mind until you have a model of a self-organizing agent that

1062
01:41:32,720 --> 01:41:37,200
discovers that what it's computing is driving the behavior of an agent in the real world and

1063
01:41:37,200 --> 01:41:42,160
it covers a first-person perspective and so on. How does that work? How can we get a system that

1064
01:41:42,160 --> 01:41:48,320
is looking for the right incentive architecture? And that is basically the main topic where I

1065
01:41:48,320 --> 01:41:54,480
think that Michael's research is pointing from my perspective that is super interesting. We have

1066
01:41:54,480 --> 01:42:02,480
this overlap between looking at cells and looking at the world of humans and animals and stuff

1067
01:42:02,480 --> 01:42:14,400
in general. Yeah, super interesting. Chris Fields and I are working on a framework to understand

1068
01:42:17,280 --> 01:42:23,840
where collective agents first come from, right? How do they organize themselves?

1069
01:42:23,840 --> 01:42:32,320
And we've got a model already about this idea of rewards and rewarding other cells with

1070
01:42:32,720 --> 01:42:37,280
neurotransmitters and things like this to keep copies of themselves nearby because they're the

1071
01:42:37,280 --> 01:42:41,840
most predictable. So this idea of reducing surprise, well, what's the least surprising thing?

1072
01:42:41,840 --> 01:42:47,200
It's a copy of yourself. And so you can sort of, Chris calls it the imperial model of multicellularity.

1073
01:42:47,200 --> 01:42:54,480
But one thing to really think about here is imagine an embryo. This is an amniote embryo,

1074
01:42:54,480 --> 01:42:59,520
let's say a human or a bird or something like that. And what you have there is you have a flat disc

1075
01:42:59,520 --> 01:43:07,200
of 10,000, 50,000 cells. And when people look at it, you say, what is that? They say it's an embryo,

1076
01:43:07,200 --> 01:43:12,080
one embryo. Well, the reason it's one embryo is that under normal conditions, what's going to

1077
01:43:12,080 --> 01:43:18,880
happen is that in this disc, one cell is symmetry breaking. One cell is going to decide that it's

1078
01:43:18,880 --> 01:43:23,280
the organizer. It's going to do local activation, long range inhibition. It's going to tell all the

1079
01:43:23,280 --> 01:43:28,000
other cells, you're not the organizer, I'm the organizer. And as a result, you get one special

1080
01:43:28,000 --> 01:43:36,400
point that begins a process that's going to walk through this memorphous space and create a particular

1081
01:43:36,400 --> 01:43:39,680
large scale structure with two eyes and four legs and whatever else it's going to have.

1082
01:43:40,240 --> 01:43:46,560
But here's the interesting thing. Those cells, that's not really one embryo. That's a weird kind

1083
01:43:46,560 --> 01:43:51,520
of Freudian ocean of potentiality. What I mean by that is if you take, and I did this as a grad

1084
01:43:51,520 --> 01:43:56,160
student, you can take a needle and you can put a little scratch through that blastoderm, put a

1085
01:43:56,160 --> 01:44:01,120
little scratch through it. What will happen is the cells on either side of that scratch don't

1086
01:44:01,120 --> 01:44:05,120
feel each other. They don't hear each other's signals. So that symmetry breaking process will

1087
01:44:05,120 --> 01:44:10,240
happen twice, once on each end. And then when it heals together, what you end up with is two

1088
01:44:10,240 --> 01:44:15,760
conjoined twins because each side organized an embryo and now you've got two conjoined twins.

1089
01:44:16,320 --> 01:44:23,120
Now, many interesting things happen there. One is that every cell is some other cell's

1090
01:44:23,360 --> 01:44:28,560
external environment. So in order to make an embryo, you have to self-organize a system that

1091
01:44:29,360 --> 01:44:33,760
puts an arbitrary boundary between itself and the outside world. You have to decide where do I end

1092
01:44:33,760 --> 01:44:40,160
and the world begins. And it's not given to you somehow from outside for a biological system.

1093
01:44:40,160 --> 01:44:44,480
Every biological system has to figure this out for itself, unlike modern robotics or whatever,

1094
01:44:44,480 --> 01:44:47,280
where it's very clear. Here's where you are. Here's where the world is. These are your

1095
01:44:47,280 --> 01:44:51,120
effectors. These are your sensors. Here's the boundary of the outside world. Living things

1096
01:44:51,120 --> 01:44:54,240
don't have any of that. They have to figure all of this out from scratch.

1097
01:44:54,240 --> 01:44:59,520
The benefit to being able to figure it out from scratch, having to figure it out from scratch,

1098
01:44:59,520 --> 01:45:04,160
is that you are then compatible with all kinds of weird initial conditions. For example,

1099
01:45:04,160 --> 01:45:10,000
if I separate you in half, you can make twins. You don't have a total failure because now

1100
01:45:10,000 --> 01:45:12,960
you have half the number of cells. You can make twins. You can make triplets,

1101
01:45:13,840 --> 01:45:18,400
probably many more than that. So if you ask the question, you look at that

1102
01:45:18,400 --> 01:45:23,120
blastoderm and you ask how many individuals are there, you actually don't know. It could be zero.

1103
01:45:23,120 --> 01:45:28,960
It could be one. It could be some small number of individuals. That process of autopolices has to

1104
01:45:28,960 --> 01:45:36,560
happen. And here are a number of things that are uniquely biological that I think relate to

1105
01:45:36,560 --> 01:45:43,360
the kind of flexibility plasticity that you need for AGI in whatever space. It doesn't have to be

1106
01:45:43,360 --> 01:45:49,120
the same space that we work in, but your boundaries are not set for you by an outside creator.

1107
01:45:49,120 --> 01:45:52,720
You have to figure out where your boundaries are. Where is the outside world? So you make

1108
01:45:52,720 --> 01:45:57,440
hypotheses about where you end and where the world begins. You don't actually know what your

1109
01:45:57,440 --> 01:46:01,840
structure is. Kind of like Vanguard's robots from 2006 where they didn't know their structure and

1110
01:46:01,840 --> 01:46:06,000
they had to make hypotheses about, well, do I have wheels? Do I have legs? What do I have? And then

1111
01:46:06,720 --> 01:46:12,640
make a model based on basically babbling, right? Like the way that babies babble. So you have to

1112
01:46:13,520 --> 01:46:18,000
make a model of where the boundary is. You have to make a model of what your structure is.

1113
01:46:18,000 --> 01:46:24,000
You are energy limited, which most AI and robotics nowadays are not. When you're energy and time

1114
01:46:24,000 --> 01:46:29,680
limited, it means that you cannot pay attention to everything. You are forced to coarse grain in

1115
01:46:29,680 --> 01:46:34,800
some way and lose a lot of information and compress it down. So you have to choose a lens,

1116
01:46:34,800 --> 01:46:38,720
a coarse graining lens on the world and figure out how you're going to represent things.

1117
01:46:38,720 --> 01:46:45,520
And all of this has to, and there are many more things that we could talk about, but all of these

1118
01:46:45,520 --> 01:46:53,920
things are self-constructions from the very beginning. And then you start to act in various

1119
01:46:53,920 --> 01:46:58,240
spaces, which again are not predefined for you. You have to solve problems that are metabolic,

1120
01:46:58,240 --> 01:47:06,080
physiological, anatomical, maybe behavioral if you have muscles, but nobody's defining the space

1121
01:47:06,080 --> 01:47:10,880
for you. For example, if you're a bacterium and Chris Fields points this out, if you're a bacterium

1122
01:47:10,880 --> 01:47:16,320
and you're in some sort of chemical gradient, you want to increase the amount of sugar in your

1123
01:47:16,320 --> 01:47:21,440
environment, you could act in three-dimensional space by physically swimming up the gradient,

1124
01:47:21,440 --> 01:47:25,840
or you can act in transcriptional space by turning on other genes that are better at

1125
01:47:25,840 --> 01:47:30,320
converting whatever sugar happens to be around and that solves your metabolic problem instead of,

1126
01:47:30,320 --> 01:47:36,000
right? So you have these hybrid problem spaces. So all of this, I think what contributes in

1127
01:47:36,000 --> 01:47:40,240
a strong sense to all the things that we were just talking about is the fact that everything

1128
01:47:40,240 --> 01:47:44,560
is in biology is self-constructed from the beginning. You can't rely on, you don't know

1129
01:47:44,560 --> 01:47:49,520
ahead of time when you're a new creature born into the world. And we have many examples of

1130
01:47:49,520 --> 01:47:53,360
this kind of stuff. You don't know how many cells you have, how big your cells are. You can't count

1131
01:47:53,360 --> 01:48:00,080
on any of the priors. So you have this weird thing that evolution makes these machines that

1132
01:48:00,080 --> 01:48:04,240
don't take the past history too seriously. It doesn't over train on them. It makes

1133
01:48:04,240 --> 01:48:08,400
problem-solving machines that use whatever hardware you have. This is why we can make

1134
01:48:08,400 --> 01:48:15,440
weird chimeras and cyborgs. And you can mix things and mix and match biology in every way

1135
01:48:16,320 --> 01:48:20,960
with other living things or with non-living things because all of this is interoperable,

1136
01:48:20,960 --> 01:48:25,280
because it does not make assumptions about what you have to have. It tries to solve whatever

1137
01:48:25,280 --> 01:48:32,640
problem is given. It plays the hands that it's dealt. And that results in that assumption that

1138
01:48:32,640 --> 01:48:38,400
you cannot trust what you come into the world with. You cannot assume that the hardware is

1139
01:48:38,400 --> 01:48:43,520
what it is. It gives rise to a lot of that intelligence, I think, and a lot of that plasticity.

1140
01:48:43,520 --> 01:48:49,120
So if you translate this into necessary and sufficient conditions, what seems to be necessary

1141
01:48:49,120 --> 01:48:57,440
for the emergence of general intelligence in a bunch of cells or units is that basically each

1142
01:48:57,440 --> 01:49:04,160
of them is a small agent, which means it's able to behave with an expectation of minimizing future

1143
01:49:04,160 --> 01:49:09,360
target value deviations. It learns that their configuration is environment that signal anticipated

1144
01:49:09,360 --> 01:49:15,200
reward. Next thing, these units need to be not just agents, they need to be connected to each other.

1145
01:49:16,080 --> 01:49:20,800
And they need to get their rewards or proxy rewards, something that allows them to anticipate

1146
01:49:20,800 --> 01:49:25,920
whether the organism is going to feed them in the future from other units that also adaptive.

1147
01:49:26,880 --> 01:49:31,680
So you need multiple message types and the ability to recognize and send them with a certain degree

1148
01:49:31,680 --> 01:49:40,160
of reliability. What else do you need? You need enough of them, of course. What's not clear to me

1149
01:49:40,160 --> 01:49:45,440
is how deterministic do the units need to be? How much memory do they need to be? How much state can

1150
01:49:45,440 --> 01:49:53,680
they store? How deep in time does their recollection need to go? And how much forward in time do they

1151
01:49:53,680 --> 01:50:00,720
need to be able to form expectations? So we see how large is this activation front that they can

1152
01:50:00,720 --> 01:50:06,560
with this shape of the distribution that they can learn and have to learn to make this whole thing

1153
01:50:06,560 --> 01:50:13,760
happen. And so basically conditions that are necessary are relatively simple. If you just

1154
01:50:13,760 --> 01:50:19,120
wait for long enough and get such a system to percolate, I imagine that the compound agency

1155
01:50:19,120 --> 01:50:27,040
will at some level emerge on the system, just in a competition of possibilities in the same way as

1156
01:50:27,040 --> 01:50:32,800
emerging agency has emerged on Twitter in a way, with devoked religion in a way that people

1157
01:50:32,800 --> 01:50:38,080
were starting to shift around their behavior to maximize likes and retweets. And there was no

1158
01:50:38,080 --> 01:50:43,840
external reward that was given on Twitter. So as a result, a local structure emerged a local agency

1159
01:50:43,840 --> 01:50:48,560
that was shifting the rewards by itself and emerging causal structure that was in some sense

1160
01:50:48,560 --> 01:50:56,000
in downward causation going to organize groups of people into behavioral things. It's really

1161
01:50:56,000 --> 01:51:02,960
as interesting to look at Twitter as something like a mind at some level, right? It's working slower,

1162
01:51:02,960 --> 01:51:07,280
but it would probably be possible to make a simulation of these dynamics in a more abstract

1163
01:51:07,280 --> 01:51:14,000
way and to use this for arbitrary problem solving. And so what would an experiment look like in which

1164
01:51:14,000 --> 01:51:18,400
we start with these necessary conditions and narrow down the sufficient conditions?

1165
01:51:20,080 --> 01:51:26,560
Yeah, right on. And yeah, we're doing some of that stuff, some of that kind of modeling.

1166
01:51:26,560 --> 01:51:32,320
I apologize. I've got to run here. Thank you both for coming out for this. I appreciate it.

1167
01:51:32,320 --> 01:51:35,680
Thank you so much. And thank you for bringing us together. So a great conversation. I really

1168
01:51:35,680 --> 01:51:41,040
enjoyed it. Likewise. I enjoyed it very much. Thank you, Kurt. Thank you so much, Kurt.

1169
01:51:41,040 --> 01:51:45,920
Thanks, Joshua. The podcast is now concluded. Thank you for watching. If you haven't subscribed

1170
01:51:45,920 --> 01:51:51,440
or clicked on that like button now would be a great time to do so as each subscribe and like

1171
01:51:51,440 --> 01:51:56,400
helps YouTube push this content to more people. Also, I recently found out that external links

1172
01:51:56,400 --> 01:52:01,680
count plenty toward the algorithm, which means that when you share on Twitter, on Facebook,

1173
01:52:01,680 --> 01:52:06,880
on Reddit, et cetera, it shows YouTube that people are talking about this outside of YouTube,

1174
01:52:06,880 --> 01:52:10,080
which in turn greatly aids the distribution on YouTube as well.

1175
01:52:10,080 --> 01:52:14,160
If you'd like to support more conversations like this, then do consider visiting theories

1176
01:52:14,160 --> 01:52:19,600
of everything.org. Again, it's support from the sponsors and you that allow me to work on

1177
01:52:19,600 --> 01:52:25,360
toe full time. You get early access to ad free audio episodes there as well. Every dollar helps

1178
01:52:25,360 --> 01:52:30,000
far more than you may think. Either way, your viewership is generosity enough. Thank you.

