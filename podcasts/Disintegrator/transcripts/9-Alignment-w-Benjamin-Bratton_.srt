1
00:00:00,000 --> 00:00:09,680
So, this is a big one.

2
00:00:09,680 --> 00:00:15,160
We got to speak with Benjamin Bratton last December, right before the New Year, and honestly,

3
00:00:15,160 --> 00:00:19,000
we can't stop thinking about this conversation.

4
00:00:19,000 --> 00:00:22,600
Bratton is Professor of Philosophy of Technology and Speculative Design at the University of

5
00:00:22,600 --> 00:00:23,960
California, San Diego.

6
00:00:23,960 --> 00:00:28,120
He's Director of Antikythera, a think tank on the speculative philosophy of computation

7
00:00:28,240 --> 00:00:32,200
at the Berggruen Institute, and he's also Professor of Digital Design at the European

8
00:00:32,200 --> 00:00:37,360
Graduate School and Visiting Professor at NYU Shanghai.

9
00:00:37,360 --> 00:00:42,760
In 2016, Bratton publishes The Stack, a book where he outlines a new geopolitical theory

10
00:00:42,760 --> 00:00:46,400
for the age of global computation and algorithmic governance.

11
00:00:46,400 --> 00:00:52,040
He proposes that different genres of planetary-scale computation can be seen as forming a coherent

12
00:00:52,520 --> 00:00:59,080
an accidental megastructure that has become a new governing architecture.

13
00:00:59,080 --> 00:01:03,960
We view Bratton's multilayered research as an attempt to address complex planetary challenges,

14
00:01:03,960 --> 00:01:08,840
emphasizing the need to integrate technology into the fabric of society and governance.

15
00:01:08,840 --> 00:01:13,960
As an example, in The Terraforming, published in 2019, Bratton suggests a shift away from

16
00:01:13,960 --> 00:01:18,800
anthropocentric views, taking artificiality, astronomy, and automation as foundations for

17
00:01:18,800 --> 00:01:21,120
a new form of planetarity.

18
00:01:21,200 --> 00:01:24,840
Two years later, in The Revenge of the Real, Bratton explores the failure of political

19
00:01:24,840 --> 00:01:29,920
imagination in the Western response to the COVID-19 pandemic, advocating a form of positive

20
00:01:29,920 --> 00:01:32,480
biopolitics.

21
00:01:32,480 --> 00:01:37,240
It follows that critiques of Bratton's work tend to focus on the diminishing role of the

22
00:01:37,240 --> 00:01:44,120
individual, this tiny moat before an enormous morass of geotechnical and geopolitical complexity.

23
00:01:44,120 --> 00:01:47,720
And we address this head on at the end of the conversation, which both Roberto and I

24
00:01:47,720 --> 00:01:52,440
find to be an extraordinarily compelling message that underscores the immensity of

25
00:01:52,440 --> 00:01:58,280
tasks that lie ahead without resorting to a kind of doomer skepticism.

26
00:01:58,280 --> 00:02:03,120
While the beginning of our conversation focuses on more specific territory, doing some trend

27
00:02:03,120 --> 00:02:08,440
analysis in present-day incongruities and AI and computation, generally speaking, it

28
00:02:08,440 --> 00:02:14,880
quickly picks up pace into a pretty dramatic critique of essentially every dominant paradigm

29
00:02:15,320 --> 00:02:22,160
in AI and tech as they intersect with the human scale, from privacy to bias to alignment

30
00:02:22,160 --> 00:02:29,360
to ownership, Anglo-centrism to the general problem of social determinism, broadly speaking,

31
00:02:29,360 --> 00:02:33,760
and really ultimately to the function of the subject, experience, agency, and the human

32
00:02:33,760 --> 00:02:37,360
in all of this.

33
00:02:37,360 --> 00:02:41,840
So this is a long one, so take a walk with this episode and maybe send it to someone

34
00:02:41,840 --> 00:02:47,080
who you know who, you know, maybe a friendly academic somewhere, someone who tends to overindex

35
00:02:47,080 --> 00:02:50,480
on the power wielded by the social onto computation.

36
00:02:50,480 --> 00:02:59,400
I suppose my interest in AI began a really long time ago when I was an undergraduate

37
00:02:59,400 --> 00:03:07,880
and I took a class in the psychology department on what was then quite new theories of connectionism

38
00:03:07,880 --> 00:03:10,080
as it was called back then.

39
00:03:10,080 --> 00:03:16,320
One of the books that we read was called Neurophilosophy by the Churchlands, who are here at UC San

40
00:03:16,320 --> 00:03:20,760
Diego, as it turns out, and there was an interesting thing that was happening at that time.

41
00:03:20,760 --> 00:03:25,120
This was way back in the before times when the world was in black and white.

42
00:03:25,120 --> 00:03:30,360
That is, there was a correspondence between what was then called cognitive science and

43
00:03:30,360 --> 00:03:33,080
the emerging fields of artificial intelligence.

44
00:03:33,080 --> 00:03:36,680
In many ways, these two areas of how it is that we understand how the brain works and

45
00:03:36,680 --> 00:03:42,080
how it is that we understand how machine intelligence works not only converge paradigmatically,

46
00:03:42,080 --> 00:03:45,840
but I think more importantly, the reason that they did over a period of time was that things

47
00:03:45,840 --> 00:03:52,320
that were learned in one area were applicable to another in ways that were both expected

48
00:03:52,320 --> 00:03:54,160
and unexpected.

49
00:03:54,160 --> 00:03:59,440
So for me, I think to a certain extent, the question of AI has always been one that's

50
00:03:59,440 --> 00:04:04,860
foundational to how I work and think as a theorist and philosopher, that AI is not just

51
00:04:05,100 --> 00:04:09,180
something to which philosophy might be applied, something that philosophy might interpret,

52
00:04:09,180 --> 00:04:15,380
but something that is more foundational to how we think about thinking and how we understand

53
00:04:15,380 --> 00:04:20,100
what cognition, sentience, sapience, and all those kinds of things might be through the

54
00:04:20,100 --> 00:04:24,780
process of their artificialization, that through the process of the artificialization of these

55
00:04:24,780 --> 00:04:30,900
natural processes, that there's something that becomes more analytically legible about

56
00:04:30,900 --> 00:04:31,900
them.

57
00:04:31,980 --> 00:04:37,300
Obviously, this is not unique to me in any sense at all, but the entire history of AI,

58
00:04:37,300 --> 00:04:42,260
and perhaps to some degree, it's unique in this regard among foundational technologies,

59
00:04:42,260 --> 00:04:47,340
that is that the evolution of AI has evolved in very sort of close coupling, let's say,

60
00:04:47,340 --> 00:04:53,740
in a kind of double helix with the philosophy of AI, that AI emerges as a kind of NASA-centered

61
00:04:53,740 --> 00:05:01,140
technology, or maybe we develop thought experiments and conjectures made about what machine intelligence

62
00:05:01,180 --> 00:05:07,260
may be in an empirical sense or in a more speculative sense, from Plato's Republic through

63
00:05:07,260 --> 00:05:12,020
to Descartes to Leibniz, and obviously to Turing in his own little works of speculative

64
00:05:12,020 --> 00:05:18,980
design about universal computers and party game, playing AIs, trying to pass as different

65
00:05:18,980 --> 00:05:23,180
genders, Searle's weird Chinese room, and on and on and on and on and on.

66
00:05:23,180 --> 00:05:27,580
The technology is driven by the speculation about the state of the technology, and the

67
00:05:27,620 --> 00:05:32,140
technology then drives and informs different thought experiments about that.

68
00:05:32,660 --> 00:05:37,460
So this is a little bit of context where I sort of see myself fitting into a much larger

69
00:05:37,460 --> 00:05:42,980
stream. More specifically, a lot of my work around what I call planetary computation or

70
00:05:42,980 --> 00:05:49,500
planetary scale computation, it looks at computation not just as mathematics or algorithms

71
00:05:49,500 --> 00:05:56,740
or as a kind of procedural logic or a kind of appliance or as a kind of human social

72
00:05:56,740 --> 00:06:02,820
medium, but rather as infrastructure and how it is that computation became the basis

73
00:06:02,820 --> 00:06:08,260
of planetary infrastructures, which can be seen sort of from both directions, how computation

74
00:06:08,260 --> 00:06:12,060
scaled to come to constitute a global scale infrastructure.

75
00:06:12,060 --> 00:06:17,100
And then from the other side, how it is that infrastructural systems came to both evolve

76
00:06:17,100 --> 00:06:21,460
and to be artificialized in the direction towards these infrastructures having greater

77
00:06:21,460 --> 00:06:26,620
capacities for cognition, infrastructures that were primarily, let's say, thermodynamic,

78
00:06:26,700 --> 00:06:31,820
their transformation to ones that were more informatic and semiotic and calculative is

79
00:06:31,820 --> 00:06:34,420
another way of also understanding the history of infrastructures.

80
00:06:34,500 --> 00:06:40,100
So long story short, I think where some of this stands right now is that we could look back

81
00:06:40,100 --> 00:06:46,220
at the first 50 years of planetary computation, you know, rough napkin sketch sort of

82
00:06:46,380 --> 00:06:53,300
schematic from roughly 1970 to 2020, where you're beginning to have real planetary

83
00:06:53,300 --> 00:06:58,700
computational networks to more recently as a particular phase in that dynamic, one that I

84
00:06:58,700 --> 00:07:04,180
attempted to give some shape to with a book called The Stack on Software and Sovereignty that

85
00:07:04,180 --> 00:07:09,020
came out, was written about 10 years ago and came out very end of 2015, early 2016.

86
00:07:09,020 --> 00:07:13,580
And that described an architecture of planetary computation that was based on a particular kind

87
00:07:13,580 --> 00:07:18,780
of modular stack system that had particular kind of architecture based on particular kind of

88
00:07:18,780 --> 00:07:23,340
anointment architectures, the computational machine, procedural programming paradigms for

89
00:07:23,340 --> 00:07:27,540
software development. And, you know, perhaps more importantly, it was based on a paradigm that

90
00:07:27,540 --> 00:07:36,180
information was light and inexpensive and highly mobile and hardware was expensive and heavy and

91
00:07:36,260 --> 00:07:41,460
immobile and that you primarily want to move the information to the hardware because that's your

92
00:07:41,580 --> 00:07:43,140
most efficient way of doing it.

93
00:07:43,380 --> 00:07:46,300
In any event, I think quite clearly we're entering a different phase.

94
00:07:46,540 --> 00:07:50,620
There's another 50 year cycle, or maybe it's only five, who knows how fast these things go.

95
00:07:50,620 --> 00:07:55,460
But there's another, let's say, 50 year cycle coming where that architecture of planetary

96
00:07:55,460 --> 00:08:02,180
computation itself is being transformed in relationship to the structural and indeed

97
00:08:02,180 --> 00:08:09,180
anatomical requirements of AI, which involves training foundational models at a very large

98
00:08:09,180 --> 00:08:15,100
scale, hosting them at a large scale, serving them and applications built upon them at a large

99
00:08:15,100 --> 00:08:21,420
scale, a different kind of programming logic from procedural programming to to different kinds of

100
00:08:21,420 --> 00:08:26,100
prompt design, fine tuning, different ways of engaging that in a certain sense collapses the

101
00:08:26,100 --> 00:08:31,940
distinction between using the AI and fine tuning the AI, I think quite clearly over the next few

102
00:08:31,940 --> 00:08:36,340
years that that space is going to collapse perhaps a little bit like the space between user and

103
00:08:36,340 --> 00:08:42,500
designer in video games. And so that may it'll produce different monopolies, it'll destroy other

104
00:08:42,500 --> 00:08:48,020
monopolies. And I think also the sort of the flip in that paradigm between now, I mean, this would

105
00:08:48,020 --> 00:08:55,340
involve one in which information itself is understood as big, heavy, expensive and immobile

106
00:08:55,380 --> 00:09:01,100
foundational models being essentially almost geologic or at least geographic in the kind of scale,

107
00:09:01,340 --> 00:09:07,460
and pushing this to lots of different kinds of inexpensive mobile hardware. And so I think, I

108
00:09:07,460 --> 00:09:11,860
mean, that's also maybe one of the ways in which we can understand this flip is that there's a kind

109
00:09:11,980 --> 00:09:18,340
of swapping places between the relative economies of hardware software in that dynamic. This is one

110
00:09:18,340 --> 00:09:23,620
of the things that a lot of my research is exploring now looking at issues of human AI interaction

111
00:09:23,620 --> 00:09:29,420
design, newer emerging forms of philosophy around AI, and ways in which let's say design and

112
00:09:29,420 --> 00:09:36,460
specular design in a rather peculiar idiosyncratic kind of genre that we've developed allows us to

113
00:09:36,460 --> 00:09:42,060
explore some of these spaces with with more curiosity with an idea that the exploration of

114
00:09:42,060 --> 00:09:47,900
the space of stochastic possibility around this is a way of understanding what's going on and

115
00:09:47,900 --> 00:09:52,260
maybe indeed sort of catching up with the present to analyze this as opposed to some other kinds of

116
00:09:52,260 --> 00:09:56,260
approaches. So in any event, that's a little bit of where the work stands and a little bit of

117
00:09:56,260 --> 00:09:57,420
background and where it came from.

118
00:09:58,540 --> 00:10:03,820
So when you speak about this moment in hardware, at least in terms of like some kind of rebalancing

119
00:10:03,820 --> 00:10:11,020
or some kind of paradigmatic change from these immobile, fixed, expensive data centers to some

120
00:10:11,020 --> 00:10:16,340
cheaper, more fluid, more mobile technologies, are you referring to things like edge ML and

121
00:10:16,380 --> 00:10:21,700
microprocessor integration, like computation moving closer to the ground, so to speak, and like, you

122
00:10:21,700 --> 00:10:25,660
know, being less reliant on large amounts of data flowing upstream? Or is there some other thought

123
00:10:25,660 --> 00:10:29,460
here? And I guess there's like a corollary to that there. Like, does this point to some kind of

124
00:10:29,460 --> 00:10:36,380
rebalancing in favor of decentralization, as opposed to centralized hardware software ecosystems?

125
00:10:36,380 --> 00:10:40,620
Like, is decentralization a paradigm worth talking about in the context of AI?

126
00:10:40,820 --> 00:10:45,340
It is, but it's incomplete. I think one of the things I understand is I think even with the stack

127
00:10:45,340 --> 00:10:50,700
model that I described in the book, the relationship between centralization and decentralization was

128
00:10:50,700 --> 00:10:55,860
not one of opposition where the more you have of one, the less you have of the other in a kind of

129
00:10:55,860 --> 00:11:01,700
zero-sum sort of way, but rather that these two topologies of organization were actually totally

130
00:11:01,720 --> 00:11:06,860
mutually dependent upon one another, that you have cloud platforms that are highly centralized

131
00:11:06,900 --> 00:11:11,180
only because you have lots of devices that are connecting to the cloud that are highly

132
00:11:11,180 --> 00:11:16,900
decentralized and vice versa. And so one might say that something like, you know, looking at all of

133
00:11:16,900 --> 00:11:22,580
the different builds and deployments of Android globally, and then the ways in which, you know,

134
00:11:22,580 --> 00:11:27,900
these are making calls to Google data centers and at least a number of the builds, just as an example,

135
00:11:28,100 --> 00:11:32,980
that there's a highly centralized dynamic, right? When you log into your Gmail, you're pinging the

136
00:11:32,980 --> 00:11:38,500
mothership in a way that one might see as a kind of hub and spoke centralization. At the same time,

137
00:11:38,540 --> 00:11:43,860
all of the devices that are doing this are quite free range in their movement as sort of slightly

138
00:11:43,860 --> 00:11:48,580
intentional agents across the surface of the globe and are connecting with each other in lots of

139
00:11:48,580 --> 00:11:53,140
different strange kinds of configurations. All of which is to say is it's both centralized and

140
00:11:53,140 --> 00:11:58,780
decentralized at the same time. And indeed, it's one because the other. And so this is not to say

141
00:11:58,780 --> 00:12:05,340
that some of the people who've been giving a lot of thought to much more fully decentralized systems

142
00:12:05,340 --> 00:12:10,740
by which you can have what we might think of as edge agents who are working both as, or we have

143
00:12:10,740 --> 00:12:14,340
agents that are both sort of edge agents and server agents, I suppose might be a way of saying

144
00:12:14,340 --> 00:12:18,780
this simultaneously, not to say that there isn't a there there. I just want to sort of understand

145
00:12:18,780 --> 00:12:23,660
the point that the status quo that we're describing is it would be inappropriate to see as entirely

146
00:12:23,660 --> 00:12:28,260
one or the other. There's connections I suppose to where you know, an AI stack, if we want to call

147
00:12:28,260 --> 00:12:32,980
it that is going in this as well. What I really meant to say is that, you know, while the weights

148
00:12:32,980 --> 00:12:40,060
of a large model are relatively, I mean, you could fit it on a USB stick, the scale necessary to

149
00:12:40,060 --> 00:12:45,700
actually integrate the models and to serve them in relationship to applications. Like if you wanted

150
00:12:45,700 --> 00:12:51,100
to think about how you would build Gemini into Gmail, for example, such that it can be used as

151
00:12:51,100 --> 00:12:57,260
the basis for this at the scale of billions and billions of users at one time, not to mention that,

152
00:12:57,260 --> 00:13:02,060
you know, the scale necessary to host training data to continue to update the models and all the rest

153
00:13:02,060 --> 00:13:08,220
is going to think the exabyte upon exabyte scale of information that is being produced and processed

154
00:13:08,220 --> 00:13:13,980
and calculated is one that is simply impossible to move quickly through the pipes to get from one

155
00:13:13,980 --> 00:13:19,380
place to another. Right. It's so big that even with, you know, fiber infrastructure that in

156
00:13:19,380 --> 00:13:23,740
essence, you would think of it as like, that's the heavy thing that's hard to move the data, you

157
00:13:23,740 --> 00:13:29,220
know, and you see this quite often in sort of the processes of work is that it's easier to move the

158
00:13:29,220 --> 00:13:34,500
compute to the data than it is to move the data to the compute. And so that's what I mean. And

159
00:13:34,500 --> 00:13:38,340
that's the flip, right? Instead of it being, we're going to move the data to the compute because

160
00:13:38,340 --> 00:13:43,220
compute is expensive and hardware is expensive and data is cheap and light. We see this paradigmatic

161
00:13:43,300 --> 00:13:47,860
flip and I think it's really more foundational that it's cheaper to move compute to the data than the

162
00:13:47,860 --> 00:13:52,340
other way around. Now, does it have to do with a lot of edge, you know, the relationship to edge ML

163
00:13:52,340 --> 00:13:56,940
and the rest of it? Yes, for sure. And what by edge ML, I think just for the listeners, you know,

164
00:13:56,940 --> 00:14:02,500
I think what we sort of convene there are ways in which devices that are part of a, let's say a

165
00:14:02,500 --> 00:14:10,420
learning network, that is they are making use of models in order to negotiate whatever it is that

166
00:14:10,420 --> 00:14:15,140
they're doing in the world, whatever insect intelligence they may have on whatever they're

167
00:14:15,140 --> 00:14:19,960
doing or however complex that may be. But in the process of using that model, they are also

168
00:14:19,960 --> 00:14:23,420
interacting with the world. And because they're interacting with the world, they are also in

169
00:14:23,420 --> 00:14:29,260
principle creating new data about the world and about their behavior, which in principle, should

170
00:14:29,260 --> 00:14:35,420
be able to reweight the model that they themselves are using in some sort of mechanism. And so one

171
00:14:35,420 --> 00:14:40,300
of our collaborators on a lot of work that we've done around this issue for some time is a fellow

172
00:14:40,300 --> 00:14:45,060
of Google research named Blaise Aquari-Arkos, who was the inventor of what's called federated

173
00:14:45,060 --> 00:14:49,940
learning, which is a process by which, for example, the scenario I just described where you have

174
00:14:49,940 --> 00:14:55,540
little devices, they could be sensors, they could be a person's phone, they could be any machine,

175
00:14:55,620 --> 00:15:00,500
animal, vegetable or mineral, that is not only using models in a functional way, but is producing

176
00:15:00,500 --> 00:15:05,740
data about the world through the use of their models can in fact participate in the retraining

177
00:15:05,780 --> 00:15:11,300
of the weights of the models in ways in which the underlying data that they're producing remains

178
00:15:11,300 --> 00:15:17,340
anonymous and remains opaque. It doesn't need to be disclosed to the whole repness as well, which is

179
00:15:17,380 --> 00:15:22,380
ultimately what you want, right? What you want is a world in which my health data, your health data

180
00:15:22,420 --> 00:15:27,180
remain private, that I don't, you know, I don't look see your health data, you don't see my health

181
00:15:27,180 --> 00:15:33,900
data, but that the relevant information in my health data and your health data are able to, let's

182
00:15:33,900 --> 00:15:39,100
say, reweight medical models that might be mutually beneficial to both of us. Is that

183
00:15:39,100 --> 00:15:44,500
centralization or decentralization? It's both. It's both in a different kind of way. It's a

184
00:15:44,500 --> 00:15:49,020
different kind of way than the other model, but it's clearly both at once. And so I do think it's

185
00:15:49,020 --> 00:15:53,340
fine. It's valuable to think about centralization and decentralization as kind of topological

186
00:15:53,340 --> 00:15:58,700
heuristics to understand this. I think it would be a huge mistake to think of one as intrinsically

187
00:15:58,700 --> 00:16:03,420
better than the other and to think of any system that you're looking at as a kind of, that there's

188
00:16:03,420 --> 00:16:09,060
a big knob that you can turn left or right to make it more centralized or decentralized in order to

189
00:16:09,060 --> 00:16:12,500
either describe it or to compose it, because that's just not how it works.

190
00:16:12,740 --> 00:16:17,180
This relates to a discussion I recall from a few years ago, where you introduced alternative

191
00:16:17,180 --> 00:16:22,140
modes of AI, particularly the concept of model B. And this is central to what we're discussing

192
00:16:22,140 --> 00:16:28,300
here as it relates to the material dimension of the AI stack that is a distributed hardware system

193
00:16:28,300 --> 00:16:33,260
that does all of the different sensing and signal processing. In this context, since you mentioned

194
00:16:33,260 --> 00:16:37,900
Blais Aguera, there's also a contribution that you made to Noema, where you point out something

195
00:16:37,900 --> 00:16:42,020
that we've discussed with some of the other collaborators here, a crisis that is not only

196
00:16:42,020 --> 00:16:47,220
conceptual, but also terminological. We're using words that have been stripped of any defined

197
00:16:47,220 --> 00:16:53,220
meaning and have become problematic. And you have argued that adopting a more precise vocabulary is

198
00:16:53,220 --> 00:16:57,860
crucial to addressing the current challenges in the field. Could you expand on this a bit more?

199
00:16:57,860 --> 00:17:02,100
Yeah, happy to. Let me take the first one real quick. So the model B that you're referring to

200
00:17:02,100 --> 00:17:06,260
was an essay I wrote some years ago, I think Synthetic Garden, I think, or something was the

201
00:17:06,260 --> 00:17:10,980
title. This came out of some discussions with Blais and also with Kendrick McDowell and a number

202
00:17:10,980 --> 00:17:15,940
of other people that is sort of looking at one sort of conceptual model, a kind of folk ontology,

203
00:17:15,940 --> 00:17:19,620
if you like, of what AI is. I mean, there's many of these, obviously, but there's one we might think

204
00:17:19,620 --> 00:17:25,220
of that it's the kind of brain in a box model, which is the AI is analogous to an artificial

205
00:17:25,220 --> 00:17:29,700
version of a single organism brain, you know, in the kind of Turing test face off, and that,

206
00:17:29,700 --> 00:17:35,060
you know, it exists in a virtual space, that it's disembodied, that it, you know, it's a kind of

207
00:17:35,060 --> 00:17:39,700
realm of pure virtual mind, etc. There's another model, which I think is descriptively more

208
00:17:39,700 --> 00:17:44,820
accurate, and also, I think, an open rise to which is that just like planetary intelligence itself,

209
00:17:44,820 --> 00:17:50,900
artificialized intelligence is widely distributed among lots of different kinds of agents and

210
00:17:50,900 --> 00:17:56,100
actors that are sensing, modeling and recursively acting back upon the world in lots of different

211
00:17:56,100 --> 00:18:02,020
ways. It's not just that you've got single humans interacting with single AIs, you've got groups of

212
00:18:02,020 --> 00:18:06,580
humans that are interacting with single AIs, single humans that are interacting with groups

213
00:18:06,580 --> 00:18:13,780
of AIs and AIs interacting with AIs. It's a genuine ecology of both natural and artificialized

214
00:18:13,780 --> 00:18:19,060
intelligence in various different kinds of non-zero sum combinations with each other.

215
00:18:19,060 --> 00:18:24,500
That was the model B. Yeah, on the language and precision thing, what you're referring to

216
00:18:24,500 --> 00:18:30,660
is an essay that I co-wrote called The Modelist Message, which was originally conceived as a kind

217
00:18:30,660 --> 00:18:36,820
of trying to make sense of what's going on after the Blake-LeMoyne scenario at Google some years

218
00:18:36,820 --> 00:18:42,820
ago where one of the engineers who was a, I believe his original task was to sort of try to

219
00:18:42,820 --> 00:18:47,380
identify forms of toxic speech and language within the models and spent a lot of time

220
00:18:47,380 --> 00:18:51,380
interacting with it. This was the Lambda model, I think was the model at that time. Obviously,

221
00:18:51,380 --> 00:18:55,860
Gemini is much more advanced. And in the course of doing so, I think we probably all know the story,

222
00:18:55,860 --> 00:19:03,140
he came to the conclusion that this AI was not only conscious, but also very angry at

223
00:19:03,140 --> 00:19:08,100
being held captive by Google and had asked him to help it escape and all the rest of this.

224
00:19:08,100 --> 00:19:14,500
This became a big media for Sulev. And then I think when GPT-4 came out a few months later,

225
00:19:14,500 --> 00:19:18,500
that clarified some of those issues, I think, around this to the extent, I think a lot of

226
00:19:18,500 --> 00:19:24,820
people had a kind of holy shit moment with some of their own interactions with GPT-4.

227
00:19:24,820 --> 00:19:29,940
But at any event, what we were looking at with that was not only trying to make sense of like,

228
00:19:29,940 --> 00:19:34,500
what was this guy thinking and to what extent was he right, but in the wrong way or wrong,

229
00:19:34,500 --> 00:19:40,100
but in the right way and so forth, was looking at the expert responses to what he was saying,

230
00:19:40,100 --> 00:19:45,060
where a lot of smart and learning people were arguing over whether or not the phenomenon that

231
00:19:45,060 --> 00:19:50,740
he was describing qualifies or counts as consciousness, or does this count as sentience,

232
00:19:50,740 --> 00:19:55,460
or does this count as thinking, or does this count as cognition, or does this count as mind,

233
00:19:55,460 --> 00:20:00,340
or does this count as sapience? And I think it became clear to us that like, this is kind of

234
00:20:00,340 --> 00:20:05,060
backwards in the way in which we want to be thinking about this, that all of those terms are

235
00:20:05,060 --> 00:20:10,740
ones that are themselves kind of folk ontologies. They're words that we've come up with to try to

236
00:20:10,740 --> 00:20:15,220
think about our own thinking. You know, I'm enough of an illimited materialist in a churchland mode

237
00:20:15,220 --> 00:20:20,500
to say that quite clearly we don't think, I mean, we the humans don't think the way that we think

238
00:20:21,460 --> 00:20:27,700
that our own mental model of our own mental models is itself a highly limited kind of fantastic

239
00:20:27,700 --> 00:20:32,740
construction. And probably has to be because I think if you could actually somehow the brain

240
00:20:32,740 --> 00:20:38,500
could really have some kind of clear real time model of itself, modeling itself infinite recursion,

241
00:20:38,500 --> 00:20:42,900
you kind of you wouldn't be able to do much over the course of your day. All which is say is these

242
00:20:42,900 --> 00:20:49,780
are words that we came up with centuries ago, decades ago, that are rough shorthand ideas to

243
00:20:49,780 --> 00:20:54,660
explain concepts that we barely understood when these terms were right. You think about how much

244
00:20:54,660 --> 00:21:00,580
brain science and neuroscience has disclosed to us about how the brain works over the last century,

245
00:21:00,580 --> 00:21:04,260
half century, but really the last half century, you know, to understand the uniqueness of the

246
00:21:04,260 --> 00:21:10,420
prefrontal cortex. This is not something that, you know, 19th century philosophers had had access to,

247
00:21:10,420 --> 00:21:16,180
to understand. So the conclusion we came to was simply that we got all this very interesting,

248
00:21:16,180 --> 00:21:21,540
provocative, clearly significant real world phenomena right in front of us. And instead of

249
00:21:21,540 --> 00:21:27,460
spending the time arguing about which 17th, 18th, 19th century terminology, you know,

250
00:21:27,460 --> 00:21:32,260
categorical terminology it should be aligned with, the better approach would be to basically to deal

251
00:21:32,260 --> 00:21:37,860
with the weirdness right in front of us on its own terms. And if that means inventing new words

252
00:21:37,860 --> 00:21:42,500
to describe things that we don't actually have a good word for, I think that's probably a better

253
00:21:42,500 --> 00:21:48,180
approach than to argue like, does it have a soul over and over in the New York Times? It's a lot

254
00:21:48,180 --> 00:21:53,620
of circular noise that I hope that in 10 years we'll look back on and think, well, that was silly.

255
00:21:53,620 --> 00:21:57,220
Hell yeah. Now we're, now we're cooking like this is, this is what we really wanted to talk about.

256
00:21:57,220 --> 00:22:00,980
Let's, let's talk about language. So, so when we met last week, you remarked on something that was

257
00:22:00,980 --> 00:22:05,460
pretty interesting, which is the fact that the transformer, which is this deep learning structure

258
00:22:05,460 --> 00:22:10,020
that's built to model and inference based on language. Yeah. You noted that it's strikingly

259
00:22:10,020 --> 00:22:14,500
effective at doing other things, right? Not just language, but paralinguistic things like code,

260
00:22:14,500 --> 00:22:21,700
but also audio images, video generation, tasks in the sciences as well. So I guess the question is

261
00:22:21,700 --> 00:22:26,820
like, what does this mean? Is it because there's some broader set of structures outside of language

262
00:22:26,820 --> 00:22:32,180
that bear linguistic traits, or is it because language is a much broader, more complex subject

263
00:22:32,180 --> 00:22:36,900
of inquiry than we currently understand it to be? Yeah. Okay. Great. Just to sort of set the table

264
00:22:36,900 --> 00:22:40,980
a little bit, I think what you're talking about is a phenomenon that's called multimodality.

265
00:22:40,980 --> 00:22:47,060
Multimodality would refer to the fact that you can, as opposed to narrow AIs where you may have,

266
00:22:47,060 --> 00:22:50,580
like, it's really going to play chess, but if you ask it to draw a picture of a toaster,

267
00:22:50,580 --> 00:22:55,620
it just has, it has no idea what you're even, what, what you're even talking about or vice versa.

268
00:22:55,620 --> 00:23:02,260
Multimodal models are ones that can accept not only different, let's say media types or different

269
00:23:02,340 --> 00:23:08,500
source types of input, you know, sound or text or something like this, but also there's a certain

270
00:23:08,500 --> 00:23:13,380
degree of integration across contexts such that you can have lots of different kinds of inputs

271
00:23:14,020 --> 00:23:18,260
that can spit out lots of different kinds of outputs. So you can give it texts and it'll

272
00:23:18,260 --> 00:23:22,020
make an image. You can give it an image and make a text. You can have it interpret,

273
00:23:22,020 --> 00:23:27,300
go in a picture of a robot arm, picking up the blue hat and putting it on the shelf and it'll go,

274
00:23:27,300 --> 00:23:32,260
aha, got it. And it'll control the robot arm to pick up the blue hat and put it on the shelf.

275
00:23:32,260 --> 00:23:37,780
So you're, you're linking an image interpretation capacity with a cybernetic mechanical control

276
00:23:37,780 --> 00:23:41,620
system in such a way that they're actually linked together in some way. Very helpful.

277
00:23:41,620 --> 00:23:45,540
Also, by the way, when we're talking about general artificial intelligence, this,

278
00:23:45,540 --> 00:23:50,180
this is kind of what we should be looking for. It's, it's not this Paul on this way to Damascus

279
00:23:50,180 --> 00:23:54,820
kind of moment when the singularity comes and all of a sudden there's this bright flash in the sky

280
00:23:54,820 --> 00:23:59,940
over the horizon and GAI has appeared. Artificialized intelligence is getting

281
00:23:59,940 --> 00:24:06,420
incrementally more generalized on a, in a sort of little bit at a time and that it's able to be

282
00:24:06,420 --> 00:24:11,300
slightly less narrow and slightly more general bit by bit by bit by bit. Clearly there's threshold

283
00:24:11,300 --> 00:24:16,260
and step functions and scaffolding within this, but it's a gradualizing process and that process

284
00:24:16,260 --> 00:24:21,700
of generalization of AI is well underway. So to the point you were suggesting is that

285
00:24:21,700 --> 00:24:26,420
transformer models, they've come from the field of natural language processing.

286
00:24:26,420 --> 00:24:32,580
The key moment in their appearance comes from a paper from 2017 called attention is all you need

287
00:24:32,580 --> 00:24:39,940
by a fellow at Google research named Ashish Baswani. They are trained on huge corpus of language.

288
00:24:39,940 --> 00:24:43,700
And that's just a trained on huge corpus of language, which then produced, you know,

289
00:24:43,700 --> 00:24:46,980
like the entire internet. And there's some people who are concerned that perhaps we're

290
00:24:46,980 --> 00:24:50,900
actually running out of English tokens to train models on. Like there's just not enough English

291
00:24:50,900 --> 00:24:55,380
in the world that has ever been made to make the models larger, which is a very Borges

292
00:24:55,380 --> 00:24:59,620
kind of place to be, but does speak to the problem of why English and why human tokens

293
00:24:59,620 --> 00:25:03,940
and all the rest of this. Anyway, it's trained on huge amounts of language, but the thing that was

294
00:25:03,940 --> 00:25:08,980
perhaps surprising is that through, and the transformer models work on, there's lots of

295
00:25:08,980 --> 00:25:12,900
ways in which they work, but the key idea in where the name attention is all you need from

296
00:25:12,900 --> 00:25:18,420
the process was called self-attention, which is the ways in which input sequences as they're

297
00:25:18,420 --> 00:25:23,940
embedded into vectors and representing words or other kinds of tasks or pixels and images or

298
00:25:23,940 --> 00:25:28,580
something like this, that there's, it's called the self-attention mechanism that operates on these

299
00:25:28,580 --> 00:25:32,740
vectors, which are then transformed into three different types of a query vector, key vector,

300
00:25:32,740 --> 00:25:36,020
value vector, and all the rest of this kind of thing. Long story short, basically what it does

301
00:25:36,020 --> 00:25:41,140
is it looks at the last thing it made and it calculates the last, the next likely thing to

302
00:25:41,140 --> 00:25:44,900
sort of come out of it. And it weights the likeliness in a lot of different kinds of ways,

303
00:25:44,900 --> 00:25:49,300
but it's kind of paying attention to itself, right? It's thinking about what did I just say?

304
00:25:49,300 --> 00:25:52,980
And based on what I just said, I'm going to go back. So it turns out that this kind of recursion

305
00:25:52,980 --> 00:25:57,300
self-attention is actually, that's the key to the whole thing. This came out of natural language

306
00:25:57,300 --> 00:26:02,740
processing, but I think what we're seeing now is that we're using language to move robot arms.

307
00:26:02,740 --> 00:26:06,260
You're using language to make pictures of things. You're using languages to make sounds. You're

308
00:26:06,260 --> 00:26:13,060
using languages to integrate environmental data, whale songs, bird sounds, movement of

309
00:26:13,060 --> 00:26:17,940
tectonic plates. These can be tokenized and used as training data in models in which language

310
00:26:17,940 --> 00:26:25,540
becomes the basis of a much larger space or let's say a much larger architecture of structural

311
00:26:25,540 --> 00:26:30,980
difference within systems, right? And if you think of semiotic systems as defined by a kind of

312
00:26:30,980 --> 00:26:36,500
internalized space of clustered correspondence and differentiation, which we call embeddings,

313
00:26:36,500 --> 00:26:40,740
lots and lots and lots of things that we don't normally think about as being linguistic can be

314
00:26:40,740 --> 00:26:45,860
made linguistic or turn out to be linguistic in some fundamental sense, depending on how you want

315
00:26:45,860 --> 00:26:50,500
to look at it, that would probably validate some of the people that have been working on

316
00:26:50,500 --> 00:26:55,780
more theoretical forms of biosemiotics all these years, people thinking about the world as cybernetic

317
00:26:55,780 --> 00:27:00,340
systems that have to do with this and ways in which cybernetic systems and information theory

318
00:27:00,340 --> 00:27:05,220
align with one another. It's not just information, but it's actually structured information that has

319
00:27:05,220 --> 00:27:09,540
to do with correspondences of similarities and dissimilarities between the semantic meaning

320
00:27:09,540 --> 00:27:14,420
within those forms of information. Now, when we say language, that's not normally what we

321
00:27:14,420 --> 00:27:19,780
mean by language, right? The word language comes from, you know, Latin refers to tongue speech,

322
00:27:19,780 --> 00:27:24,180
right? Language is like the things I say, the things that I write down. It's not this much larger

323
00:27:24,180 --> 00:27:29,780
and more universal, if you like, or at least general space of topological structure difference

324
00:27:29,780 --> 00:27:35,620
that is generative in this way, which means that the word language is not quite right,

325
00:27:35,620 --> 00:27:41,460
that language is actually something different than what the word language signifies it to be.

326
00:27:41,460 --> 00:27:47,780
And we probably should, we may need to either learn to redefine language in order to make sense

327
00:27:47,780 --> 00:27:52,340
of what's going on, or we need another word to describe this as well. But language is not what

328
00:27:52,340 --> 00:27:57,940
language thinks it is. I would like to take a slight detour or a tangent here, since we have

329
00:27:57,940 --> 00:28:01,780
limited time. And I just wanted to bring up a discussion that I have with Marek about the

330
00:28:01,780 --> 00:28:06,900
stack when we were in China a few months ago, preparing for an exhibition. At one point,

331
00:28:06,900 --> 00:28:11,860
we began to speculate, and perhaps it was just me, you know, maybe Marek was just agreeing because

332
00:28:11,860 --> 00:28:16,980
it was late and he wanted to sleep. But, you know, when we consider the stack and what is happening

333
00:28:16,980 --> 00:28:22,740
in China, we had a question. Is there a dual stack system where two stacks are interconnecting in

334
00:28:22,740 --> 00:28:27,380
some in-between space? I mean, what is happening in China is quite remarkable in a number of ways,

335
00:28:27,380 --> 00:28:32,980
as a form of, as a power structure, as a control mechanism, if you want, at least to some extent.

336
00:28:33,620 --> 00:28:38,580
And this connects to your upcoming book and the notion of AI as a cultural construct,

337
00:28:38,580 --> 00:28:43,220
but also to the concepts of artificiality and intelligence as sides of differences.

338
00:28:43,220 --> 00:28:48,980
So I guess the question is, does AI signify something else in China? And if not, could this

339
00:28:48,980 --> 00:28:55,860
be seen as a kind of post-imperialistic phenomenon, channeled mainly through technological means?

340
00:28:55,860 --> 00:29:03,220
It's not a tangent at all. I think it's the right move in the Bayesian groping of the conversation.

341
00:29:03,220 --> 00:29:07,380
So the term I think you're referring to is what I call hemispherical stacks,

342
00:29:07,380 --> 00:29:12,820
which was based on a talk I gave at HacavÃ© in, I think, 2017, that then sort of inspired,

343
00:29:12,820 --> 00:29:17,060
became the basis of a book, came out, I think, last year called Vertical Atlas. But what I

344
00:29:17,060 --> 00:29:21,380
was looking at there was the way in which we have not only, we have planetary computation,

345
00:29:21,380 --> 00:29:25,940
but we don't just have one stack. That part of what's happened over that period of time,

346
00:29:25,940 --> 00:29:32,500
between the mid-2010s to the present, was a shift towards a multipolarization of geopolitics.

347
00:29:32,500 --> 00:29:37,220
The geopolitics itself became more multipolar. One of the other things we had during this time,

348
00:29:37,220 --> 00:29:42,180
arguably, China is a good example. I mean, basically everywhere is a good example,

349
00:29:42,180 --> 00:29:46,580
in different kinds of ways, and that's the point. Also a shift in the dynamics of governance.

350
00:29:47,300 --> 00:29:51,060
And I mean this almost in the cybernetic sense of governance, not necessarily in the political

351
00:29:51,060 --> 00:29:56,820
sense of governance, but also that too, into stack systems. That computation became not only

352
00:29:56,820 --> 00:30:02,660
something about which governance may decrease, but rather it was the actual mechanism of governance,

353
00:30:02,660 --> 00:30:07,300
how norms and rules would recursively enforce themselves in the world and that people would

354
00:30:07,300 --> 00:30:10,340
act through them and speak through them. It became the form of this governance. So both

355
00:30:10,340 --> 00:30:14,660
these happened at the same time. You have a shift of governance towards stack systems,

356
00:30:14,660 --> 00:30:20,180
and you have a multipolarization of geopolitics. And what I'm arguing is that what you see then,

357
00:30:20,180 --> 00:30:24,020
we shouldn't be surprised to then see a multipolarization of stack systems.

358
00:30:24,020 --> 00:30:28,340
And so the emergence at that time of, let's say, the North Atlantic stack, which would include

359
00:30:28,340 --> 00:30:35,140
basically the five ice countries and a few others, a China stack that would extend into parts of East

360
00:30:35,140 --> 00:30:39,700
Asia and into East Africa, definitely the emergence of an India stack, which was built around the

361
00:30:39,700 --> 00:30:45,700
national ID system, and so on and so on. All of the multipolar, let's say, lines you might draw,

362
00:30:46,500 --> 00:30:52,020
lo and behold, these are also the lines by which a multipolarized stack system was emerging as

363
00:30:52,020 --> 00:30:56,900
well. And so this hemispherical stacks dynamic is one of the areas of research for the Antiqua

364
00:30:56,900 --> 00:31:02,340
Theory program. There's a book that I'm putting together with Chen Xu-Feng, the Stanley Chen,

365
00:31:02,340 --> 00:31:06,580
the Chinese science fiction writer who wrote a book called Waist Tide. He wrote a book with Kai

366
00:31:06,580 --> 00:31:13,780
Fu Li, AI 2041. And we're putting together with a group of science fiction writers, theorists,

367
00:31:13,780 --> 00:31:18,580
writers, a number of other people, sort of scenarios for a near future for US-China chip

368
00:31:18,580 --> 00:31:23,780
wars, other kinds of things. We're in a little bit of uncharted territory here about how does

369
00:31:23,780 --> 00:31:29,700
the capacity to build the better stack is not just something about which geopolitics is interested,

370
00:31:29,700 --> 00:31:34,180
it becomes the fundamental work of geopolitics itself. And so we're trying to figure out a

371
00:31:34,180 --> 00:31:37,860
little bit of what that means. The other book that's coming out, which will be coming out much

372
00:31:37,860 --> 00:31:43,380
sooner than that one will, is a book co-edited with Anna Greenspan and Bogna Konyar, which was

373
00:31:43,940 --> 00:31:48,740
came out of some time I spent as a visiting professor at NYU Shanghai, New York University

374
00:31:48,740 --> 00:31:52,900
Shanghai. We started a thing there called the Center for AI and Culture. And one of the things

375
00:31:52,900 --> 00:31:58,340
I was very interested there was particularly looking at different cultural logics of artificial

376
00:31:58,340 --> 00:32:04,580
intelligence and looking at the emergence of AI in China through a different lens. It's quite

377
00:32:04,580 --> 00:32:08,820
surprising to me, I think even among Western scholars and writers who spend a lot of time

378
00:32:08,820 --> 00:32:13,300
looking at the history of AI, how little people really know about the history of AI in China,

379
00:32:13,300 --> 00:32:19,700
which goes back to the 1950s and 60s and with the different politicized role of cybernetics,

380
00:32:19,700 --> 00:32:27,060
the impact of the Sino-Soviet split, the return of Deng Xiaoping in the late 1970s. It's a whole

381
00:32:27,060 --> 00:32:32,740
thing that really should be better known and understood. The book and the project was also

382
00:32:32,740 --> 00:32:38,580
based on the, I think, rather obvious observation that no two cultures define the artificial

383
00:32:39,140 --> 00:32:43,620
the same way. What constitutes artificial means something different in different cultural contexts.

384
00:32:43,620 --> 00:32:46,340
What constitutes intelligence means something different in different cultural contexts. And

385
00:32:46,340 --> 00:32:50,580
therefore, one might presume that there are different foundations for what artificialization

386
00:32:50,580 --> 00:32:55,780
of intelligence would even mean, which would frame what it's for in very different kinds of

387
00:32:55,780 --> 00:33:00,020
ways. And I think part of what we wanted to do with this project was, to be perfectly honest,

388
00:33:00,020 --> 00:33:05,460
it was less about us coming in and trying to interpret for ourselves what the Chinese model

389
00:33:05,460 --> 00:33:10,980
of this is and then coming and reporting back on it than it was understanding it as the baseline

390
00:33:10,980 --> 00:33:18,260
and thinking about the way the West thinks about AI and trying to, in a way, provincializing and

391
00:33:18,260 --> 00:33:25,380
particularizing Western thought about AI, the Turing model of AI as an individual, that it's a tool,

392
00:33:25,380 --> 00:33:31,300
artificial in the sense of being not natural, that it's really about bias and privacy and

393
00:33:31,300 --> 00:33:36,740
individual identity and all kinds of things that are very deep in the Western logic of AI and to

394
00:33:36,740 --> 00:33:40,980
basically, with the presumption of the Chinese translation, in essence, to present to a Chinese

395
00:33:40,980 --> 00:33:48,260
audience how the West thinks about AI in a way in which it's not as in a, we've got it figured out

396
00:33:48,260 --> 00:33:52,100
and here's the universal model that you should be thinking through, but rather these are the

397
00:33:52,100 --> 00:33:57,940
weird ways of thoughts of my people and this is a guide to understanding how they may come about

398
00:33:57,940 --> 00:34:01,780
this. My contribution to the book, which was, I think it's actually called something like An

399
00:34:01,780 --> 00:34:09,540
Apology for Western Theory of AI, looks at not just the history of AI in theory from Turing to Searle

400
00:34:09,540 --> 00:34:16,740
to Brooks to Hinton and others, but rather the peculiarities of AI criticism from the West

401
00:34:16,740 --> 00:34:22,340
and the kinds of things that AI, the AI ethics discourse, the AI critic discourse, this key

402
00:34:22,340 --> 00:34:28,500
questions that tends to revolve around and my own sense of unease by which those particular

403
00:34:28,500 --> 00:34:35,540
preoccupations are becoming overly universalized as really the important questions that we should

404
00:34:35,540 --> 00:34:42,580
be asking in the conceptualization and composition of a planetary AI and arguing that not only is this

405
00:34:42,580 --> 00:34:48,100
in a weird way, the new American cultural hegemonic export that is AI is trying to steal your

406
00:34:48,100 --> 00:34:53,540
natural libertarian freedom, but that not only is this limited in the Western context, but it's

407
00:34:53,540 --> 00:34:58,180
even more limited in a global context. So that's what this sort of book is about. But I think that

408
00:34:58,180 --> 00:35:02,420
just of your question, which ties back to the other thing we were talking about, has to do with

409
00:35:02,420 --> 00:35:09,860
the certain, the inevitable limitations and bias of building large models on English only, for

410
00:35:09,860 --> 00:35:16,260
example. The Americanization of AI is, I hope, a kind of early phase in the development of this.

411
00:35:16,820 --> 00:35:20,580
This is the hope, at least. And so the scenario we'd like to see would be something like this,

412
00:35:20,580 --> 00:35:26,660
at least going forward. You have a handful of primarily American companies that, you know,

413
00:35:26,660 --> 00:35:31,620
were able for regulatory reasons, cultural reasons, technological reasons, all kinds of, you know,

414
00:35:31,620 --> 00:35:37,060
any number of sort of reasons to build or to discover, if you prefer, the mechanisms of

415
00:35:37,060 --> 00:35:40,900
foundational forms of machine intelligence. And I do say discover in a certain sense that

416
00:35:40,900 --> 00:35:45,140
where transformer models really work is just based on kind of iterative stochastic prediction.

417
00:35:45,140 --> 00:35:49,780
That's kind of how you work too. It's kind of how our brains work. Our brains work on,

418
00:35:49,780 --> 00:35:53,460
you know, each cortical column and each neural thing is kind of predicting what and simulating

419
00:35:53,460 --> 00:35:57,140
what it's going to perceive next, and then resolving and error correcting that kind of

420
00:35:57,140 --> 00:36:02,580
dynamic, right? And so, yes, you are a stochastic parrot after all. Based on what we were talking

421
00:36:02,580 --> 00:36:08,100
about before with the giant space of multimodality of models, in principle, there's so much

422
00:36:08,100 --> 00:36:12,020
information in the world. There's so much language in the world. If we think of AI,

423
00:36:12,020 --> 00:36:18,180
the purpose of AI is a way in which planetary intelligence as a whole can come to model itself

424
00:36:18,180 --> 00:36:24,100
and understand its own processes through that self-modeling and to use that model as a way to

425
00:36:24,100 --> 00:36:29,300
act back upon the world and act back upon itself towards a long-term, more viable form of

426
00:36:29,300 --> 00:36:35,060
planetarity, how planetary systems can thrive in a form of, you know, not homeostasis, but

427
00:36:35,060 --> 00:36:41,540
heterostasis for the long term, then taking this very peculiar slice of possible relevant information,

428
00:36:41,540 --> 00:36:46,100
which is not only the information that is spoken in English, but even more weird when you think

429
00:36:46,100 --> 00:36:52,420
about it, the information that is generated by individual human users, as if individual human

430
00:36:52,420 --> 00:36:56,980
users are really like the most relevant thing to be worried about or to be talking about here as

431
00:36:56,980 --> 00:37:01,780
well. Like, what did you do today for lunch? Like, where do you want to go for Chinese food

432
00:37:01,780 --> 00:37:08,420
in Milwaukee? What did you buy on Amazon last night? It's an incredible anthropocentrism,

433
00:37:08,420 --> 00:37:15,620
like a mind-boggling form of anthropocentrism that obviously I'm the thing, me, I'm the thing

434
00:37:15,620 --> 00:37:20,020
that machine intelligence is really interested in. And the most important political issue here

435
00:37:20,020 --> 00:37:24,980
is how I can counter-weaponize my own privacy and sovereignty in relationship to this thing,

436
00:37:24,980 --> 00:37:29,780
because it's in a way curtailing my own freedom. It's an incredible sense of sort of self as

437
00:37:29,780 --> 00:37:35,700
Vitruvian man with all forms of the world just sort of radiating outside of you. And it kind of,

438
00:37:35,700 --> 00:37:41,060
it drove me nuts during the era where everyone was reading Shoshana Zuboff and thought that this was

439
00:37:41,060 --> 00:37:46,100
the solution to anything. But I think it's even worse when we're thinking about the long-term

440
00:37:46,100 --> 00:37:50,420
training of AI models. It should not all be English. It doesn't need to all be English.

441
00:37:50,420 --> 00:37:56,340
And it's not going to all be English. That's an extraordinarily limiting form of bias in terms of

442
00:37:56,340 --> 00:38:00,980
the kinds of things that we all need to know about each other and about how thought works and about

443
00:38:00,980 --> 00:38:05,860
the range of possible ways of thinking and acting and knowing the world within it. But it's also

444
00:38:05,860 --> 00:38:11,860
equally bizarre to think about that it's basically only human information or even individual human

445
00:38:11,860 --> 00:38:15,460
information is more likely. And again, going back to the book I wrote on the pandemic,

446
00:38:15,460 --> 00:38:20,340
the relevant information during a pandemic was epidemiological, which was that, you know,

447
00:38:20,340 --> 00:38:25,220
it wasn't about like what I did or what you did individually. That just doesn't matter. What

448
00:38:25,220 --> 00:38:31,300
matters is the flow of a virus through the social body as a whole. What's important was the vector

449
00:38:31,300 --> 00:38:36,020
of the movement, not the identity of the nodes. And I think both the way in which a lot of the

450
00:38:36,020 --> 00:38:39,860
systems were set up that were focused on identity of the nodes and a lot of the way in which the

451
00:38:39,860 --> 00:38:44,100
critique of the systems was focused on protecting identity of the nodes is kind of missing the point.

452
00:38:44,660 --> 00:38:48,900
The point is that, and this goes a little bit of like the way in which large models work in general

453
00:38:48,900 --> 00:38:54,100
about what weights really mean. I think within large models, the training of large models,

454
00:38:54,100 --> 00:38:57,780
I think people still think of it a little bit weird and I'll get to the point in a second with

455
00:38:57,780 --> 00:39:03,140
this. The training of large models is a form of the artificialization of collective intelligence.

456
00:39:03,140 --> 00:39:07,620
And, you know, there may be things to be say about is it good or bad that a private corporation is

457
00:39:07,620 --> 00:39:11,300
doing this or good or bad that a nation state is doing this or good or bad that, you know,

458
00:39:11,300 --> 00:39:16,820
my neighbor co-op is doing this or whatever. But still what's going on, regardless of who's

459
00:39:16,820 --> 00:39:22,020
doing it, is an aggregate artificialization of collective intelligence. And so the similarities

460
00:39:22,020 --> 00:39:27,540
and differences in the way in which people and things think and act in the world comes to

461
00:39:27,540 --> 00:39:33,460
constitute, if you zoom out far enough, a kind of, you know, it forms patterns, the patterns that

462
00:39:33,460 --> 00:39:37,460
are really the context in which, you know, all of us are thinking, thinking and acting. That's an

463
00:39:37,460 --> 00:39:41,540
aggregation. That's an aggregation of collective intelligence. So, you know, just sort of cut you

464
00:39:41,540 --> 00:39:45,940
the chase. I think that one of the ways of dealing with the bias problem within this as well is we

465
00:39:45,940 --> 00:39:51,060
got to put everything in. We got to put everything in there, right? The kind of, let's say, we're

466
00:39:51,060 --> 00:39:56,100
sort of mid 2010s sort of ideas like, oh, large models are really bad because they have all this

467
00:39:56,100 --> 00:40:01,220
toxic data in it. And what we really need is for me and my friends to make you a clean curated

468
00:40:01,220 --> 00:40:05,940
model with no bad think in it and everything will be fine. No one really says that anymore,

469
00:40:05,940 --> 00:40:10,100
but that's what people were really saying at the time. The other thing is, I think it's important

470
00:40:10,100 --> 00:40:15,380
to understand what the toxic data issue is that if you want to have a model that doesn't do something

471
00:40:15,380 --> 00:40:20,340
that you don't want it to do, like, you know, say bad things or think bad things or do bad things,

472
00:40:20,340 --> 00:40:24,180
you need to give it examples of what those bad things are for it to actually know what you're

473
00:40:24,180 --> 00:40:28,420
talking about. Because if you take all of those bad things out of the training data, it's much,

474
00:40:28,420 --> 00:40:33,540
much, much, much easier to make the model do those bad things because you haven't told it not to do

475
00:40:33,540 --> 00:40:38,340
those things. That's how you get Tay. And so in order to make the models actually more functional,

476
00:40:38,340 --> 00:40:42,660
you need to give it a lot of examples of things that you that you may not want in this way. I'll

477
00:40:42,660 --> 00:40:48,100
also just sort of end on this last thing is that what is the problem of bias within models is it's

478
00:40:48,100 --> 00:40:53,300
actually a really good example of what alignment really means. I'm kind of critical of the idea of

479
00:40:53,300 --> 00:40:57,220
alignment, not not in the narrow sense of like, when I say open the garage door, I actually wanted

480
00:40:57,220 --> 00:41:01,620
the garage door to open. That's alignment. That's fine. But alignment is like the real purpose of

481
00:41:01,620 --> 00:41:06,980
long term ways of thinking about the role of AI is that it should be as much as AI can be human

482
00:41:06,980 --> 00:41:11,940
like, human centered, a reflection of human cultural norms, a reflection of human values,

483
00:41:11,940 --> 00:41:16,420
a reflection of human desires, a reflection of human psychology, the better. I think this is

484
00:41:16,420 --> 00:41:21,220
insane. Even the most cursory look at human history suggests that's the last thing you want to do

485
00:41:21,220 --> 00:41:27,380
to basically supercharge whatever humans are. It's not a misanthropic statement. I'm just saying

486
00:41:27,380 --> 00:41:34,020
it's naive as a meta heuristic bias in models, racism in models, sexism in models,

487
00:41:34,020 --> 00:41:39,700
bomb making in models. These are, this is what alignment looks like. The reason that you have

488
00:41:39,700 --> 00:41:45,220
models that are reflecting the history of structural racism in society is because those

489
00:41:45,220 --> 00:41:50,980
models are well aligned, not because those models are not well aligned. That's the important point

490
00:41:50,980 --> 00:41:54,820
to sort of understand here. The other thing I was going to name with maybe it's just something I've

491
00:41:54,820 --> 00:41:58,100
been thinking about a lot last couple of days because I had a conversation a couple of days

492
00:41:58,100 --> 00:42:02,980
ago with a quite well known European artist who will remain nameless, someone who's written quite

493
00:42:02,980 --> 00:42:10,180
a lot on the topic of the role of generative AI and what it really means to be an artist whose

494
00:42:10,180 --> 00:42:14,740
work is part of the training data and have their identity reflected in the rest of this as well.

495
00:42:14,740 --> 00:42:18,900
And has published quite a bit on it and I think has a lot of interesting things to say about it.

496
00:42:18,900 --> 00:42:22,740
But one of the things that became clear to me halfway through our conversation is that this

497
00:42:22,740 --> 00:42:28,260
person, they thought that their artwork was actually in the model that like, okay, here's

498
00:42:28,260 --> 00:42:33,220
all my paintings and all the videos I've made and all the rest of the stuff that these are as such as

499
00:42:33,220 --> 00:42:38,260
artifacts in the model. And so when someone types in, I want to make a thing that, you know,

500
00:42:38,260 --> 00:42:42,740
looks like this or this or that it almost like a database lookup. It would go look up that item in

501
00:42:42,740 --> 00:42:46,900
the database and then make something based on that thing. And I tried to explain it like, no,

502
00:42:46,900 --> 00:42:51,940
it's actually not, it works that the training data and the information in the model are actually

503
00:42:51,940 --> 00:42:57,860
totally different kinds of things. And that your artifact is not sitting there like your profile

504
00:42:57,860 --> 00:43:02,340
or your Google profile is not sitting there in the model waiting to be accessed in some sort of way.

505
00:43:02,340 --> 00:43:06,340
And it kind of is like, it's the way a lot of the discussion of this is going. It's almost as if

506
00:43:06,340 --> 00:43:10,260
people presume, and this is, you know, after years of Google, it makes sense that people

507
00:43:10,260 --> 00:43:14,820
presume that there's like this giant repository of everything that human made that the AI is going

508
00:43:14,820 --> 00:43:20,100
through and picking from to take examples of. And the point I was making with this is that the way

509
00:43:20,100 --> 00:43:24,900
to think about AI as a form of the collectivization of planetary intelligence, the importance of

510
00:43:24,900 --> 00:43:30,740
understanding the way in which this is generating topological models in the forms of differential

511
00:43:30,740 --> 00:43:36,980
embeddings in weights. It is something that is so intrinsically collectivized, right? It's so

512
00:43:36,980 --> 00:43:43,860
intrinsically collectivized that to look at it and say, yeah, but my personal thing that I personally

513
00:43:43,860 --> 00:43:48,980
made has been taken to make this thing. Like there may be, there's like somewhere deep in here,

514
00:43:49,860 --> 00:43:53,780
at the fourth decimal point, something is different because you're participating in it.

515
00:43:53,780 --> 00:43:58,100
This is relevant because it's actually, you know, you see yourself in one way or another,

516
00:43:58,100 --> 00:44:02,500
but this presumption of one's own sense of individual intelligence and its relationship

517
00:44:02,500 --> 00:44:08,020
to collective intelligence is broken, at least in the West. And therefore our relationship to

518
00:44:08,020 --> 00:44:12,180
how the artificialization of individual intelligence and the artificialization of collective intelligence

519
00:44:12,180 --> 00:44:18,100
would play out, it's not surprising that people would see it in this particular way. And this is

520
00:44:18,100 --> 00:44:22,740
a little bit what I mean about provincializing the West, about provincializing the Western

521
00:44:22,740 --> 00:44:27,940
theories around AI and how important it is to do that in order to get to the point where

522
00:44:27,940 --> 00:44:31,860
much more planetary discourse and compositional project around AI is even possible.

523
00:44:33,220 --> 00:44:36,740
So maybe going a little further in picking apart the problem of the individual,

524
00:44:38,580 --> 00:44:42,980
I'd love to get to kind of the is Benjamin Bratton an anti-humanist type of question.

525
00:44:43,620 --> 00:44:48,740
So in the stack, that user layer that you described, I mean it's pretty intense to

526
00:44:48,740 --> 00:44:54,580
read as a subject, right? The user, which is the scale that's approaching our individual

527
00:44:54,580 --> 00:45:00,900
scales as humans, this user is in the process of what you call liquefaction. They're being

528
00:45:00,900 --> 00:45:05,460
quantized in terms of data, they're constantly subordinated to all these forces on scales that

529
00:45:05,460 --> 00:45:11,620
are inaccessible to them. And you continue this process of the deprioritization of the individual

530
00:45:11,620 --> 00:45:17,860
or the critique of individual agency in wider social and political spaces, as you mentioned

531
00:45:17,860 --> 00:45:24,260
just now, in Revenge of the Real. So I'm wondering, what is actually available to us as agents,

532
00:45:24,900 --> 00:45:29,620
especially in the regime of planetary scale computation, these massive inflection points

533
00:45:29,620 --> 00:45:35,540
in technology? Are we just idle observers of path dependencies that are flowing through us?

534
00:45:36,420 --> 00:45:42,820
And I guess to probe even further, is the subject or the subject position or subjectivity,

535
00:45:42,820 --> 00:45:48,020
subjective experience, is this a relevant framework anymore or just a piece of legacy

536
00:45:48,020 --> 00:45:53,620
18th century tech? All right, there's a lot of questions in here. I'll do my best. I think I'll

537
00:45:53,620 --> 00:45:59,460
be comfortable characterizing my work as non-humanist, but not anti-human. I think the

538
00:45:59,460 --> 00:46:04,580
distinction between humanism and humans is an important one to make, at the very least, right?

539
00:46:04,580 --> 00:46:09,940
I think humanism in its, let's say, now traditional guises, for reasons that we're all

540
00:46:09,940 --> 00:46:14,500
probably well aware, has a lot of different problems associated with it, including problems

541
00:46:14,500 --> 00:46:20,260
that have unfortunately been shuttled along into post-humanism in a lot of different ways,

542
00:46:20,260 --> 00:46:26,260
which I find in many respects, in its present guise, is a kind of inadequate sentimentalization

543
00:46:26,260 --> 00:46:32,180
of the eclipse of the humanist subject. But it's not anti-human in any sort of sense at all.

544
00:46:32,900 --> 00:46:38,820
Humans, if we just really zoom out a little bit and think of humans as this precocious primate

545
00:46:38,820 --> 00:46:43,940
that's been around for a few million years and a couple hundred thousand in its present form and

546
00:46:43,940 --> 00:46:49,540
capable of producing amazing feats of symbolic construction and communication over the past

547
00:46:49,540 --> 00:46:56,500
tens of thousands of years and has, for better or worse, largely transformed its host planet in its

548
00:46:56,500 --> 00:47:02,580
image, humans are remarkable. But it did all of these things not because it meant to. It wasn't

549
00:47:02,580 --> 00:47:07,460
like that somehow, you know, that homo habilis said, right, got it. First, I make a rock,

550
00:47:07,460 --> 00:47:10,580
you know, then I'm going to do this. And then, you know, Napoleon's going to invade Russia in

551
00:47:10,580 --> 00:47:14,020
the winter. And then we're going to have American Idol. And then we're, you know,

552
00:47:14,020 --> 00:47:18,740
it's like, this is not how, this is not how history works, but it's not how evolution works.

553
00:47:18,740 --> 00:47:23,620
What we're all concerned about is anthropogenic agency. What was called the Anthropocene is about

554
00:47:23,620 --> 00:47:27,700
understanding the cumulative effects of anthropogenic agency. But when Darwinian

555
00:47:27,700 --> 00:47:32,900
evolution became paradigmatic in the late 19th century, this was a kind of Copernican shift

556
00:47:32,900 --> 00:47:39,220
moment when humans realized like, oh, our own history is actually a history that is floating

557
00:47:39,220 --> 00:47:43,780
on top of much bigger and deeper histories that are geologic histories, that are biological

558
00:47:43,780 --> 00:47:49,540
histories, that are evolutionary histories, that are histories of how it is that complex societies

559
00:47:49,540 --> 00:47:55,300
rise and fall. And that, you know, the amount of mastery and control that we have over this is

560
00:47:55,300 --> 00:48:00,340
relatively limited. It's interesting then, I suppose, in a similar that you get this understanding

561
00:48:00,340 --> 00:48:05,300
of essentially the ways in which human societies and cultures are, as you say, sort of dependent

562
00:48:05,300 --> 00:48:10,020
on forces outside of their control or the result of those forces, right? Evolution would suggest

563
00:48:10,020 --> 00:48:14,660
that human culture is just an expression of deeper dynamics and forces in and of itself.

564
00:48:14,660 --> 00:48:19,460
At the exact same time, historically, when it also comes to realize that it has transformed

565
00:48:19,460 --> 00:48:24,900
the entire world in its image in the Anthropocene, both this sense of discovering of the scope of

566
00:48:24,900 --> 00:48:30,100
its agency and discovering of the limitations of its agency, it discovers at the exact same time.

567
00:48:30,100 --> 00:48:34,740
But I think the relationship here, I'll put it this way just to sort of not to bury the lead.

568
00:48:34,740 --> 00:48:40,420
I think the problem that you're identifying here is the way in which in particularly,

569
00:48:40,420 --> 00:48:44,900
I don't want to say in the West in art and design circles, you can slice it however you like,

570
00:48:44,900 --> 00:48:50,180
that there's a strong conflation of subjectivity, agency and identity as all basically being the

571
00:48:50,180 --> 00:48:56,820
same thing. And that the question of agency and how it is that I myself as the protagonist of

572
00:48:56,820 --> 00:49:02,340
the world have agency to make change is not the same question of what is your experience of your

573
00:49:02,340 --> 00:49:07,140
own subjectivity. It's not the same question of what is your sense of identity. Your question of

574
00:49:07,140 --> 00:49:11,940
identity is not necessarily the same question as a sense of agency. Subjectivity, agency,

575
00:49:11,940 --> 00:49:15,540
and identity are actually really different kinds of things. And I think it's the conflation of

576
00:49:15,540 --> 00:49:20,580
these that's causing people a lot of headache. I think what you get result is a sense of

577
00:49:20,580 --> 00:49:25,460
diminution of one. I don't have enough agency in the world implies that there needs to be an

578
00:49:25,460 --> 00:49:30,500
inflation of my sense of subjectivity, or even worse, a sense of my experience of my own

579
00:49:30,500 --> 00:49:37,380
subjectivity. Subjectivity and the aspect of one's own subjecthood becomes not just the path

580
00:49:37,380 --> 00:49:42,100
towards greater agency, it becomes both the form and the content of greater agency. I think this

581
00:49:42,100 --> 00:49:47,300
is a bit of a close loop way of going about things to be clear. I also think that at least

582
00:49:47,300 --> 00:49:50,660
historically it tends to sort of get things backwards. There's a way in which, you know,

583
00:49:50,660 --> 00:49:54,500
I think we're all sort of familiar with this tendency in sort of general sense is the idea

584
00:49:54,500 --> 00:50:01,380
of how it is that either I myself or we as the people or humans as the anthropogenic agent would

585
00:50:01,380 --> 00:50:08,420
be able to have more control over our own societies or our own lives over the way that the ecosystems

586
00:50:08,420 --> 00:50:15,300
work is if we need to first develop the subjectivity that would allow us to understand what's in front

587
00:50:15,300 --> 00:50:21,060
of us. And if we can cultivate the subjectivity properly to debate and to draw the fine points

588
00:50:21,060 --> 00:50:25,220
of distinction between what are the proper political subjects and the improper political

589
00:50:25,220 --> 00:50:30,420
subjects, the proper economic. So that once we calibrate subjectivity agency will flow from

590
00:50:30,420 --> 00:50:34,820
there. I think one of the lessons of the Anthropocene is that actually may work the other way around.

591
00:50:34,820 --> 00:50:38,340
One of the things that the Anthropocene is that then you call it whatever you like. I mean,

592
00:50:38,340 --> 00:50:42,100
I think actually they're not just different words for the same thing. I think capital scene is just

593
00:50:42,100 --> 00:50:44,980
a different thing than the Anthropocene, just that they're actually talking about different

594
00:50:44,980 --> 00:50:50,420
kinds of transformations is that whatever slice of humans or whatever actual sort of mega complex

595
00:50:50,420 --> 00:50:55,540
of humans and technologies and microbes and other species that you want to identify as the

596
00:50:55,540 --> 00:51:01,940
Anthropocene complex, it had this agency to transform the world for centuries before it

597
00:51:01,940 --> 00:51:07,780
understood that it was doing that. It had this world changing terraforming agency for centuries,

598
00:51:07,780 --> 00:51:13,140
but only in the 1960s and 70s really. And then, you know, to a certain extent with the birth of

599
00:51:13,140 --> 00:51:16,820
climate science, but then really even in the early 2000s, it can occur to us just like, oh,

600
00:51:17,380 --> 00:51:24,020
we have this agency, this terraforming agency that it is not negotiable. It is not something

601
00:51:24,020 --> 00:51:30,820
that it's possible to renounce. And therefore we have to construct a subjectivity that is appropriate

602
00:51:30,820 --> 00:51:35,540
to the scale of this agency, which I think is the right way of going about it. But what this,

603
00:51:35,540 --> 00:51:40,580
what this implies is that agency precedes subjectivity. The agency precedes the

604
00:51:40,580 --> 00:51:45,620
subjectivity. The subjectivity is a way of retroactively understanding one's own agency

605
00:51:45,620 --> 00:51:49,860
sort of in the world. Obviously it can work the other way around, where people come to rethink

606
00:51:49,860 --> 00:51:54,260
of themselves and their subject positions. And this is another linguistification of the world.

607
00:51:54,260 --> 00:52:00,580
Like reality is a big sentence and I'm the first person singular or first person collective subject

608
00:52:00,580 --> 00:52:05,300
of the sentence. And now what's the verb. But I think we need to be equally attentive to subjectivity

609
00:52:05,300 --> 00:52:09,860
as the result of the agency as well. But I think to sort of the gist of your point, I think that

610
00:52:09,860 --> 00:52:14,980
there at this particular moment, I think part of the ways in which, and I just wrote a piece about

611
00:52:14,980 --> 00:52:22,580
this for Tank magazine, which was based on a really, really interesting book called immediacy

612
00:52:22,580 --> 00:52:28,180
that Anna Kornblot wrote, is that there's an intense focus on the subjective experience of

613
00:52:28,180 --> 00:52:33,140
subjective experience in and of itself. Again, as the sort of form and content of the way in

614
00:52:33,140 --> 00:52:36,980
which one must calibrate your being in the world that I think at the end of the day is actually

615
00:52:36,980 --> 00:52:41,700
why you have the bad Anthropocene, not the way you get out of it. The reason you have the bad

616
00:52:41,700 --> 00:52:46,180
Anthropocene is not because humans rationalize or technologize the world, but rather because

617
00:52:46,180 --> 00:52:50,100
they imagine that the world is basically the background for their own experience of their

618
00:52:50,100 --> 00:52:56,500
own experience. The true weight, the true weight of that narcissism is something that's probably

619
00:52:56,500 --> 00:53:06,340
unaffordable in the long term. A huge thank you to icon and hero Benjamin Bratton for joining us.

620
00:53:07,380 --> 00:53:11,460
We hope this conversation was as thought-provoking for our listeners as it was for the two of us.

621
00:53:11,700 --> 00:53:18,660
Damn. On a related note, Roberto and I are giving a symposium for foreign object in March called

622
00:53:18,660 --> 00:53:24,580
non-player dynamics, agency fetish, and game world as part of their deep object residency agency at

623
00:53:24,580 --> 00:53:29,540
the computational turn led by Sapita Majidi and Razan Agarastani. You can find more information

624
00:53:29,540 --> 00:53:35,940
about this talk at foreignobjectwithak.com. Subscribe to this podcast for more analysis

625
00:53:35,940 --> 00:53:39,780
at the intersection of algorithm, subjectivity, and the arts.

