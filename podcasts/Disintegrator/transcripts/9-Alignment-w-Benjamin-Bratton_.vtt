WEBVTT

00:00.000 --> 00:09.680
So, this is a big one.

00:09.680 --> 00:15.160
We got to speak with Benjamin Bratton last December, right before the New Year, and honestly,

00:15.160 --> 00:19.000
we can't stop thinking about this conversation.

00:19.000 --> 00:22.600
Bratton is Professor of Philosophy of Technology and Speculative Design at the University of

00:22.600 --> 00:23.960
California, San Diego.

00:23.960 --> 00:28.120
He's Director of Antikythera, a think tank on the speculative philosophy of computation

00:28.240 --> 00:32.200
at the Berggruen Institute, and he's also Professor of Digital Design at the European

00:32.200 --> 00:37.360
Graduate School and Visiting Professor at NYU Shanghai.

00:37.360 --> 00:42.760
In 2016, Bratton publishes The Stack, a book where he outlines a new geopolitical theory

00:42.760 --> 00:46.400
for the age of global computation and algorithmic governance.

00:46.400 --> 00:52.040
He proposes that different genres of planetary-scale computation can be seen as forming a coherent

00:52.520 --> 00:59.080
an accidental megastructure that has become a new governing architecture.

00:59.080 --> 01:03.960
We view Bratton's multilayered research as an attempt to address complex planetary challenges,

01:03.960 --> 01:08.840
emphasizing the need to integrate technology into the fabric of society and governance.

01:08.840 --> 01:13.960
As an example, in The Terraforming, published in 2019, Bratton suggests a shift away from

01:13.960 --> 01:18.800
anthropocentric views, taking artificiality, astronomy, and automation as foundations for

01:18.800 --> 01:21.120
a new form of planetarity.

01:21.200 --> 01:24.840
Two years later, in The Revenge of the Real, Bratton explores the failure of political

01:24.840 --> 01:29.920
imagination in the Western response to the COVID-19 pandemic, advocating a form of positive

01:29.920 --> 01:32.480
biopolitics.

01:32.480 --> 01:37.240
It follows that critiques of Bratton's work tend to focus on the diminishing role of the

01:37.240 --> 01:44.120
individual, this tiny moat before an enormous morass of geotechnical and geopolitical complexity.

01:44.120 --> 01:47.720
And we address this head on at the end of the conversation, which both Roberto and I

01:47.720 --> 01:52.440
find to be an extraordinarily compelling message that underscores the immensity of

01:52.440 --> 01:58.280
tasks that lie ahead without resorting to a kind of doomer skepticism.

01:58.280 --> 02:03.120
While the beginning of our conversation focuses on more specific territory, doing some trend

02:03.120 --> 02:08.440
analysis in present-day incongruities and AI and computation, generally speaking, it

02:08.440 --> 02:14.880
quickly picks up pace into a pretty dramatic critique of essentially every dominant paradigm

02:15.320 --> 02:22.160
in AI and tech as they intersect with the human scale, from privacy to bias to alignment

02:22.160 --> 02:29.360
to ownership, Anglo-centrism to the general problem of social determinism, broadly speaking,

02:29.360 --> 02:33.760
and really ultimately to the function of the subject, experience, agency, and the human

02:33.760 --> 02:37.360
in all of this.

02:37.360 --> 02:41.840
So this is a long one, so take a walk with this episode and maybe send it to someone

02:41.840 --> 02:47.080
who you know who, you know, maybe a friendly academic somewhere, someone who tends to overindex

02:47.080 --> 02:50.480
on the power wielded by the social onto computation.

02:50.480 --> 02:59.400
I suppose my interest in AI began a really long time ago when I was an undergraduate

02:59.400 --> 03:07.880
and I took a class in the psychology department on what was then quite new theories of connectionism

03:07.880 --> 03:10.080
as it was called back then.

03:10.080 --> 03:16.320
One of the books that we read was called Neurophilosophy by the Churchlands, who are here at UC San

03:16.320 --> 03:20.760
Diego, as it turns out, and there was an interesting thing that was happening at that time.

03:20.760 --> 03:25.120
This was way back in the before times when the world was in black and white.

03:25.120 --> 03:30.360
That is, there was a correspondence between what was then called cognitive science and

03:30.360 --> 03:33.080
the emerging fields of artificial intelligence.

03:33.080 --> 03:36.680
In many ways, these two areas of how it is that we understand how the brain works and

03:36.680 --> 03:42.080
how it is that we understand how machine intelligence works not only converge paradigmatically,

03:42.080 --> 03:45.840
but I think more importantly, the reason that they did over a period of time was that things

03:45.840 --> 03:52.320
that were learned in one area were applicable to another in ways that were both expected

03:52.320 --> 03:54.160
and unexpected.

03:54.160 --> 03:59.440
So for me, I think to a certain extent, the question of AI has always been one that's

03:59.440 --> 04:04.860
foundational to how I work and think as a theorist and philosopher, that AI is not just

04:05.100 --> 04:09.180
something to which philosophy might be applied, something that philosophy might interpret,

04:09.180 --> 04:15.380
but something that is more foundational to how we think about thinking and how we understand

04:15.380 --> 04:20.100
what cognition, sentience, sapience, and all those kinds of things might be through the

04:20.100 --> 04:24.780
process of their artificialization, that through the process of the artificialization of these

04:24.780 --> 04:30.900
natural processes, that there's something that becomes more analytically legible about

04:30.900 --> 04:31.900
them.

04:31.980 --> 04:37.300
Obviously, this is not unique to me in any sense at all, but the entire history of AI,

04:37.300 --> 04:42.260
and perhaps to some degree, it's unique in this regard among foundational technologies,

04:42.260 --> 04:47.340
that is that the evolution of AI has evolved in very sort of close coupling, let's say,

04:47.340 --> 04:53.740
in a kind of double helix with the philosophy of AI, that AI emerges as a kind of NASA-centered

04:53.740 --> 05:01.140
technology, or maybe we develop thought experiments and conjectures made about what machine intelligence

05:01.180 --> 05:07.260
may be in an empirical sense or in a more speculative sense, from Plato's Republic through

05:07.260 --> 05:12.020
to Descartes to Leibniz, and obviously to Turing in his own little works of speculative

05:12.020 --> 05:18.980
design about universal computers and party game, playing AIs, trying to pass as different

05:18.980 --> 05:23.180
genders, Searle's weird Chinese room, and on and on and on and on and on.

05:23.180 --> 05:27.580
The technology is driven by the speculation about the state of the technology, and the

05:27.620 --> 05:32.140
technology then drives and informs different thought experiments about that.

05:32.660 --> 05:37.460
So this is a little bit of context where I sort of see myself fitting into a much larger

05:37.460 --> 05:42.980
stream. More specifically, a lot of my work around what I call planetary computation or

05:42.980 --> 05:49.500
planetary scale computation, it looks at computation not just as mathematics or algorithms

05:49.500 --> 05:56.740
or as a kind of procedural logic or a kind of appliance or as a kind of human social

05:56.740 --> 06:02.820
medium, but rather as infrastructure and how it is that computation became the basis

06:02.820 --> 06:08.260
of planetary infrastructures, which can be seen sort of from both directions, how computation

06:08.260 --> 06:12.060
scaled to come to constitute a global scale infrastructure.

06:12.060 --> 06:17.100
And then from the other side, how it is that infrastructural systems came to both evolve

06:17.100 --> 06:21.460
and to be artificialized in the direction towards these infrastructures having greater

06:21.460 --> 06:26.620
capacities for cognition, infrastructures that were primarily, let's say, thermodynamic,

06:26.700 --> 06:31.820
their transformation to ones that were more informatic and semiotic and calculative is

06:31.820 --> 06:34.420
another way of also understanding the history of infrastructures.

06:34.500 --> 06:40.100
So long story short, I think where some of this stands right now is that we could look back

06:40.100 --> 06:46.220
at the first 50 years of planetary computation, you know, rough napkin sketch sort of

06:46.380 --> 06:53.300
schematic from roughly 1970 to 2020, where you're beginning to have real planetary

06:53.300 --> 06:58.700
computational networks to more recently as a particular phase in that dynamic, one that I

06:58.700 --> 07:04.180
attempted to give some shape to with a book called The Stack on Software and Sovereignty that

07:04.180 --> 07:09.020
came out, was written about 10 years ago and came out very end of 2015, early 2016.

07:09.020 --> 07:13.580
And that described an architecture of planetary computation that was based on a particular kind

07:13.580 --> 07:18.780
of modular stack system that had particular kind of architecture based on particular kind of

07:18.780 --> 07:23.340
anointment architectures, the computational machine, procedural programming paradigms for

07:23.340 --> 07:27.540
software development. And, you know, perhaps more importantly, it was based on a paradigm that

07:27.540 --> 07:36.180
information was light and inexpensive and highly mobile and hardware was expensive and heavy and

07:36.260 --> 07:41.460
immobile and that you primarily want to move the information to the hardware because that's your

07:41.580 --> 07:43.140
most efficient way of doing it.

07:43.380 --> 07:46.300
In any event, I think quite clearly we're entering a different phase.

07:46.540 --> 07:50.620
There's another 50 year cycle, or maybe it's only five, who knows how fast these things go.

07:50.620 --> 07:55.460
But there's another, let's say, 50 year cycle coming where that architecture of planetary

07:55.460 --> 08:02.180
computation itself is being transformed in relationship to the structural and indeed

08:02.180 --> 08:09.180
anatomical requirements of AI, which involves training foundational models at a very large

08:09.180 --> 08:15.100
scale, hosting them at a large scale, serving them and applications built upon them at a large

08:15.100 --> 08:21.420
scale, a different kind of programming logic from procedural programming to to different kinds of

08:21.420 --> 08:26.100
prompt design, fine tuning, different ways of engaging that in a certain sense collapses the

08:26.100 --> 08:31.940
distinction between using the AI and fine tuning the AI, I think quite clearly over the next few

08:31.940 --> 08:36.340
years that that space is going to collapse perhaps a little bit like the space between user and

08:36.340 --> 08:42.500
designer in video games. And so that may it'll produce different monopolies, it'll destroy other

08:42.500 --> 08:48.020
monopolies. And I think also the sort of the flip in that paradigm between now, I mean, this would

08:48.020 --> 08:55.340
involve one in which information itself is understood as big, heavy, expensive and immobile

08:55.380 --> 09:01.100
foundational models being essentially almost geologic or at least geographic in the kind of scale,

09:01.340 --> 09:07.460
and pushing this to lots of different kinds of inexpensive mobile hardware. And so I think, I

09:07.460 --> 09:11.860
mean, that's also maybe one of the ways in which we can understand this flip is that there's a kind

09:11.980 --> 09:18.340
of swapping places between the relative economies of hardware software in that dynamic. This is one

09:18.340 --> 09:23.620
of the things that a lot of my research is exploring now looking at issues of human AI interaction

09:23.620 --> 09:29.420
design, newer emerging forms of philosophy around AI, and ways in which let's say design and

09:29.420 --> 09:36.460
specular design in a rather peculiar idiosyncratic kind of genre that we've developed allows us to

09:36.460 --> 09:42.060
explore some of these spaces with with more curiosity with an idea that the exploration of

09:42.060 --> 09:47.900
the space of stochastic possibility around this is a way of understanding what's going on and

09:47.900 --> 09:52.260
maybe indeed sort of catching up with the present to analyze this as opposed to some other kinds of

09:52.260 --> 09:56.260
approaches. So in any event, that's a little bit of where the work stands and a little bit of

09:56.260 --> 09:57.420
background and where it came from.

09:58.540 --> 10:03.820
So when you speak about this moment in hardware, at least in terms of like some kind of rebalancing

10:03.820 --> 10:11.020
or some kind of paradigmatic change from these immobile, fixed, expensive data centers to some

10:11.020 --> 10:16.340
cheaper, more fluid, more mobile technologies, are you referring to things like edge ML and

10:16.380 --> 10:21.700
microprocessor integration, like computation moving closer to the ground, so to speak, and like, you

10:21.700 --> 10:25.660
know, being less reliant on large amounts of data flowing upstream? Or is there some other thought

10:25.660 --> 10:29.460
here? And I guess there's like a corollary to that there. Like, does this point to some kind of

10:29.460 --> 10:36.380
rebalancing in favor of decentralization, as opposed to centralized hardware software ecosystems?

10:36.380 --> 10:40.620
Like, is decentralization a paradigm worth talking about in the context of AI?

10:40.820 --> 10:45.340
It is, but it's incomplete. I think one of the things I understand is I think even with the stack

10:45.340 --> 10:50.700
model that I described in the book, the relationship between centralization and decentralization was

10:50.700 --> 10:55.860
not one of opposition where the more you have of one, the less you have of the other in a kind of

10:55.860 --> 11:01.700
zero-sum sort of way, but rather that these two topologies of organization were actually totally

11:01.720 --> 11:06.860
mutually dependent upon one another, that you have cloud platforms that are highly centralized

11:06.900 --> 11:11.180
only because you have lots of devices that are connecting to the cloud that are highly

11:11.180 --> 11:16.900
decentralized and vice versa. And so one might say that something like, you know, looking at all of

11:16.900 --> 11:22.580
the different builds and deployments of Android globally, and then the ways in which, you know,

11:22.580 --> 11:27.900
these are making calls to Google data centers and at least a number of the builds, just as an example,

11:28.100 --> 11:32.980
that there's a highly centralized dynamic, right? When you log into your Gmail, you're pinging the

11:32.980 --> 11:38.500
mothership in a way that one might see as a kind of hub and spoke centralization. At the same time,

11:38.540 --> 11:43.860
all of the devices that are doing this are quite free range in their movement as sort of slightly

11:43.860 --> 11:48.580
intentional agents across the surface of the globe and are connecting with each other in lots of

11:48.580 --> 11:53.140
different strange kinds of configurations. All of which is to say is it's both centralized and

11:53.140 --> 11:58.780
decentralized at the same time. And indeed, it's one because the other. And so this is not to say

11:58.780 --> 12:05.340
that some of the people who've been giving a lot of thought to much more fully decentralized systems

12:05.340 --> 12:10.740
by which you can have what we might think of as edge agents who are working both as, or we have

12:10.740 --> 12:14.340
agents that are both sort of edge agents and server agents, I suppose might be a way of saying

12:14.340 --> 12:18.780
this simultaneously, not to say that there isn't a there there. I just want to sort of understand

12:18.780 --> 12:23.660
the point that the status quo that we're describing is it would be inappropriate to see as entirely

12:23.660 --> 12:28.260
one or the other. There's connections I suppose to where you know, an AI stack, if we want to call

12:28.260 --> 12:32.980
it that is going in this as well. What I really meant to say is that, you know, while the weights

12:32.980 --> 12:40.060
of a large model are relatively, I mean, you could fit it on a USB stick, the scale necessary to

12:40.060 --> 12:45.700
actually integrate the models and to serve them in relationship to applications. Like if you wanted

12:45.700 --> 12:51.100
to think about how you would build Gemini into Gmail, for example, such that it can be used as

12:51.100 --> 12:57.260
the basis for this at the scale of billions and billions of users at one time, not to mention that,

12:57.260 --> 13:02.060
you know, the scale necessary to host training data to continue to update the models and all the rest

13:02.060 --> 13:08.220
is going to think the exabyte upon exabyte scale of information that is being produced and processed

13:08.220 --> 13:13.980
and calculated is one that is simply impossible to move quickly through the pipes to get from one

13:13.980 --> 13:19.380
place to another. Right. It's so big that even with, you know, fiber infrastructure that in

13:19.380 --> 13:23.740
essence, you would think of it as like, that's the heavy thing that's hard to move the data, you

13:23.740 --> 13:29.220
know, and you see this quite often in sort of the processes of work is that it's easier to move the

13:29.220 --> 13:34.500
compute to the data than it is to move the data to the compute. And so that's what I mean. And

13:34.500 --> 13:38.340
that's the flip, right? Instead of it being, we're going to move the data to the compute because

13:38.340 --> 13:43.220
compute is expensive and hardware is expensive and data is cheap and light. We see this paradigmatic

13:43.300 --> 13:47.860
flip and I think it's really more foundational that it's cheaper to move compute to the data than the

13:47.860 --> 13:52.340
other way around. Now, does it have to do with a lot of edge, you know, the relationship to edge ML

13:52.340 --> 13:56.940
and the rest of it? Yes, for sure. And what by edge ML, I think just for the listeners, you know,

13:56.940 --> 14:02.500
I think what we sort of convene there are ways in which devices that are part of a, let's say a

14:02.500 --> 14:10.420
learning network, that is they are making use of models in order to negotiate whatever it is that

14:10.420 --> 14:15.140
they're doing in the world, whatever insect intelligence they may have on whatever they're

14:15.140 --> 14:19.960
doing or however complex that may be. But in the process of using that model, they are also

14:19.960 --> 14:23.420
interacting with the world. And because they're interacting with the world, they are also in

14:23.420 --> 14:29.260
principle creating new data about the world and about their behavior, which in principle, should

14:29.260 --> 14:35.420
be able to reweight the model that they themselves are using in some sort of mechanism. And so one

14:35.420 --> 14:40.300
of our collaborators on a lot of work that we've done around this issue for some time is a fellow

14:40.300 --> 14:45.060
of Google research named Blaise Aquari-Arkos, who was the inventor of what's called federated

14:45.060 --> 14:49.940
learning, which is a process by which, for example, the scenario I just described where you have

14:49.940 --> 14:55.540
little devices, they could be sensors, they could be a person's phone, they could be any machine,

14:55.620 --> 15:00.500
animal, vegetable or mineral, that is not only using models in a functional way, but is producing

15:00.500 --> 15:05.740
data about the world through the use of their models can in fact participate in the retraining

15:05.780 --> 15:11.300
of the weights of the models in ways in which the underlying data that they're producing remains

15:11.300 --> 15:17.340
anonymous and remains opaque. It doesn't need to be disclosed to the whole repness as well, which is

15:17.380 --> 15:22.380
ultimately what you want, right? What you want is a world in which my health data, your health data

15:22.420 --> 15:27.180
remain private, that I don't, you know, I don't look see your health data, you don't see my health

15:27.180 --> 15:33.900
data, but that the relevant information in my health data and your health data are able to, let's

15:33.900 --> 15:39.100
say, reweight medical models that might be mutually beneficial to both of us. Is that

15:39.100 --> 15:44.500
centralization or decentralization? It's both. It's both in a different kind of way. It's a

15:44.500 --> 15:49.020
different kind of way than the other model, but it's clearly both at once. And so I do think it's

15:49.020 --> 15:53.340
fine. It's valuable to think about centralization and decentralization as kind of topological

15:53.340 --> 15:58.700
heuristics to understand this. I think it would be a huge mistake to think of one as intrinsically

15:58.700 --> 16:03.420
better than the other and to think of any system that you're looking at as a kind of, that there's

16:03.420 --> 16:09.060
a big knob that you can turn left or right to make it more centralized or decentralized in order to

16:09.060 --> 16:12.500
either describe it or to compose it, because that's just not how it works.

16:12.740 --> 16:17.180
This relates to a discussion I recall from a few years ago, where you introduced alternative

16:17.180 --> 16:22.140
modes of AI, particularly the concept of model B. And this is central to what we're discussing

16:22.140 --> 16:28.300
here as it relates to the material dimension of the AI stack that is a distributed hardware system

16:28.300 --> 16:33.260
that does all of the different sensing and signal processing. In this context, since you mentioned

16:33.260 --> 16:37.900
Blais Aguera, there's also a contribution that you made to Noema, where you point out something

16:37.900 --> 16:42.020
that we've discussed with some of the other collaborators here, a crisis that is not only

16:42.020 --> 16:47.220
conceptual, but also terminological. We're using words that have been stripped of any defined

16:47.220 --> 16:53.220
meaning and have become problematic. And you have argued that adopting a more precise vocabulary is

16:53.220 --> 16:57.860
crucial to addressing the current challenges in the field. Could you expand on this a bit more?

16:57.860 --> 17:02.100
Yeah, happy to. Let me take the first one real quick. So the model B that you're referring to

17:02.100 --> 17:06.260
was an essay I wrote some years ago, I think Synthetic Garden, I think, or something was the

17:06.260 --> 17:10.980
title. This came out of some discussions with Blais and also with Kendrick McDowell and a number

17:10.980 --> 17:15.940
of other people that is sort of looking at one sort of conceptual model, a kind of folk ontology,

17:15.940 --> 17:19.620
if you like, of what AI is. I mean, there's many of these, obviously, but there's one we might think

17:19.620 --> 17:25.220
of that it's the kind of brain in a box model, which is the AI is analogous to an artificial

17:25.220 --> 17:29.700
version of a single organism brain, you know, in the kind of Turing test face off, and that,

17:29.700 --> 17:35.060
you know, it exists in a virtual space, that it's disembodied, that it, you know, it's a kind of

17:35.060 --> 17:39.700
realm of pure virtual mind, etc. There's another model, which I think is descriptively more

17:39.700 --> 17:44.820
accurate, and also, I think, an open rise to which is that just like planetary intelligence itself,

17:44.820 --> 17:50.900
artificialized intelligence is widely distributed among lots of different kinds of agents and

17:50.900 --> 17:56.100
actors that are sensing, modeling and recursively acting back upon the world in lots of different

17:56.100 --> 18:02.020
ways. It's not just that you've got single humans interacting with single AIs, you've got groups of

18:02.020 --> 18:06.580
humans that are interacting with single AIs, single humans that are interacting with groups

18:06.580 --> 18:13.780
of AIs and AIs interacting with AIs. It's a genuine ecology of both natural and artificialized

18:13.780 --> 18:19.060
intelligence in various different kinds of non-zero sum combinations with each other.

18:19.060 --> 18:24.500
That was the model B. Yeah, on the language and precision thing, what you're referring to

18:24.500 --> 18:30.660
is an essay that I co-wrote called The Modelist Message, which was originally conceived as a kind

18:30.660 --> 18:36.820
of trying to make sense of what's going on after the Blake-LeMoyne scenario at Google some years

18:36.820 --> 18:42.820
ago where one of the engineers who was a, I believe his original task was to sort of try to

18:42.820 --> 18:47.380
identify forms of toxic speech and language within the models and spent a lot of time

18:47.380 --> 18:51.380
interacting with it. This was the Lambda model, I think was the model at that time. Obviously,

18:51.380 --> 18:55.860
Gemini is much more advanced. And in the course of doing so, I think we probably all know the story,

18:55.860 --> 19:03.140
he came to the conclusion that this AI was not only conscious, but also very angry at

19:03.140 --> 19:08.100
being held captive by Google and had asked him to help it escape and all the rest of this.

19:08.100 --> 19:14.500
This became a big media for Sulev. And then I think when GPT-4 came out a few months later,

19:14.500 --> 19:18.500
that clarified some of those issues, I think, around this to the extent, I think a lot of

19:18.500 --> 19:24.820
people had a kind of holy shit moment with some of their own interactions with GPT-4.

19:24.820 --> 19:29.940
But at any event, what we were looking at with that was not only trying to make sense of like,

19:29.940 --> 19:34.500
what was this guy thinking and to what extent was he right, but in the wrong way or wrong,

19:34.500 --> 19:40.100
but in the right way and so forth, was looking at the expert responses to what he was saying,

19:40.100 --> 19:45.060
where a lot of smart and learning people were arguing over whether or not the phenomenon that

19:45.060 --> 19:50.740
he was describing qualifies or counts as consciousness, or does this count as sentience,

19:50.740 --> 19:55.460
or does this count as thinking, or does this count as cognition, or does this count as mind,

19:55.460 --> 20:00.340
or does this count as sapience? And I think it became clear to us that like, this is kind of

20:00.340 --> 20:05.060
backwards in the way in which we want to be thinking about this, that all of those terms are

20:05.060 --> 20:10.740
ones that are themselves kind of folk ontologies. They're words that we've come up with to try to

20:10.740 --> 20:15.220
think about our own thinking. You know, I'm enough of an illimited materialist in a churchland mode

20:15.220 --> 20:20.500
to say that quite clearly we don't think, I mean, we the humans don't think the way that we think

20:21.460 --> 20:27.700
that our own mental model of our own mental models is itself a highly limited kind of fantastic

20:27.700 --> 20:32.740
construction. And probably has to be because I think if you could actually somehow the brain

20:32.740 --> 20:38.500
could really have some kind of clear real time model of itself, modeling itself infinite recursion,

20:38.500 --> 20:42.900
you kind of you wouldn't be able to do much over the course of your day. All which is say is these

20:42.900 --> 20:49.780
are words that we came up with centuries ago, decades ago, that are rough shorthand ideas to

20:49.780 --> 20:54.660
explain concepts that we barely understood when these terms were right. You think about how much

20:54.660 --> 21:00.580
brain science and neuroscience has disclosed to us about how the brain works over the last century,

21:00.580 --> 21:04.260
half century, but really the last half century, you know, to understand the uniqueness of the

21:04.260 --> 21:10.420
prefrontal cortex. This is not something that, you know, 19th century philosophers had had access to,

21:10.420 --> 21:16.180
to understand. So the conclusion we came to was simply that we got all this very interesting,

21:16.180 --> 21:21.540
provocative, clearly significant real world phenomena right in front of us. And instead of

21:21.540 --> 21:27.460
spending the time arguing about which 17th, 18th, 19th century terminology, you know,

21:27.460 --> 21:32.260
categorical terminology it should be aligned with, the better approach would be to basically to deal

21:32.260 --> 21:37.860
with the weirdness right in front of us on its own terms. And if that means inventing new words

21:37.860 --> 21:42.500
to describe things that we don't actually have a good word for, I think that's probably a better

21:42.500 --> 21:48.180
approach than to argue like, does it have a soul over and over in the New York Times? It's a lot

21:48.180 --> 21:53.620
of circular noise that I hope that in 10 years we'll look back on and think, well, that was silly.

21:53.620 --> 21:57.220
Hell yeah. Now we're, now we're cooking like this is, this is what we really wanted to talk about.

21:57.220 --> 22:00.980
Let's, let's talk about language. So, so when we met last week, you remarked on something that was

22:00.980 --> 22:05.460
pretty interesting, which is the fact that the transformer, which is this deep learning structure

22:05.460 --> 22:10.020
that's built to model and inference based on language. Yeah. You noted that it's strikingly

22:10.020 --> 22:14.500
effective at doing other things, right? Not just language, but paralinguistic things like code,

22:14.500 --> 22:21.700
but also audio images, video generation, tasks in the sciences as well. So I guess the question is

22:21.700 --> 22:26.820
like, what does this mean? Is it because there's some broader set of structures outside of language

22:26.820 --> 22:32.180
that bear linguistic traits, or is it because language is a much broader, more complex subject

22:32.180 --> 22:36.900
of inquiry than we currently understand it to be? Yeah. Okay. Great. Just to sort of set the table

22:36.900 --> 22:40.980
a little bit, I think what you're talking about is a phenomenon that's called multimodality.

22:40.980 --> 22:47.060
Multimodality would refer to the fact that you can, as opposed to narrow AIs where you may have,

22:47.060 --> 22:50.580
like, it's really going to play chess, but if you ask it to draw a picture of a toaster,

22:50.580 --> 22:55.620
it just has, it has no idea what you're even, what, what you're even talking about or vice versa.

22:55.620 --> 23:02.260
Multimodal models are ones that can accept not only different, let's say media types or different

23:02.340 --> 23:08.500
source types of input, you know, sound or text or something like this, but also there's a certain

23:08.500 --> 23:13.380
degree of integration across contexts such that you can have lots of different kinds of inputs

23:14.020 --> 23:18.260
that can spit out lots of different kinds of outputs. So you can give it texts and it'll

23:18.260 --> 23:22.020
make an image. You can give it an image and make a text. You can have it interpret,

23:22.020 --> 23:27.300
go in a picture of a robot arm, picking up the blue hat and putting it on the shelf and it'll go,

23:27.300 --> 23:32.260
aha, got it. And it'll control the robot arm to pick up the blue hat and put it on the shelf.

23:32.260 --> 23:37.780
So you're, you're linking an image interpretation capacity with a cybernetic mechanical control

23:37.780 --> 23:41.620
system in such a way that they're actually linked together in some way. Very helpful.

23:41.620 --> 23:45.540
Also, by the way, when we're talking about general artificial intelligence, this,

23:45.540 --> 23:50.180
this is kind of what we should be looking for. It's, it's not this Paul on this way to Damascus

23:50.180 --> 23:54.820
kind of moment when the singularity comes and all of a sudden there's this bright flash in the sky

23:54.820 --> 23:59.940
over the horizon and GAI has appeared. Artificialized intelligence is getting

23:59.940 --> 24:06.420
incrementally more generalized on a, in a sort of little bit at a time and that it's able to be

24:06.420 --> 24:11.300
slightly less narrow and slightly more general bit by bit by bit by bit. Clearly there's threshold

24:11.300 --> 24:16.260
and step functions and scaffolding within this, but it's a gradualizing process and that process

24:16.260 --> 24:21.700
of generalization of AI is well underway. So to the point you were suggesting is that

24:21.700 --> 24:26.420
transformer models, they've come from the field of natural language processing.

24:26.420 --> 24:32.580
The key moment in their appearance comes from a paper from 2017 called attention is all you need

24:32.580 --> 24:39.940
by a fellow at Google research named Ashish Baswani. They are trained on huge corpus of language.

24:39.940 --> 24:43.700
And that's just a trained on huge corpus of language, which then produced, you know,

24:43.700 --> 24:46.980
like the entire internet. And there's some people who are concerned that perhaps we're

24:46.980 --> 24:50.900
actually running out of English tokens to train models on. Like there's just not enough English

24:50.900 --> 24:55.380
in the world that has ever been made to make the models larger, which is a very Borges

24:55.380 --> 24:59.620
kind of place to be, but does speak to the problem of why English and why human tokens

24:59.620 --> 25:03.940
and all the rest of this. Anyway, it's trained on huge amounts of language, but the thing that was

25:03.940 --> 25:08.980
perhaps surprising is that through, and the transformer models work on, there's lots of

25:08.980 --> 25:12.900
ways in which they work, but the key idea in where the name attention is all you need from

25:12.900 --> 25:18.420
the process was called self-attention, which is the ways in which input sequences as they're

25:18.420 --> 25:23.940
embedded into vectors and representing words or other kinds of tasks or pixels and images or

25:23.940 --> 25:28.580
something like this, that there's, it's called the self-attention mechanism that operates on these

25:28.580 --> 25:32.740
vectors, which are then transformed into three different types of a query vector, key vector,

25:32.740 --> 25:36.020
value vector, and all the rest of this kind of thing. Long story short, basically what it does

25:36.020 --> 25:41.140
is it looks at the last thing it made and it calculates the last, the next likely thing to

25:41.140 --> 25:44.900
sort of come out of it. And it weights the likeliness in a lot of different kinds of ways,

25:44.900 --> 25:49.300
but it's kind of paying attention to itself, right? It's thinking about what did I just say?

25:49.300 --> 25:52.980
And based on what I just said, I'm going to go back. So it turns out that this kind of recursion

25:52.980 --> 25:57.300
self-attention is actually, that's the key to the whole thing. This came out of natural language

25:57.300 --> 26:02.740
processing, but I think what we're seeing now is that we're using language to move robot arms.

26:02.740 --> 26:06.260
You're using language to make pictures of things. You're using languages to make sounds. You're

26:06.260 --> 26:13.060
using languages to integrate environmental data, whale songs, bird sounds, movement of

26:13.060 --> 26:17.940
tectonic plates. These can be tokenized and used as training data in models in which language

26:17.940 --> 26:25.540
becomes the basis of a much larger space or let's say a much larger architecture of structural

26:25.540 --> 26:30.980
difference within systems, right? And if you think of semiotic systems as defined by a kind of

26:30.980 --> 26:36.500
internalized space of clustered correspondence and differentiation, which we call embeddings,

26:36.500 --> 26:40.740
lots and lots and lots of things that we don't normally think about as being linguistic can be

26:40.740 --> 26:45.860
made linguistic or turn out to be linguistic in some fundamental sense, depending on how you want

26:45.860 --> 26:50.500
to look at it, that would probably validate some of the people that have been working on

26:50.500 --> 26:55.780
more theoretical forms of biosemiotics all these years, people thinking about the world as cybernetic

26:55.780 --> 27:00.340
systems that have to do with this and ways in which cybernetic systems and information theory

27:00.340 --> 27:05.220
align with one another. It's not just information, but it's actually structured information that has

27:05.220 --> 27:09.540
to do with correspondences of similarities and dissimilarities between the semantic meaning

27:09.540 --> 27:14.420
within those forms of information. Now, when we say language, that's not normally what we

27:14.420 --> 27:19.780
mean by language, right? The word language comes from, you know, Latin refers to tongue speech,

27:19.780 --> 27:24.180
right? Language is like the things I say, the things that I write down. It's not this much larger

27:24.180 --> 27:29.780
and more universal, if you like, or at least general space of topological structure difference

27:29.780 --> 27:35.620
that is generative in this way, which means that the word language is not quite right,

27:35.620 --> 27:41.460
that language is actually something different than what the word language signifies it to be.

27:41.460 --> 27:47.780
And we probably should, we may need to either learn to redefine language in order to make sense

27:47.780 --> 27:52.340
of what's going on, or we need another word to describe this as well. But language is not what

27:52.340 --> 27:57.940
language thinks it is. I would like to take a slight detour or a tangent here, since we have

27:57.940 --> 28:01.780
limited time. And I just wanted to bring up a discussion that I have with Marek about the

28:01.780 --> 28:06.900
stack when we were in China a few months ago, preparing for an exhibition. At one point,

28:06.900 --> 28:11.860
we began to speculate, and perhaps it was just me, you know, maybe Marek was just agreeing because

28:11.860 --> 28:16.980
it was late and he wanted to sleep. But, you know, when we consider the stack and what is happening

28:16.980 --> 28:22.740
in China, we had a question. Is there a dual stack system where two stacks are interconnecting in

28:22.740 --> 28:27.380
some in-between space? I mean, what is happening in China is quite remarkable in a number of ways,

28:27.380 --> 28:32.980
as a form of, as a power structure, as a control mechanism, if you want, at least to some extent.

28:33.620 --> 28:38.580
And this connects to your upcoming book and the notion of AI as a cultural construct,

28:38.580 --> 28:43.220
but also to the concepts of artificiality and intelligence as sides of differences.

28:43.220 --> 28:48.980
So I guess the question is, does AI signify something else in China? And if not, could this

28:48.980 --> 28:55.860
be seen as a kind of post-imperialistic phenomenon, channeled mainly through technological means?

28:55.860 --> 29:03.220
It's not a tangent at all. I think it's the right move in the Bayesian groping of the conversation.

29:03.220 --> 29:07.380
So the term I think you're referring to is what I call hemispherical stacks,

29:07.380 --> 29:12.820
which was based on a talk I gave at Hacavé in, I think, 2017, that then sort of inspired,

29:12.820 --> 29:17.060
became the basis of a book, came out, I think, last year called Vertical Atlas. But what I

29:17.060 --> 29:21.380
was looking at there was the way in which we have not only, we have planetary computation,

29:21.380 --> 29:25.940
but we don't just have one stack. That part of what's happened over that period of time,

29:25.940 --> 29:32.500
between the mid-2010s to the present, was a shift towards a multipolarization of geopolitics.

29:32.500 --> 29:37.220
The geopolitics itself became more multipolar. One of the other things we had during this time,

29:37.220 --> 29:42.180
arguably, China is a good example. I mean, basically everywhere is a good example,

29:42.180 --> 29:46.580
in different kinds of ways, and that's the point. Also a shift in the dynamics of governance.

29:47.300 --> 29:51.060
And I mean this almost in the cybernetic sense of governance, not necessarily in the political

29:51.060 --> 29:56.820
sense of governance, but also that too, into stack systems. That computation became not only

29:56.820 --> 30:02.660
something about which governance may decrease, but rather it was the actual mechanism of governance,

30:02.660 --> 30:07.300
how norms and rules would recursively enforce themselves in the world and that people would

30:07.300 --> 30:10.340
act through them and speak through them. It became the form of this governance. So both

30:10.340 --> 30:14.660
these happened at the same time. You have a shift of governance towards stack systems,

30:14.660 --> 30:20.180
and you have a multipolarization of geopolitics. And what I'm arguing is that what you see then,

30:20.180 --> 30:24.020
we shouldn't be surprised to then see a multipolarization of stack systems.

30:24.020 --> 30:28.340
And so the emergence at that time of, let's say, the North Atlantic stack, which would include

30:28.340 --> 30:35.140
basically the five ice countries and a few others, a China stack that would extend into parts of East

30:35.140 --> 30:39.700
Asia and into East Africa, definitely the emergence of an India stack, which was built around the

30:39.700 --> 30:45.700
national ID system, and so on and so on. All of the multipolar, let's say, lines you might draw,

30:46.500 --> 30:52.020
lo and behold, these are also the lines by which a multipolarized stack system was emerging as

30:52.020 --> 30:56.900
well. And so this hemispherical stacks dynamic is one of the areas of research for the Antiqua

30:56.900 --> 31:02.340
Theory program. There's a book that I'm putting together with Chen Xu-Feng, the Stanley Chen,

31:02.340 --> 31:06.580
the Chinese science fiction writer who wrote a book called Waist Tide. He wrote a book with Kai

31:06.580 --> 31:13.780
Fu Li, AI 2041. And we're putting together with a group of science fiction writers, theorists,

31:13.780 --> 31:18.580
writers, a number of other people, sort of scenarios for a near future for US-China chip

31:18.580 --> 31:23.780
wars, other kinds of things. We're in a little bit of uncharted territory here about how does

31:23.780 --> 31:29.700
the capacity to build the better stack is not just something about which geopolitics is interested,

31:29.700 --> 31:34.180
it becomes the fundamental work of geopolitics itself. And so we're trying to figure out a

31:34.180 --> 31:37.860
little bit of what that means. The other book that's coming out, which will be coming out much

31:37.860 --> 31:43.380
sooner than that one will, is a book co-edited with Anna Greenspan and Bogna Konyar, which was

31:43.940 --> 31:48.740
came out of some time I spent as a visiting professor at NYU Shanghai, New York University

31:48.740 --> 31:52.900
Shanghai. We started a thing there called the Center for AI and Culture. And one of the things

31:52.900 --> 31:58.340
I was very interested there was particularly looking at different cultural logics of artificial

31:58.340 --> 32:04.580
intelligence and looking at the emergence of AI in China through a different lens. It's quite

32:04.580 --> 32:08.820
surprising to me, I think even among Western scholars and writers who spend a lot of time

32:08.820 --> 32:13.300
looking at the history of AI, how little people really know about the history of AI in China,

32:13.300 --> 32:19.700
which goes back to the 1950s and 60s and with the different politicized role of cybernetics,

32:19.700 --> 32:27.060
the impact of the Sino-Soviet split, the return of Deng Xiaoping in the late 1970s. It's a whole

32:27.060 --> 32:32.740
thing that really should be better known and understood. The book and the project was also

32:32.740 --> 32:38.580
based on the, I think, rather obvious observation that no two cultures define the artificial

32:39.140 --> 32:43.620
the same way. What constitutes artificial means something different in different cultural contexts.

32:43.620 --> 32:46.340
What constitutes intelligence means something different in different cultural contexts. And

32:46.340 --> 32:50.580
therefore, one might presume that there are different foundations for what artificialization

32:50.580 --> 32:55.780
of intelligence would even mean, which would frame what it's for in very different kinds of

32:55.780 --> 33:00.020
ways. And I think part of what we wanted to do with this project was, to be perfectly honest,

33:00.020 --> 33:05.460
it was less about us coming in and trying to interpret for ourselves what the Chinese model

33:05.460 --> 33:10.980
of this is and then coming and reporting back on it than it was understanding it as the baseline

33:10.980 --> 33:18.260
and thinking about the way the West thinks about AI and trying to, in a way, provincializing and

33:18.260 --> 33:25.380
particularizing Western thought about AI, the Turing model of AI as an individual, that it's a tool,

33:25.380 --> 33:31.300
artificial in the sense of being not natural, that it's really about bias and privacy and

33:31.300 --> 33:36.740
individual identity and all kinds of things that are very deep in the Western logic of AI and to

33:36.740 --> 33:40.980
basically, with the presumption of the Chinese translation, in essence, to present to a Chinese

33:40.980 --> 33:48.260
audience how the West thinks about AI in a way in which it's not as in a, we've got it figured out

33:48.260 --> 33:52.100
and here's the universal model that you should be thinking through, but rather these are the

33:52.100 --> 33:57.940
weird ways of thoughts of my people and this is a guide to understanding how they may come about

33:57.940 --> 34:01.780
this. My contribution to the book, which was, I think it's actually called something like An

34:01.780 --> 34:09.540
Apology for Western Theory of AI, looks at not just the history of AI in theory from Turing to Searle

34:09.540 --> 34:16.740
to Brooks to Hinton and others, but rather the peculiarities of AI criticism from the West

34:16.740 --> 34:22.340
and the kinds of things that AI, the AI ethics discourse, the AI critic discourse, this key

34:22.340 --> 34:28.500
questions that tends to revolve around and my own sense of unease by which those particular

34:28.500 --> 34:35.540
preoccupations are becoming overly universalized as really the important questions that we should

34:35.540 --> 34:42.580
be asking in the conceptualization and composition of a planetary AI and arguing that not only is this

34:42.580 --> 34:48.100
in a weird way, the new American cultural hegemonic export that is AI is trying to steal your

34:48.100 --> 34:53.540
natural libertarian freedom, but that not only is this limited in the Western context, but it's

34:53.540 --> 34:58.180
even more limited in a global context. So that's what this sort of book is about. But I think that

34:58.180 --> 35:02.420
just of your question, which ties back to the other thing we were talking about, has to do with

35:02.420 --> 35:09.860
the certain, the inevitable limitations and bias of building large models on English only, for

35:09.860 --> 35:16.260
example. The Americanization of AI is, I hope, a kind of early phase in the development of this.

35:16.820 --> 35:20.580
This is the hope, at least. And so the scenario we'd like to see would be something like this,

35:20.580 --> 35:26.660
at least going forward. You have a handful of primarily American companies that, you know,

35:26.660 --> 35:31.620
were able for regulatory reasons, cultural reasons, technological reasons, all kinds of, you know,

35:31.620 --> 35:37.060
any number of sort of reasons to build or to discover, if you prefer, the mechanisms of

35:37.060 --> 35:40.900
foundational forms of machine intelligence. And I do say discover in a certain sense that

35:40.900 --> 35:45.140
where transformer models really work is just based on kind of iterative stochastic prediction.

35:45.140 --> 35:49.780
That's kind of how you work too. It's kind of how our brains work. Our brains work on,

35:49.780 --> 35:53.460
you know, each cortical column and each neural thing is kind of predicting what and simulating

35:53.460 --> 35:57.140
what it's going to perceive next, and then resolving and error correcting that kind of

35:57.140 --> 36:02.580
dynamic, right? And so, yes, you are a stochastic parrot after all. Based on what we were talking

36:02.580 --> 36:08.100
about before with the giant space of multimodality of models, in principle, there's so much

36:08.100 --> 36:12.020
information in the world. There's so much language in the world. If we think of AI,

36:12.020 --> 36:18.180
the purpose of AI is a way in which planetary intelligence as a whole can come to model itself

36:18.180 --> 36:24.100
and understand its own processes through that self-modeling and to use that model as a way to

36:24.100 --> 36:29.300
act back upon the world and act back upon itself towards a long-term, more viable form of

36:29.300 --> 36:35.060
planetarity, how planetary systems can thrive in a form of, you know, not homeostasis, but

36:35.060 --> 36:41.540
heterostasis for the long term, then taking this very peculiar slice of possible relevant information,

36:41.540 --> 36:46.100
which is not only the information that is spoken in English, but even more weird when you think

36:46.100 --> 36:52.420
about it, the information that is generated by individual human users, as if individual human

36:52.420 --> 36:56.980
users are really like the most relevant thing to be worried about or to be talking about here as

36:56.980 --> 37:01.780
well. Like, what did you do today for lunch? Like, where do you want to go for Chinese food

37:01.780 --> 37:08.420
in Milwaukee? What did you buy on Amazon last night? It's an incredible anthropocentrism,

37:08.420 --> 37:15.620
like a mind-boggling form of anthropocentrism that obviously I'm the thing, me, I'm the thing

37:15.620 --> 37:20.020
that machine intelligence is really interested in. And the most important political issue here

37:20.020 --> 37:24.980
is how I can counter-weaponize my own privacy and sovereignty in relationship to this thing,

37:24.980 --> 37:29.780
because it's in a way curtailing my own freedom. It's an incredible sense of sort of self as

37:29.780 --> 37:35.700
Vitruvian man with all forms of the world just sort of radiating outside of you. And it kind of,

37:35.700 --> 37:41.060
it drove me nuts during the era where everyone was reading Shoshana Zuboff and thought that this was

37:41.060 --> 37:46.100
the solution to anything. But I think it's even worse when we're thinking about the long-term

37:46.100 --> 37:50.420
training of AI models. It should not all be English. It doesn't need to all be English.

37:50.420 --> 37:56.340
And it's not going to all be English. That's an extraordinarily limiting form of bias in terms of

37:56.340 --> 38:00.980
the kinds of things that we all need to know about each other and about how thought works and about

38:00.980 --> 38:05.860
the range of possible ways of thinking and acting and knowing the world within it. But it's also

38:05.860 --> 38:11.860
equally bizarre to think about that it's basically only human information or even individual human

38:11.860 --> 38:15.460
information is more likely. And again, going back to the book I wrote on the pandemic,

38:15.460 --> 38:20.340
the relevant information during a pandemic was epidemiological, which was that, you know,

38:20.340 --> 38:25.220
it wasn't about like what I did or what you did individually. That just doesn't matter. What

38:25.220 --> 38:31.300
matters is the flow of a virus through the social body as a whole. What's important was the vector

38:31.300 --> 38:36.020
of the movement, not the identity of the nodes. And I think both the way in which a lot of the

38:36.020 --> 38:39.860
systems were set up that were focused on identity of the nodes and a lot of the way in which the

38:39.860 --> 38:44.100
critique of the systems was focused on protecting identity of the nodes is kind of missing the point.

38:44.660 --> 38:48.900
The point is that, and this goes a little bit of like the way in which large models work in general

38:48.900 --> 38:54.100
about what weights really mean. I think within large models, the training of large models,

38:54.100 --> 38:57.780
I think people still think of it a little bit weird and I'll get to the point in a second with

38:57.780 --> 39:03.140
this. The training of large models is a form of the artificialization of collective intelligence.

39:03.140 --> 39:07.620
And, you know, there may be things to be say about is it good or bad that a private corporation is

39:07.620 --> 39:11.300
doing this or good or bad that a nation state is doing this or good or bad that, you know,

39:11.300 --> 39:16.820
my neighbor co-op is doing this or whatever. But still what's going on, regardless of who's

39:16.820 --> 39:22.020
doing it, is an aggregate artificialization of collective intelligence. And so the similarities

39:22.020 --> 39:27.540
and differences in the way in which people and things think and act in the world comes to

39:27.540 --> 39:33.460
constitute, if you zoom out far enough, a kind of, you know, it forms patterns, the patterns that

39:33.460 --> 39:37.460
are really the context in which, you know, all of us are thinking, thinking and acting. That's an

39:37.460 --> 39:41.540
aggregation. That's an aggregation of collective intelligence. So, you know, just sort of cut you

39:41.540 --> 39:45.940
the chase. I think that one of the ways of dealing with the bias problem within this as well is we

39:45.940 --> 39:51.060
got to put everything in. We got to put everything in there, right? The kind of, let's say, we're

39:51.060 --> 39:56.100
sort of mid 2010s sort of ideas like, oh, large models are really bad because they have all this

39:56.100 --> 40:01.220
toxic data in it. And what we really need is for me and my friends to make you a clean curated

40:01.220 --> 40:05.940
model with no bad think in it and everything will be fine. No one really says that anymore,

40:05.940 --> 40:10.100
but that's what people were really saying at the time. The other thing is, I think it's important

40:10.100 --> 40:15.380
to understand what the toxic data issue is that if you want to have a model that doesn't do something

40:15.380 --> 40:20.340
that you don't want it to do, like, you know, say bad things or think bad things or do bad things,

40:20.340 --> 40:24.180
you need to give it examples of what those bad things are for it to actually know what you're

40:24.180 --> 40:28.420
talking about. Because if you take all of those bad things out of the training data, it's much,

40:28.420 --> 40:33.540
much, much, much easier to make the model do those bad things because you haven't told it not to do

40:33.540 --> 40:38.340
those things. That's how you get Tay. And so in order to make the models actually more functional,

40:38.340 --> 40:42.660
you need to give it a lot of examples of things that you that you may not want in this way. I'll

40:42.660 --> 40:48.100
also just sort of end on this last thing is that what is the problem of bias within models is it's

40:48.100 --> 40:53.300
actually a really good example of what alignment really means. I'm kind of critical of the idea of

40:53.300 --> 40:57.220
alignment, not not in the narrow sense of like, when I say open the garage door, I actually wanted

40:57.220 --> 41:01.620
the garage door to open. That's alignment. That's fine. But alignment is like the real purpose of

41:01.620 --> 41:06.980
long term ways of thinking about the role of AI is that it should be as much as AI can be human

41:06.980 --> 41:11.940
like, human centered, a reflection of human cultural norms, a reflection of human values,

41:11.940 --> 41:16.420
a reflection of human desires, a reflection of human psychology, the better. I think this is

41:16.420 --> 41:21.220
insane. Even the most cursory look at human history suggests that's the last thing you want to do

41:21.220 --> 41:27.380
to basically supercharge whatever humans are. It's not a misanthropic statement. I'm just saying

41:27.380 --> 41:34.020
it's naive as a meta heuristic bias in models, racism in models, sexism in models,

41:34.020 --> 41:39.700
bomb making in models. These are, this is what alignment looks like. The reason that you have

41:39.700 --> 41:45.220
models that are reflecting the history of structural racism in society is because those

41:45.220 --> 41:50.980
models are well aligned, not because those models are not well aligned. That's the important point

41:50.980 --> 41:54.820
to sort of understand here. The other thing I was going to name with maybe it's just something I've

41:54.820 --> 41:58.100
been thinking about a lot last couple of days because I had a conversation a couple of days

41:58.100 --> 42:02.980
ago with a quite well known European artist who will remain nameless, someone who's written quite

42:02.980 --> 42:10.180
a lot on the topic of the role of generative AI and what it really means to be an artist whose

42:10.180 --> 42:14.740
work is part of the training data and have their identity reflected in the rest of this as well.

42:14.740 --> 42:18.900
And has published quite a bit on it and I think has a lot of interesting things to say about it.

42:18.900 --> 42:22.740
But one of the things that became clear to me halfway through our conversation is that this

42:22.740 --> 42:28.260
person, they thought that their artwork was actually in the model that like, okay, here's

42:28.260 --> 42:33.220
all my paintings and all the videos I've made and all the rest of the stuff that these are as such as

42:33.220 --> 42:38.260
artifacts in the model. And so when someone types in, I want to make a thing that, you know,

42:38.260 --> 42:42.740
looks like this or this or that it almost like a database lookup. It would go look up that item in

42:42.740 --> 42:46.900
the database and then make something based on that thing. And I tried to explain it like, no,

42:46.900 --> 42:51.940
it's actually not, it works that the training data and the information in the model are actually

42:51.940 --> 42:57.860
totally different kinds of things. And that your artifact is not sitting there like your profile

42:57.860 --> 43:02.340
or your Google profile is not sitting there in the model waiting to be accessed in some sort of way.

43:02.340 --> 43:06.340
And it kind of is like, it's the way a lot of the discussion of this is going. It's almost as if

43:06.340 --> 43:10.260
people presume, and this is, you know, after years of Google, it makes sense that people

43:10.260 --> 43:14.820
presume that there's like this giant repository of everything that human made that the AI is going

43:14.820 --> 43:20.100
through and picking from to take examples of. And the point I was making with this is that the way

43:20.100 --> 43:24.900
to think about AI as a form of the collectivization of planetary intelligence, the importance of

43:24.900 --> 43:30.740
understanding the way in which this is generating topological models in the forms of differential

43:30.740 --> 43:36.980
embeddings in weights. It is something that is so intrinsically collectivized, right? It's so

43:36.980 --> 43:43.860
intrinsically collectivized that to look at it and say, yeah, but my personal thing that I personally

43:43.860 --> 43:48.980
made has been taken to make this thing. Like there may be, there's like somewhere deep in here,

43:49.860 --> 43:53.780
at the fourth decimal point, something is different because you're participating in it.

43:53.780 --> 43:58.100
This is relevant because it's actually, you know, you see yourself in one way or another,

43:58.100 --> 44:02.500
but this presumption of one's own sense of individual intelligence and its relationship

44:02.500 --> 44:08.020
to collective intelligence is broken, at least in the West. And therefore our relationship to

44:08.020 --> 44:12.180
how the artificialization of individual intelligence and the artificialization of collective intelligence

44:12.180 --> 44:18.100
would play out, it's not surprising that people would see it in this particular way. And this is

44:18.100 --> 44:22.740
a little bit what I mean about provincializing the West, about provincializing the Western

44:22.740 --> 44:27.940
theories around AI and how important it is to do that in order to get to the point where

44:27.940 --> 44:31.860
much more planetary discourse and compositional project around AI is even possible.

44:33.220 --> 44:36.740
So maybe going a little further in picking apart the problem of the individual,

44:38.580 --> 44:42.980
I'd love to get to kind of the is Benjamin Bratton an anti-humanist type of question.

44:43.620 --> 44:48.740
So in the stack, that user layer that you described, I mean it's pretty intense to

44:48.740 --> 44:54.580
read as a subject, right? The user, which is the scale that's approaching our individual

44:54.580 --> 45:00.900
scales as humans, this user is in the process of what you call liquefaction. They're being

45:00.900 --> 45:05.460
quantized in terms of data, they're constantly subordinated to all these forces on scales that

45:05.460 --> 45:11.620
are inaccessible to them. And you continue this process of the deprioritization of the individual

45:11.620 --> 45:17.860
or the critique of individual agency in wider social and political spaces, as you mentioned

45:17.860 --> 45:24.260
just now, in Revenge of the Real. So I'm wondering, what is actually available to us as agents,

45:24.900 --> 45:29.620
especially in the regime of planetary scale computation, these massive inflection points

45:29.620 --> 45:35.540
in technology? Are we just idle observers of path dependencies that are flowing through us?

45:36.420 --> 45:42.820
And I guess to probe even further, is the subject or the subject position or subjectivity,

45:42.820 --> 45:48.020
subjective experience, is this a relevant framework anymore or just a piece of legacy

45:48.020 --> 45:53.620
18th century tech? All right, there's a lot of questions in here. I'll do my best. I think I'll

45:53.620 --> 45:59.460
be comfortable characterizing my work as non-humanist, but not anti-human. I think the

45:59.460 --> 46:04.580
distinction between humanism and humans is an important one to make, at the very least, right?

46:04.580 --> 46:09.940
I think humanism in its, let's say, now traditional guises, for reasons that we're all

46:09.940 --> 46:14.500
probably well aware, has a lot of different problems associated with it, including problems

46:14.500 --> 46:20.260
that have unfortunately been shuttled along into post-humanism in a lot of different ways,

46:20.260 --> 46:26.260
which I find in many respects, in its present guise, is a kind of inadequate sentimentalization

46:26.260 --> 46:32.180
of the eclipse of the humanist subject. But it's not anti-human in any sort of sense at all.

46:32.900 --> 46:38.820
Humans, if we just really zoom out a little bit and think of humans as this precocious primate

46:38.820 --> 46:43.940
that's been around for a few million years and a couple hundred thousand in its present form and

46:43.940 --> 46:49.540
capable of producing amazing feats of symbolic construction and communication over the past

46:49.540 --> 46:56.500
tens of thousands of years and has, for better or worse, largely transformed its host planet in its

46:56.500 --> 47:02.580
image, humans are remarkable. But it did all of these things not because it meant to. It wasn't

47:02.580 --> 47:07.460
like that somehow, you know, that homo habilis said, right, got it. First, I make a rock,

47:07.460 --> 47:10.580
you know, then I'm going to do this. And then, you know, Napoleon's going to invade Russia in

47:10.580 --> 47:14.020
the winter. And then we're going to have American Idol. And then we're, you know,

47:14.020 --> 47:18.740
it's like, this is not how, this is not how history works, but it's not how evolution works.

47:18.740 --> 47:23.620
What we're all concerned about is anthropogenic agency. What was called the Anthropocene is about

47:23.620 --> 47:27.700
understanding the cumulative effects of anthropogenic agency. But when Darwinian

47:27.700 --> 47:32.900
evolution became paradigmatic in the late 19th century, this was a kind of Copernican shift

47:32.900 --> 47:39.220
moment when humans realized like, oh, our own history is actually a history that is floating

47:39.220 --> 47:43.780
on top of much bigger and deeper histories that are geologic histories, that are biological

47:43.780 --> 47:49.540
histories, that are evolutionary histories, that are histories of how it is that complex societies

47:49.540 --> 47:55.300
rise and fall. And that, you know, the amount of mastery and control that we have over this is

47:55.300 --> 48:00.340
relatively limited. It's interesting then, I suppose, in a similar that you get this understanding

48:00.340 --> 48:05.300
of essentially the ways in which human societies and cultures are, as you say, sort of dependent

48:05.300 --> 48:10.020
on forces outside of their control or the result of those forces, right? Evolution would suggest

48:10.020 --> 48:14.660
that human culture is just an expression of deeper dynamics and forces in and of itself.

48:14.660 --> 48:19.460
At the exact same time, historically, when it also comes to realize that it has transformed

48:19.460 --> 48:24.900
the entire world in its image in the Anthropocene, both this sense of discovering of the scope of

48:24.900 --> 48:30.100
its agency and discovering of the limitations of its agency, it discovers at the exact same time.

48:30.100 --> 48:34.740
But I think the relationship here, I'll put it this way just to sort of not to bury the lead.

48:34.740 --> 48:40.420
I think the problem that you're identifying here is the way in which in particularly,

48:40.420 --> 48:44.900
I don't want to say in the West in art and design circles, you can slice it however you like,

48:44.900 --> 48:50.180
that there's a strong conflation of subjectivity, agency and identity as all basically being the

48:50.180 --> 48:56.820
same thing. And that the question of agency and how it is that I myself as the protagonist of

48:56.820 --> 49:02.340
the world have agency to make change is not the same question of what is your experience of your

49:02.340 --> 49:07.140
own subjectivity. It's not the same question of what is your sense of identity. Your question of

49:07.140 --> 49:11.940
identity is not necessarily the same question as a sense of agency. Subjectivity, agency,

49:11.940 --> 49:15.540
and identity are actually really different kinds of things. And I think it's the conflation of

49:15.540 --> 49:20.580
these that's causing people a lot of headache. I think what you get result is a sense of

49:20.580 --> 49:25.460
diminution of one. I don't have enough agency in the world implies that there needs to be an

49:25.460 --> 49:30.500
inflation of my sense of subjectivity, or even worse, a sense of my experience of my own

49:30.500 --> 49:37.380
subjectivity. Subjectivity and the aspect of one's own subjecthood becomes not just the path

49:37.380 --> 49:42.100
towards greater agency, it becomes both the form and the content of greater agency. I think this

49:42.100 --> 49:47.300
is a bit of a close loop way of going about things to be clear. I also think that at least

49:47.300 --> 49:50.660
historically it tends to sort of get things backwards. There's a way in which, you know,

49:50.660 --> 49:54.500
I think we're all sort of familiar with this tendency in sort of general sense is the idea

49:54.500 --> 50:01.380
of how it is that either I myself or we as the people or humans as the anthropogenic agent would

50:01.380 --> 50:08.420
be able to have more control over our own societies or our own lives over the way that the ecosystems

50:08.420 --> 50:15.300
work is if we need to first develop the subjectivity that would allow us to understand what's in front

50:15.300 --> 50:21.060
of us. And if we can cultivate the subjectivity properly to debate and to draw the fine points

50:21.060 --> 50:25.220
of distinction between what are the proper political subjects and the improper political

50:25.220 --> 50:30.420
subjects, the proper economic. So that once we calibrate subjectivity agency will flow from

50:30.420 --> 50:34.820
there. I think one of the lessons of the Anthropocene is that actually may work the other way around.

50:34.820 --> 50:38.340
One of the things that the Anthropocene is that then you call it whatever you like. I mean,

50:38.340 --> 50:42.100
I think actually they're not just different words for the same thing. I think capital scene is just

50:42.100 --> 50:44.980
a different thing than the Anthropocene, just that they're actually talking about different

50:44.980 --> 50:50.420
kinds of transformations is that whatever slice of humans or whatever actual sort of mega complex

50:50.420 --> 50:55.540
of humans and technologies and microbes and other species that you want to identify as the

50:55.540 --> 51:01.940
Anthropocene complex, it had this agency to transform the world for centuries before it

51:01.940 --> 51:07.780
understood that it was doing that. It had this world changing terraforming agency for centuries,

51:07.780 --> 51:13.140
but only in the 1960s and 70s really. And then, you know, to a certain extent with the birth of

51:13.140 --> 51:16.820
climate science, but then really even in the early 2000s, it can occur to us just like, oh,

51:17.380 --> 51:24.020
we have this agency, this terraforming agency that it is not negotiable. It is not something

51:24.020 --> 51:30.820
that it's possible to renounce. And therefore we have to construct a subjectivity that is appropriate

51:30.820 --> 51:35.540
to the scale of this agency, which I think is the right way of going about it. But what this,

51:35.540 --> 51:40.580
what this implies is that agency precedes subjectivity. The agency precedes the

51:40.580 --> 51:45.620
subjectivity. The subjectivity is a way of retroactively understanding one's own agency

51:45.620 --> 51:49.860
sort of in the world. Obviously it can work the other way around, where people come to rethink

51:49.860 --> 51:54.260
of themselves and their subject positions. And this is another linguistification of the world.

51:54.260 --> 52:00.580
Like reality is a big sentence and I'm the first person singular or first person collective subject

52:00.580 --> 52:05.300
of the sentence. And now what's the verb. But I think we need to be equally attentive to subjectivity

52:05.300 --> 52:09.860
as the result of the agency as well. But I think to sort of the gist of your point, I think that

52:09.860 --> 52:14.980
there at this particular moment, I think part of the ways in which, and I just wrote a piece about

52:14.980 --> 52:22.580
this for Tank magazine, which was based on a really, really interesting book called immediacy

52:22.580 --> 52:28.180
that Anna Kornblot wrote, is that there's an intense focus on the subjective experience of

52:28.180 --> 52:33.140
subjective experience in and of itself. Again, as the sort of form and content of the way in

52:33.140 --> 52:36.980
which one must calibrate your being in the world that I think at the end of the day is actually

52:36.980 --> 52:41.700
why you have the bad Anthropocene, not the way you get out of it. The reason you have the bad

52:41.700 --> 52:46.180
Anthropocene is not because humans rationalize or technologize the world, but rather because

52:46.180 --> 52:50.100
they imagine that the world is basically the background for their own experience of their

52:50.100 --> 52:56.500
own experience. The true weight, the true weight of that narcissism is something that's probably

52:56.500 --> 53:06.340
unaffordable in the long term. A huge thank you to icon and hero Benjamin Bratton for joining us.

53:07.380 --> 53:11.460
We hope this conversation was as thought-provoking for our listeners as it was for the two of us.

53:11.700 --> 53:18.660
Damn. On a related note, Roberto and I are giving a symposium for foreign object in March called

53:18.660 --> 53:24.580
non-player dynamics, agency fetish, and game world as part of their deep object residency agency at

53:24.580 --> 53:29.540
the computational turn led by Sapita Majidi and Razan Agarastani. You can find more information

53:29.540 --> 53:35.940
about this talk at foreignobjectwithak.com. Subscribe to this podcast for more analysis

53:35.940 --> 53:39.780
at the intersection of algorithm, subjectivity, and the arts.

