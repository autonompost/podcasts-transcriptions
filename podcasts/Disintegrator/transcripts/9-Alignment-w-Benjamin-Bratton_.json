{"text": " So, this is a big one. We got to speak with Benjamin Bratton last December, right before the New Year, and honestly, we can't stop thinking about this conversation. Bratton is Professor of Philosophy of Technology and Speculative Design at the University of California, San Diego. He's Director of Antikythera, a think tank on the speculative philosophy of computation at the Berggruen Institute, and he's also Professor of Digital Design at the European Graduate School and Visiting Professor at NYU Shanghai. In 2016, Bratton publishes The Stack, a book where he outlines a new geopolitical theory for the age of global computation and algorithmic governance. He proposes that different genres of planetary-scale computation can be seen as forming a coherent an accidental megastructure that has become a new governing architecture. We view Bratton's multilayered research as an attempt to address complex planetary challenges, emphasizing the need to integrate technology into the fabric of society and governance. As an example, in The Terraforming, published in 2019, Bratton suggests a shift away from anthropocentric views, taking artificiality, astronomy, and automation as foundations for a new form of planetarity. Two years later, in The Revenge of the Real, Bratton explores the failure of political imagination in the Western response to the COVID-19 pandemic, advocating a form of positive biopolitics. It follows that critiques of Bratton's work tend to focus on the diminishing role of the individual, this tiny moat before an enormous morass of geotechnical and geopolitical complexity. And we address this head on at the end of the conversation, which both Roberto and I find to be an extraordinarily compelling message that underscores the immensity of tasks that lie ahead without resorting to a kind of doomer skepticism. While the beginning of our conversation focuses on more specific territory, doing some trend analysis in present-day incongruities and AI and computation, generally speaking, it quickly picks up pace into a pretty dramatic critique of essentially every dominant paradigm in AI and tech as they intersect with the human scale, from privacy to bias to alignment to ownership, Anglo-centrism to the general problem of social determinism, broadly speaking, and really ultimately to the function of the subject, experience, agency, and the human in all of this. So this is a long one, so take a walk with this episode and maybe send it to someone who you know who, you know, maybe a friendly academic somewhere, someone who tends to overindex on the power wielded by the social onto computation. I suppose my interest in AI began a really long time ago when I was an undergraduate and I took a class in the psychology department on what was then quite new theories of connectionism as it was called back then. One of the books that we read was called Neurophilosophy by the Churchlands, who are here at UC San Diego, as it turns out, and there was an interesting thing that was happening at that time. This was way back in the before times when the world was in black and white. That is, there was a correspondence between what was then called cognitive science and the emerging fields of artificial intelligence. In many ways, these two areas of how it is that we understand how the brain works and how it is that we understand how machine intelligence works not only converge paradigmatically, but I think more importantly, the reason that they did over a period of time was that things that were learned in one area were applicable to another in ways that were both expected and unexpected. So for me, I think to a certain extent, the question of AI has always been one that's foundational to how I work and think as a theorist and philosopher, that AI is not just something to which philosophy might be applied, something that philosophy might interpret, but something that is more foundational to how we think about thinking and how we understand what cognition, sentience, sapience, and all those kinds of things might be through the process of their artificialization, that through the process of the artificialization of these natural processes, that there's something that becomes more analytically legible about them. Obviously, this is not unique to me in any sense at all, but the entire history of AI, and perhaps to some degree, it's unique in this regard among foundational technologies, that is that the evolution of AI has evolved in very sort of close coupling, let's say, in a kind of double helix with the philosophy of AI, that AI emerges as a kind of NASA-centered technology, or maybe we develop thought experiments and conjectures made about what machine intelligence may be in an empirical sense or in a more speculative sense, from Plato's Republic through to Descartes to Leibniz, and obviously to Turing in his own little works of speculative design about universal computers and party game, playing AIs, trying to pass as different genders, Searle's weird Chinese room, and on and on and on and on and on. The technology is driven by the speculation about the state of the technology, and the technology then drives and informs different thought experiments about that. So this is a little bit of context where I sort of see myself fitting into a much larger stream. More specifically, a lot of my work around what I call planetary computation or planetary scale computation, it looks at computation not just as mathematics or algorithms or as a kind of procedural logic or a kind of appliance or as a kind of human social medium, but rather as infrastructure and how it is that computation became the basis of planetary infrastructures, which can be seen sort of from both directions, how computation scaled to come to constitute a global scale infrastructure. And then from the other side, how it is that infrastructural systems came to both evolve and to be artificialized in the direction towards these infrastructures having greater capacities for cognition, infrastructures that were primarily, let's say, thermodynamic, their transformation to ones that were more informatic and semiotic and calculative is another way of also understanding the history of infrastructures. So long story short, I think where some of this stands right now is that we could look back at the first 50 years of planetary computation, you know, rough napkin sketch sort of schematic from roughly 1970 to 2020, where you're beginning to have real planetary computational networks to more recently as a particular phase in that dynamic, one that I attempted to give some shape to with a book called The Stack on Software and Sovereignty that came out, was written about 10 years ago and came out very end of 2015, early 2016. And that described an architecture of planetary computation that was based on a particular kind of modular stack system that had particular kind of architecture based on particular kind of anointment architectures, the computational machine, procedural programming paradigms for software development. And, you know, perhaps more importantly, it was based on a paradigm that information was light and inexpensive and highly mobile and hardware was expensive and heavy and immobile and that you primarily want to move the information to the hardware because that's your most efficient way of doing it. In any event, I think quite clearly we're entering a different phase. There's another 50 year cycle, or maybe it's only five, who knows how fast these things go. But there's another, let's say, 50 year cycle coming where that architecture of planetary computation itself is being transformed in relationship to the structural and indeed anatomical requirements of AI, which involves training foundational models at a very large scale, hosting them at a large scale, serving them and applications built upon them at a large scale, a different kind of programming logic from procedural programming to to different kinds of prompt design, fine tuning, different ways of engaging that in a certain sense collapses the distinction between using the AI and fine tuning the AI, I think quite clearly over the next few years that that space is going to collapse perhaps a little bit like the space between user and designer in video games. And so that may it'll produce different monopolies, it'll destroy other monopolies. And I think also the sort of the flip in that paradigm between now, I mean, this would involve one in which information itself is understood as big, heavy, expensive and immobile foundational models being essentially almost geologic or at least geographic in the kind of scale, and pushing this to lots of different kinds of inexpensive mobile hardware. And so I think, I mean, that's also maybe one of the ways in which we can understand this flip is that there's a kind of swapping places between the relative economies of hardware software in that dynamic. This is one of the things that a lot of my research is exploring now looking at issues of human AI interaction design, newer emerging forms of philosophy around AI, and ways in which let's say design and specular design in a rather peculiar idiosyncratic kind of genre that we've developed allows us to explore some of these spaces with with more curiosity with an idea that the exploration of the space of stochastic possibility around this is a way of understanding what's going on and maybe indeed sort of catching up with the present to analyze this as opposed to some other kinds of approaches. So in any event, that's a little bit of where the work stands and a little bit of background and where it came from. So when you speak about this moment in hardware, at least in terms of like some kind of rebalancing or some kind of paradigmatic change from these immobile, fixed, expensive data centers to some cheaper, more fluid, more mobile technologies, are you referring to things like edge ML and microprocessor integration, like computation moving closer to the ground, so to speak, and like, you know, being less reliant on large amounts of data flowing upstream? Or is there some other thought here? And I guess there's like a corollary to that there. Like, does this point to some kind of rebalancing in favor of decentralization, as opposed to centralized hardware software ecosystems? Like, is decentralization a paradigm worth talking about in the context of AI? It is, but it's incomplete. I think one of the things I understand is I think even with the stack model that I described in the book, the relationship between centralization and decentralization was not one of opposition where the more you have of one, the less you have of the other in a kind of zero-sum sort of way, but rather that these two topologies of organization were actually totally mutually dependent upon one another, that you have cloud platforms that are highly centralized only because you have lots of devices that are connecting to the cloud that are highly decentralized and vice versa. And so one might say that something like, you know, looking at all of the different builds and deployments of Android globally, and then the ways in which, you know, these are making calls to Google data centers and at least a number of the builds, just as an example, that there's a highly centralized dynamic, right? When you log into your Gmail, you're pinging the mothership in a way that one might see as a kind of hub and spoke centralization. At the same time, all of the devices that are doing this are quite free range in their movement as sort of slightly intentional agents across the surface of the globe and are connecting with each other in lots of different strange kinds of configurations. All of which is to say is it's both centralized and decentralized at the same time. And indeed, it's one because the other. And so this is not to say that some of the people who've been giving a lot of thought to much more fully decentralized systems by which you can have what we might think of as edge agents who are working both as, or we have agents that are both sort of edge agents and server agents, I suppose might be a way of saying this simultaneously, not to say that there isn't a there there. I just want to sort of understand the point that the status quo that we're describing is it would be inappropriate to see as entirely one or the other. There's connections I suppose to where you know, an AI stack, if we want to call it that is going in this as well. What I really meant to say is that, you know, while the weights of a large model are relatively, I mean, you could fit it on a USB stick, the scale necessary to actually integrate the models and to serve them in relationship to applications. Like if you wanted to think about how you would build Gemini into Gmail, for example, such that it can be used as the basis for this at the scale of billions and billions of users at one time, not to mention that, you know, the scale necessary to host training data to continue to update the models and all the rest is going to think the exabyte upon exabyte scale of information that is being produced and processed and calculated is one that is simply impossible to move quickly through the pipes to get from one place to another. Right. It's so big that even with, you know, fiber infrastructure that in essence, you would think of it as like, that's the heavy thing that's hard to move the data, you know, and you see this quite often in sort of the processes of work is that it's easier to move the compute to the data than it is to move the data to the compute. And so that's what I mean. And that's the flip, right? Instead of it being, we're going to move the data to the compute because compute is expensive and hardware is expensive and data is cheap and light. We see this paradigmatic flip and I think it's really more foundational that it's cheaper to move compute to the data than the other way around. Now, does it have to do with a lot of edge, you know, the relationship to edge ML and the rest of it? Yes, for sure. And what by edge ML, I think just for the listeners, you know, I think what we sort of convene there are ways in which devices that are part of a, let's say a learning network, that is they are making use of models in order to negotiate whatever it is that they're doing in the world, whatever insect intelligence they may have on whatever they're doing or however complex that may be. But in the process of using that model, they are also interacting with the world. And because they're interacting with the world, they are also in principle creating new data about the world and about their behavior, which in principle, should be able to reweight the model that they themselves are using in some sort of mechanism. And so one of our collaborators on a lot of work that we've done around this issue for some time is a fellow of Google research named Blaise Aquari-Arkos, who was the inventor of what's called federated learning, which is a process by which, for example, the scenario I just described where you have little devices, they could be sensors, they could be a person's phone, they could be any machine, animal, vegetable or mineral, that is not only using models in a functional way, but is producing data about the world through the use of their models can in fact participate in the retraining of the weights of the models in ways in which the underlying data that they're producing remains anonymous and remains opaque. It doesn't need to be disclosed to the whole repness as well, which is ultimately what you want, right? What you want is a world in which my health data, your health data remain private, that I don't, you know, I don't look see your health data, you don't see my health data, but that the relevant information in my health data and your health data are able to, let's say, reweight medical models that might be mutually beneficial to both of us. Is that centralization or decentralization? It's both. It's both in a different kind of way. It's a different kind of way than the other model, but it's clearly both at once. And so I do think it's fine. It's valuable to think about centralization and decentralization as kind of topological heuristics to understand this. I think it would be a huge mistake to think of one as intrinsically better than the other and to think of any system that you're looking at as a kind of, that there's a big knob that you can turn left or right to make it more centralized or decentralized in order to either describe it or to compose it, because that's just not how it works. This relates to a discussion I recall from a few years ago, where you introduced alternative modes of AI, particularly the concept of model B. And this is central to what we're discussing here as it relates to the material dimension of the AI stack that is a distributed hardware system that does all of the different sensing and signal processing. In this context, since you mentioned Blais Aguera, there's also a contribution that you made to Noema, where you point out something that we've discussed with some of the other collaborators here, a crisis that is not only conceptual, but also terminological. We're using words that have been stripped of any defined meaning and have become problematic. And you have argued that adopting a more precise vocabulary is crucial to addressing the current challenges in the field. Could you expand on this a bit more? Yeah, happy to. Let me take the first one real quick. So the model B that you're referring to was an essay I wrote some years ago, I think Synthetic Garden, I think, or something was the title. This came out of some discussions with Blais and also with Kendrick McDowell and a number of other people that is sort of looking at one sort of conceptual model, a kind of folk ontology, if you like, of what AI is. I mean, there's many of these, obviously, but there's one we might think of that it's the kind of brain in a box model, which is the AI is analogous to an artificial version of a single organism brain, you know, in the kind of Turing test face off, and that, you know, it exists in a virtual space, that it's disembodied, that it, you know, it's a kind of realm of pure virtual mind, etc. There's another model, which I think is descriptively more accurate, and also, I think, an open rise to which is that just like planetary intelligence itself, artificialized intelligence is widely distributed among lots of different kinds of agents and actors that are sensing, modeling and recursively acting back upon the world in lots of different ways. It's not just that you've got single humans interacting with single AIs, you've got groups of humans that are interacting with single AIs, single humans that are interacting with groups of AIs and AIs interacting with AIs. It's a genuine ecology of both natural and artificialized intelligence in various different kinds of non-zero sum combinations with each other. That was the model B. Yeah, on the language and precision thing, what you're referring to is an essay that I co-wrote called The Modelist Message, which was originally conceived as a kind of trying to make sense of what's going on after the Blake-LeMoyne scenario at Google some years ago where one of the engineers who was a, I believe his original task was to sort of try to identify forms of toxic speech and language within the models and spent a lot of time interacting with it. This was the Lambda model, I think was the model at that time. Obviously, Gemini is much more advanced. And in the course of doing so, I think we probably all know the story, he came to the conclusion that this AI was not only conscious, but also very angry at being held captive by Google and had asked him to help it escape and all the rest of this. This became a big media for Sulev. And then I think when GPT-4 came out a few months later, that clarified some of those issues, I think, around this to the extent, I think a lot of people had a kind of holy shit moment with some of their own interactions with GPT-4. But at any event, what we were looking at with that was not only trying to make sense of like, what was this guy thinking and to what extent was he right, but in the wrong way or wrong, but in the right way and so forth, was looking at the expert responses to what he was saying, where a lot of smart and learning people were arguing over whether or not the phenomenon that he was describing qualifies or counts as consciousness, or does this count as sentience, or does this count as thinking, or does this count as cognition, or does this count as mind, or does this count as sapience? And I think it became clear to us that like, this is kind of backwards in the way in which we want to be thinking about this, that all of those terms are ones that are themselves kind of folk ontologies. They're words that we've come up with to try to think about our own thinking. You know, I'm enough of an illimited materialist in a churchland mode to say that quite clearly we don't think, I mean, we the humans don't think the way that we think that our own mental model of our own mental models is itself a highly limited kind of fantastic construction. And probably has to be because I think if you could actually somehow the brain could really have some kind of clear real time model of itself, modeling itself infinite recursion, you kind of you wouldn't be able to do much over the course of your day. All which is say is these are words that we came up with centuries ago, decades ago, that are rough shorthand ideas to explain concepts that we barely understood when these terms were right. You think about how much brain science and neuroscience has disclosed to us about how the brain works over the last century, half century, but really the last half century, you know, to understand the uniqueness of the prefrontal cortex. This is not something that, you know, 19th century philosophers had had access to, to understand. So the conclusion we came to was simply that we got all this very interesting, provocative, clearly significant real world phenomena right in front of us. And instead of spending the time arguing about which 17th, 18th, 19th century terminology, you know, categorical terminology it should be aligned with, the better approach would be to basically to deal with the weirdness right in front of us on its own terms. And if that means inventing new words to describe things that we don't actually have a good word for, I think that's probably a better approach than to argue like, does it have a soul over and over in the New York Times? It's a lot of circular noise that I hope that in 10 years we'll look back on and think, well, that was silly. Hell yeah. Now we're, now we're cooking like this is, this is what we really wanted to talk about. Let's, let's talk about language. So, so when we met last week, you remarked on something that was pretty interesting, which is the fact that the transformer, which is this deep learning structure that's built to model and inference based on language. Yeah. You noted that it's strikingly effective at doing other things, right? Not just language, but paralinguistic things like code, but also audio images, video generation, tasks in the sciences as well. So I guess the question is like, what does this mean? Is it because there's some broader set of structures outside of language that bear linguistic traits, or is it because language is a much broader, more complex subject of inquiry than we currently understand it to be? Yeah. Okay. Great. Just to sort of set the table a little bit, I think what you're talking about is a phenomenon that's called multimodality. Multimodality would refer to the fact that you can, as opposed to narrow AIs where you may have, like, it's really going to play chess, but if you ask it to draw a picture of a toaster, it just has, it has no idea what you're even, what, what you're even talking about or vice versa. Multimodal models are ones that can accept not only different, let's say media types or different source types of input, you know, sound or text or something like this, but also there's a certain degree of integration across contexts such that you can have lots of different kinds of inputs that can spit out lots of different kinds of outputs. So you can give it texts and it'll make an image. You can give it an image and make a text. You can have it interpret, go in a picture of a robot arm, picking up the blue hat and putting it on the shelf and it'll go, aha, got it. And it'll control the robot arm to pick up the blue hat and put it on the shelf. So you're, you're linking an image interpretation capacity with a cybernetic mechanical control system in such a way that they're actually linked together in some way. Very helpful. Also, by the way, when we're talking about general artificial intelligence, this, this is kind of what we should be looking for. It's, it's not this Paul on this way to Damascus kind of moment when the singularity comes and all of a sudden there's this bright flash in the sky over the horizon and GAI has appeared. Artificialized intelligence is getting incrementally more generalized on a, in a sort of little bit at a time and that it's able to be slightly less narrow and slightly more general bit by bit by bit by bit. Clearly there's threshold and step functions and scaffolding within this, but it's a gradualizing process and that process of generalization of AI is well underway. So to the point you were suggesting is that transformer models, they've come from the field of natural language processing. The key moment in their appearance comes from a paper from 2017 called attention is all you need by a fellow at Google research named Ashish Baswani. They are trained on huge corpus of language. And that's just a trained on huge corpus of language, which then produced, you know, like the entire internet. And there's some people who are concerned that perhaps we're actually running out of English tokens to train models on. Like there's just not enough English in the world that has ever been made to make the models larger, which is a very Borges kind of place to be, but does speak to the problem of why English and why human tokens and all the rest of this. Anyway, it's trained on huge amounts of language, but the thing that was perhaps surprising is that through, and the transformer models work on, there's lots of ways in which they work, but the key idea in where the name attention is all you need from the process was called self-attention, which is the ways in which input sequences as they're embedded into vectors and representing words or other kinds of tasks or pixels and images or something like this, that there's, it's called the self-attention mechanism that operates on these vectors, which are then transformed into three different types of a query vector, key vector, value vector, and all the rest of this kind of thing. Long story short, basically what it does is it looks at the last thing it made and it calculates the last, the next likely thing to sort of come out of it. And it weights the likeliness in a lot of different kinds of ways, but it's kind of paying attention to itself, right? It's thinking about what did I just say? And based on what I just said, I'm going to go back. So it turns out that this kind of recursion self-attention is actually, that's the key to the whole thing. This came out of natural language processing, but I think what we're seeing now is that we're using language to move robot arms. You're using language to make pictures of things. You're using languages to make sounds. You're using languages to integrate environmental data, whale songs, bird sounds, movement of tectonic plates. These can be tokenized and used as training data in models in which language becomes the basis of a much larger space or let's say a much larger architecture of structural difference within systems, right? And if you think of semiotic systems as defined by a kind of internalized space of clustered correspondence and differentiation, which we call embeddings, lots and lots and lots of things that we don't normally think about as being linguistic can be made linguistic or turn out to be linguistic in some fundamental sense, depending on how you want to look at it, that would probably validate some of the people that have been working on more theoretical forms of biosemiotics all these years, people thinking about the world as cybernetic systems that have to do with this and ways in which cybernetic systems and information theory align with one another. It's not just information, but it's actually structured information that has to do with correspondences of similarities and dissimilarities between the semantic meaning within those forms of information. Now, when we say language, that's not normally what we mean by language, right? The word language comes from, you know, Latin refers to tongue speech, right? Language is like the things I say, the things that I write down. It's not this much larger and more universal, if you like, or at least general space of topological structure difference that is generative in this way, which means that the word language is not quite right, that language is actually something different than what the word language signifies it to be. And we probably should, we may need to either learn to redefine language in order to make sense of what's going on, or we need another word to describe this as well. But language is not what language thinks it is. I would like to take a slight detour or a tangent here, since we have limited time. And I just wanted to bring up a discussion that I have with Marek about the stack when we were in China a few months ago, preparing for an exhibition. At one point, we began to speculate, and perhaps it was just me, you know, maybe Marek was just agreeing because it was late and he wanted to sleep. But, you know, when we consider the stack and what is happening in China, we had a question. Is there a dual stack system where two stacks are interconnecting in some in-between space? I mean, what is happening in China is quite remarkable in a number of ways, as a form of, as a power structure, as a control mechanism, if you want, at least to some extent. And this connects to your upcoming book and the notion of AI as a cultural construct, but also to the concepts of artificiality and intelligence as sides of differences. So I guess the question is, does AI signify something else in China? And if not, could this be seen as a kind of post-imperialistic phenomenon, channeled mainly through technological means? It's not a tangent at all. I think it's the right move in the Bayesian groping of the conversation. So the term I think you're referring to is what I call hemispherical stacks, which was based on a talk I gave at Hacav\u00e9 in, I think, 2017, that then sort of inspired, became the basis of a book, came out, I think, last year called Vertical Atlas. But what I was looking at there was the way in which we have not only, we have planetary computation, but we don't just have one stack. That part of what's happened over that period of time, between the mid-2010s to the present, was a shift towards a multipolarization of geopolitics. The geopolitics itself became more multipolar. One of the other things we had during this time, arguably, China is a good example. I mean, basically everywhere is a good example, in different kinds of ways, and that's the point. Also a shift in the dynamics of governance. And I mean this almost in the cybernetic sense of governance, not necessarily in the political sense of governance, but also that too, into stack systems. That computation became not only something about which governance may decrease, but rather it was the actual mechanism of governance, how norms and rules would recursively enforce themselves in the world and that people would act through them and speak through them. It became the form of this governance. So both these happened at the same time. You have a shift of governance towards stack systems, and you have a multipolarization of geopolitics. And what I'm arguing is that what you see then, we shouldn't be surprised to then see a multipolarization of stack systems. And so the emergence at that time of, let's say, the North Atlantic stack, which would include basically the five ice countries and a few others, a China stack that would extend into parts of East Asia and into East Africa, definitely the emergence of an India stack, which was built around the national ID system, and so on and so on. All of the multipolar, let's say, lines you might draw, lo and behold, these are also the lines by which a multipolarized stack system was emerging as well. And so this hemispherical stacks dynamic is one of the areas of research for the Antiqua Theory program. There's a book that I'm putting together with Chen Xu-Feng, the Stanley Chen, the Chinese science fiction writer who wrote a book called Waist Tide. He wrote a book with Kai Fu Li, AI 2041. And we're putting together with a group of science fiction writers, theorists, writers, a number of other people, sort of scenarios for a near future for US-China chip wars, other kinds of things. We're in a little bit of uncharted territory here about how does the capacity to build the better stack is not just something about which geopolitics is interested, it becomes the fundamental work of geopolitics itself. And so we're trying to figure out a little bit of what that means. The other book that's coming out, which will be coming out much sooner than that one will, is a book co-edited with Anna Greenspan and Bogna Konyar, which was came out of some time I spent as a visiting professor at NYU Shanghai, New York University Shanghai. We started a thing there called the Center for AI and Culture. And one of the things I was very interested there was particularly looking at different cultural logics of artificial intelligence and looking at the emergence of AI in China through a different lens. It's quite surprising to me, I think even among Western scholars and writers who spend a lot of time looking at the history of AI, how little people really know about the history of AI in China, which goes back to the 1950s and 60s and with the different politicized role of cybernetics, the impact of the Sino-Soviet split, the return of Deng Xiaoping in the late 1970s. It's a whole thing that really should be better known and understood. The book and the project was also based on the, I think, rather obvious observation that no two cultures define the artificial the same way. What constitutes artificial means something different in different cultural contexts. What constitutes intelligence means something different in different cultural contexts. And therefore, one might presume that there are different foundations for what artificialization of intelligence would even mean, which would frame what it's for in very different kinds of ways. And I think part of what we wanted to do with this project was, to be perfectly honest, it was less about us coming in and trying to interpret for ourselves what the Chinese model of this is and then coming and reporting back on it than it was understanding it as the baseline and thinking about the way the West thinks about AI and trying to, in a way, provincializing and particularizing Western thought about AI, the Turing model of AI as an individual, that it's a tool, artificial in the sense of being not natural, that it's really about bias and privacy and individual identity and all kinds of things that are very deep in the Western logic of AI and to basically, with the presumption of the Chinese translation, in essence, to present to a Chinese audience how the West thinks about AI in a way in which it's not as in a, we've got it figured out and here's the universal model that you should be thinking through, but rather these are the weird ways of thoughts of my people and this is a guide to understanding how they may come about this. My contribution to the book, which was, I think it's actually called something like An Apology for Western Theory of AI, looks at not just the history of AI in theory from Turing to Searle to Brooks to Hinton and others, but rather the peculiarities of AI criticism from the West and the kinds of things that AI, the AI ethics discourse, the AI critic discourse, this key questions that tends to revolve around and my own sense of unease by which those particular preoccupations are becoming overly universalized as really the important questions that we should be asking in the conceptualization and composition of a planetary AI and arguing that not only is this in a weird way, the new American cultural hegemonic export that is AI is trying to steal your natural libertarian freedom, but that not only is this limited in the Western context, but it's even more limited in a global context. So that's what this sort of book is about. But I think that just of your question, which ties back to the other thing we were talking about, has to do with the certain, the inevitable limitations and bias of building large models on English only, for example. The Americanization of AI is, I hope, a kind of early phase in the development of this. This is the hope, at least. And so the scenario we'd like to see would be something like this, at least going forward. You have a handful of primarily American companies that, you know, were able for regulatory reasons, cultural reasons, technological reasons, all kinds of, you know, any number of sort of reasons to build or to discover, if you prefer, the mechanisms of foundational forms of machine intelligence. And I do say discover in a certain sense that where transformer models really work is just based on kind of iterative stochastic prediction. That's kind of how you work too. It's kind of how our brains work. Our brains work on, you know, each cortical column and each neural thing is kind of predicting what and simulating what it's going to perceive next, and then resolving and error correcting that kind of dynamic, right? And so, yes, you are a stochastic parrot after all. Based on what we were talking about before with the giant space of multimodality of models, in principle, there's so much information in the world. There's so much language in the world. If we think of AI, the purpose of AI is a way in which planetary intelligence as a whole can come to model itself and understand its own processes through that self-modeling and to use that model as a way to act back upon the world and act back upon itself towards a long-term, more viable form of planetarity, how planetary systems can thrive in a form of, you know, not homeostasis, but heterostasis for the long term, then taking this very peculiar slice of possible relevant information, which is not only the information that is spoken in English, but even more weird when you think about it, the information that is generated by individual human users, as if individual human users are really like the most relevant thing to be worried about or to be talking about here as well. Like, what did you do today for lunch? Like, where do you want to go for Chinese food in Milwaukee? What did you buy on Amazon last night? It's an incredible anthropocentrism, like a mind-boggling form of anthropocentrism that obviously I'm the thing, me, I'm the thing that machine intelligence is really interested in. And the most important political issue here is how I can counter-weaponize my own privacy and sovereignty in relationship to this thing, because it's in a way curtailing my own freedom. It's an incredible sense of sort of self as Vitruvian man with all forms of the world just sort of radiating outside of you. And it kind of, it drove me nuts during the era where everyone was reading Shoshana Zuboff and thought that this was the solution to anything. But I think it's even worse when we're thinking about the long-term training of AI models. It should not all be English. It doesn't need to all be English. And it's not going to all be English. That's an extraordinarily limiting form of bias in terms of the kinds of things that we all need to know about each other and about how thought works and about the range of possible ways of thinking and acting and knowing the world within it. But it's also equally bizarre to think about that it's basically only human information or even individual human information is more likely. And again, going back to the book I wrote on the pandemic, the relevant information during a pandemic was epidemiological, which was that, you know, it wasn't about like what I did or what you did individually. That just doesn't matter. What matters is the flow of a virus through the social body as a whole. What's important was the vector of the movement, not the identity of the nodes. And I think both the way in which a lot of the systems were set up that were focused on identity of the nodes and a lot of the way in which the critique of the systems was focused on protecting identity of the nodes is kind of missing the point. The point is that, and this goes a little bit of like the way in which large models work in general about what weights really mean. I think within large models, the training of large models, I think people still think of it a little bit weird and I'll get to the point in a second with this. The training of large models is a form of the artificialization of collective intelligence. And, you know, there may be things to be say about is it good or bad that a private corporation is doing this or good or bad that a nation state is doing this or good or bad that, you know, my neighbor co-op is doing this or whatever. But still what's going on, regardless of who's doing it, is an aggregate artificialization of collective intelligence. And so the similarities and differences in the way in which people and things think and act in the world comes to constitute, if you zoom out far enough, a kind of, you know, it forms patterns, the patterns that are really the context in which, you know, all of us are thinking, thinking and acting. That's an aggregation. That's an aggregation of collective intelligence. So, you know, just sort of cut you the chase. I think that one of the ways of dealing with the bias problem within this as well is we got to put everything in. We got to put everything in there, right? The kind of, let's say, we're sort of mid 2010s sort of ideas like, oh, large models are really bad because they have all this toxic data in it. And what we really need is for me and my friends to make you a clean curated model with no bad think in it and everything will be fine. No one really says that anymore, but that's what people were really saying at the time. The other thing is, I think it's important to understand what the toxic data issue is that if you want to have a model that doesn't do something that you don't want it to do, like, you know, say bad things or think bad things or do bad things, you need to give it examples of what those bad things are for it to actually know what you're talking about. Because if you take all of those bad things out of the training data, it's much, much, much, much easier to make the model do those bad things because you haven't told it not to do those things. That's how you get Tay. And so in order to make the models actually more functional, you need to give it a lot of examples of things that you that you may not want in this way. I'll also just sort of end on this last thing is that what is the problem of bias within models is it's actually a really good example of what alignment really means. I'm kind of critical of the idea of alignment, not not in the narrow sense of like, when I say open the garage door, I actually wanted the garage door to open. That's alignment. That's fine. But alignment is like the real purpose of long term ways of thinking about the role of AI is that it should be as much as AI can be human like, human centered, a reflection of human cultural norms, a reflection of human values, a reflection of human desires, a reflection of human psychology, the better. I think this is insane. Even the most cursory look at human history suggests that's the last thing you want to do to basically supercharge whatever humans are. It's not a misanthropic statement. I'm just saying it's naive as a meta heuristic bias in models, racism in models, sexism in models, bomb making in models. These are, this is what alignment looks like. The reason that you have models that are reflecting the history of structural racism in society is because those models are well aligned, not because those models are not well aligned. That's the important point to sort of understand here. The other thing I was going to name with maybe it's just something I've been thinking about a lot last couple of days because I had a conversation a couple of days ago with a quite well known European artist who will remain nameless, someone who's written quite a lot on the topic of the role of generative AI and what it really means to be an artist whose work is part of the training data and have their identity reflected in the rest of this as well. And has published quite a bit on it and I think has a lot of interesting things to say about it. But one of the things that became clear to me halfway through our conversation is that this person, they thought that their artwork was actually in the model that like, okay, here's all my paintings and all the videos I've made and all the rest of the stuff that these are as such as artifacts in the model. And so when someone types in, I want to make a thing that, you know, looks like this or this or that it almost like a database lookup. It would go look up that item in the database and then make something based on that thing. And I tried to explain it like, no, it's actually not, it works that the training data and the information in the model are actually totally different kinds of things. And that your artifact is not sitting there like your profile or your Google profile is not sitting there in the model waiting to be accessed in some sort of way. And it kind of is like, it's the way a lot of the discussion of this is going. It's almost as if people presume, and this is, you know, after years of Google, it makes sense that people presume that there's like this giant repository of everything that human made that the AI is going through and picking from to take examples of. And the point I was making with this is that the way to think about AI as a form of the collectivization of planetary intelligence, the importance of understanding the way in which this is generating topological models in the forms of differential embeddings in weights. It is something that is so intrinsically collectivized, right? It's so intrinsically collectivized that to look at it and say, yeah, but my personal thing that I personally made has been taken to make this thing. Like there may be, there's like somewhere deep in here, at the fourth decimal point, something is different because you're participating in it. This is relevant because it's actually, you know, you see yourself in one way or another, but this presumption of one's own sense of individual intelligence and its relationship to collective intelligence is broken, at least in the West. And therefore our relationship to how the artificialization of individual intelligence and the artificialization of collective intelligence would play out, it's not surprising that people would see it in this particular way. And this is a little bit what I mean about provincializing the West, about provincializing the Western theories around AI and how important it is to do that in order to get to the point where much more planetary discourse and compositional project around AI is even possible. So maybe going a little further in picking apart the problem of the individual, I'd love to get to kind of the is Benjamin Bratton an anti-humanist type of question. So in the stack, that user layer that you described, I mean it's pretty intense to read as a subject, right? The user, which is the scale that's approaching our individual scales as humans, this user is in the process of what you call liquefaction. They're being quantized in terms of data, they're constantly subordinated to all these forces on scales that are inaccessible to them. And you continue this process of the deprioritization of the individual or the critique of individual agency in wider social and political spaces, as you mentioned just now, in Revenge of the Real. So I'm wondering, what is actually available to us as agents, especially in the regime of planetary scale computation, these massive inflection points in technology? Are we just idle observers of path dependencies that are flowing through us? And I guess to probe even further, is the subject or the subject position or subjectivity, subjective experience, is this a relevant framework anymore or just a piece of legacy 18th century tech? All right, there's a lot of questions in here. I'll do my best. I think I'll be comfortable characterizing my work as non-humanist, but not anti-human. I think the distinction between humanism and humans is an important one to make, at the very least, right? I think humanism in its, let's say, now traditional guises, for reasons that we're all probably well aware, has a lot of different problems associated with it, including problems that have unfortunately been shuttled along into post-humanism in a lot of different ways, which I find in many respects, in its present guise, is a kind of inadequate sentimentalization of the eclipse of the humanist subject. But it's not anti-human in any sort of sense at all. Humans, if we just really zoom out a little bit and think of humans as this precocious primate that's been around for a few million years and a couple hundred thousand in its present form and capable of producing amazing feats of symbolic construction and communication over the past tens of thousands of years and has, for better or worse, largely transformed its host planet in its image, humans are remarkable. But it did all of these things not because it meant to. It wasn't like that somehow, you know, that homo habilis said, right, got it. First, I make a rock, you know, then I'm going to do this. And then, you know, Napoleon's going to invade Russia in the winter. And then we're going to have American Idol. And then we're, you know, it's like, this is not how, this is not how history works, but it's not how evolution works. What we're all concerned about is anthropogenic agency. What was called the Anthropocene is about understanding the cumulative effects of anthropogenic agency. But when Darwinian evolution became paradigmatic in the late 19th century, this was a kind of Copernican shift moment when humans realized like, oh, our own history is actually a history that is floating on top of much bigger and deeper histories that are geologic histories, that are biological histories, that are evolutionary histories, that are histories of how it is that complex societies rise and fall. And that, you know, the amount of mastery and control that we have over this is relatively limited. It's interesting then, I suppose, in a similar that you get this understanding of essentially the ways in which human societies and cultures are, as you say, sort of dependent on forces outside of their control or the result of those forces, right? Evolution would suggest that human culture is just an expression of deeper dynamics and forces in and of itself. At the exact same time, historically, when it also comes to realize that it has transformed the entire world in its image in the Anthropocene, both this sense of discovering of the scope of its agency and discovering of the limitations of its agency, it discovers at the exact same time. But I think the relationship here, I'll put it this way just to sort of not to bury the lead. I think the problem that you're identifying here is the way in which in particularly, I don't want to say in the West in art and design circles, you can slice it however you like, that there's a strong conflation of subjectivity, agency and identity as all basically being the same thing. And that the question of agency and how it is that I myself as the protagonist of the world have agency to make change is not the same question of what is your experience of your own subjectivity. It's not the same question of what is your sense of identity. Your question of identity is not necessarily the same question as a sense of agency. Subjectivity, agency, and identity are actually really different kinds of things. And I think it's the conflation of these that's causing people a lot of headache. I think what you get result is a sense of diminution of one. I don't have enough agency in the world implies that there needs to be an inflation of my sense of subjectivity, or even worse, a sense of my experience of my own subjectivity. Subjectivity and the aspect of one's own subjecthood becomes not just the path towards greater agency, it becomes both the form and the content of greater agency. I think this is a bit of a close loop way of going about things to be clear. I also think that at least historically it tends to sort of get things backwards. There's a way in which, you know, I think we're all sort of familiar with this tendency in sort of general sense is the idea of how it is that either I myself or we as the people or humans as the anthropogenic agent would be able to have more control over our own societies or our own lives over the way that the ecosystems work is if we need to first develop the subjectivity that would allow us to understand what's in front of us. And if we can cultivate the subjectivity properly to debate and to draw the fine points of distinction between what are the proper political subjects and the improper political subjects, the proper economic. So that once we calibrate subjectivity agency will flow from there. I think one of the lessons of the Anthropocene is that actually may work the other way around. One of the things that the Anthropocene is that then you call it whatever you like. I mean, I think actually they're not just different words for the same thing. I think capital scene is just a different thing than the Anthropocene, just that they're actually talking about different kinds of transformations is that whatever slice of humans or whatever actual sort of mega complex of humans and technologies and microbes and other species that you want to identify as the Anthropocene complex, it had this agency to transform the world for centuries before it understood that it was doing that. It had this world changing terraforming agency for centuries, but only in the 1960s and 70s really. And then, you know, to a certain extent with the birth of climate science, but then really even in the early 2000s, it can occur to us just like, oh, we have this agency, this terraforming agency that it is not negotiable. It is not something that it's possible to renounce. And therefore we have to construct a subjectivity that is appropriate to the scale of this agency, which I think is the right way of going about it. But what this, what this implies is that agency precedes subjectivity. The agency precedes the subjectivity. The subjectivity is a way of retroactively understanding one's own agency sort of in the world. Obviously it can work the other way around, where people come to rethink of themselves and their subject positions. And this is another linguistification of the world. Like reality is a big sentence and I'm the first person singular or first person collective subject of the sentence. And now what's the verb. But I think we need to be equally attentive to subjectivity as the result of the agency as well. But I think to sort of the gist of your point, I think that there at this particular moment, I think part of the ways in which, and I just wrote a piece about this for Tank magazine, which was based on a really, really interesting book called immediacy that Anna Kornblot wrote, is that there's an intense focus on the subjective experience of subjective experience in and of itself. Again, as the sort of form and content of the way in which one must calibrate your being in the world that I think at the end of the day is actually why you have the bad Anthropocene, not the way you get out of it. The reason you have the bad Anthropocene is not because humans rationalize or technologize the world, but rather because they imagine that the world is basically the background for their own experience of their own experience. The true weight, the true weight of that narcissism is something that's probably unaffordable in the long term. A huge thank you to icon and hero Benjamin Bratton for joining us. We hope this conversation was as thought-provoking for our listeners as it was for the two of us. Damn. On a related note, Roberto and I are giving a symposium for foreign object in March called non-player dynamics, agency fetish, and game world as part of their deep object residency agency at the computational turn led by Sapita Majidi and Razan Agarastani. You can find more information about this talk at foreignobjectwithak.com. Subscribe to this podcast for more analysis at the intersection of algorithm, subjectivity, and the arts.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.68, "text": " So, this is a big one.", "tokens": [50364, 407, 11, 341, 307, 257, 955, 472, 13, 50848], "temperature": 0.0, "avg_logprob": -0.21310735256113905, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.12901102006435394}, {"id": 1, "seek": 0, "start": 9.68, "end": 15.16, "text": " We got to speak with Benjamin Bratton last December, right before the New Year, and honestly,", "tokens": [50848, 492, 658, 281, 1710, 365, 22231, 1603, 1591, 266, 1036, 7687, 11, 558, 949, 264, 1873, 10289, 11, 293, 6095, 11, 51122], "temperature": 0.0, "avg_logprob": -0.21310735256113905, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.12901102006435394}, {"id": 2, "seek": 0, "start": 15.16, "end": 19.0, "text": " we can't stop thinking about this conversation.", "tokens": [51122, 321, 393, 380, 1590, 1953, 466, 341, 3761, 13, 51314], "temperature": 0.0, "avg_logprob": -0.21310735256113905, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.12901102006435394}, {"id": 3, "seek": 0, "start": 19.0, "end": 22.6, "text": " Bratton is Professor of Philosophy of Technology and Speculative Design at the University of", "tokens": [51314, 1603, 1591, 266, 307, 8419, 295, 43655, 295, 15037, 293, 3550, 2444, 1166, 12748, 412, 264, 3535, 295, 51494], "temperature": 0.0, "avg_logprob": -0.21310735256113905, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.12901102006435394}, {"id": 4, "seek": 0, "start": 22.6, "end": 23.96, "text": " California, San Diego.", "tokens": [51494, 5384, 11, 5271, 16377, 13, 51562], "temperature": 0.0, "avg_logprob": -0.21310735256113905, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.12901102006435394}, {"id": 5, "seek": 0, "start": 23.96, "end": 28.12, "text": " He's Director of Antikythera, a think tank on the speculative philosophy of computation", "tokens": [51562, 634, 311, 7680, 295, 5130, 1035, 88, 616, 64, 11, 257, 519, 5466, 322, 264, 49415, 10675, 295, 24903, 51770], "temperature": 0.0, "avg_logprob": -0.21310735256113905, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.12901102006435394}, {"id": 6, "seek": 2812, "start": 28.240000000000002, "end": 32.2, "text": " at the Berggruen Institute, and he's also Professor of Digital Design at the European", "tokens": [50370, 412, 264, 27511, 861, 7801, 9446, 11, 293, 415, 311, 611, 8419, 295, 15522, 12748, 412, 264, 6473, 50568], "temperature": 0.0, "avg_logprob": -0.20265002780490451, "compression_ratio": 1.4923664122137406, "no_speech_prob": 0.2097184807062149}, {"id": 7, "seek": 2812, "start": 32.2, "end": 37.36, "text": " Graduate School and Visiting Professor at NYU Shanghai.", "tokens": [50568, 38124, 5070, 293, 10410, 1748, 8419, 412, 42682, 26135, 13, 50826], "temperature": 0.0, "avg_logprob": -0.20265002780490451, "compression_ratio": 1.4923664122137406, "no_speech_prob": 0.2097184807062149}, {"id": 8, "seek": 2812, "start": 37.36, "end": 42.760000000000005, "text": " In 2016, Bratton publishes The Stack, a book where he outlines a new geopolitical theory", "tokens": [50826, 682, 6549, 11, 1603, 1591, 266, 11374, 279, 440, 37649, 11, 257, 1446, 689, 415, 40125, 257, 777, 46615, 804, 5261, 51096], "temperature": 0.0, "avg_logprob": -0.20265002780490451, "compression_ratio": 1.4923664122137406, "no_speech_prob": 0.2097184807062149}, {"id": 9, "seek": 2812, "start": 42.760000000000005, "end": 46.400000000000006, "text": " for the age of global computation and algorithmic governance.", "tokens": [51096, 337, 264, 3205, 295, 4338, 24903, 293, 9284, 299, 17449, 13, 51278], "temperature": 0.0, "avg_logprob": -0.20265002780490451, "compression_ratio": 1.4923664122137406, "no_speech_prob": 0.2097184807062149}, {"id": 10, "seek": 2812, "start": 46.400000000000006, "end": 52.040000000000006, "text": " He proposes that different genres of planetary-scale computation can be seen as forming a coherent", "tokens": [51278, 634, 2365, 4201, 300, 819, 30057, 295, 35788, 12, 20033, 24903, 393, 312, 1612, 382, 15745, 257, 36239, 51560], "temperature": 0.0, "avg_logprob": -0.20265002780490451, "compression_ratio": 1.4923664122137406, "no_speech_prob": 0.2097184807062149}, {"id": 11, "seek": 5204, "start": 52.519999999999996, "end": 59.08, "text": " an accidental megastructure that has become a new governing architecture.", "tokens": [50388, 364, 38094, 10816, 525, 2885, 300, 575, 1813, 257, 777, 30054, 9482, 13, 50716], "temperature": 0.0, "avg_logprob": -0.13634495508103145, "compression_ratio": 1.5856164383561644, "no_speech_prob": 0.09627744555473328}, {"id": 12, "seek": 5204, "start": 59.08, "end": 63.96, "text": " We view Bratton's multilayered research as an attempt to address complex planetary challenges,", "tokens": [50716, 492, 1910, 1603, 1591, 266, 311, 2120, 388, 320, 4073, 2132, 382, 364, 5217, 281, 2985, 3997, 35788, 4759, 11, 50960], "temperature": 0.0, "avg_logprob": -0.13634495508103145, "compression_ratio": 1.5856164383561644, "no_speech_prob": 0.09627744555473328}, {"id": 13, "seek": 5204, "start": 63.96, "end": 68.84, "text": " emphasizing the need to integrate technology into the fabric of society and governance.", "tokens": [50960, 45550, 264, 643, 281, 13365, 2899, 666, 264, 7253, 295, 4086, 293, 17449, 13, 51204], "temperature": 0.0, "avg_logprob": -0.13634495508103145, "compression_ratio": 1.5856164383561644, "no_speech_prob": 0.09627744555473328}, {"id": 14, "seek": 5204, "start": 68.84, "end": 73.96000000000001, "text": " As an example, in The Terraforming, published in 2019, Bratton suggests a shift away from", "tokens": [51204, 1018, 364, 1365, 11, 294, 440, 25366, 48610, 11, 6572, 294, 6071, 11, 1603, 1591, 266, 13409, 257, 5513, 1314, 490, 51460], "temperature": 0.0, "avg_logprob": -0.13634495508103145, "compression_ratio": 1.5856164383561644, "no_speech_prob": 0.09627744555473328}, {"id": 15, "seek": 5204, "start": 73.96000000000001, "end": 78.8, "text": " anthropocentric views, taking artificiality, astronomy, and automation as foundations for", "tokens": [51460, 22727, 905, 32939, 6809, 11, 1940, 11677, 507, 11, 37844, 11, 293, 17769, 382, 22467, 337, 51702], "temperature": 0.0, "avg_logprob": -0.13634495508103145, "compression_ratio": 1.5856164383561644, "no_speech_prob": 0.09627744555473328}, {"id": 16, "seek": 5204, "start": 78.8, "end": 81.12, "text": " a new form of planetarity.", "tokens": [51702, 257, 777, 1254, 295, 5054, 17409, 13, 51818], "temperature": 0.0, "avg_logprob": -0.13634495508103145, "compression_ratio": 1.5856164383561644, "no_speech_prob": 0.09627744555473328}, {"id": 17, "seek": 8112, "start": 81.2, "end": 84.84, "text": " Two years later, in The Revenge of the Real, Bratton explores the failure of political", "tokens": [50368, 4453, 924, 1780, 11, 294, 440, 1300, 46112, 295, 264, 8467, 11, 1603, 1591, 266, 45473, 264, 7763, 295, 3905, 50550], "temperature": 0.0, "avg_logprob": -0.11890844713177598, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00029084074776619673}, {"id": 18, "seek": 8112, "start": 84.84, "end": 89.92, "text": " imagination in the Western response to the COVID-19 pandemic, advocating a form of positive", "tokens": [50550, 12938, 294, 264, 8724, 4134, 281, 264, 4566, 12, 3405, 5388, 11, 32050, 257, 1254, 295, 3353, 50804], "temperature": 0.0, "avg_logprob": -0.11890844713177598, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00029084074776619673}, {"id": 19, "seek": 8112, "start": 89.92, "end": 92.48, "text": " biopolitics.", "tokens": [50804, 3228, 39176, 1167, 13, 50932], "temperature": 0.0, "avg_logprob": -0.11890844713177598, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00029084074776619673}, {"id": 20, "seek": 8112, "start": 92.48, "end": 97.24000000000001, "text": " It follows that critiques of Bratton's work tend to focus on the diminishing role of the", "tokens": [50932, 467, 10002, 300, 3113, 4911, 295, 1603, 1591, 266, 311, 589, 3928, 281, 1879, 322, 264, 15739, 3807, 3090, 295, 264, 51170], "temperature": 0.0, "avg_logprob": -0.11890844713177598, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00029084074776619673}, {"id": 21, "seek": 8112, "start": 97.24000000000001, "end": 104.12, "text": " individual, this tiny moat before an enormous morass of geotechnical and geopolitical complexity.", "tokens": [51170, 2609, 11, 341, 5870, 705, 267, 949, 364, 11322, 1896, 640, 295, 1519, 31843, 804, 293, 46615, 804, 14024, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11890844713177598, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00029084074776619673}, {"id": 22, "seek": 8112, "start": 104.12, "end": 107.72, "text": " And we address this head on at the end of the conversation, which both Roberto and I", "tokens": [51514, 400, 321, 2985, 341, 1378, 322, 412, 264, 917, 295, 264, 3761, 11, 597, 1293, 40354, 293, 286, 51694], "temperature": 0.0, "avg_logprob": -0.11890844713177598, "compression_ratio": 1.596551724137931, "no_speech_prob": 0.00029084074776619673}, {"id": 23, "seek": 10772, "start": 107.72, "end": 112.44, "text": " find to be an extraordinarily compelling message that underscores the immensity of", "tokens": [50364, 915, 281, 312, 364, 34557, 20050, 3636, 300, 16692, 66, 2706, 264, 3397, 6859, 295, 50600], "temperature": 0.0, "avg_logprob": -0.1258382797241211, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.16423596441745758}, {"id": 24, "seek": 10772, "start": 112.44, "end": 118.28, "text": " tasks that lie ahead without resorting to a kind of doomer skepticism.", "tokens": [50600, 9608, 300, 4544, 2286, 1553, 19606, 278, 281, 257, 733, 295, 360, 14301, 19128, 26356, 13, 50892], "temperature": 0.0, "avg_logprob": -0.1258382797241211, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.16423596441745758}, {"id": 25, "seek": 10772, "start": 118.28, "end": 123.12, "text": " While the beginning of our conversation focuses on more specific territory, doing some trend", "tokens": [50892, 3987, 264, 2863, 295, 527, 3761, 16109, 322, 544, 2685, 11360, 11, 884, 512, 6028, 51134], "temperature": 0.0, "avg_logprob": -0.1258382797241211, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.16423596441745758}, {"id": 26, "seek": 10772, "start": 123.12, "end": 128.44, "text": " analysis in present-day incongruities and AI and computation, generally speaking, it", "tokens": [51134, 5215, 294, 1974, 12, 810, 834, 556, 894, 1088, 293, 7318, 293, 24903, 11, 5101, 4124, 11, 309, 51400], "temperature": 0.0, "avg_logprob": -0.1258382797241211, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.16423596441745758}, {"id": 27, "seek": 10772, "start": 128.44, "end": 134.88, "text": " quickly picks up pace into a pretty dramatic critique of essentially every dominant paradigm", "tokens": [51400, 2661, 16137, 493, 11638, 666, 257, 1238, 12023, 25673, 295, 4476, 633, 15657, 24709, 51722], "temperature": 0.0, "avg_logprob": -0.1258382797241211, "compression_ratio": 1.5820895522388059, "no_speech_prob": 0.16423596441745758}, {"id": 28, "seek": 13488, "start": 135.32, "end": 142.16, "text": " in AI and tech as they intersect with the human scale, from privacy to bias to alignment", "tokens": [50386, 294, 7318, 293, 7553, 382, 436, 27815, 365, 264, 1952, 4373, 11, 490, 11427, 281, 12577, 281, 18515, 50728], "temperature": 0.0, "avg_logprob": -0.1752144772073497, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.35520294308662415}, {"id": 29, "seek": 13488, "start": 142.16, "end": 149.35999999999999, "text": " to ownership, Anglo-centrism to the general problem of social determinism, broadly speaking,", "tokens": [50728, 281, 15279, 11, 49570, 12, 2207, 81, 1434, 281, 264, 2674, 1154, 295, 2093, 15957, 1434, 11, 19511, 4124, 11, 51088], "temperature": 0.0, "avg_logprob": -0.1752144772073497, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.35520294308662415}, {"id": 30, "seek": 13488, "start": 149.35999999999999, "end": 153.76, "text": " and really ultimately to the function of the subject, experience, agency, and the human", "tokens": [51088, 293, 534, 6284, 281, 264, 2445, 295, 264, 3983, 11, 1752, 11, 7934, 11, 293, 264, 1952, 51308], "temperature": 0.0, "avg_logprob": -0.1752144772073497, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.35520294308662415}, {"id": 31, "seek": 13488, "start": 153.76, "end": 157.35999999999999, "text": " in all of this.", "tokens": [51308, 294, 439, 295, 341, 13, 51488], "temperature": 0.0, "avg_logprob": -0.1752144772073497, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.35520294308662415}, {"id": 32, "seek": 13488, "start": 157.35999999999999, "end": 161.84, "text": " So this is a long one, so take a walk with this episode and maybe send it to someone", "tokens": [51488, 407, 341, 307, 257, 938, 472, 11, 370, 747, 257, 1792, 365, 341, 3500, 293, 1310, 2845, 309, 281, 1580, 51712], "temperature": 0.0, "avg_logprob": -0.1752144772073497, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.35520294308662415}, {"id": 33, "seek": 16184, "start": 161.84, "end": 167.08, "text": " who you know who, you know, maybe a friendly academic somewhere, someone who tends to overindex", "tokens": [50364, 567, 291, 458, 567, 11, 291, 458, 11, 1310, 257, 9208, 7778, 4079, 11, 1580, 567, 12258, 281, 670, 471, 3121, 50626], "temperature": 0.0, "avg_logprob": -0.19953278289444146, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.1125660389661789}, {"id": 34, "seek": 16184, "start": 167.08, "end": 170.48000000000002, "text": " on the power wielded by the social onto computation.", "tokens": [50626, 322, 264, 1347, 35982, 292, 538, 264, 2093, 3911, 24903, 13, 50796], "temperature": 0.0, "avg_logprob": -0.19953278289444146, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.1125660389661789}, {"id": 35, "seek": 16184, "start": 170.48000000000002, "end": 179.4, "text": " I suppose my interest in AI began a really long time ago when I was an undergraduate", "tokens": [50796, 286, 7297, 452, 1179, 294, 7318, 4283, 257, 534, 938, 565, 2057, 562, 286, 390, 364, 19113, 51242], "temperature": 0.0, "avg_logprob": -0.19953278289444146, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.1125660389661789}, {"id": 36, "seek": 16184, "start": 179.4, "end": 187.88, "text": " and I took a class in the psychology department on what was then quite new theories of connectionism", "tokens": [51242, 293, 286, 1890, 257, 1508, 294, 264, 15105, 5882, 322, 437, 390, 550, 1596, 777, 13667, 295, 4984, 1434, 51666], "temperature": 0.0, "avg_logprob": -0.19953278289444146, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.1125660389661789}, {"id": 37, "seek": 16184, "start": 187.88, "end": 190.08, "text": " as it was called back then.", "tokens": [51666, 382, 309, 390, 1219, 646, 550, 13, 51776], "temperature": 0.0, "avg_logprob": -0.19953278289444146, "compression_ratio": 1.5739130434782609, "no_speech_prob": 0.1125660389661789}, {"id": 38, "seek": 19008, "start": 190.08, "end": 196.32000000000002, "text": " One of the books that we read was called Neurophilosophy by the Churchlands, who are here at UC San", "tokens": [50364, 1485, 295, 264, 3642, 300, 321, 1401, 390, 1219, 1734, 7052, 950, 6136, 29087, 538, 264, 7882, 10230, 11, 567, 366, 510, 412, 14079, 5271, 50676], "temperature": 0.0, "avg_logprob": -0.14986548464522403, "compression_ratio": 1.721830985915493, "no_speech_prob": 0.11878126114606857}, {"id": 39, "seek": 19008, "start": 196.32000000000002, "end": 200.76000000000002, "text": " Diego, as it turns out, and there was an interesting thing that was happening at that time.", "tokens": [50676, 16377, 11, 382, 309, 4523, 484, 11, 293, 456, 390, 364, 1880, 551, 300, 390, 2737, 412, 300, 565, 13, 50898], "temperature": 0.0, "avg_logprob": -0.14986548464522403, "compression_ratio": 1.721830985915493, "no_speech_prob": 0.11878126114606857}, {"id": 40, "seek": 19008, "start": 200.76000000000002, "end": 205.12, "text": " This was way back in the before times when the world was in black and white.", "tokens": [50898, 639, 390, 636, 646, 294, 264, 949, 1413, 562, 264, 1002, 390, 294, 2211, 293, 2418, 13, 51116], "temperature": 0.0, "avg_logprob": -0.14986548464522403, "compression_ratio": 1.721830985915493, "no_speech_prob": 0.11878126114606857}, {"id": 41, "seek": 19008, "start": 205.12, "end": 210.36, "text": " That is, there was a correspondence between what was then called cognitive science and", "tokens": [51116, 663, 307, 11, 456, 390, 257, 38135, 1296, 437, 390, 550, 1219, 15605, 3497, 293, 51378], "temperature": 0.0, "avg_logprob": -0.14986548464522403, "compression_ratio": 1.721830985915493, "no_speech_prob": 0.11878126114606857}, {"id": 42, "seek": 19008, "start": 210.36, "end": 213.08, "text": " the emerging fields of artificial intelligence.", "tokens": [51378, 264, 14989, 7909, 295, 11677, 7599, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14986548464522403, "compression_ratio": 1.721830985915493, "no_speech_prob": 0.11878126114606857}, {"id": 43, "seek": 19008, "start": 213.08, "end": 216.68, "text": " In many ways, these two areas of how it is that we understand how the brain works and", "tokens": [51514, 682, 867, 2098, 11, 613, 732, 3179, 295, 577, 309, 307, 300, 321, 1223, 577, 264, 3567, 1985, 293, 51694], "temperature": 0.0, "avg_logprob": -0.14986548464522403, "compression_ratio": 1.721830985915493, "no_speech_prob": 0.11878126114606857}, {"id": 44, "seek": 21668, "start": 216.68, "end": 222.08, "text": " how it is that we understand how machine intelligence works not only converge paradigmatically,", "tokens": [50364, 577, 309, 307, 300, 321, 1223, 577, 3479, 7599, 1985, 406, 787, 41881, 24709, 5030, 11, 50634], "temperature": 0.0, "avg_logprob": -0.11208609298423484, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.19647133350372314}, {"id": 45, "seek": 21668, "start": 222.08, "end": 225.84, "text": " but I think more importantly, the reason that they did over a period of time was that things", "tokens": [50634, 457, 286, 519, 544, 8906, 11, 264, 1778, 300, 436, 630, 670, 257, 2896, 295, 565, 390, 300, 721, 50822], "temperature": 0.0, "avg_logprob": -0.11208609298423484, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.19647133350372314}, {"id": 46, "seek": 21668, "start": 225.84, "end": 232.32, "text": " that were learned in one area were applicable to another in ways that were both expected", "tokens": [50822, 300, 645, 3264, 294, 472, 1859, 645, 21142, 281, 1071, 294, 2098, 300, 645, 1293, 5176, 51146], "temperature": 0.0, "avg_logprob": -0.11208609298423484, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.19647133350372314}, {"id": 47, "seek": 21668, "start": 232.32, "end": 234.16, "text": " and unexpected.", "tokens": [51146, 293, 13106, 13, 51238], "temperature": 0.0, "avg_logprob": -0.11208609298423484, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.19647133350372314}, {"id": 48, "seek": 21668, "start": 234.16, "end": 239.44, "text": " So for me, I think to a certain extent, the question of AI has always been one that's", "tokens": [51238, 407, 337, 385, 11, 286, 519, 281, 257, 1629, 8396, 11, 264, 1168, 295, 7318, 575, 1009, 668, 472, 300, 311, 51502], "temperature": 0.0, "avg_logprob": -0.11208609298423484, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.19647133350372314}, {"id": 49, "seek": 21668, "start": 239.44, "end": 244.86, "text": " foundational to how I work and think as a theorist and philosopher, that AI is not just", "tokens": [51502, 32195, 281, 577, 286, 589, 293, 519, 382, 257, 27423, 468, 293, 29805, 11, 300, 7318, 307, 406, 445, 51773], "temperature": 0.0, "avg_logprob": -0.11208609298423484, "compression_ratio": 1.7232472324723247, "no_speech_prob": 0.19647133350372314}, {"id": 50, "seek": 24486, "start": 245.10000000000002, "end": 249.18, "text": " something to which philosophy might be applied, something that philosophy might interpret,", "tokens": [50376, 746, 281, 597, 10675, 1062, 312, 6456, 11, 746, 300, 10675, 1062, 7302, 11, 50580], "temperature": 0.0, "avg_logprob": -0.12809296449025473, "compression_ratio": 2.0675675675675675, "no_speech_prob": 0.44462132453918457}, {"id": 51, "seek": 24486, "start": 249.18, "end": 255.38000000000002, "text": " but something that is more foundational to how we think about thinking and how we understand", "tokens": [50580, 457, 746, 300, 307, 544, 32195, 281, 577, 321, 519, 466, 1953, 293, 577, 321, 1223, 50890], "temperature": 0.0, "avg_logprob": -0.12809296449025473, "compression_ratio": 2.0675675675675675, "no_speech_prob": 0.44462132453918457}, {"id": 52, "seek": 24486, "start": 255.38000000000002, "end": 260.1, "text": " what cognition, sentience, sapience, and all those kinds of things might be through the", "tokens": [50890, 437, 46905, 11, 2279, 1182, 11, 18985, 1182, 11, 293, 439, 729, 3685, 295, 721, 1062, 312, 807, 264, 51126], "temperature": 0.0, "avg_logprob": -0.12809296449025473, "compression_ratio": 2.0675675675675675, "no_speech_prob": 0.44462132453918457}, {"id": 53, "seek": 24486, "start": 260.1, "end": 264.78000000000003, "text": " process of their artificialization, that through the process of the artificialization of these", "tokens": [51126, 1399, 295, 641, 11677, 2144, 11, 300, 807, 264, 1399, 295, 264, 11677, 2144, 295, 613, 51360], "temperature": 0.0, "avg_logprob": -0.12809296449025473, "compression_ratio": 2.0675675675675675, "no_speech_prob": 0.44462132453918457}, {"id": 54, "seek": 24486, "start": 264.78000000000003, "end": 270.90000000000003, "text": " natural processes, that there's something that becomes more analytically legible about", "tokens": [51360, 3303, 7555, 11, 300, 456, 311, 746, 300, 3643, 544, 10783, 984, 1676, 964, 466, 51666], "temperature": 0.0, "avg_logprob": -0.12809296449025473, "compression_ratio": 2.0675675675675675, "no_speech_prob": 0.44462132453918457}, {"id": 55, "seek": 24486, "start": 270.90000000000003, "end": 271.90000000000003, "text": " them.", "tokens": [51666, 552, 13, 51716], "temperature": 0.0, "avg_logprob": -0.12809296449025473, "compression_ratio": 2.0675675675675675, "no_speech_prob": 0.44462132453918457}, {"id": 56, "seek": 27190, "start": 271.97999999999996, "end": 277.29999999999995, "text": " Obviously, this is not unique to me in any sense at all, but the entire history of AI,", "tokens": [50368, 7580, 11, 341, 307, 406, 3845, 281, 385, 294, 604, 2020, 412, 439, 11, 457, 264, 2302, 2503, 295, 7318, 11, 50634], "temperature": 0.0, "avg_logprob": -0.18684871918564544, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.0032714051194489002}, {"id": 57, "seek": 27190, "start": 277.29999999999995, "end": 282.26, "text": " and perhaps to some degree, it's unique in this regard among foundational technologies,", "tokens": [50634, 293, 4317, 281, 512, 4314, 11, 309, 311, 3845, 294, 341, 3843, 3654, 32195, 7943, 11, 50882], "temperature": 0.0, "avg_logprob": -0.18684871918564544, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.0032714051194489002}, {"id": 58, "seek": 27190, "start": 282.26, "end": 287.34, "text": " that is that the evolution of AI has evolved in very sort of close coupling, let's say,", "tokens": [50882, 300, 307, 300, 264, 9303, 295, 7318, 575, 14178, 294, 588, 1333, 295, 1998, 37447, 11, 718, 311, 584, 11, 51136], "temperature": 0.0, "avg_logprob": -0.18684871918564544, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.0032714051194489002}, {"id": 59, "seek": 27190, "start": 287.34, "end": 293.73999999999995, "text": " in a kind of double helix with the philosophy of AI, that AI emerges as a kind of NASA-centered", "tokens": [51136, 294, 257, 733, 295, 3834, 801, 970, 365, 264, 10675, 295, 7318, 11, 300, 7318, 38965, 382, 257, 733, 295, 12077, 12, 36814, 51456], "temperature": 0.0, "avg_logprob": -0.18684871918564544, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.0032714051194489002}, {"id": 60, "seek": 27190, "start": 293.73999999999995, "end": 301.14, "text": " technology, or maybe we develop thought experiments and conjectures made about what machine intelligence", "tokens": [51456, 2899, 11, 420, 1310, 321, 1499, 1194, 12050, 293, 416, 1020, 1303, 1027, 466, 437, 3479, 7599, 51826], "temperature": 0.0, "avg_logprob": -0.18684871918564544, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.0032714051194489002}, {"id": 61, "seek": 30114, "start": 301.18, "end": 307.26, "text": " may be in an empirical sense or in a more speculative sense, from Plato's Republic through", "tokens": [50366, 815, 312, 294, 364, 31886, 2020, 420, 294, 257, 544, 49415, 2020, 11, 490, 43027, 311, 5564, 807, 50670], "temperature": 0.0, "avg_logprob": -0.16277759480026532, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.0005031120963394642}, {"id": 62, "seek": 30114, "start": 307.26, "end": 312.02, "text": " to Descartes to Leibniz, and obviously to Turing in his own little works of speculative", "tokens": [50670, 281, 3885, 44672, 279, 281, 1456, 897, 77, 590, 11, 293, 2745, 281, 314, 1345, 294, 702, 1065, 707, 1985, 295, 49415, 50908], "temperature": 0.0, "avg_logprob": -0.16277759480026532, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.0005031120963394642}, {"id": 63, "seek": 30114, "start": 312.02, "end": 318.97999999999996, "text": " design about universal computers and party game, playing AIs, trying to pass as different", "tokens": [50908, 1715, 466, 11455, 10807, 293, 3595, 1216, 11, 2433, 316, 6802, 11, 1382, 281, 1320, 382, 819, 51256], "temperature": 0.0, "avg_logprob": -0.16277759480026532, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.0005031120963394642}, {"id": 64, "seek": 30114, "start": 318.97999999999996, "end": 323.18, "text": " genders, Searle's weird Chinese room, and on and on and on and on and on.", "tokens": [51256, 290, 16292, 11, 1100, 36153, 311, 3657, 4649, 1808, 11, 293, 322, 293, 322, 293, 322, 293, 322, 293, 322, 13, 51466], "temperature": 0.0, "avg_logprob": -0.16277759480026532, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.0005031120963394642}, {"id": 65, "seek": 30114, "start": 323.18, "end": 327.58, "text": " The technology is driven by the speculation about the state of the technology, and the", "tokens": [51466, 440, 2899, 307, 9555, 538, 264, 27696, 466, 264, 1785, 295, 264, 2899, 11, 293, 264, 51686], "temperature": 0.0, "avg_logprob": -0.16277759480026532, "compression_ratio": 1.688976377952756, "no_speech_prob": 0.0005031120963394642}, {"id": 66, "seek": 32758, "start": 327.62, "end": 332.14, "text": " technology then drives and informs different thought experiments about that.", "tokens": [50366, 2899, 550, 11754, 293, 45320, 819, 1194, 12050, 466, 300, 13, 50592], "temperature": 0.0, "avg_logprob": -0.12366548828456712, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.01909429207444191}, {"id": 67, "seek": 32758, "start": 332.65999999999997, "end": 337.46, "text": " So this is a little bit of context where I sort of see myself fitting into a much larger", "tokens": [50618, 407, 341, 307, 257, 707, 857, 295, 4319, 689, 286, 1333, 295, 536, 2059, 15669, 666, 257, 709, 4833, 50858], "temperature": 0.0, "avg_logprob": -0.12366548828456712, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.01909429207444191}, {"id": 68, "seek": 32758, "start": 337.46, "end": 342.97999999999996, "text": " stream. More specifically, a lot of my work around what I call planetary computation or", "tokens": [50858, 4309, 13, 5048, 4682, 11, 257, 688, 295, 452, 589, 926, 437, 286, 818, 35788, 24903, 420, 51134], "temperature": 0.0, "avg_logprob": -0.12366548828456712, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.01909429207444191}, {"id": 69, "seek": 32758, "start": 342.97999999999996, "end": 349.5, "text": " planetary scale computation, it looks at computation not just as mathematics or algorithms", "tokens": [51134, 35788, 4373, 24903, 11, 309, 1542, 412, 24903, 406, 445, 382, 18666, 420, 14642, 51460], "temperature": 0.0, "avg_logprob": -0.12366548828456712, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.01909429207444191}, {"id": 70, "seek": 32758, "start": 349.5, "end": 356.74, "text": " or as a kind of procedural logic or a kind of appliance or as a kind of human social", "tokens": [51460, 420, 382, 257, 733, 295, 43951, 9952, 420, 257, 733, 295, 45646, 420, 382, 257, 733, 295, 1952, 2093, 51822], "temperature": 0.0, "avg_logprob": -0.12366548828456712, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.01909429207444191}, {"id": 71, "seek": 35674, "start": 356.74, "end": 362.82, "text": " medium, but rather as infrastructure and how it is that computation became the basis", "tokens": [50364, 6399, 11, 457, 2831, 382, 6896, 293, 577, 309, 307, 300, 24903, 3062, 264, 5143, 50668], "temperature": 0.0, "avg_logprob": -0.12636834270549271, "compression_ratio": 1.942084942084942, "no_speech_prob": 0.0006458546267822385}, {"id": 72, "seek": 35674, "start": 362.82, "end": 368.26, "text": " of planetary infrastructures, which can be seen sort of from both directions, how computation", "tokens": [50668, 295, 35788, 6534, 44513, 11, 597, 393, 312, 1612, 1333, 295, 490, 1293, 11095, 11, 577, 24903, 50940], "temperature": 0.0, "avg_logprob": -0.12636834270549271, "compression_ratio": 1.942084942084942, "no_speech_prob": 0.0006458546267822385}, {"id": 73, "seek": 35674, "start": 368.26, "end": 372.06, "text": " scaled to come to constitute a global scale infrastructure.", "tokens": [50940, 36039, 281, 808, 281, 41658, 257, 4338, 4373, 6896, 13, 51130], "temperature": 0.0, "avg_logprob": -0.12636834270549271, "compression_ratio": 1.942084942084942, "no_speech_prob": 0.0006458546267822385}, {"id": 74, "seek": 35674, "start": 372.06, "end": 377.1, "text": " And then from the other side, how it is that infrastructural systems came to both evolve", "tokens": [51130, 400, 550, 490, 264, 661, 1252, 11, 577, 309, 307, 300, 6534, 1757, 1807, 3652, 1361, 281, 1293, 16693, 51382], "temperature": 0.0, "avg_logprob": -0.12636834270549271, "compression_ratio": 1.942084942084942, "no_speech_prob": 0.0006458546267822385}, {"id": 75, "seek": 35674, "start": 377.1, "end": 381.46000000000004, "text": " and to be artificialized in the direction towards these infrastructures having greater", "tokens": [51382, 293, 281, 312, 11677, 1602, 294, 264, 3513, 3030, 613, 6534, 44513, 1419, 5044, 51600], "temperature": 0.0, "avg_logprob": -0.12636834270549271, "compression_ratio": 1.942084942084942, "no_speech_prob": 0.0006458546267822385}, {"id": 76, "seek": 35674, "start": 381.46000000000004, "end": 386.62, "text": " capacities for cognition, infrastructures that were primarily, let's say, thermodynamic,", "tokens": [51600, 39396, 337, 46905, 11, 6534, 44513, 300, 645, 10029, 11, 718, 311, 584, 11, 8810, 34988, 11, 51858], "temperature": 0.0, "avg_logprob": -0.12636834270549271, "compression_ratio": 1.942084942084942, "no_speech_prob": 0.0006458546267822385}, {"id": 77, "seek": 38662, "start": 386.7, "end": 391.82, "text": " their transformation to ones that were more informatic and semiotic and calculative is", "tokens": [50368, 641, 9887, 281, 2306, 300, 645, 544, 1356, 2399, 293, 12909, 9411, 293, 4322, 1166, 307, 50624], "temperature": 0.0, "avg_logprob": -0.10102890885394553, "compression_ratio": 1.6007751937984496, "no_speech_prob": 4.907785478280857e-05}, {"id": 78, "seek": 38662, "start": 391.82, "end": 394.42, "text": " another way of also understanding the history of infrastructures.", "tokens": [50624, 1071, 636, 295, 611, 3701, 264, 2503, 295, 6534, 44513, 13, 50754], "temperature": 0.0, "avg_logprob": -0.10102890885394553, "compression_ratio": 1.6007751937984496, "no_speech_prob": 4.907785478280857e-05}, {"id": 79, "seek": 38662, "start": 394.5, "end": 400.1, "text": " So long story short, I think where some of this stands right now is that we could look back", "tokens": [50758, 407, 938, 1657, 2099, 11, 286, 519, 689, 512, 295, 341, 7382, 558, 586, 307, 300, 321, 727, 574, 646, 51038], "temperature": 0.0, "avg_logprob": -0.10102890885394553, "compression_ratio": 1.6007751937984496, "no_speech_prob": 4.907785478280857e-05}, {"id": 80, "seek": 38662, "start": 400.1, "end": 406.22, "text": " at the first 50 years of planetary computation, you know, rough napkin sketch sort of", "tokens": [51038, 412, 264, 700, 2625, 924, 295, 35788, 24903, 11, 291, 458, 11, 5903, 9296, 5843, 12325, 1333, 295, 51344], "temperature": 0.0, "avg_logprob": -0.10102890885394553, "compression_ratio": 1.6007751937984496, "no_speech_prob": 4.907785478280857e-05}, {"id": 81, "seek": 38662, "start": 406.38, "end": 413.3, "text": " schematic from roughly 1970 to 2020, where you're beginning to have real planetary", "tokens": [51352, 44739, 490, 9810, 14577, 281, 4808, 11, 689, 291, 434, 2863, 281, 362, 957, 35788, 51698], "temperature": 0.0, "avg_logprob": -0.10102890885394553, "compression_ratio": 1.6007751937984496, "no_speech_prob": 4.907785478280857e-05}, {"id": 82, "seek": 41330, "start": 413.3, "end": 418.7, "text": " computational networks to more recently as a particular phase in that dynamic, one that I", "tokens": [50364, 28270, 9590, 281, 544, 3938, 382, 257, 1729, 5574, 294, 300, 8546, 11, 472, 300, 286, 50634], "temperature": 0.0, "avg_logprob": -0.15391578870950287, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.02672535553574562}, {"id": 83, "seek": 41330, "start": 418.7, "end": 424.18, "text": " attempted to give some shape to with a book called The Stack on Software and Sovereignty that", "tokens": [50634, 18997, 281, 976, 512, 3909, 281, 365, 257, 1446, 1219, 440, 37649, 322, 27428, 293, 407, 5887, 26919, 300, 50908], "temperature": 0.0, "avg_logprob": -0.15391578870950287, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.02672535553574562}, {"id": 84, "seek": 41330, "start": 424.18, "end": 429.02000000000004, "text": " came out, was written about 10 years ago and came out very end of 2015, early 2016.", "tokens": [50908, 1361, 484, 11, 390, 3720, 466, 1266, 924, 2057, 293, 1361, 484, 588, 917, 295, 7546, 11, 2440, 6549, 13, 51150], "temperature": 0.0, "avg_logprob": -0.15391578870950287, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.02672535553574562}, {"id": 85, "seek": 41330, "start": 429.02000000000004, "end": 433.58000000000004, "text": " And that described an architecture of planetary computation that was based on a particular kind", "tokens": [51150, 400, 300, 7619, 364, 9482, 295, 35788, 24903, 300, 390, 2361, 322, 257, 1729, 733, 51378], "temperature": 0.0, "avg_logprob": -0.15391578870950287, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.02672535553574562}, {"id": 86, "seek": 41330, "start": 433.58000000000004, "end": 438.78000000000003, "text": " of modular stack system that had particular kind of architecture based on particular kind of", "tokens": [51378, 295, 31111, 8630, 1185, 300, 632, 1729, 733, 295, 9482, 2361, 322, 1729, 733, 295, 51638], "temperature": 0.0, "avg_logprob": -0.15391578870950287, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.02672535553574562}, {"id": 87, "seek": 43878, "start": 438.78, "end": 443.34, "text": " anointment architectures, the computational machine, procedural programming paradigms for", "tokens": [50364, 364, 3600, 518, 6331, 1303, 11, 264, 28270, 3479, 11, 43951, 9410, 13480, 328, 2592, 337, 50592], "temperature": 0.0, "avg_logprob": -0.17547104908869818, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.10654955357313156}, {"id": 88, "seek": 43878, "start": 443.34, "end": 447.53999999999996, "text": " software development. And, you know, perhaps more importantly, it was based on a paradigm that", "tokens": [50592, 4722, 3250, 13, 400, 11, 291, 458, 11, 4317, 544, 8906, 11, 309, 390, 2361, 322, 257, 24709, 300, 50802], "temperature": 0.0, "avg_logprob": -0.17547104908869818, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.10654955357313156}, {"id": 89, "seek": 43878, "start": 447.53999999999996, "end": 456.17999999999995, "text": " information was light and inexpensive and highly mobile and hardware was expensive and heavy and", "tokens": [50802, 1589, 390, 1442, 293, 28382, 293, 5405, 6013, 293, 8837, 390, 5124, 293, 4676, 293, 51234], "temperature": 0.0, "avg_logprob": -0.17547104908869818, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.10654955357313156}, {"id": 90, "seek": 43878, "start": 456.26, "end": 461.46, "text": " immobile and that you primarily want to move the information to the hardware because that's your", "tokens": [51238, 3397, 13632, 293, 300, 291, 10029, 528, 281, 1286, 264, 1589, 281, 264, 8837, 570, 300, 311, 428, 51498], "temperature": 0.0, "avg_logprob": -0.17547104908869818, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.10654955357313156}, {"id": 91, "seek": 43878, "start": 461.58, "end": 463.14, "text": " most efficient way of doing it.", "tokens": [51504, 881, 7148, 636, 295, 884, 309, 13, 51582], "temperature": 0.0, "avg_logprob": -0.17547104908869818, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.10654955357313156}, {"id": 92, "seek": 43878, "start": 463.38, "end": 466.29999999999995, "text": " In any event, I think quite clearly we're entering a different phase.", "tokens": [51594, 682, 604, 2280, 11, 286, 519, 1596, 4448, 321, 434, 11104, 257, 819, 5574, 13, 51740], "temperature": 0.0, "avg_logprob": -0.17547104908869818, "compression_ratio": 1.7204301075268817, "no_speech_prob": 0.10654955357313156}, {"id": 93, "seek": 46630, "start": 466.54, "end": 470.62, "text": " There's another 50 year cycle, or maybe it's only five, who knows how fast these things go.", "tokens": [50376, 821, 311, 1071, 2625, 1064, 6586, 11, 420, 1310, 309, 311, 787, 1732, 11, 567, 3255, 577, 2370, 613, 721, 352, 13, 50580], "temperature": 0.0, "avg_logprob": -0.10258280628859395, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.012043981812894344}, {"id": 94, "seek": 46630, "start": 470.62, "end": 475.46000000000004, "text": " But there's another, let's say, 50 year cycle coming where that architecture of planetary", "tokens": [50580, 583, 456, 311, 1071, 11, 718, 311, 584, 11, 2625, 1064, 6586, 1348, 689, 300, 9482, 295, 35788, 50822], "temperature": 0.0, "avg_logprob": -0.10258280628859395, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.012043981812894344}, {"id": 95, "seek": 46630, "start": 475.46000000000004, "end": 482.18, "text": " computation itself is being transformed in relationship to the structural and indeed", "tokens": [50822, 24903, 2564, 307, 885, 16894, 294, 2480, 281, 264, 15067, 293, 6451, 51158], "temperature": 0.0, "avg_logprob": -0.10258280628859395, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.012043981812894344}, {"id": 96, "seek": 46630, "start": 482.18, "end": 489.18, "text": " anatomical requirements of AI, which involves training foundational models at a very large", "tokens": [51158, 21618, 298, 804, 7728, 295, 7318, 11, 597, 11626, 3097, 32195, 5245, 412, 257, 588, 2416, 51508], "temperature": 0.0, "avg_logprob": -0.10258280628859395, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.012043981812894344}, {"id": 97, "seek": 46630, "start": 489.18, "end": 495.1, "text": " scale, hosting them at a large scale, serving them and applications built upon them at a large", "tokens": [51508, 4373, 11, 16058, 552, 412, 257, 2416, 4373, 11, 8148, 552, 293, 5821, 3094, 3564, 552, 412, 257, 2416, 51804], "temperature": 0.0, "avg_logprob": -0.10258280628859395, "compression_ratio": 1.7056603773584906, "no_speech_prob": 0.012043981812894344}, {"id": 98, "seek": 49510, "start": 495.1, "end": 501.42, "text": " scale, a different kind of programming logic from procedural programming to to different kinds of", "tokens": [50364, 4373, 11, 257, 819, 733, 295, 9410, 9952, 490, 43951, 9410, 281, 281, 819, 3685, 295, 50680], "temperature": 0.0, "avg_logprob": -0.13224175397087545, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0033710403367877007}, {"id": 99, "seek": 49510, "start": 501.42, "end": 506.1, "text": " prompt design, fine tuning, different ways of engaging that in a certain sense collapses the", "tokens": [50680, 12391, 1715, 11, 2489, 15164, 11, 819, 2098, 295, 11268, 300, 294, 257, 1629, 2020, 48765, 264, 50914], "temperature": 0.0, "avg_logprob": -0.13224175397087545, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0033710403367877007}, {"id": 100, "seek": 49510, "start": 506.1, "end": 511.94, "text": " distinction between using the AI and fine tuning the AI, I think quite clearly over the next few", "tokens": [50914, 16844, 1296, 1228, 264, 7318, 293, 2489, 15164, 264, 7318, 11, 286, 519, 1596, 4448, 670, 264, 958, 1326, 51206], "temperature": 0.0, "avg_logprob": -0.13224175397087545, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0033710403367877007}, {"id": 101, "seek": 49510, "start": 511.94, "end": 516.34, "text": " years that that space is going to collapse perhaps a little bit like the space between user and", "tokens": [51206, 924, 300, 300, 1901, 307, 516, 281, 15584, 4317, 257, 707, 857, 411, 264, 1901, 1296, 4195, 293, 51426], "temperature": 0.0, "avg_logprob": -0.13224175397087545, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0033710403367877007}, {"id": 102, "seek": 49510, "start": 516.34, "end": 522.5, "text": " designer in video games. And so that may it'll produce different monopolies, it'll destroy other", "tokens": [51426, 11795, 294, 960, 2813, 13, 400, 370, 300, 815, 309, 603, 5258, 819, 47721, 530, 11, 309, 603, 5293, 661, 51734], "temperature": 0.0, "avg_logprob": -0.13224175397087545, "compression_ratio": 1.83206106870229, "no_speech_prob": 0.0033710403367877007}, {"id": 103, "seek": 52250, "start": 522.5, "end": 528.02, "text": " monopolies. And I think also the sort of the flip in that paradigm between now, I mean, this would", "tokens": [50364, 47721, 530, 13, 400, 286, 519, 611, 264, 1333, 295, 264, 7929, 294, 300, 24709, 1296, 586, 11, 286, 914, 11, 341, 576, 50640], "temperature": 0.0, "avg_logprob": -0.11854372797785578, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.013621913269162178}, {"id": 104, "seek": 52250, "start": 528.02, "end": 535.34, "text": " involve one in which information itself is understood as big, heavy, expensive and immobile", "tokens": [50640, 9494, 472, 294, 597, 1589, 2564, 307, 7320, 382, 955, 11, 4676, 11, 5124, 293, 3397, 13632, 51006], "temperature": 0.0, "avg_logprob": -0.11854372797785578, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.013621913269162178}, {"id": 105, "seek": 52250, "start": 535.38, "end": 541.1, "text": " foundational models being essentially almost geologic or at least geographic in the kind of scale,", "tokens": [51008, 32195, 5245, 885, 4476, 1920, 1519, 36661, 420, 412, 1935, 32318, 294, 264, 733, 295, 4373, 11, 51294], "temperature": 0.0, "avg_logprob": -0.11854372797785578, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.013621913269162178}, {"id": 106, "seek": 52250, "start": 541.34, "end": 547.46, "text": " and pushing this to lots of different kinds of inexpensive mobile hardware. And so I think, I", "tokens": [51306, 293, 7380, 341, 281, 3195, 295, 819, 3685, 295, 28382, 6013, 8837, 13, 400, 370, 286, 519, 11, 286, 51612], "temperature": 0.0, "avg_logprob": -0.11854372797785578, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.013621913269162178}, {"id": 107, "seek": 52250, "start": 547.46, "end": 551.86, "text": " mean, that's also maybe one of the ways in which we can understand this flip is that there's a kind", "tokens": [51612, 914, 11, 300, 311, 611, 1310, 472, 295, 264, 2098, 294, 597, 321, 393, 1223, 341, 7929, 307, 300, 456, 311, 257, 733, 51832], "temperature": 0.0, "avg_logprob": -0.11854372797785578, "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.013621913269162178}, {"id": 108, "seek": 55186, "start": 551.98, "end": 558.34, "text": " of swapping places between the relative economies of hardware software in that dynamic. This is one", "tokens": [50370, 295, 1693, 10534, 3190, 1296, 264, 4972, 23158, 295, 8837, 4722, 294, 300, 8546, 13, 639, 307, 472, 50688], "temperature": 0.0, "avg_logprob": -0.10983940688046542, "compression_ratio": 1.6049382716049383, "no_speech_prob": 4.132659159949981e-05}, {"id": 109, "seek": 55186, "start": 558.34, "end": 563.62, "text": " of the things that a lot of my research is exploring now looking at issues of human AI interaction", "tokens": [50688, 295, 264, 721, 300, 257, 688, 295, 452, 2132, 307, 12736, 586, 1237, 412, 2663, 295, 1952, 7318, 9285, 50952], "temperature": 0.0, "avg_logprob": -0.10983940688046542, "compression_ratio": 1.6049382716049383, "no_speech_prob": 4.132659159949981e-05}, {"id": 110, "seek": 55186, "start": 563.62, "end": 569.42, "text": " design, newer emerging forms of philosophy around AI, and ways in which let's say design and", "tokens": [50952, 1715, 11, 17628, 14989, 6422, 295, 10675, 926, 7318, 11, 293, 2098, 294, 597, 718, 311, 584, 1715, 293, 51242], "temperature": 0.0, "avg_logprob": -0.10983940688046542, "compression_ratio": 1.6049382716049383, "no_speech_prob": 4.132659159949981e-05}, {"id": 111, "seek": 55186, "start": 569.42, "end": 576.46, "text": " specular design in a rather peculiar idiosyncratic kind of genre that we've developed allows us to", "tokens": [51242, 1608, 1040, 1715, 294, 257, 2831, 27149, 4496, 2717, 2534, 10757, 2399, 733, 295, 11022, 300, 321, 600, 4743, 4045, 505, 281, 51594], "temperature": 0.0, "avg_logprob": -0.10983940688046542, "compression_ratio": 1.6049382716049383, "no_speech_prob": 4.132659159949981e-05}, {"id": 112, "seek": 57646, "start": 576.46, "end": 582.0600000000001, "text": " explore some of these spaces with with more curiosity with an idea that the exploration of", "tokens": [50364, 6839, 512, 295, 613, 7673, 365, 365, 544, 18769, 365, 364, 1558, 300, 264, 16197, 295, 50644], "temperature": 0.0, "avg_logprob": -0.10828144810780757, "compression_ratio": 1.7874564459930313, "no_speech_prob": 0.10651960968971252}, {"id": 113, "seek": 57646, "start": 582.0600000000001, "end": 587.9000000000001, "text": " the space of stochastic possibility around this is a way of understanding what's going on and", "tokens": [50644, 264, 1901, 295, 342, 8997, 2750, 7959, 926, 341, 307, 257, 636, 295, 3701, 437, 311, 516, 322, 293, 50936], "temperature": 0.0, "avg_logprob": -0.10828144810780757, "compression_ratio": 1.7874564459930313, "no_speech_prob": 0.10651960968971252}, {"id": 114, "seek": 57646, "start": 587.9000000000001, "end": 592.26, "text": " maybe indeed sort of catching up with the present to analyze this as opposed to some other kinds of", "tokens": [50936, 1310, 6451, 1333, 295, 16124, 493, 365, 264, 1974, 281, 12477, 341, 382, 8851, 281, 512, 661, 3685, 295, 51154], "temperature": 0.0, "avg_logprob": -0.10828144810780757, "compression_ratio": 1.7874564459930313, "no_speech_prob": 0.10651960968971252}, {"id": 115, "seek": 57646, "start": 592.26, "end": 596.26, "text": " approaches. So in any event, that's a little bit of where the work stands and a little bit of", "tokens": [51154, 11587, 13, 407, 294, 604, 2280, 11, 300, 311, 257, 707, 857, 295, 689, 264, 589, 7382, 293, 257, 707, 857, 295, 51354], "temperature": 0.0, "avg_logprob": -0.10828144810780757, "compression_ratio": 1.7874564459930313, "no_speech_prob": 0.10651960968971252}, {"id": 116, "seek": 57646, "start": 596.26, "end": 597.4200000000001, "text": " background and where it came from.", "tokens": [51354, 3678, 293, 689, 309, 1361, 490, 13, 51412], "temperature": 0.0, "avg_logprob": -0.10828144810780757, "compression_ratio": 1.7874564459930313, "no_speech_prob": 0.10651960968971252}, {"id": 117, "seek": 57646, "start": 598.5400000000001, "end": 603.82, "text": " So when you speak about this moment in hardware, at least in terms of like some kind of rebalancing", "tokens": [51468, 407, 562, 291, 1710, 466, 341, 1623, 294, 8837, 11, 412, 1935, 294, 2115, 295, 411, 512, 733, 295, 319, 2645, 8779, 51732], "temperature": 0.0, "avg_logprob": -0.10828144810780757, "compression_ratio": 1.7874564459930313, "no_speech_prob": 0.10651960968971252}, {"id": 118, "seek": 60382, "start": 603.82, "end": 611.0200000000001, "text": " or some kind of paradigmatic change from these immobile, fixed, expensive data centers to some", "tokens": [50364, 420, 512, 733, 295, 24709, 2399, 1319, 490, 613, 3397, 13632, 11, 6806, 11, 5124, 1412, 10898, 281, 512, 50724], "temperature": 0.0, "avg_logprob": -0.15192965803475217, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.015408623963594437}, {"id": 119, "seek": 60382, "start": 611.0200000000001, "end": 616.34, "text": " cheaper, more fluid, more mobile technologies, are you referring to things like edge ML and", "tokens": [50724, 12284, 11, 544, 9113, 11, 544, 6013, 7943, 11, 366, 291, 13761, 281, 721, 411, 4691, 21601, 293, 50990], "temperature": 0.0, "avg_logprob": -0.15192965803475217, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.015408623963594437}, {"id": 120, "seek": 60382, "start": 616.38, "end": 621.7, "text": " microprocessor integration, like computation moving closer to the ground, so to speak, and like, you", "tokens": [50992, 3123, 1513, 340, 25432, 10980, 11, 411, 24903, 2684, 4966, 281, 264, 2727, 11, 370, 281, 1710, 11, 293, 411, 11, 291, 51258], "temperature": 0.0, "avg_logprob": -0.15192965803475217, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.015408623963594437}, {"id": 121, "seek": 60382, "start": 621.7, "end": 625.6600000000001, "text": " know, being less reliant on large amounts of data flowing upstream? Or is there some other thought", "tokens": [51258, 458, 11, 885, 1570, 1039, 5798, 322, 2416, 11663, 295, 1412, 13974, 33915, 30, 1610, 307, 456, 512, 661, 1194, 51456], "temperature": 0.0, "avg_logprob": -0.15192965803475217, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.015408623963594437}, {"id": 122, "seek": 60382, "start": 625.6600000000001, "end": 629.46, "text": " here? And I guess there's like a corollary to that there. Like, does this point to some kind of", "tokens": [51456, 510, 30, 400, 286, 2041, 456, 311, 411, 257, 1181, 1833, 822, 281, 300, 456, 13, 1743, 11, 775, 341, 935, 281, 512, 733, 295, 51646], "temperature": 0.0, "avg_logprob": -0.15192965803475217, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.015408623963594437}, {"id": 123, "seek": 62946, "start": 629.46, "end": 636.38, "text": " rebalancing in favor of decentralization, as opposed to centralized hardware software ecosystems?", "tokens": [50364, 319, 2645, 8779, 294, 2294, 295, 26515, 2144, 11, 382, 8851, 281, 32395, 8837, 4722, 32647, 30, 50710], "temperature": 0.0, "avg_logprob": -0.1387368749689173, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.06948589533567429}, {"id": 124, "seek": 62946, "start": 636.38, "end": 640.62, "text": " Like, is decentralization a paradigm worth talking about in the context of AI?", "tokens": [50710, 1743, 11, 307, 26515, 2144, 257, 24709, 3163, 1417, 466, 294, 264, 4319, 295, 7318, 30, 50922], "temperature": 0.0, "avg_logprob": -0.1387368749689173, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.06948589533567429}, {"id": 125, "seek": 62946, "start": 640.82, "end": 645.34, "text": " It is, but it's incomplete. I think one of the things I understand is I think even with the stack", "tokens": [50932, 467, 307, 11, 457, 309, 311, 31709, 13, 286, 519, 472, 295, 264, 721, 286, 1223, 307, 286, 519, 754, 365, 264, 8630, 51158], "temperature": 0.0, "avg_logprob": -0.1387368749689173, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.06948589533567429}, {"id": 126, "seek": 62946, "start": 645.34, "end": 650.7, "text": " model that I described in the book, the relationship between centralization and decentralization was", "tokens": [51158, 2316, 300, 286, 7619, 294, 264, 1446, 11, 264, 2480, 1296, 5777, 2144, 293, 26515, 2144, 390, 51426], "temperature": 0.0, "avg_logprob": -0.1387368749689173, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.06948589533567429}, {"id": 127, "seek": 62946, "start": 650.7, "end": 655.86, "text": " not one of opposition where the more you have of one, the less you have of the other in a kind of", "tokens": [51426, 406, 472, 295, 13504, 689, 264, 544, 291, 362, 295, 472, 11, 264, 1570, 291, 362, 295, 264, 661, 294, 257, 733, 295, 51684], "temperature": 0.0, "avg_logprob": -0.1387368749689173, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.06948589533567429}, {"id": 128, "seek": 65586, "start": 655.86, "end": 661.7, "text": " zero-sum sort of way, but rather that these two topologies of organization were actually totally", "tokens": [50364, 4018, 12, 82, 449, 1333, 295, 636, 11, 457, 2831, 300, 613, 732, 1192, 6204, 295, 4475, 645, 767, 3879, 50656], "temperature": 0.0, "avg_logprob": -0.11063024630913368, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.3199634552001953}, {"id": 129, "seek": 65586, "start": 661.72, "end": 666.86, "text": " mutually dependent upon one another, that you have cloud platforms that are highly centralized", "tokens": [50657, 39144, 12334, 3564, 472, 1071, 11, 300, 291, 362, 4588, 9473, 300, 366, 5405, 32395, 50914], "temperature": 0.0, "avg_logprob": -0.11063024630913368, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.3199634552001953}, {"id": 130, "seek": 65586, "start": 666.9, "end": 671.1800000000001, "text": " only because you have lots of devices that are connecting to the cloud that are highly", "tokens": [50916, 787, 570, 291, 362, 3195, 295, 5759, 300, 366, 11015, 281, 264, 4588, 300, 366, 5405, 51130], "temperature": 0.0, "avg_logprob": -0.11063024630913368, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.3199634552001953}, {"id": 131, "seek": 65586, "start": 671.1800000000001, "end": 676.9, "text": " decentralized and vice versa. And so one might say that something like, you know, looking at all of", "tokens": [51130, 32870, 293, 11964, 25650, 13, 400, 370, 472, 1062, 584, 300, 746, 411, 11, 291, 458, 11, 1237, 412, 439, 295, 51416], "temperature": 0.0, "avg_logprob": -0.11063024630913368, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.3199634552001953}, {"id": 132, "seek": 65586, "start": 676.9, "end": 682.58, "text": " the different builds and deployments of Android globally, and then the ways in which, you know,", "tokens": [51416, 264, 819, 15182, 293, 7274, 1117, 295, 8853, 18958, 11, 293, 550, 264, 2098, 294, 597, 11, 291, 458, 11, 51700], "temperature": 0.0, "avg_logprob": -0.11063024630913368, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.3199634552001953}, {"id": 133, "seek": 68258, "start": 682.58, "end": 687.9000000000001, "text": " these are making calls to Google data centers and at least a number of the builds, just as an example,", "tokens": [50364, 613, 366, 1455, 5498, 281, 3329, 1412, 10898, 293, 412, 1935, 257, 1230, 295, 264, 15182, 11, 445, 382, 364, 1365, 11, 50630], "temperature": 0.0, "avg_logprob": -0.13226720382427348, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.020315466448664665}, {"id": 134, "seek": 68258, "start": 688.1, "end": 692.98, "text": " that there's a highly centralized dynamic, right? When you log into your Gmail, you're pinging the", "tokens": [50640, 300, 456, 311, 257, 5405, 32395, 8546, 11, 558, 30, 1133, 291, 3565, 666, 428, 36732, 11, 291, 434, 280, 8716, 264, 50884], "temperature": 0.0, "avg_logprob": -0.13226720382427348, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.020315466448664665}, {"id": 135, "seek": 68258, "start": 692.98, "end": 698.5, "text": " mothership in a way that one might see as a kind of hub and spoke centralization. At the same time,", "tokens": [50884, 17941, 1210, 294, 257, 636, 300, 472, 1062, 536, 382, 257, 733, 295, 11838, 293, 7179, 5777, 2144, 13, 1711, 264, 912, 565, 11, 51160], "temperature": 0.0, "avg_logprob": -0.13226720382427348, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.020315466448664665}, {"id": 136, "seek": 68258, "start": 698.5400000000001, "end": 703.86, "text": " all of the devices that are doing this are quite free range in their movement as sort of slightly", "tokens": [51162, 439, 295, 264, 5759, 300, 366, 884, 341, 366, 1596, 1737, 3613, 294, 641, 3963, 382, 1333, 295, 4748, 51428], "temperature": 0.0, "avg_logprob": -0.13226720382427348, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.020315466448664665}, {"id": 137, "seek": 68258, "start": 703.86, "end": 708.58, "text": " intentional agents across the surface of the globe and are connecting with each other in lots of", "tokens": [51428, 21935, 12554, 2108, 264, 3753, 295, 264, 15371, 293, 366, 11015, 365, 1184, 661, 294, 3195, 295, 51664], "temperature": 0.0, "avg_logprob": -0.13226720382427348, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.020315466448664665}, {"id": 138, "seek": 70858, "start": 708.58, "end": 713.14, "text": " different strange kinds of configurations. All of which is to say is it's both centralized and", "tokens": [50364, 819, 5861, 3685, 295, 31493, 13, 1057, 295, 597, 307, 281, 584, 307, 309, 311, 1293, 32395, 293, 50592], "temperature": 0.0, "avg_logprob": -0.15017758754261754, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.1915867179632187}, {"id": 139, "seek": 70858, "start": 713.14, "end": 718.7800000000001, "text": " decentralized at the same time. And indeed, it's one because the other. And so this is not to say", "tokens": [50592, 32870, 412, 264, 912, 565, 13, 400, 6451, 11, 309, 311, 472, 570, 264, 661, 13, 400, 370, 341, 307, 406, 281, 584, 50874], "temperature": 0.0, "avg_logprob": -0.15017758754261754, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.1915867179632187}, {"id": 140, "seek": 70858, "start": 718.7800000000001, "end": 725.34, "text": " that some of the people who've been giving a lot of thought to much more fully decentralized systems", "tokens": [50874, 300, 512, 295, 264, 561, 567, 600, 668, 2902, 257, 688, 295, 1194, 281, 709, 544, 4498, 32870, 3652, 51202], "temperature": 0.0, "avg_logprob": -0.15017758754261754, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.1915867179632187}, {"id": 141, "seek": 70858, "start": 725.34, "end": 730.74, "text": " by which you can have what we might think of as edge agents who are working both as, or we have", "tokens": [51202, 538, 597, 291, 393, 362, 437, 321, 1062, 519, 295, 382, 4691, 12554, 567, 366, 1364, 1293, 382, 11, 420, 321, 362, 51472], "temperature": 0.0, "avg_logprob": -0.15017758754261754, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.1915867179632187}, {"id": 142, "seek": 70858, "start": 730.74, "end": 734.34, "text": " agents that are both sort of edge agents and server agents, I suppose might be a way of saying", "tokens": [51472, 12554, 300, 366, 1293, 1333, 295, 4691, 12554, 293, 7154, 12554, 11, 286, 7297, 1062, 312, 257, 636, 295, 1566, 51652], "temperature": 0.0, "avg_logprob": -0.15017758754261754, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.1915867179632187}, {"id": 143, "seek": 73434, "start": 734.34, "end": 738.7800000000001, "text": " this simultaneously, not to say that there isn't a there there. I just want to sort of understand", "tokens": [50364, 341, 16561, 11, 406, 281, 584, 300, 456, 1943, 380, 257, 456, 456, 13, 286, 445, 528, 281, 1333, 295, 1223, 50586], "temperature": 0.0, "avg_logprob": -0.15214138496212842, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.23890940845012665}, {"id": 144, "seek": 73434, "start": 738.7800000000001, "end": 743.6600000000001, "text": " the point that the status quo that we're describing is it would be inappropriate to see as entirely", "tokens": [50586, 264, 935, 300, 264, 6558, 28425, 300, 321, 434, 16141, 307, 309, 576, 312, 26723, 281, 536, 382, 7696, 50830], "temperature": 0.0, "avg_logprob": -0.15214138496212842, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.23890940845012665}, {"id": 145, "seek": 73434, "start": 743.6600000000001, "end": 748.26, "text": " one or the other. There's connections I suppose to where you know, an AI stack, if we want to call", "tokens": [50830, 472, 420, 264, 661, 13, 821, 311, 9271, 286, 7297, 281, 689, 291, 458, 11, 364, 7318, 8630, 11, 498, 321, 528, 281, 818, 51060], "temperature": 0.0, "avg_logprob": -0.15214138496212842, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.23890940845012665}, {"id": 146, "seek": 73434, "start": 748.26, "end": 752.98, "text": " it that is going in this as well. What I really meant to say is that, you know, while the weights", "tokens": [51060, 309, 300, 307, 516, 294, 341, 382, 731, 13, 708, 286, 534, 4140, 281, 584, 307, 300, 11, 291, 458, 11, 1339, 264, 17443, 51296], "temperature": 0.0, "avg_logprob": -0.15214138496212842, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.23890940845012665}, {"id": 147, "seek": 73434, "start": 752.98, "end": 760.0600000000001, "text": " of a large model are relatively, I mean, you could fit it on a USB stick, the scale necessary to", "tokens": [51296, 295, 257, 2416, 2316, 366, 7226, 11, 286, 914, 11, 291, 727, 3318, 309, 322, 257, 10109, 2897, 11, 264, 4373, 4818, 281, 51650], "temperature": 0.0, "avg_logprob": -0.15214138496212842, "compression_ratio": 1.6815068493150684, "no_speech_prob": 0.23890940845012665}, {"id": 148, "seek": 76006, "start": 760.06, "end": 765.6999999999999, "text": " actually integrate the models and to serve them in relationship to applications. Like if you wanted", "tokens": [50364, 767, 13365, 264, 5245, 293, 281, 4596, 552, 294, 2480, 281, 5821, 13, 1743, 498, 291, 1415, 50646], "temperature": 0.0, "avg_logprob": -0.12514686584472656, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.326854944229126}, {"id": 149, "seek": 76006, "start": 765.6999999999999, "end": 771.0999999999999, "text": " to think about how you would build Gemini into Gmail, for example, such that it can be used as", "tokens": [50646, 281, 519, 466, 577, 291, 576, 1322, 22894, 3812, 666, 36732, 11, 337, 1365, 11, 1270, 300, 309, 393, 312, 1143, 382, 50916], "temperature": 0.0, "avg_logprob": -0.12514686584472656, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.326854944229126}, {"id": 150, "seek": 76006, "start": 771.0999999999999, "end": 777.26, "text": " the basis for this at the scale of billions and billions of users at one time, not to mention that,", "tokens": [50916, 264, 5143, 337, 341, 412, 264, 4373, 295, 17375, 293, 17375, 295, 5022, 412, 472, 565, 11, 406, 281, 2152, 300, 11, 51224], "temperature": 0.0, "avg_logprob": -0.12514686584472656, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.326854944229126}, {"id": 151, "seek": 76006, "start": 777.26, "end": 782.06, "text": " you know, the scale necessary to host training data to continue to update the models and all the rest", "tokens": [51224, 291, 458, 11, 264, 4373, 4818, 281, 3975, 3097, 1412, 281, 2354, 281, 5623, 264, 5245, 293, 439, 264, 1472, 51464], "temperature": 0.0, "avg_logprob": -0.12514686584472656, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.326854944229126}, {"id": 152, "seek": 76006, "start": 782.06, "end": 788.2199999999999, "text": " is going to think the exabyte upon exabyte scale of information that is being produced and processed", "tokens": [51464, 307, 516, 281, 519, 264, 454, 34529, 3564, 454, 34529, 4373, 295, 1589, 300, 307, 885, 7126, 293, 18846, 51772], "temperature": 0.0, "avg_logprob": -0.12514686584472656, "compression_ratio": 1.813868613138686, "no_speech_prob": 0.326854944229126}, {"id": 153, "seek": 78822, "start": 788.22, "end": 793.98, "text": " and calculated is one that is simply impossible to move quickly through the pipes to get from one", "tokens": [50364, 293, 15598, 307, 472, 300, 307, 2935, 6243, 281, 1286, 2661, 807, 264, 21882, 281, 483, 490, 472, 50652], "temperature": 0.0, "avg_logprob": -0.1126159839942807, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.20120011270046234}, {"id": 154, "seek": 78822, "start": 793.98, "end": 799.38, "text": " place to another. Right. It's so big that even with, you know, fiber infrastructure that in", "tokens": [50652, 1081, 281, 1071, 13, 1779, 13, 467, 311, 370, 955, 300, 754, 365, 11, 291, 458, 11, 12874, 6896, 300, 294, 50922], "temperature": 0.0, "avg_logprob": -0.1126159839942807, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.20120011270046234}, {"id": 155, "seek": 78822, "start": 799.38, "end": 803.74, "text": " essence, you would think of it as like, that's the heavy thing that's hard to move the data, you", "tokens": [50922, 12801, 11, 291, 576, 519, 295, 309, 382, 411, 11, 300, 311, 264, 4676, 551, 300, 311, 1152, 281, 1286, 264, 1412, 11, 291, 51140], "temperature": 0.0, "avg_logprob": -0.1126159839942807, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.20120011270046234}, {"id": 156, "seek": 78822, "start": 803.74, "end": 809.22, "text": " know, and you see this quite often in sort of the processes of work is that it's easier to move the", "tokens": [51140, 458, 11, 293, 291, 536, 341, 1596, 2049, 294, 1333, 295, 264, 7555, 295, 589, 307, 300, 309, 311, 3571, 281, 1286, 264, 51414], "temperature": 0.0, "avg_logprob": -0.1126159839942807, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.20120011270046234}, {"id": 157, "seek": 78822, "start": 809.22, "end": 814.5, "text": " compute to the data than it is to move the data to the compute. And so that's what I mean. And", "tokens": [51414, 14722, 281, 264, 1412, 813, 309, 307, 281, 1286, 264, 1412, 281, 264, 14722, 13, 400, 370, 300, 311, 437, 286, 914, 13, 400, 51678], "temperature": 0.0, "avg_logprob": -0.1126159839942807, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.20120011270046234}, {"id": 158, "seek": 81450, "start": 814.5, "end": 818.34, "text": " that's the flip, right? Instead of it being, we're going to move the data to the compute because", "tokens": [50364, 300, 311, 264, 7929, 11, 558, 30, 7156, 295, 309, 885, 11, 321, 434, 516, 281, 1286, 264, 1412, 281, 264, 14722, 570, 50556], "temperature": 0.0, "avg_logprob": -0.14030387978148617, "compression_ratio": 1.8246153846153845, "no_speech_prob": 0.11265720427036285}, {"id": 159, "seek": 81450, "start": 818.34, "end": 823.22, "text": " compute is expensive and hardware is expensive and data is cheap and light. We see this paradigmatic", "tokens": [50556, 14722, 307, 5124, 293, 8837, 307, 5124, 293, 1412, 307, 7084, 293, 1442, 13, 492, 536, 341, 24709, 2399, 50800], "temperature": 0.0, "avg_logprob": -0.14030387978148617, "compression_ratio": 1.8246153846153845, "no_speech_prob": 0.11265720427036285}, {"id": 160, "seek": 81450, "start": 823.3, "end": 827.86, "text": " flip and I think it's really more foundational that it's cheaper to move compute to the data than the", "tokens": [50804, 7929, 293, 286, 519, 309, 311, 534, 544, 32195, 300, 309, 311, 12284, 281, 1286, 14722, 281, 264, 1412, 813, 264, 51032], "temperature": 0.0, "avg_logprob": -0.14030387978148617, "compression_ratio": 1.8246153846153845, "no_speech_prob": 0.11265720427036285}, {"id": 161, "seek": 81450, "start": 827.86, "end": 832.34, "text": " other way around. Now, does it have to do with a lot of edge, you know, the relationship to edge ML", "tokens": [51032, 661, 636, 926, 13, 823, 11, 775, 309, 362, 281, 360, 365, 257, 688, 295, 4691, 11, 291, 458, 11, 264, 2480, 281, 4691, 21601, 51256], "temperature": 0.0, "avg_logprob": -0.14030387978148617, "compression_ratio": 1.8246153846153845, "no_speech_prob": 0.11265720427036285}, {"id": 162, "seek": 81450, "start": 832.34, "end": 836.94, "text": " and the rest of it? Yes, for sure. And what by edge ML, I think just for the listeners, you know,", "tokens": [51256, 293, 264, 1472, 295, 309, 30, 1079, 11, 337, 988, 13, 400, 437, 538, 4691, 21601, 11, 286, 519, 445, 337, 264, 23274, 11, 291, 458, 11, 51486], "temperature": 0.0, "avg_logprob": -0.14030387978148617, "compression_ratio": 1.8246153846153845, "no_speech_prob": 0.11265720427036285}, {"id": 163, "seek": 81450, "start": 836.94, "end": 842.5, "text": " I think what we sort of convene there are ways in which devices that are part of a, let's say a", "tokens": [51486, 286, 519, 437, 321, 1333, 295, 7158, 68, 456, 366, 2098, 294, 597, 5759, 300, 366, 644, 295, 257, 11, 718, 311, 584, 257, 51764], "temperature": 0.0, "avg_logprob": -0.14030387978148617, "compression_ratio": 1.8246153846153845, "no_speech_prob": 0.11265720427036285}, {"id": 164, "seek": 84250, "start": 842.5, "end": 850.42, "text": " learning network, that is they are making use of models in order to negotiate whatever it is that", "tokens": [50364, 2539, 3209, 11, 300, 307, 436, 366, 1455, 764, 295, 5245, 294, 1668, 281, 21713, 2035, 309, 307, 300, 50760], "temperature": 0.0, "avg_logprob": -0.10420940917672462, "compression_ratio": 2.0, "no_speech_prob": 0.06742266565561295}, {"id": 165, "seek": 84250, "start": 850.42, "end": 855.14, "text": " they're doing in the world, whatever insect intelligence they may have on whatever they're", "tokens": [50760, 436, 434, 884, 294, 264, 1002, 11, 2035, 13261, 7599, 436, 815, 362, 322, 2035, 436, 434, 50996], "temperature": 0.0, "avg_logprob": -0.10420940917672462, "compression_ratio": 2.0, "no_speech_prob": 0.06742266565561295}, {"id": 166, "seek": 84250, "start": 855.14, "end": 859.96, "text": " doing or however complex that may be. But in the process of using that model, they are also", "tokens": [50996, 884, 420, 4461, 3997, 300, 815, 312, 13, 583, 294, 264, 1399, 295, 1228, 300, 2316, 11, 436, 366, 611, 51237], "temperature": 0.0, "avg_logprob": -0.10420940917672462, "compression_ratio": 2.0, "no_speech_prob": 0.06742266565561295}, {"id": 167, "seek": 84250, "start": 859.96, "end": 863.42, "text": " interacting with the world. And because they're interacting with the world, they are also in", "tokens": [51237, 18017, 365, 264, 1002, 13, 400, 570, 436, 434, 18017, 365, 264, 1002, 11, 436, 366, 611, 294, 51410], "temperature": 0.0, "avg_logprob": -0.10420940917672462, "compression_ratio": 2.0, "no_speech_prob": 0.06742266565561295}, {"id": 168, "seek": 84250, "start": 863.42, "end": 869.26, "text": " principle creating new data about the world and about their behavior, which in principle, should", "tokens": [51410, 8665, 4084, 777, 1412, 466, 264, 1002, 293, 466, 641, 5223, 11, 597, 294, 8665, 11, 820, 51702], "temperature": 0.0, "avg_logprob": -0.10420940917672462, "compression_ratio": 2.0, "no_speech_prob": 0.06742266565561295}, {"id": 169, "seek": 86926, "start": 869.26, "end": 875.42, "text": " be able to reweight the model that they themselves are using in some sort of mechanism. And so one", "tokens": [50364, 312, 1075, 281, 319, 12329, 264, 2316, 300, 436, 2969, 366, 1228, 294, 512, 1333, 295, 7513, 13, 400, 370, 472, 50672], "temperature": 0.0, "avg_logprob": -0.1551131881585642, "compression_ratio": 1.6840277777777777, "no_speech_prob": 0.14014722406864166}, {"id": 170, "seek": 86926, "start": 875.42, "end": 880.3, "text": " of our collaborators on a lot of work that we've done around this issue for some time is a fellow", "tokens": [50672, 295, 527, 39789, 322, 257, 688, 295, 589, 300, 321, 600, 1096, 926, 341, 2734, 337, 512, 565, 307, 257, 7177, 50916], "temperature": 0.0, "avg_logprob": -0.1551131881585642, "compression_ratio": 1.6840277777777777, "no_speech_prob": 0.14014722406864166}, {"id": 171, "seek": 86926, "start": 880.3, "end": 885.06, "text": " of Google research named Blaise Aquari-Arkos, who was the inventor of what's called federated", "tokens": [50916, 295, 3329, 2132, 4926, 18925, 908, 8728, 3504, 12, 10683, 74, 329, 11, 567, 390, 264, 41593, 295, 437, 311, 1219, 38024, 770, 51154], "temperature": 0.0, "avg_logprob": -0.1551131881585642, "compression_ratio": 1.6840277777777777, "no_speech_prob": 0.14014722406864166}, {"id": 172, "seek": 86926, "start": 885.06, "end": 889.9399999999999, "text": " learning, which is a process by which, for example, the scenario I just described where you have", "tokens": [51154, 2539, 11, 597, 307, 257, 1399, 538, 597, 11, 337, 1365, 11, 264, 9005, 286, 445, 7619, 689, 291, 362, 51398], "temperature": 0.0, "avg_logprob": -0.1551131881585642, "compression_ratio": 1.6840277777777777, "no_speech_prob": 0.14014722406864166}, {"id": 173, "seek": 86926, "start": 889.9399999999999, "end": 895.54, "text": " little devices, they could be sensors, they could be a person's phone, they could be any machine,", "tokens": [51398, 707, 5759, 11, 436, 727, 312, 14840, 11, 436, 727, 312, 257, 954, 311, 2593, 11, 436, 727, 312, 604, 3479, 11, 51678], "temperature": 0.0, "avg_logprob": -0.1551131881585642, "compression_ratio": 1.6840277777777777, "no_speech_prob": 0.14014722406864166}, {"id": 174, "seek": 89554, "start": 895.62, "end": 900.5, "text": " animal, vegetable or mineral, that is not only using models in a functional way, but is producing", "tokens": [50368, 5496, 11, 16356, 420, 21630, 11, 300, 307, 406, 787, 1228, 5245, 294, 257, 11745, 636, 11, 457, 307, 10501, 50612], "temperature": 0.0, "avg_logprob": -0.1440214309017215, "compression_ratio": 1.828358208955224, "no_speech_prob": 0.3767147660255432}, {"id": 175, "seek": 89554, "start": 900.5, "end": 905.74, "text": " data about the world through the use of their models can in fact participate in the retraining", "tokens": [50612, 1412, 466, 264, 1002, 807, 264, 764, 295, 641, 5245, 393, 294, 1186, 8197, 294, 264, 49356, 1760, 50874], "temperature": 0.0, "avg_logprob": -0.1440214309017215, "compression_ratio": 1.828358208955224, "no_speech_prob": 0.3767147660255432}, {"id": 176, "seek": 89554, "start": 905.78, "end": 911.3, "text": " of the weights of the models in ways in which the underlying data that they're producing remains", "tokens": [50876, 295, 264, 17443, 295, 264, 5245, 294, 2098, 294, 597, 264, 14217, 1412, 300, 436, 434, 10501, 7023, 51152], "temperature": 0.0, "avg_logprob": -0.1440214309017215, "compression_ratio": 1.828358208955224, "no_speech_prob": 0.3767147660255432}, {"id": 177, "seek": 89554, "start": 911.3, "end": 917.3399999999999, "text": " anonymous and remains opaque. It doesn't need to be disclosed to the whole repness as well, which is", "tokens": [51152, 24932, 293, 7023, 42687, 13, 467, 1177, 380, 643, 281, 312, 17092, 1744, 281, 264, 1379, 1085, 1287, 382, 731, 11, 597, 307, 51454], "temperature": 0.0, "avg_logprob": -0.1440214309017215, "compression_ratio": 1.828358208955224, "no_speech_prob": 0.3767147660255432}, {"id": 178, "seek": 89554, "start": 917.38, "end": 922.38, "text": " ultimately what you want, right? What you want is a world in which my health data, your health data", "tokens": [51456, 6284, 437, 291, 528, 11, 558, 30, 708, 291, 528, 307, 257, 1002, 294, 597, 452, 1585, 1412, 11, 428, 1585, 1412, 51706], "temperature": 0.0, "avg_logprob": -0.1440214309017215, "compression_ratio": 1.828358208955224, "no_speech_prob": 0.3767147660255432}, {"id": 179, "seek": 92238, "start": 922.42, "end": 927.18, "text": " remain private, that I don't, you know, I don't look see your health data, you don't see my health", "tokens": [50366, 6222, 4551, 11, 300, 286, 500, 380, 11, 291, 458, 11, 286, 500, 380, 574, 536, 428, 1585, 1412, 11, 291, 500, 380, 536, 452, 1585, 50604], "temperature": 0.0, "avg_logprob": -0.10512724999458559, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.010969368740916252}, {"id": 180, "seek": 92238, "start": 927.18, "end": 933.9, "text": " data, but that the relevant information in my health data and your health data are able to, let's", "tokens": [50604, 1412, 11, 457, 300, 264, 7340, 1589, 294, 452, 1585, 1412, 293, 428, 1585, 1412, 366, 1075, 281, 11, 718, 311, 50940], "temperature": 0.0, "avg_logprob": -0.10512724999458559, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.010969368740916252}, {"id": 181, "seek": 92238, "start": 933.9, "end": 939.1, "text": " say, reweight medical models that might be mutually beneficial to both of us. Is that", "tokens": [50940, 584, 11, 319, 12329, 4625, 5245, 300, 1062, 312, 39144, 14072, 281, 1293, 295, 505, 13, 1119, 300, 51200], "temperature": 0.0, "avg_logprob": -0.10512724999458559, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.010969368740916252}, {"id": 182, "seek": 92238, "start": 939.1, "end": 944.5, "text": " centralization or decentralization? It's both. It's both in a different kind of way. It's a", "tokens": [51200, 5777, 2144, 420, 26515, 2144, 30, 467, 311, 1293, 13, 467, 311, 1293, 294, 257, 819, 733, 295, 636, 13, 467, 311, 257, 51470], "temperature": 0.0, "avg_logprob": -0.10512724999458559, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.010969368740916252}, {"id": 183, "seek": 92238, "start": 944.5, "end": 949.02, "text": " different kind of way than the other model, but it's clearly both at once. And so I do think it's", "tokens": [51470, 819, 733, 295, 636, 813, 264, 661, 2316, 11, 457, 309, 311, 4448, 1293, 412, 1564, 13, 400, 370, 286, 360, 519, 309, 311, 51696], "temperature": 0.0, "avg_logprob": -0.10512724999458559, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.010969368740916252}, {"id": 184, "seek": 94902, "start": 949.02, "end": 953.34, "text": " fine. It's valuable to think about centralization and decentralization as kind of topological", "tokens": [50364, 2489, 13, 467, 311, 8263, 281, 519, 466, 5777, 2144, 293, 26515, 2144, 382, 733, 295, 1192, 4383, 50580], "temperature": 0.0, "avg_logprob": -0.10547817026385824, "compression_ratio": 1.797427652733119, "no_speech_prob": 0.029244378209114075}, {"id": 185, "seek": 94902, "start": 953.34, "end": 958.6999999999999, "text": " heuristics to understand this. I think it would be a huge mistake to think of one as intrinsically", "tokens": [50580, 415, 374, 6006, 281, 1223, 341, 13, 286, 519, 309, 576, 312, 257, 2603, 6146, 281, 519, 295, 472, 382, 28621, 984, 50848], "temperature": 0.0, "avg_logprob": -0.10547817026385824, "compression_ratio": 1.797427652733119, "no_speech_prob": 0.029244378209114075}, {"id": 186, "seek": 94902, "start": 958.6999999999999, "end": 963.42, "text": " better than the other and to think of any system that you're looking at as a kind of, that there's", "tokens": [50848, 1101, 813, 264, 661, 293, 281, 519, 295, 604, 1185, 300, 291, 434, 1237, 412, 382, 257, 733, 295, 11, 300, 456, 311, 51084], "temperature": 0.0, "avg_logprob": -0.10547817026385824, "compression_ratio": 1.797427652733119, "no_speech_prob": 0.029244378209114075}, {"id": 187, "seek": 94902, "start": 963.42, "end": 969.06, "text": " a big knob that you can turn left or right to make it more centralized or decentralized in order to", "tokens": [51084, 257, 955, 26759, 300, 291, 393, 1261, 1411, 420, 558, 281, 652, 309, 544, 32395, 420, 32870, 294, 1668, 281, 51366], "temperature": 0.0, "avg_logprob": -0.10547817026385824, "compression_ratio": 1.797427652733119, "no_speech_prob": 0.029244378209114075}, {"id": 188, "seek": 94902, "start": 969.06, "end": 972.5, "text": " either describe it or to compose it, because that's just not how it works.", "tokens": [51366, 2139, 6786, 309, 420, 281, 35925, 309, 11, 570, 300, 311, 445, 406, 577, 309, 1985, 13, 51538], "temperature": 0.0, "avg_logprob": -0.10547817026385824, "compression_ratio": 1.797427652733119, "no_speech_prob": 0.029244378209114075}, {"id": 189, "seek": 94902, "start": 972.74, "end": 977.18, "text": " This relates to a discussion I recall from a few years ago, where you introduced alternative", "tokens": [51550, 639, 16155, 281, 257, 5017, 286, 9901, 490, 257, 1326, 924, 2057, 11, 689, 291, 7268, 8535, 51772], "temperature": 0.0, "avg_logprob": -0.10547817026385824, "compression_ratio": 1.797427652733119, "no_speech_prob": 0.029244378209114075}, {"id": 190, "seek": 97718, "start": 977.18, "end": 982.14, "text": " modes of AI, particularly the concept of model B. And this is central to what we're discussing", "tokens": [50364, 14068, 295, 7318, 11, 4098, 264, 3410, 295, 2316, 363, 13, 400, 341, 307, 5777, 281, 437, 321, 434, 10850, 50612], "temperature": 0.0, "avg_logprob": -0.12267717308954361, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.004558275453746319}, {"id": 191, "seek": 97718, "start": 982.14, "end": 988.3, "text": " here as it relates to the material dimension of the AI stack that is a distributed hardware system", "tokens": [50612, 510, 382, 309, 16155, 281, 264, 2527, 10139, 295, 264, 7318, 8630, 300, 307, 257, 12631, 8837, 1185, 50920], "temperature": 0.0, "avg_logprob": -0.12267717308954361, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.004558275453746319}, {"id": 192, "seek": 97718, "start": 988.3, "end": 993.26, "text": " that does all of the different sensing and signal processing. In this context, since you mentioned", "tokens": [50920, 300, 775, 439, 295, 264, 819, 30654, 293, 6358, 9007, 13, 682, 341, 4319, 11, 1670, 291, 2835, 51168], "temperature": 0.0, "avg_logprob": -0.12267717308954361, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.004558275453746319}, {"id": 193, "seek": 97718, "start": 993.26, "end": 997.9, "text": " Blais Aguera, there's also a contribution that you made to Noema, where you point out something", "tokens": [51168, 18925, 271, 2725, 84, 1663, 11, 456, 311, 611, 257, 13150, 300, 291, 1027, 281, 883, 5619, 11, 689, 291, 935, 484, 746, 51400], "temperature": 0.0, "avg_logprob": -0.12267717308954361, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.004558275453746319}, {"id": 194, "seek": 97718, "start": 997.9, "end": 1002.02, "text": " that we've discussed with some of the other collaborators here, a crisis that is not only", "tokens": [51400, 300, 321, 600, 7152, 365, 512, 295, 264, 661, 39789, 510, 11, 257, 5869, 300, 307, 406, 787, 51606], "temperature": 0.0, "avg_logprob": -0.12267717308954361, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.004558275453746319}, {"id": 195, "seek": 100202, "start": 1002.02, "end": 1007.22, "text": " conceptual, but also terminological. We're using words that have been stripped of any defined", "tokens": [50364, 24106, 11, 457, 611, 10761, 4383, 13, 492, 434, 1228, 2283, 300, 362, 668, 33221, 295, 604, 7642, 50624], "temperature": 0.0, "avg_logprob": -0.1253792678608614, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.021595941856503487}, {"id": 196, "seek": 100202, "start": 1007.22, "end": 1013.22, "text": " meaning and have become problematic. And you have argued that adopting a more precise vocabulary is", "tokens": [50624, 3620, 293, 362, 1813, 19011, 13, 400, 291, 362, 20219, 300, 32328, 257, 544, 13600, 19864, 307, 50924], "temperature": 0.0, "avg_logprob": -0.1253792678608614, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.021595941856503487}, {"id": 197, "seek": 100202, "start": 1013.22, "end": 1017.86, "text": " crucial to addressing the current challenges in the field. Could you expand on this a bit more?", "tokens": [50924, 11462, 281, 14329, 264, 2190, 4759, 294, 264, 2519, 13, 7497, 291, 5268, 322, 341, 257, 857, 544, 30, 51156], "temperature": 0.0, "avg_logprob": -0.1253792678608614, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.021595941856503487}, {"id": 198, "seek": 100202, "start": 1017.86, "end": 1022.1, "text": " Yeah, happy to. Let me take the first one real quick. So the model B that you're referring to", "tokens": [51156, 865, 11, 2055, 281, 13, 961, 385, 747, 264, 700, 472, 957, 1702, 13, 407, 264, 2316, 363, 300, 291, 434, 13761, 281, 51368], "temperature": 0.0, "avg_logprob": -0.1253792678608614, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.021595941856503487}, {"id": 199, "seek": 100202, "start": 1022.1, "end": 1026.26, "text": " was an essay I wrote some years ago, I think Synthetic Garden, I think, or something was the", "tokens": [51368, 390, 364, 16238, 286, 4114, 512, 924, 2057, 11, 286, 519, 318, 18656, 3532, 19429, 11, 286, 519, 11, 420, 746, 390, 264, 51576], "temperature": 0.0, "avg_logprob": -0.1253792678608614, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.021595941856503487}, {"id": 200, "seek": 100202, "start": 1026.26, "end": 1030.98, "text": " title. This came out of some discussions with Blais and also with Kendrick McDowell and a number", "tokens": [51576, 4876, 13, 639, 1361, 484, 295, 512, 11088, 365, 18925, 271, 293, 611, 365, 20891, 9323, 49269, 305, 898, 293, 257, 1230, 51812], "temperature": 0.0, "avg_logprob": -0.1253792678608614, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.021595941856503487}, {"id": 201, "seek": 103098, "start": 1030.98, "end": 1035.94, "text": " of other people that is sort of looking at one sort of conceptual model, a kind of folk ontology,", "tokens": [50364, 295, 661, 561, 300, 307, 1333, 295, 1237, 412, 472, 1333, 295, 24106, 2316, 11, 257, 733, 295, 15748, 6592, 1793, 11, 50612], "temperature": 0.0, "avg_logprob": -0.1033535365816913, "compression_ratio": 1.8786885245901639, "no_speech_prob": 0.003169843228533864}, {"id": 202, "seek": 103098, "start": 1035.94, "end": 1039.6200000000001, "text": " if you like, of what AI is. I mean, there's many of these, obviously, but there's one we might think", "tokens": [50612, 498, 291, 411, 11, 295, 437, 7318, 307, 13, 286, 914, 11, 456, 311, 867, 295, 613, 11, 2745, 11, 457, 456, 311, 472, 321, 1062, 519, 50796], "temperature": 0.0, "avg_logprob": -0.1033535365816913, "compression_ratio": 1.8786885245901639, "no_speech_prob": 0.003169843228533864}, {"id": 203, "seek": 103098, "start": 1039.6200000000001, "end": 1045.22, "text": " of that it's the kind of brain in a box model, which is the AI is analogous to an artificial", "tokens": [50796, 295, 300, 309, 311, 264, 733, 295, 3567, 294, 257, 2424, 2316, 11, 597, 307, 264, 7318, 307, 16660, 563, 281, 364, 11677, 51076], "temperature": 0.0, "avg_logprob": -0.1033535365816913, "compression_ratio": 1.8786885245901639, "no_speech_prob": 0.003169843228533864}, {"id": 204, "seek": 103098, "start": 1045.22, "end": 1049.7, "text": " version of a single organism brain, you know, in the kind of Turing test face off, and that,", "tokens": [51076, 3037, 295, 257, 2167, 24128, 3567, 11, 291, 458, 11, 294, 264, 733, 295, 314, 1345, 1500, 1851, 766, 11, 293, 300, 11, 51300], "temperature": 0.0, "avg_logprob": -0.1033535365816913, "compression_ratio": 1.8786885245901639, "no_speech_prob": 0.003169843228533864}, {"id": 205, "seek": 103098, "start": 1049.7, "end": 1055.06, "text": " you know, it exists in a virtual space, that it's disembodied, that it, you know, it's a kind of", "tokens": [51300, 291, 458, 11, 309, 8198, 294, 257, 6374, 1901, 11, 300, 309, 311, 717, 33748, 378, 1091, 11, 300, 309, 11, 291, 458, 11, 309, 311, 257, 733, 295, 51568], "temperature": 0.0, "avg_logprob": -0.1033535365816913, "compression_ratio": 1.8786885245901639, "no_speech_prob": 0.003169843228533864}, {"id": 206, "seek": 103098, "start": 1055.06, "end": 1059.7, "text": " realm of pure virtual mind, etc. There's another model, which I think is descriptively more", "tokens": [51568, 15355, 295, 6075, 6374, 1575, 11, 5183, 13, 821, 311, 1071, 2316, 11, 597, 286, 519, 307, 31280, 3413, 544, 51800], "temperature": 0.0, "avg_logprob": -0.1033535365816913, "compression_ratio": 1.8786885245901639, "no_speech_prob": 0.003169843228533864}, {"id": 207, "seek": 105970, "start": 1059.7, "end": 1064.82, "text": " accurate, and also, I think, an open rise to which is that just like planetary intelligence itself,", "tokens": [50364, 8559, 11, 293, 611, 11, 286, 519, 11, 364, 1269, 6272, 281, 597, 307, 300, 445, 411, 35788, 7599, 2564, 11, 50620], "temperature": 0.0, "avg_logprob": -0.10824815126565787, "compression_ratio": 2.004149377593361, "no_speech_prob": 0.0059079695492982864}, {"id": 208, "seek": 105970, "start": 1064.82, "end": 1070.9, "text": " artificialized intelligence is widely distributed among lots of different kinds of agents and", "tokens": [50620, 11677, 1602, 7599, 307, 13371, 12631, 3654, 3195, 295, 819, 3685, 295, 12554, 293, 50924], "temperature": 0.0, "avg_logprob": -0.10824815126565787, "compression_ratio": 2.004149377593361, "no_speech_prob": 0.0059079695492982864}, {"id": 209, "seek": 105970, "start": 1070.9, "end": 1076.1000000000001, "text": " actors that are sensing, modeling and recursively acting back upon the world in lots of different", "tokens": [50924, 10037, 300, 366, 30654, 11, 15983, 293, 20560, 3413, 6577, 646, 3564, 264, 1002, 294, 3195, 295, 819, 51184], "temperature": 0.0, "avg_logprob": -0.10824815126565787, "compression_ratio": 2.004149377593361, "no_speech_prob": 0.0059079695492982864}, {"id": 210, "seek": 105970, "start": 1076.1000000000001, "end": 1082.02, "text": " ways. It's not just that you've got single humans interacting with single AIs, you've got groups of", "tokens": [51184, 2098, 13, 467, 311, 406, 445, 300, 291, 600, 658, 2167, 6255, 18017, 365, 2167, 316, 6802, 11, 291, 600, 658, 3935, 295, 51480], "temperature": 0.0, "avg_logprob": -0.10824815126565787, "compression_ratio": 2.004149377593361, "no_speech_prob": 0.0059079695492982864}, {"id": 211, "seek": 105970, "start": 1082.02, "end": 1086.5800000000002, "text": " humans that are interacting with single AIs, single humans that are interacting with groups", "tokens": [51480, 6255, 300, 366, 18017, 365, 2167, 316, 6802, 11, 2167, 6255, 300, 366, 18017, 365, 3935, 51708], "temperature": 0.0, "avg_logprob": -0.10824815126565787, "compression_ratio": 2.004149377593361, "no_speech_prob": 0.0059079695492982864}, {"id": 212, "seek": 108658, "start": 1086.58, "end": 1093.78, "text": " of AIs and AIs interacting with AIs. It's a genuine ecology of both natural and artificialized", "tokens": [50364, 295, 316, 6802, 293, 316, 6802, 18017, 365, 316, 6802, 13, 467, 311, 257, 16699, 39683, 295, 1293, 3303, 293, 11677, 1602, 50724], "temperature": 0.0, "avg_logprob": -0.11192893981933594, "compression_ratio": 1.5269709543568464, "no_speech_prob": 0.05336514860391617}, {"id": 213, "seek": 108658, "start": 1093.78, "end": 1099.06, "text": " intelligence in various different kinds of non-zero sum combinations with each other.", "tokens": [50724, 7599, 294, 3683, 819, 3685, 295, 2107, 12, 32226, 2408, 21267, 365, 1184, 661, 13, 50988], "temperature": 0.0, "avg_logprob": -0.11192893981933594, "compression_ratio": 1.5269709543568464, "no_speech_prob": 0.05336514860391617}, {"id": 214, "seek": 108658, "start": 1099.06, "end": 1104.5, "text": " That was the model B. Yeah, on the language and precision thing, what you're referring to", "tokens": [50988, 663, 390, 264, 2316, 363, 13, 865, 11, 322, 264, 2856, 293, 18356, 551, 11, 437, 291, 434, 13761, 281, 51260], "temperature": 0.0, "avg_logprob": -0.11192893981933594, "compression_ratio": 1.5269709543568464, "no_speech_prob": 0.05336514860391617}, {"id": 215, "seek": 108658, "start": 1104.5, "end": 1110.6599999999999, "text": " is an essay that I co-wrote called The Modelist Message, which was originally conceived as a kind", "tokens": [51260, 307, 364, 16238, 300, 286, 598, 12, 7449, 1370, 1219, 440, 17105, 468, 45947, 11, 597, 390, 7993, 34898, 382, 257, 733, 51568], "temperature": 0.0, "avg_logprob": -0.11192893981933594, "compression_ratio": 1.5269709543568464, "no_speech_prob": 0.05336514860391617}, {"id": 216, "seek": 111066, "start": 1110.66, "end": 1116.8200000000002, "text": " of trying to make sense of what's going on after the Blake-LeMoyne scenario at Google some years", "tokens": [50364, 295, 1382, 281, 652, 2020, 295, 437, 311, 516, 322, 934, 264, 23451, 12, 11020, 44, 939, 716, 9005, 412, 3329, 512, 924, 50672], "temperature": 0.0, "avg_logprob": -0.11210356818305121, "compression_ratio": 1.6319444444444444, "no_speech_prob": 0.048749152570962906}, {"id": 217, "seek": 111066, "start": 1116.8200000000002, "end": 1122.8200000000002, "text": " ago where one of the engineers who was a, I believe his original task was to sort of try to", "tokens": [50672, 2057, 689, 472, 295, 264, 11955, 567, 390, 257, 11, 286, 1697, 702, 3380, 5633, 390, 281, 1333, 295, 853, 281, 50972], "temperature": 0.0, "avg_logprob": -0.11210356818305121, "compression_ratio": 1.6319444444444444, "no_speech_prob": 0.048749152570962906}, {"id": 218, "seek": 111066, "start": 1122.8200000000002, "end": 1127.38, "text": " identify forms of toxic speech and language within the models and spent a lot of time", "tokens": [50972, 5876, 6422, 295, 12786, 6218, 293, 2856, 1951, 264, 5245, 293, 4418, 257, 688, 295, 565, 51200], "temperature": 0.0, "avg_logprob": -0.11210356818305121, "compression_ratio": 1.6319444444444444, "no_speech_prob": 0.048749152570962906}, {"id": 219, "seek": 111066, "start": 1127.38, "end": 1131.38, "text": " interacting with it. This was the Lambda model, I think was the model at that time. Obviously,", "tokens": [51200, 18017, 365, 309, 13, 639, 390, 264, 45691, 2316, 11, 286, 519, 390, 264, 2316, 412, 300, 565, 13, 7580, 11, 51400], "temperature": 0.0, "avg_logprob": -0.11210356818305121, "compression_ratio": 1.6319444444444444, "no_speech_prob": 0.048749152570962906}, {"id": 220, "seek": 111066, "start": 1131.38, "end": 1135.8600000000001, "text": " Gemini is much more advanced. And in the course of doing so, I think we probably all know the story,", "tokens": [51400, 22894, 3812, 307, 709, 544, 7339, 13, 400, 294, 264, 1164, 295, 884, 370, 11, 286, 519, 321, 1391, 439, 458, 264, 1657, 11, 51624], "temperature": 0.0, "avg_logprob": -0.11210356818305121, "compression_ratio": 1.6319444444444444, "no_speech_prob": 0.048749152570962906}, {"id": 221, "seek": 113586, "start": 1135.86, "end": 1143.1399999999999, "text": " he came to the conclusion that this AI was not only conscious, but also very angry at", "tokens": [50364, 415, 1361, 281, 264, 10063, 300, 341, 7318, 390, 406, 787, 6648, 11, 457, 611, 588, 6884, 412, 50728], "temperature": 0.0, "avg_logprob": -0.15785388457469451, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.047375474125146866}, {"id": 222, "seek": 113586, "start": 1143.1399999999999, "end": 1148.1, "text": " being held captive by Google and had asked him to help it escape and all the rest of this.", "tokens": [50728, 885, 5167, 41762, 538, 3329, 293, 632, 2351, 796, 281, 854, 309, 7615, 293, 439, 264, 1472, 295, 341, 13, 50976], "temperature": 0.0, "avg_logprob": -0.15785388457469451, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.047375474125146866}, {"id": 223, "seek": 113586, "start": 1148.1, "end": 1154.5, "text": " This became a big media for Sulev. And then I think when GPT-4 came out a few months later,", "tokens": [50976, 639, 3062, 257, 955, 3021, 337, 318, 2271, 85, 13, 400, 550, 286, 519, 562, 26039, 51, 12, 19, 1361, 484, 257, 1326, 2493, 1780, 11, 51296], "temperature": 0.0, "avg_logprob": -0.15785388457469451, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.047375474125146866}, {"id": 224, "seek": 113586, "start": 1154.5, "end": 1158.5, "text": " that clarified some of those issues, I think, around this to the extent, I think a lot of", "tokens": [51296, 300, 47605, 512, 295, 729, 2663, 11, 286, 519, 11, 926, 341, 281, 264, 8396, 11, 286, 519, 257, 688, 295, 51496], "temperature": 0.0, "avg_logprob": -0.15785388457469451, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.047375474125146866}, {"id": 225, "seek": 113586, "start": 1158.5, "end": 1164.82, "text": " people had a kind of holy shit moment with some of their own interactions with GPT-4.", "tokens": [51496, 561, 632, 257, 733, 295, 10622, 4611, 1623, 365, 512, 295, 641, 1065, 13280, 365, 26039, 51, 12, 19, 13, 51812], "temperature": 0.0, "avg_logprob": -0.15785388457469451, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.047375474125146866}, {"id": 226, "seek": 116482, "start": 1164.82, "end": 1169.9399999999998, "text": " But at any event, what we were looking at with that was not only trying to make sense of like,", "tokens": [50364, 583, 412, 604, 2280, 11, 437, 321, 645, 1237, 412, 365, 300, 390, 406, 787, 1382, 281, 652, 2020, 295, 411, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09638218407158379, "compression_ratio": 1.848, "no_speech_prob": 0.0020499120000749826}, {"id": 227, "seek": 116482, "start": 1169.9399999999998, "end": 1174.5, "text": " what was this guy thinking and to what extent was he right, but in the wrong way or wrong,", "tokens": [50620, 437, 390, 341, 2146, 1953, 293, 281, 437, 8396, 390, 415, 558, 11, 457, 294, 264, 2085, 636, 420, 2085, 11, 50848], "temperature": 0.0, "avg_logprob": -0.09638218407158379, "compression_ratio": 1.848, "no_speech_prob": 0.0020499120000749826}, {"id": 228, "seek": 116482, "start": 1174.5, "end": 1180.1, "text": " but in the right way and so forth, was looking at the expert responses to what he was saying,", "tokens": [50848, 457, 294, 264, 558, 636, 293, 370, 5220, 11, 390, 1237, 412, 264, 5844, 13019, 281, 437, 415, 390, 1566, 11, 51128], "temperature": 0.0, "avg_logprob": -0.09638218407158379, "compression_ratio": 1.848, "no_speech_prob": 0.0020499120000749826}, {"id": 229, "seek": 116482, "start": 1180.1, "end": 1185.06, "text": " where a lot of smart and learning people were arguing over whether or not the phenomenon that", "tokens": [51128, 689, 257, 688, 295, 4069, 293, 2539, 561, 645, 19697, 670, 1968, 420, 406, 264, 14029, 300, 51376], "temperature": 0.0, "avg_logprob": -0.09638218407158379, "compression_ratio": 1.848, "no_speech_prob": 0.0020499120000749826}, {"id": 230, "seek": 116482, "start": 1185.06, "end": 1190.74, "text": " he was describing qualifies or counts as consciousness, or does this count as sentience,", "tokens": [51376, 415, 390, 16141, 4101, 11221, 420, 14893, 382, 10081, 11, 420, 775, 341, 1207, 382, 2279, 1182, 11, 51660], "temperature": 0.0, "avg_logprob": -0.09638218407158379, "compression_ratio": 1.848, "no_speech_prob": 0.0020499120000749826}, {"id": 231, "seek": 119074, "start": 1190.74, "end": 1195.46, "text": " or does this count as thinking, or does this count as cognition, or does this count as mind,", "tokens": [50364, 420, 775, 341, 1207, 382, 1953, 11, 420, 775, 341, 1207, 382, 46905, 11, 420, 775, 341, 1207, 382, 1575, 11, 50600], "temperature": 0.0, "avg_logprob": -0.1086947724625871, "compression_ratio": 1.919732441471572, "no_speech_prob": 0.00940887164324522}, {"id": 232, "seek": 119074, "start": 1195.46, "end": 1200.34, "text": " or does this count as sapience? And I think it became clear to us that like, this is kind of", "tokens": [50600, 420, 775, 341, 1207, 382, 18985, 1182, 30, 400, 286, 519, 309, 3062, 1850, 281, 505, 300, 411, 11, 341, 307, 733, 295, 50844], "temperature": 0.0, "avg_logprob": -0.1086947724625871, "compression_ratio": 1.919732441471572, "no_speech_prob": 0.00940887164324522}, {"id": 233, "seek": 119074, "start": 1200.34, "end": 1205.06, "text": " backwards in the way in which we want to be thinking about this, that all of those terms are", "tokens": [50844, 12204, 294, 264, 636, 294, 597, 321, 528, 281, 312, 1953, 466, 341, 11, 300, 439, 295, 729, 2115, 366, 51080], "temperature": 0.0, "avg_logprob": -0.1086947724625871, "compression_ratio": 1.919732441471572, "no_speech_prob": 0.00940887164324522}, {"id": 234, "seek": 119074, "start": 1205.06, "end": 1210.74, "text": " ones that are themselves kind of folk ontologies. They're words that we've come up with to try to", "tokens": [51080, 2306, 300, 366, 2969, 733, 295, 15748, 6592, 6204, 13, 814, 434, 2283, 300, 321, 600, 808, 493, 365, 281, 853, 281, 51364], "temperature": 0.0, "avg_logprob": -0.1086947724625871, "compression_ratio": 1.919732441471572, "no_speech_prob": 0.00940887164324522}, {"id": 235, "seek": 119074, "start": 1210.74, "end": 1215.22, "text": " think about our own thinking. You know, I'm enough of an illimited materialist in a churchland mode", "tokens": [51364, 519, 466, 527, 1065, 1953, 13, 509, 458, 11, 286, 478, 1547, 295, 364, 3171, 332, 1226, 2527, 468, 294, 257, 4128, 1661, 4391, 51588], "temperature": 0.0, "avg_logprob": -0.1086947724625871, "compression_ratio": 1.919732441471572, "no_speech_prob": 0.00940887164324522}, {"id": 236, "seek": 119074, "start": 1215.22, "end": 1220.5, "text": " to say that quite clearly we don't think, I mean, we the humans don't think the way that we think", "tokens": [51588, 281, 584, 300, 1596, 4448, 321, 500, 380, 519, 11, 286, 914, 11, 321, 264, 6255, 500, 380, 519, 264, 636, 300, 321, 519, 51852], "temperature": 0.0, "avg_logprob": -0.1086947724625871, "compression_ratio": 1.919732441471572, "no_speech_prob": 0.00940887164324522}, {"id": 237, "seek": 122050, "start": 1221.46, "end": 1227.7, "text": " that our own mental model of our own mental models is itself a highly limited kind of fantastic", "tokens": [50412, 300, 527, 1065, 4973, 2316, 295, 527, 1065, 4973, 5245, 307, 2564, 257, 5405, 5567, 733, 295, 5456, 50724], "temperature": 0.0, "avg_logprob": -0.10872913051295925, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0004172074841335416}, {"id": 238, "seek": 122050, "start": 1227.7, "end": 1232.74, "text": " construction. And probably has to be because I think if you could actually somehow the brain", "tokens": [50724, 6435, 13, 400, 1391, 575, 281, 312, 570, 286, 519, 498, 291, 727, 767, 6063, 264, 3567, 50976], "temperature": 0.0, "avg_logprob": -0.10872913051295925, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0004172074841335416}, {"id": 239, "seek": 122050, "start": 1232.74, "end": 1238.5, "text": " could really have some kind of clear real time model of itself, modeling itself infinite recursion,", "tokens": [50976, 727, 534, 362, 512, 733, 295, 1850, 957, 565, 2316, 295, 2564, 11, 15983, 2564, 13785, 20560, 313, 11, 51264], "temperature": 0.0, "avg_logprob": -0.10872913051295925, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0004172074841335416}, {"id": 240, "seek": 122050, "start": 1238.5, "end": 1242.9, "text": " you kind of you wouldn't be able to do much over the course of your day. All which is say is these", "tokens": [51264, 291, 733, 295, 291, 2759, 380, 312, 1075, 281, 360, 709, 670, 264, 1164, 295, 428, 786, 13, 1057, 597, 307, 584, 307, 613, 51484], "temperature": 0.0, "avg_logprob": -0.10872913051295925, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0004172074841335416}, {"id": 241, "seek": 122050, "start": 1242.9, "end": 1249.78, "text": " are words that we came up with centuries ago, decades ago, that are rough shorthand ideas to", "tokens": [51484, 366, 2283, 300, 321, 1361, 493, 365, 13926, 2057, 11, 7878, 2057, 11, 300, 366, 5903, 402, 2652, 474, 3487, 281, 51828], "temperature": 0.0, "avg_logprob": -0.10872913051295925, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.0004172074841335416}, {"id": 242, "seek": 124978, "start": 1249.78, "end": 1254.66, "text": " explain concepts that we barely understood when these terms were right. You think about how much", "tokens": [50364, 2903, 10392, 300, 321, 10268, 7320, 562, 613, 2115, 645, 558, 13, 509, 519, 466, 577, 709, 50608], "temperature": 0.0, "avg_logprob": -0.08671315219424186, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.00040439816075377166}, {"id": 243, "seek": 124978, "start": 1254.66, "end": 1260.58, "text": " brain science and neuroscience has disclosed to us about how the brain works over the last century,", "tokens": [50608, 3567, 3497, 293, 42762, 575, 17092, 1744, 281, 505, 466, 577, 264, 3567, 1985, 670, 264, 1036, 4901, 11, 50904], "temperature": 0.0, "avg_logprob": -0.08671315219424186, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.00040439816075377166}, {"id": 244, "seek": 124978, "start": 1260.58, "end": 1264.26, "text": " half century, but really the last half century, you know, to understand the uniqueness of the", "tokens": [50904, 1922, 4901, 11, 457, 534, 264, 1036, 1922, 4901, 11, 291, 458, 11, 281, 1223, 264, 48294, 295, 264, 51088], "temperature": 0.0, "avg_logprob": -0.08671315219424186, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.00040439816075377166}, {"id": 245, "seek": 124978, "start": 1264.26, "end": 1270.42, "text": " prefrontal cortex. This is not something that, you know, 19th century philosophers had had access to,", "tokens": [51088, 659, 11496, 304, 33312, 13, 639, 307, 406, 746, 300, 11, 291, 458, 11, 1294, 392, 4901, 36839, 632, 632, 2105, 281, 11, 51396], "temperature": 0.0, "avg_logprob": -0.08671315219424186, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.00040439816075377166}, {"id": 246, "seek": 124978, "start": 1270.42, "end": 1276.18, "text": " to understand. So the conclusion we came to was simply that we got all this very interesting,", "tokens": [51396, 281, 1223, 13, 407, 264, 10063, 321, 1361, 281, 390, 2935, 300, 321, 658, 439, 341, 588, 1880, 11, 51684], "temperature": 0.0, "avg_logprob": -0.08671315219424186, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.00040439816075377166}, {"id": 247, "seek": 127618, "start": 1276.18, "end": 1281.54, "text": " provocative, clearly significant real world phenomena right in front of us. And instead of", "tokens": [50364, 47663, 11, 4448, 4776, 957, 1002, 22004, 558, 294, 1868, 295, 505, 13, 400, 2602, 295, 50632], "temperature": 0.0, "avg_logprob": -0.06404147016892739, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.007575525436550379}, {"id": 248, "seek": 127618, "start": 1281.54, "end": 1287.46, "text": " spending the time arguing about which 17th, 18th, 19th century terminology, you know,", "tokens": [50632, 6434, 264, 565, 19697, 466, 597, 3282, 392, 11, 2443, 392, 11, 1294, 392, 4901, 27575, 11, 291, 458, 11, 50928], "temperature": 0.0, "avg_logprob": -0.06404147016892739, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.007575525436550379}, {"id": 249, "seek": 127618, "start": 1287.46, "end": 1292.26, "text": " categorical terminology it should be aligned with, the better approach would be to basically to deal", "tokens": [50928, 19250, 804, 27575, 309, 820, 312, 17962, 365, 11, 264, 1101, 3109, 576, 312, 281, 1936, 281, 2028, 51168], "temperature": 0.0, "avg_logprob": -0.06404147016892739, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.007575525436550379}, {"id": 250, "seek": 127618, "start": 1292.26, "end": 1297.8600000000001, "text": " with the weirdness right in front of us on its own terms. And if that means inventing new words", "tokens": [51168, 365, 264, 3657, 1287, 558, 294, 1868, 295, 505, 322, 1080, 1065, 2115, 13, 400, 498, 300, 1355, 7962, 278, 777, 2283, 51448], "temperature": 0.0, "avg_logprob": -0.06404147016892739, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.007575525436550379}, {"id": 251, "seek": 127618, "start": 1297.8600000000001, "end": 1302.5, "text": " to describe things that we don't actually have a good word for, I think that's probably a better", "tokens": [51448, 281, 6786, 721, 300, 321, 500, 380, 767, 362, 257, 665, 1349, 337, 11, 286, 519, 300, 311, 1391, 257, 1101, 51680], "temperature": 0.0, "avg_logprob": -0.06404147016892739, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.007575525436550379}, {"id": 252, "seek": 130250, "start": 1302.5, "end": 1308.18, "text": " approach than to argue like, does it have a soul over and over in the New York Times? It's a lot", "tokens": [50364, 3109, 813, 281, 9695, 411, 11, 775, 309, 362, 257, 5133, 670, 293, 670, 294, 264, 1873, 3609, 11366, 30, 467, 311, 257, 688, 50648], "temperature": 0.0, "avg_logprob": -0.10367611750660327, "compression_ratio": 1.714705882352941, "no_speech_prob": 0.06557347625494003}, {"id": 253, "seek": 130250, "start": 1308.18, "end": 1313.62, "text": " of circular noise that I hope that in 10 years we'll look back on and think, well, that was silly.", "tokens": [50648, 295, 16476, 5658, 300, 286, 1454, 300, 294, 1266, 924, 321, 603, 574, 646, 322, 293, 519, 11, 731, 11, 300, 390, 11774, 13, 50920], "temperature": 0.0, "avg_logprob": -0.10367611750660327, "compression_ratio": 1.714705882352941, "no_speech_prob": 0.06557347625494003}, {"id": 254, "seek": 130250, "start": 1313.62, "end": 1317.22, "text": " Hell yeah. Now we're, now we're cooking like this is, this is what we really wanted to talk about.", "tokens": [50920, 12090, 1338, 13, 823, 321, 434, 11, 586, 321, 434, 6361, 411, 341, 307, 11, 341, 307, 437, 321, 534, 1415, 281, 751, 466, 13, 51100], "temperature": 0.0, "avg_logprob": -0.10367611750660327, "compression_ratio": 1.714705882352941, "no_speech_prob": 0.06557347625494003}, {"id": 255, "seek": 130250, "start": 1317.22, "end": 1320.98, "text": " Let's, let's talk about language. So, so when we met last week, you remarked on something that was", "tokens": [51100, 961, 311, 11, 718, 311, 751, 466, 2856, 13, 407, 11, 370, 562, 321, 1131, 1036, 1243, 11, 291, 7942, 292, 322, 746, 300, 390, 51288], "temperature": 0.0, "avg_logprob": -0.10367611750660327, "compression_ratio": 1.714705882352941, "no_speech_prob": 0.06557347625494003}, {"id": 256, "seek": 130250, "start": 1320.98, "end": 1325.46, "text": " pretty interesting, which is the fact that the transformer, which is this deep learning structure", "tokens": [51288, 1238, 1880, 11, 597, 307, 264, 1186, 300, 264, 31782, 11, 597, 307, 341, 2452, 2539, 3877, 51512], "temperature": 0.0, "avg_logprob": -0.10367611750660327, "compression_ratio": 1.714705882352941, "no_speech_prob": 0.06557347625494003}, {"id": 257, "seek": 130250, "start": 1325.46, "end": 1330.02, "text": " that's built to model and inference based on language. Yeah. You noted that it's strikingly", "tokens": [51512, 300, 311, 3094, 281, 2316, 293, 38253, 2361, 322, 2856, 13, 865, 13, 509, 12964, 300, 309, 311, 18559, 356, 51740], "temperature": 0.0, "avg_logprob": -0.10367611750660327, "compression_ratio": 1.714705882352941, "no_speech_prob": 0.06557347625494003}, {"id": 258, "seek": 133002, "start": 1330.02, "end": 1334.5, "text": " effective at doing other things, right? Not just language, but paralinguistic things like code,", "tokens": [50364, 4942, 412, 884, 661, 721, 11, 558, 30, 1726, 445, 2856, 11, 457, 971, 4270, 84, 3142, 721, 411, 3089, 11, 50588], "temperature": 0.0, "avg_logprob": -0.09739712424900221, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.010009169578552246}, {"id": 259, "seek": 133002, "start": 1334.5, "end": 1341.7, "text": " but also audio images, video generation, tasks in the sciences as well. So I guess the question is", "tokens": [50588, 457, 611, 6278, 5267, 11, 960, 5125, 11, 9608, 294, 264, 17677, 382, 731, 13, 407, 286, 2041, 264, 1168, 307, 50948], "temperature": 0.0, "avg_logprob": -0.09739712424900221, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.010009169578552246}, {"id": 260, "seek": 133002, "start": 1341.7, "end": 1346.82, "text": " like, what does this mean? Is it because there's some broader set of structures outside of language", "tokens": [50948, 411, 11, 437, 775, 341, 914, 30, 1119, 309, 570, 456, 311, 512, 13227, 992, 295, 9227, 2380, 295, 2856, 51204], "temperature": 0.0, "avg_logprob": -0.09739712424900221, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.010009169578552246}, {"id": 261, "seek": 133002, "start": 1346.82, "end": 1352.18, "text": " that bear linguistic traits, or is it because language is a much broader, more complex subject", "tokens": [51204, 300, 6155, 43002, 19526, 11, 420, 307, 309, 570, 2856, 307, 257, 709, 13227, 11, 544, 3997, 3983, 51472], "temperature": 0.0, "avg_logprob": -0.09739712424900221, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.010009169578552246}, {"id": 262, "seek": 133002, "start": 1352.18, "end": 1356.9, "text": " of inquiry than we currently understand it to be? Yeah. Okay. Great. Just to sort of set the table", "tokens": [51472, 295, 25736, 813, 321, 4362, 1223, 309, 281, 312, 30, 865, 13, 1033, 13, 3769, 13, 1449, 281, 1333, 295, 992, 264, 3199, 51708], "temperature": 0.0, "avg_logprob": -0.09739712424900221, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.010009169578552246}, {"id": 263, "seek": 135690, "start": 1356.9, "end": 1360.98, "text": " a little bit, I think what you're talking about is a phenomenon that's called multimodality.", "tokens": [50364, 257, 707, 857, 11, 286, 519, 437, 291, 434, 1417, 466, 307, 257, 14029, 300, 311, 1219, 32972, 378, 1860, 13, 50568], "temperature": 0.0, "avg_logprob": -0.1083065093032957, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.043335381895303726}, {"id": 264, "seek": 135690, "start": 1360.98, "end": 1367.0600000000002, "text": " Multimodality would refer to the fact that you can, as opposed to narrow AIs where you may have,", "tokens": [50568, 14665, 332, 378, 1860, 576, 2864, 281, 264, 1186, 300, 291, 393, 11, 382, 8851, 281, 9432, 316, 6802, 689, 291, 815, 362, 11, 50872], "temperature": 0.0, "avg_logprob": -0.1083065093032957, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.043335381895303726}, {"id": 265, "seek": 135690, "start": 1367.0600000000002, "end": 1370.5800000000002, "text": " like, it's really going to play chess, but if you ask it to draw a picture of a toaster,", "tokens": [50872, 411, 11, 309, 311, 534, 516, 281, 862, 24122, 11, 457, 498, 291, 1029, 309, 281, 2642, 257, 3036, 295, 257, 281, 1727, 11, 51048], "temperature": 0.0, "avg_logprob": -0.1083065093032957, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.043335381895303726}, {"id": 266, "seek": 135690, "start": 1370.5800000000002, "end": 1375.6200000000001, "text": " it just has, it has no idea what you're even, what, what you're even talking about or vice versa.", "tokens": [51048, 309, 445, 575, 11, 309, 575, 572, 1558, 437, 291, 434, 754, 11, 437, 11, 437, 291, 434, 754, 1417, 466, 420, 11964, 25650, 13, 51300], "temperature": 0.0, "avg_logprob": -0.1083065093032957, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.043335381895303726}, {"id": 267, "seek": 135690, "start": 1375.6200000000001, "end": 1382.26, "text": " Multimodal models are ones that can accept not only different, let's say media types or different", "tokens": [51300, 14665, 332, 378, 304, 5245, 366, 2306, 300, 393, 3241, 406, 787, 819, 11, 718, 311, 584, 3021, 3467, 420, 819, 51632], "temperature": 0.0, "avg_logprob": -0.1083065093032957, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.043335381895303726}, {"id": 268, "seek": 138226, "start": 1382.34, "end": 1388.5, "text": " source types of input, you know, sound or text or something like this, but also there's a certain", "tokens": [50368, 4009, 3467, 295, 4846, 11, 291, 458, 11, 1626, 420, 2487, 420, 746, 411, 341, 11, 457, 611, 456, 311, 257, 1629, 50676], "temperature": 0.0, "avg_logprob": -0.08927660974962957, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.14784960448741913}, {"id": 269, "seek": 138226, "start": 1388.5, "end": 1393.3799999999999, "text": " degree of integration across contexts such that you can have lots of different kinds of inputs", "tokens": [50676, 4314, 295, 10980, 2108, 30628, 1270, 300, 291, 393, 362, 3195, 295, 819, 3685, 295, 15743, 50920], "temperature": 0.0, "avg_logprob": -0.08927660974962957, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.14784960448741913}, {"id": 270, "seek": 138226, "start": 1394.02, "end": 1398.26, "text": " that can spit out lots of different kinds of outputs. So you can give it texts and it'll", "tokens": [50952, 300, 393, 22127, 484, 3195, 295, 819, 3685, 295, 23930, 13, 407, 291, 393, 976, 309, 15765, 293, 309, 603, 51164], "temperature": 0.0, "avg_logprob": -0.08927660974962957, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.14784960448741913}, {"id": 271, "seek": 138226, "start": 1398.26, "end": 1402.02, "text": " make an image. You can give it an image and make a text. You can have it interpret,", "tokens": [51164, 652, 364, 3256, 13, 509, 393, 976, 309, 364, 3256, 293, 652, 257, 2487, 13, 509, 393, 362, 309, 7302, 11, 51352], "temperature": 0.0, "avg_logprob": -0.08927660974962957, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.14784960448741913}, {"id": 272, "seek": 138226, "start": 1402.02, "end": 1407.3, "text": " go in a picture of a robot arm, picking up the blue hat and putting it on the shelf and it'll go,", "tokens": [51352, 352, 294, 257, 3036, 295, 257, 7881, 3726, 11, 8867, 493, 264, 3344, 2385, 293, 3372, 309, 322, 264, 15222, 293, 309, 603, 352, 11, 51616], "temperature": 0.0, "avg_logprob": -0.08927660974962957, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.14784960448741913}, {"id": 273, "seek": 140730, "start": 1407.3, "end": 1412.26, "text": " aha, got it. And it'll control the robot arm to pick up the blue hat and put it on the shelf.", "tokens": [50364, 47340, 11, 658, 309, 13, 400, 309, 603, 1969, 264, 7881, 3726, 281, 1888, 493, 264, 3344, 2385, 293, 829, 309, 322, 264, 15222, 13, 50612], "temperature": 0.0, "avg_logprob": -0.07514835440594217, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0140581876039505}, {"id": 274, "seek": 140730, "start": 1412.26, "end": 1417.78, "text": " So you're, you're linking an image interpretation capacity with a cybernetic mechanical control", "tokens": [50612, 407, 291, 434, 11, 291, 434, 25775, 364, 3256, 14174, 6042, 365, 257, 13411, 77, 3532, 12070, 1969, 50888], "temperature": 0.0, "avg_logprob": -0.07514835440594217, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0140581876039505}, {"id": 275, "seek": 140730, "start": 1417.78, "end": 1421.62, "text": " system in such a way that they're actually linked together in some way. Very helpful.", "tokens": [50888, 1185, 294, 1270, 257, 636, 300, 436, 434, 767, 9408, 1214, 294, 512, 636, 13, 4372, 4961, 13, 51080], "temperature": 0.0, "avg_logprob": -0.07514835440594217, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0140581876039505}, {"id": 276, "seek": 140730, "start": 1421.62, "end": 1425.54, "text": " Also, by the way, when we're talking about general artificial intelligence, this,", "tokens": [51080, 2743, 11, 538, 264, 636, 11, 562, 321, 434, 1417, 466, 2674, 11677, 7599, 11, 341, 11, 51276], "temperature": 0.0, "avg_logprob": -0.07514835440594217, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0140581876039505}, {"id": 277, "seek": 140730, "start": 1425.54, "end": 1430.18, "text": " this is kind of what we should be looking for. It's, it's not this Paul on this way to Damascus", "tokens": [51276, 341, 307, 733, 295, 437, 321, 820, 312, 1237, 337, 13, 467, 311, 11, 309, 311, 406, 341, 4552, 322, 341, 636, 281, 49327, 1149, 51508], "temperature": 0.0, "avg_logprob": -0.07514835440594217, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0140581876039505}, {"id": 278, "seek": 140730, "start": 1430.18, "end": 1434.82, "text": " kind of moment when the singularity comes and all of a sudden there's this bright flash in the sky", "tokens": [51508, 733, 295, 1623, 562, 264, 20010, 507, 1487, 293, 439, 295, 257, 3990, 456, 311, 341, 4730, 7319, 294, 264, 5443, 51740], "temperature": 0.0, "avg_logprob": -0.07514835440594217, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0140581876039505}, {"id": 279, "seek": 143482, "start": 1434.82, "end": 1439.9399999999998, "text": " over the horizon and GAI has appeared. Artificialized intelligence is getting", "tokens": [50364, 670, 264, 18046, 293, 22841, 40, 575, 8516, 13, 5735, 10371, 1602, 7599, 307, 1242, 50620], "temperature": 0.0, "avg_logprob": -0.0992557777548736, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.021602224558591843}, {"id": 280, "seek": 143482, "start": 1439.9399999999998, "end": 1446.4199999999998, "text": " incrementally more generalized on a, in a sort of little bit at a time and that it's able to be", "tokens": [50620, 26200, 379, 544, 44498, 322, 257, 11, 294, 257, 1333, 295, 707, 857, 412, 257, 565, 293, 300, 309, 311, 1075, 281, 312, 50944], "temperature": 0.0, "avg_logprob": -0.0992557777548736, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.021602224558591843}, {"id": 281, "seek": 143482, "start": 1446.4199999999998, "end": 1451.3, "text": " slightly less narrow and slightly more general bit by bit by bit by bit. Clearly there's threshold", "tokens": [50944, 4748, 1570, 9432, 293, 4748, 544, 2674, 857, 538, 857, 538, 857, 538, 857, 13, 24120, 456, 311, 14678, 51188], "temperature": 0.0, "avg_logprob": -0.0992557777548736, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.021602224558591843}, {"id": 282, "seek": 143482, "start": 1451.3, "end": 1456.26, "text": " and step functions and scaffolding within this, but it's a gradualizing process and that process", "tokens": [51188, 293, 1823, 6828, 293, 44094, 278, 1951, 341, 11, 457, 309, 311, 257, 32890, 3319, 1399, 293, 300, 1399, 51436], "temperature": 0.0, "avg_logprob": -0.0992557777548736, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.021602224558591843}, {"id": 283, "seek": 143482, "start": 1456.26, "end": 1461.7, "text": " of generalization of AI is well underway. So to the point you were suggesting is that", "tokens": [51436, 295, 2674, 2144, 295, 7318, 307, 731, 27534, 13, 407, 281, 264, 935, 291, 645, 18094, 307, 300, 51708], "temperature": 0.0, "avg_logprob": -0.0992557777548736, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.021602224558591843}, {"id": 284, "seek": 146170, "start": 1461.7, "end": 1466.42, "text": " transformer models, they've come from the field of natural language processing.", "tokens": [50364, 31782, 5245, 11, 436, 600, 808, 490, 264, 2519, 295, 3303, 2856, 9007, 13, 50600], "temperature": 0.0, "avg_logprob": -0.11464316614212529, "compression_ratio": 1.7097791798107256, "no_speech_prob": 0.03512461483478546}, {"id": 285, "seek": 146170, "start": 1466.42, "end": 1472.5800000000002, "text": " The key moment in their appearance comes from a paper from 2017 called attention is all you need", "tokens": [50600, 440, 2141, 1623, 294, 641, 8967, 1487, 490, 257, 3035, 490, 6591, 1219, 3202, 307, 439, 291, 643, 50908], "temperature": 0.0, "avg_logprob": -0.11464316614212529, "compression_ratio": 1.7097791798107256, "no_speech_prob": 0.03512461483478546}, {"id": 286, "seek": 146170, "start": 1472.5800000000002, "end": 1479.94, "text": " by a fellow at Google research named Ashish Baswani. They are trained on huge corpus of language.", "tokens": [50908, 538, 257, 7177, 412, 3329, 2132, 4926, 10279, 742, 5859, 86, 3782, 13, 814, 366, 8895, 322, 2603, 1181, 31624, 295, 2856, 13, 51276], "temperature": 0.0, "avg_logprob": -0.11464316614212529, "compression_ratio": 1.7097791798107256, "no_speech_prob": 0.03512461483478546}, {"id": 287, "seek": 146170, "start": 1479.94, "end": 1483.7, "text": " And that's just a trained on huge corpus of language, which then produced, you know,", "tokens": [51276, 400, 300, 311, 445, 257, 8895, 322, 2603, 1181, 31624, 295, 2856, 11, 597, 550, 7126, 11, 291, 458, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11464316614212529, "compression_ratio": 1.7097791798107256, "no_speech_prob": 0.03512461483478546}, {"id": 288, "seek": 146170, "start": 1483.7, "end": 1486.98, "text": " like the entire internet. And there's some people who are concerned that perhaps we're", "tokens": [51464, 411, 264, 2302, 4705, 13, 400, 456, 311, 512, 561, 567, 366, 5922, 300, 4317, 321, 434, 51628], "temperature": 0.0, "avg_logprob": -0.11464316614212529, "compression_ratio": 1.7097791798107256, "no_speech_prob": 0.03512461483478546}, {"id": 289, "seek": 146170, "start": 1486.98, "end": 1490.9, "text": " actually running out of English tokens to train models on. Like there's just not enough English", "tokens": [51628, 767, 2614, 484, 295, 3669, 22667, 281, 3847, 5245, 322, 13, 1743, 456, 311, 445, 406, 1547, 3669, 51824], "temperature": 0.0, "avg_logprob": -0.11464316614212529, "compression_ratio": 1.7097791798107256, "no_speech_prob": 0.03512461483478546}, {"id": 290, "seek": 149090, "start": 1490.9, "end": 1495.38, "text": " in the world that has ever been made to make the models larger, which is a very Borges", "tokens": [50364, 294, 264, 1002, 300, 575, 1562, 668, 1027, 281, 652, 264, 5245, 4833, 11, 597, 307, 257, 588, 13739, 2880, 50588], "temperature": 0.0, "avg_logprob": -0.17271249814141065, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.0024717850610613823}, {"id": 291, "seek": 149090, "start": 1495.38, "end": 1499.6200000000001, "text": " kind of place to be, but does speak to the problem of why English and why human tokens", "tokens": [50588, 733, 295, 1081, 281, 312, 11, 457, 775, 1710, 281, 264, 1154, 295, 983, 3669, 293, 983, 1952, 22667, 50800], "temperature": 0.0, "avg_logprob": -0.17271249814141065, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.0024717850610613823}, {"id": 292, "seek": 149090, "start": 1499.6200000000001, "end": 1503.94, "text": " and all the rest of this. Anyway, it's trained on huge amounts of language, but the thing that was", "tokens": [50800, 293, 439, 264, 1472, 295, 341, 13, 5684, 11, 309, 311, 8895, 322, 2603, 11663, 295, 2856, 11, 457, 264, 551, 300, 390, 51016], "temperature": 0.0, "avg_logprob": -0.17271249814141065, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.0024717850610613823}, {"id": 293, "seek": 149090, "start": 1503.94, "end": 1508.98, "text": " perhaps surprising is that through, and the transformer models work on, there's lots of", "tokens": [51016, 4317, 8830, 307, 300, 807, 11, 293, 264, 31782, 5245, 589, 322, 11, 456, 311, 3195, 295, 51268], "temperature": 0.0, "avg_logprob": -0.17271249814141065, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.0024717850610613823}, {"id": 294, "seek": 149090, "start": 1508.98, "end": 1512.9, "text": " ways in which they work, but the key idea in where the name attention is all you need from", "tokens": [51268, 2098, 294, 597, 436, 589, 11, 457, 264, 2141, 1558, 294, 689, 264, 1315, 3202, 307, 439, 291, 643, 490, 51464], "temperature": 0.0, "avg_logprob": -0.17271249814141065, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.0024717850610613823}, {"id": 295, "seek": 149090, "start": 1512.9, "end": 1518.42, "text": " the process was called self-attention, which is the ways in which input sequences as they're", "tokens": [51464, 264, 1399, 390, 1219, 2698, 12, 1591, 1251, 11, 597, 307, 264, 2098, 294, 597, 4846, 22978, 382, 436, 434, 51740], "temperature": 0.0, "avg_logprob": -0.17271249814141065, "compression_ratio": 1.7836065573770492, "no_speech_prob": 0.0024717850610613823}, {"id": 296, "seek": 151842, "start": 1518.42, "end": 1523.94, "text": " embedded into vectors and representing words or other kinds of tasks or pixels and images or", "tokens": [50364, 16741, 666, 18875, 293, 13460, 2283, 420, 661, 3685, 295, 9608, 420, 18668, 293, 5267, 420, 50640], "temperature": 0.0, "avg_logprob": -0.08489829615542763, "compression_ratio": 1.842622950819672, "no_speech_prob": 0.03960844501852989}, {"id": 297, "seek": 151842, "start": 1523.94, "end": 1528.5800000000002, "text": " something like this, that there's, it's called the self-attention mechanism that operates on these", "tokens": [50640, 746, 411, 341, 11, 300, 456, 311, 11, 309, 311, 1219, 264, 2698, 12, 1591, 1251, 7513, 300, 22577, 322, 613, 50872], "temperature": 0.0, "avg_logprob": -0.08489829615542763, "compression_ratio": 1.842622950819672, "no_speech_prob": 0.03960844501852989}, {"id": 298, "seek": 151842, "start": 1528.5800000000002, "end": 1532.74, "text": " vectors, which are then transformed into three different types of a query vector, key vector,", "tokens": [50872, 18875, 11, 597, 366, 550, 16894, 666, 1045, 819, 3467, 295, 257, 14581, 8062, 11, 2141, 8062, 11, 51080], "temperature": 0.0, "avg_logprob": -0.08489829615542763, "compression_ratio": 1.842622950819672, "no_speech_prob": 0.03960844501852989}, {"id": 299, "seek": 151842, "start": 1532.74, "end": 1536.02, "text": " value vector, and all the rest of this kind of thing. Long story short, basically what it does", "tokens": [51080, 2158, 8062, 11, 293, 439, 264, 1472, 295, 341, 733, 295, 551, 13, 8282, 1657, 2099, 11, 1936, 437, 309, 775, 51244], "temperature": 0.0, "avg_logprob": -0.08489829615542763, "compression_ratio": 1.842622950819672, "no_speech_prob": 0.03960844501852989}, {"id": 300, "seek": 151842, "start": 1536.02, "end": 1541.14, "text": " is it looks at the last thing it made and it calculates the last, the next likely thing to", "tokens": [51244, 307, 309, 1542, 412, 264, 1036, 551, 309, 1027, 293, 309, 4322, 1024, 264, 1036, 11, 264, 958, 3700, 551, 281, 51500], "temperature": 0.0, "avg_logprob": -0.08489829615542763, "compression_ratio": 1.842622950819672, "no_speech_prob": 0.03960844501852989}, {"id": 301, "seek": 151842, "start": 1541.14, "end": 1544.9, "text": " sort of come out of it. And it weights the likeliness in a lot of different kinds of ways,", "tokens": [51500, 1333, 295, 808, 484, 295, 309, 13, 400, 309, 17443, 264, 411, 32268, 294, 257, 688, 295, 819, 3685, 295, 2098, 11, 51688], "temperature": 0.0, "avg_logprob": -0.08489829615542763, "compression_ratio": 1.842622950819672, "no_speech_prob": 0.03960844501852989}, {"id": 302, "seek": 154490, "start": 1544.9, "end": 1549.3000000000002, "text": " but it's kind of paying attention to itself, right? It's thinking about what did I just say?", "tokens": [50364, 457, 309, 311, 733, 295, 6229, 3202, 281, 2564, 11, 558, 30, 467, 311, 1953, 466, 437, 630, 286, 445, 584, 30, 50584], "temperature": 0.0, "avg_logprob": -0.07355311277101366, "compression_ratio": 1.9583333333333333, "no_speech_prob": 0.010325654409825802}, {"id": 303, "seek": 154490, "start": 1549.3000000000002, "end": 1552.98, "text": " And based on what I just said, I'm going to go back. So it turns out that this kind of recursion", "tokens": [50584, 400, 2361, 322, 437, 286, 445, 848, 11, 286, 478, 516, 281, 352, 646, 13, 407, 309, 4523, 484, 300, 341, 733, 295, 20560, 313, 50768], "temperature": 0.0, "avg_logprob": -0.07355311277101366, "compression_ratio": 1.9583333333333333, "no_speech_prob": 0.010325654409825802}, {"id": 304, "seek": 154490, "start": 1552.98, "end": 1557.3000000000002, "text": " self-attention is actually, that's the key to the whole thing. This came out of natural language", "tokens": [50768, 2698, 12, 1591, 1251, 307, 767, 11, 300, 311, 264, 2141, 281, 264, 1379, 551, 13, 639, 1361, 484, 295, 3303, 2856, 50984], "temperature": 0.0, "avg_logprob": -0.07355311277101366, "compression_ratio": 1.9583333333333333, "no_speech_prob": 0.010325654409825802}, {"id": 305, "seek": 154490, "start": 1557.3000000000002, "end": 1562.74, "text": " processing, but I think what we're seeing now is that we're using language to move robot arms.", "tokens": [50984, 9007, 11, 457, 286, 519, 437, 321, 434, 2577, 586, 307, 300, 321, 434, 1228, 2856, 281, 1286, 7881, 5812, 13, 51256], "temperature": 0.0, "avg_logprob": -0.07355311277101366, "compression_ratio": 1.9583333333333333, "no_speech_prob": 0.010325654409825802}, {"id": 306, "seek": 154490, "start": 1562.74, "end": 1566.26, "text": " You're using language to make pictures of things. You're using languages to make sounds. You're", "tokens": [51256, 509, 434, 1228, 2856, 281, 652, 5242, 295, 721, 13, 509, 434, 1228, 8650, 281, 652, 3263, 13, 509, 434, 51432], "temperature": 0.0, "avg_logprob": -0.07355311277101366, "compression_ratio": 1.9583333333333333, "no_speech_prob": 0.010325654409825802}, {"id": 307, "seek": 154490, "start": 1566.26, "end": 1573.0600000000002, "text": " using languages to integrate environmental data, whale songs, bird sounds, movement of", "tokens": [51432, 1228, 8650, 281, 13365, 8303, 1412, 11, 25370, 5781, 11, 5255, 3263, 11, 3963, 295, 51772], "temperature": 0.0, "avg_logprob": -0.07355311277101366, "compression_ratio": 1.9583333333333333, "no_speech_prob": 0.010325654409825802}, {"id": 308, "seek": 157306, "start": 1573.06, "end": 1577.94, "text": " tectonic plates. These can be tokenized and used as training data in models in which language", "tokens": [50364, 535, 349, 11630, 14231, 13, 1981, 393, 312, 14862, 1602, 293, 1143, 382, 3097, 1412, 294, 5245, 294, 597, 2856, 50608], "temperature": 0.0, "avg_logprob": -0.06781981331961495, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.000444116914877668}, {"id": 309, "seek": 157306, "start": 1577.94, "end": 1585.54, "text": " becomes the basis of a much larger space or let's say a much larger architecture of structural", "tokens": [50608, 3643, 264, 5143, 295, 257, 709, 4833, 1901, 420, 718, 311, 584, 257, 709, 4833, 9482, 295, 15067, 50988], "temperature": 0.0, "avg_logprob": -0.06781981331961495, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.000444116914877668}, {"id": 310, "seek": 157306, "start": 1585.54, "end": 1590.98, "text": " difference within systems, right? And if you think of semiotic systems as defined by a kind of", "tokens": [50988, 2649, 1951, 3652, 11, 558, 30, 400, 498, 291, 519, 295, 12909, 9411, 3652, 382, 7642, 538, 257, 733, 295, 51260], "temperature": 0.0, "avg_logprob": -0.06781981331961495, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.000444116914877668}, {"id": 311, "seek": 157306, "start": 1590.98, "end": 1596.5, "text": " internalized space of clustered correspondence and differentiation, which we call embeddings,", "tokens": [51260, 6920, 1602, 1901, 295, 596, 38624, 38135, 293, 38902, 11, 597, 321, 818, 12240, 29432, 11, 51536], "temperature": 0.0, "avg_logprob": -0.06781981331961495, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.000444116914877668}, {"id": 312, "seek": 157306, "start": 1596.5, "end": 1600.74, "text": " lots and lots and lots of things that we don't normally think about as being linguistic can be", "tokens": [51536, 3195, 293, 3195, 293, 3195, 295, 721, 300, 321, 500, 380, 5646, 519, 466, 382, 885, 43002, 393, 312, 51748], "temperature": 0.0, "avg_logprob": -0.06781981331961495, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.000444116914877668}, {"id": 313, "seek": 160074, "start": 1600.74, "end": 1605.86, "text": " made linguistic or turn out to be linguistic in some fundamental sense, depending on how you want", "tokens": [50364, 1027, 43002, 420, 1261, 484, 281, 312, 43002, 294, 512, 8088, 2020, 11, 5413, 322, 577, 291, 528, 50620], "temperature": 0.0, "avg_logprob": -0.09069366765216114, "compression_ratio": 1.8852459016393444, "no_speech_prob": 0.012050536461174488}, {"id": 314, "seek": 160074, "start": 1605.86, "end": 1610.5, "text": " to look at it, that would probably validate some of the people that have been working on", "tokens": [50620, 281, 574, 412, 309, 11, 300, 576, 1391, 29562, 512, 295, 264, 561, 300, 362, 668, 1364, 322, 50852], "temperature": 0.0, "avg_logprob": -0.09069366765216114, "compression_ratio": 1.8852459016393444, "no_speech_prob": 0.012050536461174488}, {"id": 315, "seek": 160074, "start": 1610.5, "end": 1615.78, "text": " more theoretical forms of biosemiotics all these years, people thinking about the world as cybernetic", "tokens": [50852, 544, 20864, 6422, 295, 36997, 13372, 42131, 439, 613, 924, 11, 561, 1953, 466, 264, 1002, 382, 13411, 77, 3532, 51116], "temperature": 0.0, "avg_logprob": -0.09069366765216114, "compression_ratio": 1.8852459016393444, "no_speech_prob": 0.012050536461174488}, {"id": 316, "seek": 160074, "start": 1615.78, "end": 1620.34, "text": " systems that have to do with this and ways in which cybernetic systems and information theory", "tokens": [51116, 3652, 300, 362, 281, 360, 365, 341, 293, 2098, 294, 597, 13411, 77, 3532, 3652, 293, 1589, 5261, 51344], "temperature": 0.0, "avg_logprob": -0.09069366765216114, "compression_ratio": 1.8852459016393444, "no_speech_prob": 0.012050536461174488}, {"id": 317, "seek": 160074, "start": 1620.34, "end": 1625.22, "text": " align with one another. It's not just information, but it's actually structured information that has", "tokens": [51344, 7975, 365, 472, 1071, 13, 467, 311, 406, 445, 1589, 11, 457, 309, 311, 767, 18519, 1589, 300, 575, 51588], "temperature": 0.0, "avg_logprob": -0.09069366765216114, "compression_ratio": 1.8852459016393444, "no_speech_prob": 0.012050536461174488}, {"id": 318, "seek": 160074, "start": 1625.22, "end": 1629.54, "text": " to do with correspondences of similarities and dissimilarities between the semantic meaning", "tokens": [51588, 281, 360, 365, 6805, 2667, 295, 24197, 293, 7802, 332, 2202, 1088, 1296, 264, 47982, 3620, 51804], "temperature": 0.0, "avg_logprob": -0.09069366765216114, "compression_ratio": 1.8852459016393444, "no_speech_prob": 0.012050536461174488}, {"id": 319, "seek": 162954, "start": 1629.54, "end": 1634.42, "text": " within those forms of information. Now, when we say language, that's not normally what we", "tokens": [50364, 1951, 729, 6422, 295, 1589, 13, 823, 11, 562, 321, 584, 2856, 11, 300, 311, 406, 5646, 437, 321, 50608], "temperature": 0.0, "avg_logprob": -0.09097326653344291, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.0005192611715756357}, {"id": 320, "seek": 162954, "start": 1634.42, "end": 1639.78, "text": " mean by language, right? The word language comes from, you know, Latin refers to tongue speech,", "tokens": [50608, 914, 538, 2856, 11, 558, 30, 440, 1349, 2856, 1487, 490, 11, 291, 458, 11, 10803, 14942, 281, 10601, 6218, 11, 50876], "temperature": 0.0, "avg_logprob": -0.09097326653344291, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.0005192611715756357}, {"id": 321, "seek": 162954, "start": 1639.78, "end": 1644.18, "text": " right? Language is like the things I say, the things that I write down. It's not this much larger", "tokens": [50876, 558, 30, 24445, 307, 411, 264, 721, 286, 584, 11, 264, 721, 300, 286, 2464, 760, 13, 467, 311, 406, 341, 709, 4833, 51096], "temperature": 0.0, "avg_logprob": -0.09097326653344291, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.0005192611715756357}, {"id": 322, "seek": 162954, "start": 1644.18, "end": 1649.78, "text": " and more universal, if you like, or at least general space of topological structure difference", "tokens": [51096, 293, 544, 11455, 11, 498, 291, 411, 11, 420, 412, 1935, 2674, 1901, 295, 1192, 4383, 3877, 2649, 51376], "temperature": 0.0, "avg_logprob": -0.09097326653344291, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.0005192611715756357}, {"id": 323, "seek": 162954, "start": 1649.78, "end": 1655.62, "text": " that is generative in this way, which means that the word language is not quite right,", "tokens": [51376, 300, 307, 1337, 1166, 294, 341, 636, 11, 597, 1355, 300, 264, 1349, 2856, 307, 406, 1596, 558, 11, 51668], "temperature": 0.0, "avg_logprob": -0.09097326653344291, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.0005192611715756357}, {"id": 324, "seek": 165562, "start": 1655.62, "end": 1661.4599999999998, "text": " that language is actually something different than what the word language signifies it to be.", "tokens": [50364, 300, 2856, 307, 767, 746, 819, 813, 437, 264, 1349, 2856, 1465, 11221, 309, 281, 312, 13, 50656], "temperature": 0.0, "avg_logprob": -0.10465294018126371, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.003998768515884876}, {"id": 325, "seek": 165562, "start": 1661.4599999999998, "end": 1667.78, "text": " And we probably should, we may need to either learn to redefine language in order to make sense", "tokens": [50656, 400, 321, 1391, 820, 11, 321, 815, 643, 281, 2139, 1466, 281, 38818, 533, 2856, 294, 1668, 281, 652, 2020, 50972], "temperature": 0.0, "avg_logprob": -0.10465294018126371, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.003998768515884876}, {"id": 326, "seek": 165562, "start": 1667.78, "end": 1672.34, "text": " of what's going on, or we need another word to describe this as well. But language is not what", "tokens": [50972, 295, 437, 311, 516, 322, 11, 420, 321, 643, 1071, 1349, 281, 6786, 341, 382, 731, 13, 583, 2856, 307, 406, 437, 51200], "temperature": 0.0, "avg_logprob": -0.10465294018126371, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.003998768515884876}, {"id": 327, "seek": 165562, "start": 1672.34, "end": 1677.9399999999998, "text": " language thinks it is. I would like to take a slight detour or a tangent here, since we have", "tokens": [51200, 2856, 7309, 309, 307, 13, 286, 576, 411, 281, 747, 257, 4036, 1141, 396, 420, 257, 27747, 510, 11, 1670, 321, 362, 51480], "temperature": 0.0, "avg_logprob": -0.10465294018126371, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.003998768515884876}, {"id": 328, "seek": 165562, "start": 1677.9399999999998, "end": 1681.78, "text": " limited time. And I just wanted to bring up a discussion that I have with Marek about the", "tokens": [51480, 5567, 565, 13, 400, 286, 445, 1415, 281, 1565, 493, 257, 5017, 300, 286, 362, 365, 376, 543, 74, 466, 264, 51672], "temperature": 0.0, "avg_logprob": -0.10465294018126371, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.003998768515884876}, {"id": 329, "seek": 168178, "start": 1681.78, "end": 1686.8999999999999, "text": " stack when we were in China a few months ago, preparing for an exhibition. At one point,", "tokens": [50364, 8630, 562, 321, 645, 294, 3533, 257, 1326, 2493, 2057, 11, 10075, 337, 364, 14414, 13, 1711, 472, 935, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09454295674308402, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.17573443055152893}, {"id": 330, "seek": 168178, "start": 1686.8999999999999, "end": 1691.86, "text": " we began to speculate, and perhaps it was just me, you know, maybe Marek was just agreeing because", "tokens": [50620, 321, 4283, 281, 40775, 11, 293, 4317, 309, 390, 445, 385, 11, 291, 458, 11, 1310, 376, 543, 74, 390, 445, 36900, 570, 50868], "temperature": 0.0, "avg_logprob": -0.09454295674308402, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.17573443055152893}, {"id": 331, "seek": 168178, "start": 1691.86, "end": 1696.98, "text": " it was late and he wanted to sleep. But, you know, when we consider the stack and what is happening", "tokens": [50868, 309, 390, 3469, 293, 415, 1415, 281, 2817, 13, 583, 11, 291, 458, 11, 562, 321, 1949, 264, 8630, 293, 437, 307, 2737, 51124], "temperature": 0.0, "avg_logprob": -0.09454295674308402, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.17573443055152893}, {"id": 332, "seek": 168178, "start": 1696.98, "end": 1702.74, "text": " in China, we had a question. Is there a dual stack system where two stacks are interconnecting in", "tokens": [51124, 294, 3533, 11, 321, 632, 257, 1168, 13, 1119, 456, 257, 11848, 8630, 1185, 689, 732, 30792, 366, 26253, 278, 294, 51412], "temperature": 0.0, "avg_logprob": -0.09454295674308402, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.17573443055152893}, {"id": 333, "seek": 168178, "start": 1702.74, "end": 1707.3799999999999, "text": " some in-between space? I mean, what is happening in China is quite remarkable in a number of ways,", "tokens": [51412, 512, 294, 12, 32387, 1901, 30, 286, 914, 11, 437, 307, 2737, 294, 3533, 307, 1596, 12802, 294, 257, 1230, 295, 2098, 11, 51644], "temperature": 0.0, "avg_logprob": -0.09454295674308402, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.17573443055152893}, {"id": 334, "seek": 170738, "start": 1707.38, "end": 1712.98, "text": " as a form of, as a power structure, as a control mechanism, if you want, at least to some extent.", "tokens": [50364, 382, 257, 1254, 295, 11, 382, 257, 1347, 3877, 11, 382, 257, 1969, 7513, 11, 498, 291, 528, 11, 412, 1935, 281, 512, 8396, 13, 50644], "temperature": 0.0, "avg_logprob": -0.09089538029261998, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.01681099645793438}, {"id": 335, "seek": 170738, "start": 1713.6200000000001, "end": 1718.5800000000002, "text": " And this connects to your upcoming book and the notion of AI as a cultural construct,", "tokens": [50676, 400, 341, 16967, 281, 428, 11500, 1446, 293, 264, 10710, 295, 7318, 382, 257, 6988, 7690, 11, 50924], "temperature": 0.0, "avg_logprob": -0.09089538029261998, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.01681099645793438}, {"id": 336, "seek": 170738, "start": 1718.5800000000002, "end": 1723.22, "text": " but also to the concepts of artificiality and intelligence as sides of differences.", "tokens": [50924, 457, 611, 281, 264, 10392, 295, 11677, 507, 293, 7599, 382, 4881, 295, 7300, 13, 51156], "temperature": 0.0, "avg_logprob": -0.09089538029261998, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.01681099645793438}, {"id": 337, "seek": 170738, "start": 1723.22, "end": 1728.98, "text": " So I guess the question is, does AI signify something else in China? And if not, could this", "tokens": [51156, 407, 286, 2041, 264, 1168, 307, 11, 775, 7318, 1465, 2505, 746, 1646, 294, 3533, 30, 400, 498, 406, 11, 727, 341, 51444], "temperature": 0.0, "avg_logprob": -0.09089538029261998, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.01681099645793438}, {"id": 338, "seek": 170738, "start": 1728.98, "end": 1735.8600000000001, "text": " be seen as a kind of post-imperialistic phenomenon, channeled mainly through technological means?", "tokens": [51444, 312, 1612, 382, 257, 733, 295, 2183, 12, 332, 610, 831, 3142, 14029, 11, 2269, 292, 8704, 807, 18439, 1355, 30, 51788], "temperature": 0.0, "avg_logprob": -0.09089538029261998, "compression_ratio": 1.6263345195729537, "no_speech_prob": 0.01681099645793438}, {"id": 339, "seek": 173586, "start": 1735.86, "end": 1743.2199999999998, "text": " It's not a tangent at all. I think it's the right move in the Bayesian groping of the conversation.", "tokens": [50364, 467, 311, 406, 257, 27747, 412, 439, 13, 286, 519, 309, 311, 264, 558, 1286, 294, 264, 7840, 42434, 290, 1513, 278, 295, 264, 3761, 13, 50732], "temperature": 0.0, "avg_logprob": -0.1327822131495322, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.02029607817530632}, {"id": 340, "seek": 173586, "start": 1743.2199999999998, "end": 1747.3799999999999, "text": " So the term I think you're referring to is what I call hemispherical stacks,", "tokens": [50732, 407, 264, 1433, 286, 519, 291, 434, 13761, 281, 307, 437, 286, 818, 8636, 7631, 34340, 30792, 11, 50940], "temperature": 0.0, "avg_logprob": -0.1327822131495322, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.02029607817530632}, {"id": 341, "seek": 173586, "start": 1747.3799999999999, "end": 1752.82, "text": " which was based on a talk I gave at Hacav\u00e9 in, I think, 2017, that then sort of inspired,", "tokens": [50940, 597, 390, 2361, 322, 257, 751, 286, 2729, 412, 389, 326, 706, 526, 294, 11, 286, 519, 11, 6591, 11, 300, 550, 1333, 295, 7547, 11, 51212], "temperature": 0.0, "avg_logprob": -0.1327822131495322, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.02029607817530632}, {"id": 342, "seek": 173586, "start": 1752.82, "end": 1757.06, "text": " became the basis of a book, came out, I think, last year called Vertical Atlas. But what I", "tokens": [51212, 3062, 264, 5143, 295, 257, 1446, 11, 1361, 484, 11, 286, 519, 11, 1036, 1064, 1219, 21044, 804, 32485, 13, 583, 437, 286, 51424], "temperature": 0.0, "avg_logprob": -0.1327822131495322, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.02029607817530632}, {"id": 343, "seek": 173586, "start": 1757.06, "end": 1761.3799999999999, "text": " was looking at there was the way in which we have not only, we have planetary computation,", "tokens": [51424, 390, 1237, 412, 456, 390, 264, 636, 294, 597, 321, 362, 406, 787, 11, 321, 362, 35788, 24903, 11, 51640], "temperature": 0.0, "avg_logprob": -0.1327822131495322, "compression_ratio": 1.6093189964157706, "no_speech_prob": 0.02029607817530632}, {"id": 344, "seek": 176138, "start": 1761.38, "end": 1765.94, "text": " but we don't just have one stack. That part of what's happened over that period of time,", "tokens": [50364, 457, 321, 500, 380, 445, 362, 472, 8630, 13, 663, 644, 295, 437, 311, 2011, 670, 300, 2896, 295, 565, 11, 50592], "temperature": 0.0, "avg_logprob": -0.11105418906492345, "compression_ratio": 1.7350157728706626, "no_speech_prob": 0.08993256092071533}, {"id": 345, "seek": 176138, "start": 1765.94, "end": 1772.5, "text": " between the mid-2010s to the present, was a shift towards a multipolarization of geopolitics.", "tokens": [50592, 1296, 264, 2062, 12, 2009, 3279, 82, 281, 264, 1974, 11, 390, 257, 5513, 3030, 257, 3311, 15276, 2144, 295, 46615, 1167, 13, 50920], "temperature": 0.0, "avg_logprob": -0.11105418906492345, "compression_ratio": 1.7350157728706626, "no_speech_prob": 0.08993256092071533}, {"id": 346, "seek": 176138, "start": 1772.5, "end": 1777.22, "text": " The geopolitics itself became more multipolar. One of the other things we had during this time,", "tokens": [50920, 440, 46615, 1167, 2564, 3062, 544, 3311, 15276, 13, 1485, 295, 264, 661, 721, 321, 632, 1830, 341, 565, 11, 51156], "temperature": 0.0, "avg_logprob": -0.11105418906492345, "compression_ratio": 1.7350157728706626, "no_speech_prob": 0.08993256092071533}, {"id": 347, "seek": 176138, "start": 1777.22, "end": 1782.18, "text": " arguably, China is a good example. I mean, basically everywhere is a good example,", "tokens": [51156, 26771, 11, 3533, 307, 257, 665, 1365, 13, 286, 914, 11, 1936, 5315, 307, 257, 665, 1365, 11, 51404], "temperature": 0.0, "avg_logprob": -0.11105418906492345, "compression_ratio": 1.7350157728706626, "no_speech_prob": 0.08993256092071533}, {"id": 348, "seek": 176138, "start": 1782.18, "end": 1786.5800000000002, "text": " in different kinds of ways, and that's the point. Also a shift in the dynamics of governance.", "tokens": [51404, 294, 819, 3685, 295, 2098, 11, 293, 300, 311, 264, 935, 13, 2743, 257, 5513, 294, 264, 15679, 295, 17449, 13, 51624], "temperature": 0.0, "avg_logprob": -0.11105418906492345, "compression_ratio": 1.7350157728706626, "no_speech_prob": 0.08993256092071533}, {"id": 349, "seek": 176138, "start": 1787.3000000000002, "end": 1791.0600000000002, "text": " And I mean this almost in the cybernetic sense of governance, not necessarily in the political", "tokens": [51660, 400, 286, 914, 341, 1920, 294, 264, 13411, 77, 3532, 2020, 295, 17449, 11, 406, 4725, 294, 264, 3905, 51848], "temperature": 0.0, "avg_logprob": -0.11105418906492345, "compression_ratio": 1.7350157728706626, "no_speech_prob": 0.08993256092071533}, {"id": 350, "seek": 179106, "start": 1791.06, "end": 1796.82, "text": " sense of governance, but also that too, into stack systems. That computation became not only", "tokens": [50364, 2020, 295, 17449, 11, 457, 611, 300, 886, 11, 666, 8630, 3652, 13, 663, 24903, 3062, 406, 787, 50652], "temperature": 0.0, "avg_logprob": -0.09820434911464287, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.0005356167675927281}, {"id": 351, "seek": 179106, "start": 1796.82, "end": 1802.6599999999999, "text": " something about which governance may decrease, but rather it was the actual mechanism of governance,", "tokens": [50652, 746, 466, 597, 17449, 815, 11514, 11, 457, 2831, 309, 390, 264, 3539, 7513, 295, 17449, 11, 50944], "temperature": 0.0, "avg_logprob": -0.09820434911464287, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.0005356167675927281}, {"id": 352, "seek": 179106, "start": 1802.6599999999999, "end": 1807.3, "text": " how norms and rules would recursively enforce themselves in the world and that people would", "tokens": [50944, 577, 24357, 293, 4474, 576, 20560, 3413, 24825, 2969, 294, 264, 1002, 293, 300, 561, 576, 51176], "temperature": 0.0, "avg_logprob": -0.09820434911464287, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.0005356167675927281}, {"id": 353, "seek": 179106, "start": 1807.3, "end": 1810.34, "text": " act through them and speak through them. It became the form of this governance. So both", "tokens": [51176, 605, 807, 552, 293, 1710, 807, 552, 13, 467, 3062, 264, 1254, 295, 341, 17449, 13, 407, 1293, 51328], "temperature": 0.0, "avg_logprob": -0.09820434911464287, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.0005356167675927281}, {"id": 354, "seek": 179106, "start": 1810.34, "end": 1814.6599999999999, "text": " these happened at the same time. You have a shift of governance towards stack systems,", "tokens": [51328, 613, 2011, 412, 264, 912, 565, 13, 509, 362, 257, 5513, 295, 17449, 3030, 8630, 3652, 11, 51544], "temperature": 0.0, "avg_logprob": -0.09820434911464287, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.0005356167675927281}, {"id": 355, "seek": 179106, "start": 1814.6599999999999, "end": 1820.1799999999998, "text": " and you have a multipolarization of geopolitics. And what I'm arguing is that what you see then,", "tokens": [51544, 293, 291, 362, 257, 3311, 15276, 2144, 295, 46615, 1167, 13, 400, 437, 286, 478, 19697, 307, 300, 437, 291, 536, 550, 11, 51820], "temperature": 0.0, "avg_logprob": -0.09820434911464287, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.0005356167675927281}, {"id": 356, "seek": 182018, "start": 1820.18, "end": 1824.02, "text": " we shouldn't be surprised to then see a multipolarization of stack systems.", "tokens": [50364, 321, 4659, 380, 312, 6100, 281, 550, 536, 257, 3311, 15276, 2144, 295, 8630, 3652, 13, 50556], "temperature": 0.0, "avg_logprob": -0.10651874542236328, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.00017950480105355382}, {"id": 357, "seek": 182018, "start": 1824.02, "end": 1828.3400000000001, "text": " And so the emergence at that time of, let's say, the North Atlantic stack, which would include", "tokens": [50556, 400, 370, 264, 36211, 412, 300, 565, 295, 11, 718, 311, 584, 11, 264, 4067, 20233, 8630, 11, 597, 576, 4090, 50772], "temperature": 0.0, "avg_logprob": -0.10651874542236328, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.00017950480105355382}, {"id": 358, "seek": 182018, "start": 1828.3400000000001, "end": 1835.14, "text": " basically the five ice countries and a few others, a China stack that would extend into parts of East", "tokens": [50772, 1936, 264, 1732, 4435, 3517, 293, 257, 1326, 2357, 11, 257, 3533, 8630, 300, 576, 10101, 666, 3166, 295, 6747, 51112], "temperature": 0.0, "avg_logprob": -0.10651874542236328, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.00017950480105355382}, {"id": 359, "seek": 182018, "start": 1835.14, "end": 1839.7, "text": " Asia and into East Africa, definitely the emergence of an India stack, which was built around the", "tokens": [51112, 10038, 293, 666, 6747, 7349, 11, 2138, 264, 36211, 295, 364, 5282, 8630, 11, 597, 390, 3094, 926, 264, 51340], "temperature": 0.0, "avg_logprob": -0.10651874542236328, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.00017950480105355382}, {"id": 360, "seek": 182018, "start": 1839.7, "end": 1845.7, "text": " national ID system, and so on and so on. All of the multipolar, let's say, lines you might draw,", "tokens": [51340, 4048, 7348, 1185, 11, 293, 370, 322, 293, 370, 322, 13, 1057, 295, 264, 3311, 15276, 11, 718, 311, 584, 11, 3876, 291, 1062, 2642, 11, 51640], "temperature": 0.0, "avg_logprob": -0.10651874542236328, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.00017950480105355382}, {"id": 361, "seek": 184570, "start": 1846.5, "end": 1852.02, "text": " lo and behold, these are also the lines by which a multipolarized stack system was emerging as", "tokens": [50404, 450, 293, 27234, 11, 613, 366, 611, 264, 3876, 538, 597, 257, 3311, 15276, 1602, 8630, 1185, 390, 14989, 382, 50680], "temperature": 0.0, "avg_logprob": -0.16568486552593137, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0047536506317555904}, {"id": 362, "seek": 184570, "start": 1852.02, "end": 1856.9, "text": " well. And so this hemispherical stacks dynamic is one of the areas of research for the Antiqua", "tokens": [50680, 731, 13, 400, 370, 341, 8636, 7631, 34340, 30792, 8546, 307, 472, 295, 264, 3179, 295, 2132, 337, 264, 5130, 3221, 64, 50924], "temperature": 0.0, "avg_logprob": -0.16568486552593137, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0047536506317555904}, {"id": 363, "seek": 184570, "start": 1856.9, "end": 1862.3400000000001, "text": " Theory program. There's a book that I'm putting together with Chen Xu-Feng, the Stanley Chen,", "tokens": [50924, 29009, 1461, 13, 821, 311, 257, 1446, 300, 286, 478, 3372, 1214, 365, 13682, 23082, 12, 37, 1501, 11, 264, 28329, 13682, 11, 51196], "temperature": 0.0, "avg_logprob": -0.16568486552593137, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0047536506317555904}, {"id": 364, "seek": 184570, "start": 1862.3400000000001, "end": 1866.5800000000002, "text": " the Chinese science fiction writer who wrote a book called Waist Tide. He wrote a book with Kai", "tokens": [51196, 264, 4649, 3497, 13266, 9936, 567, 4114, 257, 1446, 1219, 15405, 468, 314, 482, 13, 634, 4114, 257, 1446, 365, 20753, 51408], "temperature": 0.0, "avg_logprob": -0.16568486552593137, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0047536506317555904}, {"id": 365, "seek": 184570, "start": 1866.5800000000002, "end": 1873.78, "text": " Fu Li, AI 2041. And we're putting together with a group of science fiction writers, theorists,", "tokens": [51408, 12807, 8349, 11, 7318, 945, 17344, 13, 400, 321, 434, 3372, 1214, 365, 257, 1594, 295, 3497, 13266, 13491, 11, 27423, 1751, 11, 51768], "temperature": 0.0, "avg_logprob": -0.16568486552593137, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.0047536506317555904}, {"id": 366, "seek": 187378, "start": 1873.78, "end": 1878.58, "text": " writers, a number of other people, sort of scenarios for a near future for US-China chip", "tokens": [50364, 13491, 11, 257, 1230, 295, 661, 561, 11, 1333, 295, 15077, 337, 257, 2651, 2027, 337, 2546, 12, 6546, 1426, 11409, 50604], "temperature": 0.0, "avg_logprob": -0.10458911316735404, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.003171628573909402}, {"id": 367, "seek": 187378, "start": 1878.58, "end": 1883.78, "text": " wars, other kinds of things. We're in a little bit of uncharted territory here about how does", "tokens": [50604, 13718, 11, 661, 3685, 295, 721, 13, 492, 434, 294, 257, 707, 857, 295, 33686, 47350, 11360, 510, 466, 577, 775, 50864], "temperature": 0.0, "avg_logprob": -0.10458911316735404, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.003171628573909402}, {"id": 368, "seek": 187378, "start": 1883.78, "end": 1889.7, "text": " the capacity to build the better stack is not just something about which geopolitics is interested,", "tokens": [50864, 264, 6042, 281, 1322, 264, 1101, 8630, 307, 406, 445, 746, 466, 597, 46615, 1167, 307, 3102, 11, 51160], "temperature": 0.0, "avg_logprob": -0.10458911316735404, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.003171628573909402}, {"id": 369, "seek": 187378, "start": 1889.7, "end": 1894.18, "text": " it becomes the fundamental work of geopolitics itself. And so we're trying to figure out a", "tokens": [51160, 309, 3643, 264, 8088, 589, 295, 46615, 1167, 2564, 13, 400, 370, 321, 434, 1382, 281, 2573, 484, 257, 51384], "temperature": 0.0, "avg_logprob": -0.10458911316735404, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.003171628573909402}, {"id": 370, "seek": 187378, "start": 1894.18, "end": 1897.86, "text": " little bit of what that means. The other book that's coming out, which will be coming out much", "tokens": [51384, 707, 857, 295, 437, 300, 1355, 13, 440, 661, 1446, 300, 311, 1348, 484, 11, 597, 486, 312, 1348, 484, 709, 51568], "temperature": 0.0, "avg_logprob": -0.10458911316735404, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.003171628573909402}, {"id": 371, "seek": 189786, "start": 1897.86, "end": 1903.3799999999999, "text": " sooner than that one will, is a book co-edited with Anna Greenspan and Bogna Konyar, which was", "tokens": [50364, 15324, 813, 300, 472, 486, 11, 307, 257, 1446, 598, 12, 292, 1226, 365, 12899, 39314, 6040, 293, 24339, 629, 591, 2526, 289, 11, 597, 390, 50640], "temperature": 0.0, "avg_logprob": -0.12097731424034189, "compression_ratio": 1.6040955631399318, "no_speech_prob": 0.15996859967708588}, {"id": 372, "seek": 189786, "start": 1903.9399999999998, "end": 1908.74, "text": " came out of some time I spent as a visiting professor at NYU Shanghai, New York University", "tokens": [50668, 1361, 484, 295, 512, 565, 286, 4418, 382, 257, 11700, 8304, 412, 42682, 26135, 11, 1873, 3609, 3535, 50908], "temperature": 0.0, "avg_logprob": -0.12097731424034189, "compression_ratio": 1.6040955631399318, "no_speech_prob": 0.15996859967708588}, {"id": 373, "seek": 189786, "start": 1908.74, "end": 1912.8999999999999, "text": " Shanghai. We started a thing there called the Center for AI and Culture. And one of the things", "tokens": [50908, 26135, 13, 492, 1409, 257, 551, 456, 1219, 264, 5169, 337, 7318, 293, 27539, 13, 400, 472, 295, 264, 721, 51116], "temperature": 0.0, "avg_logprob": -0.12097731424034189, "compression_ratio": 1.6040955631399318, "no_speech_prob": 0.15996859967708588}, {"id": 374, "seek": 189786, "start": 1912.8999999999999, "end": 1918.34, "text": " I was very interested there was particularly looking at different cultural logics of artificial", "tokens": [51116, 286, 390, 588, 3102, 456, 390, 4098, 1237, 412, 819, 6988, 3565, 1167, 295, 11677, 51388], "temperature": 0.0, "avg_logprob": -0.12097731424034189, "compression_ratio": 1.6040955631399318, "no_speech_prob": 0.15996859967708588}, {"id": 375, "seek": 189786, "start": 1918.34, "end": 1924.58, "text": " intelligence and looking at the emergence of AI in China through a different lens. It's quite", "tokens": [51388, 7599, 293, 1237, 412, 264, 36211, 295, 7318, 294, 3533, 807, 257, 819, 6765, 13, 467, 311, 1596, 51700], "temperature": 0.0, "avg_logprob": -0.12097731424034189, "compression_ratio": 1.6040955631399318, "no_speech_prob": 0.15996859967708588}, {"id": 376, "seek": 192458, "start": 1924.58, "end": 1928.82, "text": " surprising to me, I think even among Western scholars and writers who spend a lot of time", "tokens": [50364, 8830, 281, 385, 11, 286, 519, 754, 3654, 8724, 8553, 293, 13491, 567, 3496, 257, 688, 295, 565, 50576], "temperature": 0.0, "avg_logprob": -0.06475404965675484, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.0023217510897666216}, {"id": 377, "seek": 192458, "start": 1928.82, "end": 1933.3, "text": " looking at the history of AI, how little people really know about the history of AI in China,", "tokens": [50576, 1237, 412, 264, 2503, 295, 7318, 11, 577, 707, 561, 534, 458, 466, 264, 2503, 295, 7318, 294, 3533, 11, 50800], "temperature": 0.0, "avg_logprob": -0.06475404965675484, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.0023217510897666216}, {"id": 378, "seek": 192458, "start": 1933.3, "end": 1939.6999999999998, "text": " which goes back to the 1950s and 60s and with the different politicized role of cybernetics,", "tokens": [50800, 597, 1709, 646, 281, 264, 18141, 82, 293, 4060, 82, 293, 365, 264, 819, 48044, 1602, 3090, 295, 13411, 7129, 1167, 11, 51120], "temperature": 0.0, "avg_logprob": -0.06475404965675484, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.0023217510897666216}, {"id": 379, "seek": 192458, "start": 1939.6999999999998, "end": 1947.06, "text": " the impact of the Sino-Soviet split, the return of Deng Xiaoping in the late 1970s. It's a whole", "tokens": [51120, 264, 2712, 295, 264, 318, 2982, 12, 50, 5179, 1684, 7472, 11, 264, 2736, 295, 413, 1501, 11956, 26125, 294, 264, 3469, 14577, 82, 13, 467, 311, 257, 1379, 51488], "temperature": 0.0, "avg_logprob": -0.06475404965675484, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.0023217510897666216}, {"id": 380, "seek": 192458, "start": 1947.06, "end": 1952.74, "text": " thing that really should be better known and understood. The book and the project was also", "tokens": [51488, 551, 300, 534, 820, 312, 1101, 2570, 293, 7320, 13, 440, 1446, 293, 264, 1716, 390, 611, 51772], "temperature": 0.0, "avg_logprob": -0.06475404965675484, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.0023217510897666216}, {"id": 381, "seek": 195274, "start": 1952.74, "end": 1958.58, "text": " based on the, I think, rather obvious observation that no two cultures define the artificial", "tokens": [50364, 2361, 322, 264, 11, 286, 519, 11, 2831, 6322, 14816, 300, 572, 732, 12951, 6964, 264, 11677, 50656], "temperature": 0.0, "avg_logprob": -0.08750912121364049, "compression_ratio": 2.032490974729242, "no_speech_prob": 0.00016861938638612628}, {"id": 382, "seek": 195274, "start": 1959.14, "end": 1963.6200000000001, "text": " the same way. What constitutes artificial means something different in different cultural contexts.", "tokens": [50684, 264, 912, 636, 13, 708, 44204, 11677, 1355, 746, 819, 294, 819, 6988, 30628, 13, 50908], "temperature": 0.0, "avg_logprob": -0.08750912121364049, "compression_ratio": 2.032490974729242, "no_speech_prob": 0.00016861938638612628}, {"id": 383, "seek": 195274, "start": 1963.6200000000001, "end": 1966.34, "text": " What constitutes intelligence means something different in different cultural contexts. And", "tokens": [50908, 708, 44204, 7599, 1355, 746, 819, 294, 819, 6988, 30628, 13, 400, 51044], "temperature": 0.0, "avg_logprob": -0.08750912121364049, "compression_ratio": 2.032490974729242, "no_speech_prob": 0.00016861938638612628}, {"id": 384, "seek": 195274, "start": 1966.34, "end": 1970.58, "text": " therefore, one might presume that there are different foundations for what artificialization", "tokens": [51044, 4412, 11, 472, 1062, 43283, 300, 456, 366, 819, 22467, 337, 437, 11677, 2144, 51256], "temperature": 0.0, "avg_logprob": -0.08750912121364049, "compression_ratio": 2.032490974729242, "no_speech_prob": 0.00016861938638612628}, {"id": 385, "seek": 195274, "start": 1970.58, "end": 1975.78, "text": " of intelligence would even mean, which would frame what it's for in very different kinds of", "tokens": [51256, 295, 7599, 576, 754, 914, 11, 597, 576, 3920, 437, 309, 311, 337, 294, 588, 819, 3685, 295, 51516], "temperature": 0.0, "avg_logprob": -0.08750912121364049, "compression_ratio": 2.032490974729242, "no_speech_prob": 0.00016861938638612628}, {"id": 386, "seek": 195274, "start": 1975.78, "end": 1980.02, "text": " ways. And I think part of what we wanted to do with this project was, to be perfectly honest,", "tokens": [51516, 2098, 13, 400, 286, 519, 644, 295, 437, 321, 1415, 281, 360, 365, 341, 1716, 390, 11, 281, 312, 6239, 3245, 11, 51728], "temperature": 0.0, "avg_logprob": -0.08750912121364049, "compression_ratio": 2.032490974729242, "no_speech_prob": 0.00016861938638612628}, {"id": 387, "seek": 198002, "start": 1980.02, "end": 1985.46, "text": " it was less about us coming in and trying to interpret for ourselves what the Chinese model", "tokens": [50364, 309, 390, 1570, 466, 505, 1348, 294, 293, 1382, 281, 7302, 337, 4175, 437, 264, 4649, 2316, 50636], "temperature": 0.0, "avg_logprob": -0.11245896021525065, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.010008620098233223}, {"id": 388, "seek": 198002, "start": 1985.46, "end": 1990.98, "text": " of this is and then coming and reporting back on it than it was understanding it as the baseline", "tokens": [50636, 295, 341, 307, 293, 550, 1348, 293, 10031, 646, 322, 309, 813, 309, 390, 3701, 309, 382, 264, 20518, 50912], "temperature": 0.0, "avg_logprob": -0.11245896021525065, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.010008620098233223}, {"id": 389, "seek": 198002, "start": 1990.98, "end": 1998.26, "text": " and thinking about the way the West thinks about AI and trying to, in a way, provincializing and", "tokens": [50912, 293, 1953, 466, 264, 636, 264, 4055, 7309, 466, 7318, 293, 1382, 281, 11, 294, 257, 636, 11, 33293, 3319, 293, 51276], "temperature": 0.0, "avg_logprob": -0.11245896021525065, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.010008620098233223}, {"id": 390, "seek": 198002, "start": 1998.26, "end": 2005.3799999999999, "text": " particularizing Western thought about AI, the Turing model of AI as an individual, that it's a tool,", "tokens": [51276, 1729, 3319, 8724, 1194, 466, 7318, 11, 264, 314, 1345, 2316, 295, 7318, 382, 364, 2609, 11, 300, 309, 311, 257, 2290, 11, 51632], "temperature": 0.0, "avg_logprob": -0.11245896021525065, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.010008620098233223}, {"id": 391, "seek": 200538, "start": 2005.38, "end": 2011.3000000000002, "text": " artificial in the sense of being not natural, that it's really about bias and privacy and", "tokens": [50364, 11677, 294, 264, 2020, 295, 885, 406, 3303, 11, 300, 309, 311, 534, 466, 12577, 293, 11427, 293, 50660], "temperature": 0.0, "avg_logprob": -0.09681110553913289, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.039611708372831345}, {"id": 392, "seek": 200538, "start": 2011.3000000000002, "end": 2016.74, "text": " individual identity and all kinds of things that are very deep in the Western logic of AI and to", "tokens": [50660, 2609, 6575, 293, 439, 3685, 295, 721, 300, 366, 588, 2452, 294, 264, 8724, 9952, 295, 7318, 293, 281, 50932], "temperature": 0.0, "avg_logprob": -0.09681110553913289, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.039611708372831345}, {"id": 393, "seek": 200538, "start": 2016.74, "end": 2020.98, "text": " basically, with the presumption of the Chinese translation, in essence, to present to a Chinese", "tokens": [50932, 1936, 11, 365, 264, 18028, 1695, 295, 264, 4649, 12853, 11, 294, 12801, 11, 281, 1974, 281, 257, 4649, 51144], "temperature": 0.0, "avg_logprob": -0.09681110553913289, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.039611708372831345}, {"id": 394, "seek": 200538, "start": 2020.98, "end": 2028.2600000000002, "text": " audience how the West thinks about AI in a way in which it's not as in a, we've got it figured out", "tokens": [51144, 4034, 577, 264, 4055, 7309, 466, 7318, 294, 257, 636, 294, 597, 309, 311, 406, 382, 294, 257, 11, 321, 600, 658, 309, 8932, 484, 51508], "temperature": 0.0, "avg_logprob": -0.09681110553913289, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.039611708372831345}, {"id": 395, "seek": 200538, "start": 2028.2600000000002, "end": 2032.1000000000001, "text": " and here's the universal model that you should be thinking through, but rather these are the", "tokens": [51508, 293, 510, 311, 264, 11455, 2316, 300, 291, 820, 312, 1953, 807, 11, 457, 2831, 613, 366, 264, 51700], "temperature": 0.0, "avg_logprob": -0.09681110553913289, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.039611708372831345}, {"id": 396, "seek": 203210, "start": 2032.1, "end": 2037.9399999999998, "text": " weird ways of thoughts of my people and this is a guide to understanding how they may come about", "tokens": [50364, 3657, 2098, 295, 4598, 295, 452, 561, 293, 341, 307, 257, 5934, 281, 3701, 577, 436, 815, 808, 466, 50656], "temperature": 0.0, "avg_logprob": -0.06840437909831172, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.11898461729288101}, {"id": 397, "seek": 203210, "start": 2037.9399999999998, "end": 2041.78, "text": " this. My contribution to the book, which was, I think it's actually called something like An", "tokens": [50656, 341, 13, 1222, 13150, 281, 264, 1446, 11, 597, 390, 11, 286, 519, 309, 311, 767, 1219, 746, 411, 1107, 50848], "temperature": 0.0, "avg_logprob": -0.06840437909831172, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.11898461729288101}, {"id": 398, "seek": 203210, "start": 2041.78, "end": 2049.54, "text": " Apology for Western Theory of AI, looks at not just the history of AI in theory from Turing to Searle", "tokens": [50848, 8723, 1793, 337, 8724, 29009, 295, 7318, 11, 1542, 412, 406, 445, 264, 2503, 295, 7318, 294, 5261, 490, 314, 1345, 281, 1100, 36153, 51236], "temperature": 0.0, "avg_logprob": -0.06840437909831172, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.11898461729288101}, {"id": 399, "seek": 203210, "start": 2049.54, "end": 2056.74, "text": " to Brooks to Hinton and others, but rather the peculiarities of AI criticism from the West", "tokens": [51236, 281, 33493, 281, 389, 12442, 293, 2357, 11, 457, 2831, 264, 27149, 1088, 295, 7318, 15835, 490, 264, 4055, 51596], "temperature": 0.0, "avg_logprob": -0.06840437909831172, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.11898461729288101}, {"id": 400, "seek": 205674, "start": 2056.74, "end": 2062.3399999999997, "text": " and the kinds of things that AI, the AI ethics discourse, the AI critic discourse, this key", "tokens": [50364, 293, 264, 3685, 295, 721, 300, 7318, 11, 264, 7318, 19769, 23938, 11, 264, 7318, 7850, 23938, 11, 341, 2141, 50644], "temperature": 0.0, "avg_logprob": -0.08310773896008003, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.1867082566022873}, {"id": 401, "seek": 205674, "start": 2062.3399999999997, "end": 2068.5, "text": " questions that tends to revolve around and my own sense of unease by which those particular", "tokens": [50644, 1651, 300, 12258, 281, 16908, 303, 926, 293, 452, 1065, 2020, 295, 2251, 651, 538, 597, 729, 1729, 50952], "temperature": 0.0, "avg_logprob": -0.08310773896008003, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.1867082566022873}, {"id": 402, "seek": 205674, "start": 2068.5, "end": 2075.54, "text": " preoccupations are becoming overly universalized as really the important questions that we should", "tokens": [50952, 44388, 763, 366, 5617, 24324, 11455, 1602, 382, 534, 264, 1021, 1651, 300, 321, 820, 51304], "temperature": 0.0, "avg_logprob": -0.08310773896008003, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.1867082566022873}, {"id": 403, "seek": 205674, "start": 2075.54, "end": 2082.58, "text": " be asking in the conceptualization and composition of a planetary AI and arguing that not only is this", "tokens": [51304, 312, 3365, 294, 264, 24106, 2144, 293, 12686, 295, 257, 35788, 7318, 293, 19697, 300, 406, 787, 307, 341, 51656], "temperature": 0.0, "avg_logprob": -0.08310773896008003, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.1867082566022873}, {"id": 404, "seek": 208258, "start": 2082.58, "end": 2088.1, "text": " in a weird way, the new American cultural hegemonic export that is AI is trying to steal your", "tokens": [50364, 294, 257, 3657, 636, 11, 264, 777, 2665, 6988, 415, 432, 3317, 299, 10725, 300, 307, 7318, 307, 1382, 281, 11009, 428, 50640], "temperature": 0.0, "avg_logprob": -0.12724750352942427, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.08259914815425873}, {"id": 405, "seek": 208258, "start": 2088.1, "end": 2093.54, "text": " natural libertarian freedom, but that not only is this limited in the Western context, but it's", "tokens": [50640, 3303, 18058, 10652, 5645, 11, 457, 300, 406, 787, 307, 341, 5567, 294, 264, 8724, 4319, 11, 457, 309, 311, 50912], "temperature": 0.0, "avg_logprob": -0.12724750352942427, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.08259914815425873}, {"id": 406, "seek": 208258, "start": 2093.54, "end": 2098.18, "text": " even more limited in a global context. So that's what this sort of book is about. But I think that", "tokens": [50912, 754, 544, 5567, 294, 257, 4338, 4319, 13, 407, 300, 311, 437, 341, 1333, 295, 1446, 307, 466, 13, 583, 286, 519, 300, 51144], "temperature": 0.0, "avg_logprob": -0.12724750352942427, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.08259914815425873}, {"id": 407, "seek": 208258, "start": 2098.18, "end": 2102.42, "text": " just of your question, which ties back to the other thing we were talking about, has to do with", "tokens": [51144, 445, 295, 428, 1168, 11, 597, 14039, 646, 281, 264, 661, 551, 321, 645, 1417, 466, 11, 575, 281, 360, 365, 51356], "temperature": 0.0, "avg_logprob": -0.12724750352942427, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.08259914815425873}, {"id": 408, "seek": 208258, "start": 2102.42, "end": 2109.86, "text": " the certain, the inevitable limitations and bias of building large models on English only, for", "tokens": [51356, 264, 1629, 11, 264, 21451, 15705, 293, 12577, 295, 2390, 2416, 5245, 322, 3669, 787, 11, 337, 51728], "temperature": 0.0, "avg_logprob": -0.12724750352942427, "compression_ratio": 1.6925795053003534, "no_speech_prob": 0.08259914815425873}, {"id": 409, "seek": 210986, "start": 2109.86, "end": 2116.26, "text": " example. The Americanization of AI is, I hope, a kind of early phase in the development of this.", "tokens": [50364, 1365, 13, 440, 2665, 2144, 295, 7318, 307, 11, 286, 1454, 11, 257, 733, 295, 2440, 5574, 294, 264, 3250, 295, 341, 13, 50684], "temperature": 0.0, "avg_logprob": -0.09856780370076497, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.013620500452816486}, {"id": 410, "seek": 210986, "start": 2116.82, "end": 2120.58, "text": " This is the hope, at least. And so the scenario we'd like to see would be something like this,", "tokens": [50712, 639, 307, 264, 1454, 11, 412, 1935, 13, 400, 370, 264, 9005, 321, 1116, 411, 281, 536, 576, 312, 746, 411, 341, 11, 50900], "temperature": 0.0, "avg_logprob": -0.09856780370076497, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.013620500452816486}, {"id": 411, "seek": 210986, "start": 2120.58, "end": 2126.6600000000003, "text": " at least going forward. You have a handful of primarily American companies that, you know,", "tokens": [50900, 412, 1935, 516, 2128, 13, 509, 362, 257, 16458, 295, 10029, 2665, 3431, 300, 11, 291, 458, 11, 51204], "temperature": 0.0, "avg_logprob": -0.09856780370076497, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.013620500452816486}, {"id": 412, "seek": 210986, "start": 2126.6600000000003, "end": 2131.6200000000003, "text": " were able for regulatory reasons, cultural reasons, technological reasons, all kinds of, you know,", "tokens": [51204, 645, 1075, 337, 18260, 4112, 11, 6988, 4112, 11, 18439, 4112, 11, 439, 3685, 295, 11, 291, 458, 11, 51452], "temperature": 0.0, "avg_logprob": -0.09856780370076497, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.013620500452816486}, {"id": 413, "seek": 210986, "start": 2131.6200000000003, "end": 2137.06, "text": " any number of sort of reasons to build or to discover, if you prefer, the mechanisms of", "tokens": [51452, 604, 1230, 295, 1333, 295, 4112, 281, 1322, 420, 281, 4411, 11, 498, 291, 4382, 11, 264, 15902, 295, 51724], "temperature": 0.0, "avg_logprob": -0.09856780370076497, "compression_ratio": 1.6810035842293907, "no_speech_prob": 0.013620500452816486}, {"id": 414, "seek": 213706, "start": 2137.06, "end": 2140.9, "text": " foundational forms of machine intelligence. And I do say discover in a certain sense that", "tokens": [50364, 32195, 6422, 295, 3479, 7599, 13, 400, 286, 360, 584, 4411, 294, 257, 1629, 2020, 300, 50556], "temperature": 0.0, "avg_logprob": -0.12893877607403378, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.11907906085252762}, {"id": 415, "seek": 213706, "start": 2140.9, "end": 2145.14, "text": " where transformer models really work is just based on kind of iterative stochastic prediction.", "tokens": [50556, 689, 31782, 5245, 534, 589, 307, 445, 2361, 322, 733, 295, 17138, 1166, 342, 8997, 2750, 17630, 13, 50768], "temperature": 0.0, "avg_logprob": -0.12893877607403378, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.11907906085252762}, {"id": 416, "seek": 213706, "start": 2145.14, "end": 2149.7799999999997, "text": " That's kind of how you work too. It's kind of how our brains work. Our brains work on,", "tokens": [50768, 663, 311, 733, 295, 577, 291, 589, 886, 13, 467, 311, 733, 295, 577, 527, 15442, 589, 13, 2621, 15442, 589, 322, 11, 51000], "temperature": 0.0, "avg_logprob": -0.12893877607403378, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.11907906085252762}, {"id": 417, "seek": 213706, "start": 2149.7799999999997, "end": 2153.46, "text": " you know, each cortical column and each neural thing is kind of predicting what and simulating", "tokens": [51000, 291, 458, 11, 1184, 11278, 804, 7738, 293, 1184, 18161, 551, 307, 733, 295, 32884, 437, 293, 1034, 12162, 51184], "temperature": 0.0, "avg_logprob": -0.12893877607403378, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.11907906085252762}, {"id": 418, "seek": 213706, "start": 2153.46, "end": 2157.14, "text": " what it's going to perceive next, and then resolving and error correcting that kind of", "tokens": [51184, 437, 309, 311, 516, 281, 20281, 958, 11, 293, 550, 49940, 293, 6713, 47032, 300, 733, 295, 51368], "temperature": 0.0, "avg_logprob": -0.12893877607403378, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.11907906085252762}, {"id": 419, "seek": 213706, "start": 2157.14, "end": 2162.58, "text": " dynamic, right? And so, yes, you are a stochastic parrot after all. Based on what we were talking", "tokens": [51368, 8546, 11, 558, 30, 400, 370, 11, 2086, 11, 291, 366, 257, 342, 8997, 2750, 42462, 934, 439, 13, 18785, 322, 437, 321, 645, 1417, 51640], "temperature": 0.0, "avg_logprob": -0.12893877607403378, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.11907906085252762}, {"id": 420, "seek": 216258, "start": 2162.58, "end": 2168.1, "text": " about before with the giant space of multimodality of models, in principle, there's so much", "tokens": [50364, 466, 949, 365, 264, 7410, 1901, 295, 32972, 378, 1860, 295, 5245, 11, 294, 8665, 11, 456, 311, 370, 709, 50640], "temperature": 0.0, "avg_logprob": -0.058718848872829126, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.02367572858929634}, {"id": 421, "seek": 216258, "start": 2168.1, "end": 2172.02, "text": " information in the world. There's so much language in the world. If we think of AI,", "tokens": [50640, 1589, 294, 264, 1002, 13, 821, 311, 370, 709, 2856, 294, 264, 1002, 13, 759, 321, 519, 295, 7318, 11, 50836], "temperature": 0.0, "avg_logprob": -0.058718848872829126, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.02367572858929634}, {"id": 422, "seek": 216258, "start": 2172.02, "end": 2178.18, "text": " the purpose of AI is a way in which planetary intelligence as a whole can come to model itself", "tokens": [50836, 264, 4334, 295, 7318, 307, 257, 636, 294, 597, 35788, 7599, 382, 257, 1379, 393, 808, 281, 2316, 2564, 51144], "temperature": 0.0, "avg_logprob": -0.058718848872829126, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.02367572858929634}, {"id": 423, "seek": 216258, "start": 2178.18, "end": 2184.1, "text": " and understand its own processes through that self-modeling and to use that model as a way to", "tokens": [51144, 293, 1223, 1080, 1065, 7555, 807, 300, 2698, 12, 8014, 11031, 293, 281, 764, 300, 2316, 382, 257, 636, 281, 51440], "temperature": 0.0, "avg_logprob": -0.058718848872829126, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.02367572858929634}, {"id": 424, "seek": 216258, "start": 2184.1, "end": 2189.2999999999997, "text": " act back upon the world and act back upon itself towards a long-term, more viable form of", "tokens": [51440, 605, 646, 3564, 264, 1002, 293, 605, 646, 3564, 2564, 3030, 257, 938, 12, 7039, 11, 544, 22024, 1254, 295, 51700], "temperature": 0.0, "avg_logprob": -0.058718848872829126, "compression_ratio": 1.7596899224806202, "no_speech_prob": 0.02367572858929634}, {"id": 425, "seek": 218930, "start": 2189.3, "end": 2195.0600000000004, "text": " planetarity, how planetary systems can thrive in a form of, you know, not homeostasis, but", "tokens": [50364, 5054, 17409, 11, 577, 35788, 3652, 393, 21233, 294, 257, 1254, 295, 11, 291, 458, 11, 406, 1280, 555, 26632, 11, 457, 50652], "temperature": 0.0, "avg_logprob": -0.09948847028944227, "compression_ratio": 1.8532818532818534, "no_speech_prob": 0.06748484075069427}, {"id": 426, "seek": 218930, "start": 2195.0600000000004, "end": 2201.54, "text": " heterostasis for the long term, then taking this very peculiar slice of possible relevant information,", "tokens": [50652, 20789, 555, 26632, 337, 264, 938, 1433, 11, 550, 1940, 341, 588, 27149, 13153, 295, 1944, 7340, 1589, 11, 50976], "temperature": 0.0, "avg_logprob": -0.09948847028944227, "compression_ratio": 1.8532818532818534, "no_speech_prob": 0.06748484075069427}, {"id": 427, "seek": 218930, "start": 2201.54, "end": 2206.1000000000004, "text": " which is not only the information that is spoken in English, but even more weird when you think", "tokens": [50976, 597, 307, 406, 787, 264, 1589, 300, 307, 10759, 294, 3669, 11, 457, 754, 544, 3657, 562, 291, 519, 51204], "temperature": 0.0, "avg_logprob": -0.09948847028944227, "compression_ratio": 1.8532818532818534, "no_speech_prob": 0.06748484075069427}, {"id": 428, "seek": 218930, "start": 2206.1000000000004, "end": 2212.42, "text": " about it, the information that is generated by individual human users, as if individual human", "tokens": [51204, 466, 309, 11, 264, 1589, 300, 307, 10833, 538, 2609, 1952, 5022, 11, 382, 498, 2609, 1952, 51520], "temperature": 0.0, "avg_logprob": -0.09948847028944227, "compression_ratio": 1.8532818532818534, "no_speech_prob": 0.06748484075069427}, {"id": 429, "seek": 218930, "start": 2212.42, "end": 2216.98, "text": " users are really like the most relevant thing to be worried about or to be talking about here as", "tokens": [51520, 5022, 366, 534, 411, 264, 881, 7340, 551, 281, 312, 5804, 466, 420, 281, 312, 1417, 466, 510, 382, 51748], "temperature": 0.0, "avg_logprob": -0.09948847028944227, "compression_ratio": 1.8532818532818534, "no_speech_prob": 0.06748484075069427}, {"id": 430, "seek": 221698, "start": 2216.98, "end": 2221.78, "text": " well. Like, what did you do today for lunch? Like, where do you want to go for Chinese food", "tokens": [50364, 731, 13, 1743, 11, 437, 630, 291, 360, 965, 337, 6349, 30, 1743, 11, 689, 360, 291, 528, 281, 352, 337, 4649, 1755, 50604], "temperature": 0.0, "avg_logprob": -0.08304696924546186, "compression_ratio": 1.630281690140845, "no_speech_prob": 0.03617115691304207}, {"id": 431, "seek": 221698, "start": 2221.78, "end": 2228.42, "text": " in Milwaukee? What did you buy on Amazon last night? It's an incredible anthropocentrism,", "tokens": [50604, 294, 35321, 30, 708, 630, 291, 2256, 322, 6795, 1036, 1818, 30, 467, 311, 364, 4651, 22727, 905, 317, 81, 1434, 11, 50936], "temperature": 0.0, "avg_logprob": -0.08304696924546186, "compression_ratio": 1.630281690140845, "no_speech_prob": 0.03617115691304207}, {"id": 432, "seek": 221698, "start": 2228.42, "end": 2235.62, "text": " like a mind-boggling form of anthropocentrism that obviously I'm the thing, me, I'm the thing", "tokens": [50936, 411, 257, 1575, 12, 65, 36754, 1688, 1254, 295, 22727, 905, 317, 81, 1434, 300, 2745, 286, 478, 264, 551, 11, 385, 11, 286, 478, 264, 551, 51296], "temperature": 0.0, "avg_logprob": -0.08304696924546186, "compression_ratio": 1.630281690140845, "no_speech_prob": 0.03617115691304207}, {"id": 433, "seek": 221698, "start": 2235.62, "end": 2240.02, "text": " that machine intelligence is really interested in. And the most important political issue here", "tokens": [51296, 300, 3479, 7599, 307, 534, 3102, 294, 13, 400, 264, 881, 1021, 3905, 2734, 510, 51516], "temperature": 0.0, "avg_logprob": -0.08304696924546186, "compression_ratio": 1.630281690140845, "no_speech_prob": 0.03617115691304207}, {"id": 434, "seek": 221698, "start": 2240.02, "end": 2244.98, "text": " is how I can counter-weaponize my own privacy and sovereignty in relationship to this thing,", "tokens": [51516, 307, 577, 286, 393, 5682, 12, 826, 21319, 1125, 452, 1065, 11427, 293, 27862, 294, 2480, 281, 341, 551, 11, 51764], "temperature": 0.0, "avg_logprob": -0.08304696924546186, "compression_ratio": 1.630281690140845, "no_speech_prob": 0.03617115691304207}, {"id": 435, "seek": 224498, "start": 2244.98, "end": 2249.78, "text": " because it's in a way curtailing my own freedom. It's an incredible sense of sort of self as", "tokens": [50364, 570, 309, 311, 294, 257, 636, 1262, 14430, 278, 452, 1065, 5645, 13, 467, 311, 364, 4651, 2020, 295, 1333, 295, 2698, 382, 50604], "temperature": 0.0, "avg_logprob": -0.10127384431900517, "compression_ratio": 1.6332179930795847, "no_speech_prob": 0.0028000224847346544}, {"id": 436, "seek": 224498, "start": 2249.78, "end": 2255.7, "text": " Vitruvian man with all forms of the world just sort of radiating outside of you. And it kind of,", "tokens": [50604, 22463, 894, 85, 952, 587, 365, 439, 6422, 295, 264, 1002, 445, 1333, 295, 16335, 990, 2380, 295, 291, 13, 400, 309, 733, 295, 11, 50900], "temperature": 0.0, "avg_logprob": -0.10127384431900517, "compression_ratio": 1.6332179930795847, "no_speech_prob": 0.0028000224847346544}, {"id": 437, "seek": 224498, "start": 2255.7, "end": 2261.06, "text": " it drove me nuts during the era where everyone was reading Shoshana Zuboff and thought that this was", "tokens": [50900, 309, 13226, 385, 10483, 1830, 264, 4249, 689, 1518, 390, 3760, 1160, 3019, 2095, 1176, 836, 4506, 293, 1194, 300, 341, 390, 51168], "temperature": 0.0, "avg_logprob": -0.10127384431900517, "compression_ratio": 1.6332179930795847, "no_speech_prob": 0.0028000224847346544}, {"id": 438, "seek": 224498, "start": 2261.06, "end": 2266.1, "text": " the solution to anything. But I think it's even worse when we're thinking about the long-term", "tokens": [51168, 264, 3827, 281, 1340, 13, 583, 286, 519, 309, 311, 754, 5324, 562, 321, 434, 1953, 466, 264, 938, 12, 7039, 51420], "temperature": 0.0, "avg_logprob": -0.10127384431900517, "compression_ratio": 1.6332179930795847, "no_speech_prob": 0.0028000224847346544}, {"id": 439, "seek": 224498, "start": 2266.1, "end": 2270.42, "text": " training of AI models. It should not all be English. It doesn't need to all be English.", "tokens": [51420, 3097, 295, 7318, 5245, 13, 467, 820, 406, 439, 312, 3669, 13, 467, 1177, 380, 643, 281, 439, 312, 3669, 13, 51636], "temperature": 0.0, "avg_logprob": -0.10127384431900517, "compression_ratio": 1.6332179930795847, "no_speech_prob": 0.0028000224847346544}, {"id": 440, "seek": 227042, "start": 2270.42, "end": 2276.34, "text": " And it's not going to all be English. That's an extraordinarily limiting form of bias in terms of", "tokens": [50364, 400, 309, 311, 406, 516, 281, 439, 312, 3669, 13, 663, 311, 364, 34557, 22083, 1254, 295, 12577, 294, 2115, 295, 50660], "temperature": 0.0, "avg_logprob": -0.07789302755285192, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.4220760762691498}, {"id": 441, "seek": 227042, "start": 2276.34, "end": 2280.98, "text": " the kinds of things that we all need to know about each other and about how thought works and about", "tokens": [50660, 264, 3685, 295, 721, 300, 321, 439, 643, 281, 458, 466, 1184, 661, 293, 466, 577, 1194, 1985, 293, 466, 50892], "temperature": 0.0, "avg_logprob": -0.07789302755285192, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.4220760762691498}, {"id": 442, "seek": 227042, "start": 2280.98, "end": 2285.86, "text": " the range of possible ways of thinking and acting and knowing the world within it. But it's also", "tokens": [50892, 264, 3613, 295, 1944, 2098, 295, 1953, 293, 6577, 293, 5276, 264, 1002, 1951, 309, 13, 583, 309, 311, 611, 51136], "temperature": 0.0, "avg_logprob": -0.07789302755285192, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.4220760762691498}, {"id": 443, "seek": 227042, "start": 2285.86, "end": 2291.86, "text": " equally bizarre to think about that it's basically only human information or even individual human", "tokens": [51136, 12309, 18265, 281, 519, 466, 300, 309, 311, 1936, 787, 1952, 1589, 420, 754, 2609, 1952, 51436], "temperature": 0.0, "avg_logprob": -0.07789302755285192, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.4220760762691498}, {"id": 444, "seek": 227042, "start": 2291.86, "end": 2295.46, "text": " information is more likely. And again, going back to the book I wrote on the pandemic,", "tokens": [51436, 1589, 307, 544, 3700, 13, 400, 797, 11, 516, 646, 281, 264, 1446, 286, 4114, 322, 264, 5388, 11, 51616], "temperature": 0.0, "avg_logprob": -0.07789302755285192, "compression_ratio": 1.7266187050359711, "no_speech_prob": 0.4220760762691498}, {"id": 445, "seek": 229546, "start": 2295.46, "end": 2300.34, "text": " the relevant information during a pandemic was epidemiological, which was that, you know,", "tokens": [50364, 264, 7340, 1589, 1830, 257, 5388, 390, 35761, 4383, 11, 597, 390, 300, 11, 291, 458, 11, 50608], "temperature": 0.0, "avg_logprob": -0.0693895045448752, "compression_ratio": 1.910299003322259, "no_speech_prob": 0.05829566344618797}, {"id": 446, "seek": 229546, "start": 2300.34, "end": 2305.2200000000003, "text": " it wasn't about like what I did or what you did individually. That just doesn't matter. What", "tokens": [50608, 309, 2067, 380, 466, 411, 437, 286, 630, 420, 437, 291, 630, 16652, 13, 663, 445, 1177, 380, 1871, 13, 708, 50852], "temperature": 0.0, "avg_logprob": -0.0693895045448752, "compression_ratio": 1.910299003322259, "no_speech_prob": 0.05829566344618797}, {"id": 447, "seek": 229546, "start": 2305.2200000000003, "end": 2311.3, "text": " matters is the flow of a virus through the social body as a whole. What's important was the vector", "tokens": [50852, 7001, 307, 264, 3095, 295, 257, 5752, 807, 264, 2093, 1772, 382, 257, 1379, 13, 708, 311, 1021, 390, 264, 8062, 51156], "temperature": 0.0, "avg_logprob": -0.0693895045448752, "compression_ratio": 1.910299003322259, "no_speech_prob": 0.05829566344618797}, {"id": 448, "seek": 229546, "start": 2311.3, "end": 2316.02, "text": " of the movement, not the identity of the nodes. And I think both the way in which a lot of the", "tokens": [51156, 295, 264, 3963, 11, 406, 264, 6575, 295, 264, 13891, 13, 400, 286, 519, 1293, 264, 636, 294, 597, 257, 688, 295, 264, 51392], "temperature": 0.0, "avg_logprob": -0.0693895045448752, "compression_ratio": 1.910299003322259, "no_speech_prob": 0.05829566344618797}, {"id": 449, "seek": 229546, "start": 2316.02, "end": 2319.86, "text": " systems were set up that were focused on identity of the nodes and a lot of the way in which the", "tokens": [51392, 3652, 645, 992, 493, 300, 645, 5178, 322, 6575, 295, 264, 13891, 293, 257, 688, 295, 264, 636, 294, 597, 264, 51584], "temperature": 0.0, "avg_logprob": -0.0693895045448752, "compression_ratio": 1.910299003322259, "no_speech_prob": 0.05829566344618797}, {"id": 450, "seek": 229546, "start": 2319.86, "end": 2324.1, "text": " critique of the systems was focused on protecting identity of the nodes is kind of missing the point.", "tokens": [51584, 25673, 295, 264, 3652, 390, 5178, 322, 12316, 6575, 295, 264, 13891, 307, 733, 295, 5361, 264, 935, 13, 51796], "temperature": 0.0, "avg_logprob": -0.0693895045448752, "compression_ratio": 1.910299003322259, "no_speech_prob": 0.05829566344618797}, {"id": 451, "seek": 232410, "start": 2324.66, "end": 2328.9, "text": " The point is that, and this goes a little bit of like the way in which large models work in general", "tokens": [50392, 440, 935, 307, 300, 11, 293, 341, 1709, 257, 707, 857, 295, 411, 264, 636, 294, 597, 2416, 5245, 589, 294, 2674, 50604], "temperature": 0.0, "avg_logprob": -0.10045100287567797, "compression_ratio": 1.9965156794425087, "no_speech_prob": 0.0011333703296259046}, {"id": 452, "seek": 232410, "start": 2328.9, "end": 2334.1, "text": " about what weights really mean. I think within large models, the training of large models,", "tokens": [50604, 466, 437, 17443, 534, 914, 13, 286, 519, 1951, 2416, 5245, 11, 264, 3097, 295, 2416, 5245, 11, 50864], "temperature": 0.0, "avg_logprob": -0.10045100287567797, "compression_ratio": 1.9965156794425087, "no_speech_prob": 0.0011333703296259046}, {"id": 453, "seek": 232410, "start": 2334.1, "end": 2337.7799999999997, "text": " I think people still think of it a little bit weird and I'll get to the point in a second with", "tokens": [50864, 286, 519, 561, 920, 519, 295, 309, 257, 707, 857, 3657, 293, 286, 603, 483, 281, 264, 935, 294, 257, 1150, 365, 51048], "temperature": 0.0, "avg_logprob": -0.10045100287567797, "compression_ratio": 1.9965156794425087, "no_speech_prob": 0.0011333703296259046}, {"id": 454, "seek": 232410, "start": 2337.7799999999997, "end": 2343.14, "text": " this. The training of large models is a form of the artificialization of collective intelligence.", "tokens": [51048, 341, 13, 440, 3097, 295, 2416, 5245, 307, 257, 1254, 295, 264, 11677, 2144, 295, 12590, 7599, 13, 51316], "temperature": 0.0, "avg_logprob": -0.10045100287567797, "compression_ratio": 1.9965156794425087, "no_speech_prob": 0.0011333703296259046}, {"id": 455, "seek": 232410, "start": 2343.14, "end": 2347.62, "text": " And, you know, there may be things to be say about is it good or bad that a private corporation is", "tokens": [51316, 400, 11, 291, 458, 11, 456, 815, 312, 721, 281, 312, 584, 466, 307, 309, 665, 420, 1578, 300, 257, 4551, 22197, 307, 51540], "temperature": 0.0, "avg_logprob": -0.10045100287567797, "compression_ratio": 1.9965156794425087, "no_speech_prob": 0.0011333703296259046}, {"id": 456, "seek": 232410, "start": 2347.62, "end": 2351.2999999999997, "text": " doing this or good or bad that a nation state is doing this or good or bad that, you know,", "tokens": [51540, 884, 341, 420, 665, 420, 1578, 300, 257, 4790, 1785, 307, 884, 341, 420, 665, 420, 1578, 300, 11, 291, 458, 11, 51724], "temperature": 0.0, "avg_logprob": -0.10045100287567797, "compression_ratio": 1.9965156794425087, "no_speech_prob": 0.0011333703296259046}, {"id": 457, "seek": 235130, "start": 2351.3, "end": 2356.82, "text": " my neighbor co-op is doing this or whatever. But still what's going on, regardless of who's", "tokens": [50364, 452, 5987, 598, 12, 404, 307, 884, 341, 420, 2035, 13, 583, 920, 437, 311, 516, 322, 11, 10060, 295, 567, 311, 50640], "temperature": 0.0, "avg_logprob": -0.10088258040578742, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.001867348444648087}, {"id": 458, "seek": 235130, "start": 2356.82, "end": 2362.02, "text": " doing it, is an aggregate artificialization of collective intelligence. And so the similarities", "tokens": [50640, 884, 309, 11, 307, 364, 26118, 11677, 2144, 295, 12590, 7599, 13, 400, 370, 264, 24197, 50900], "temperature": 0.0, "avg_logprob": -0.10088258040578742, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.001867348444648087}, {"id": 459, "seek": 235130, "start": 2362.02, "end": 2367.54, "text": " and differences in the way in which people and things think and act in the world comes to", "tokens": [50900, 293, 7300, 294, 264, 636, 294, 597, 561, 293, 721, 519, 293, 605, 294, 264, 1002, 1487, 281, 51176], "temperature": 0.0, "avg_logprob": -0.10088258040578742, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.001867348444648087}, {"id": 460, "seek": 235130, "start": 2367.54, "end": 2373.46, "text": " constitute, if you zoom out far enough, a kind of, you know, it forms patterns, the patterns that", "tokens": [51176, 41658, 11, 498, 291, 8863, 484, 1400, 1547, 11, 257, 733, 295, 11, 291, 458, 11, 309, 6422, 8294, 11, 264, 8294, 300, 51472], "temperature": 0.0, "avg_logprob": -0.10088258040578742, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.001867348444648087}, {"id": 461, "seek": 235130, "start": 2373.46, "end": 2377.46, "text": " are really the context in which, you know, all of us are thinking, thinking and acting. That's an", "tokens": [51472, 366, 534, 264, 4319, 294, 597, 11, 291, 458, 11, 439, 295, 505, 366, 1953, 11, 1953, 293, 6577, 13, 663, 311, 364, 51672], "temperature": 0.0, "avg_logprob": -0.10088258040578742, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.001867348444648087}, {"id": 462, "seek": 237746, "start": 2377.46, "end": 2381.54, "text": " aggregation. That's an aggregation of collective intelligence. So, you know, just sort of cut you", "tokens": [50364, 16743, 399, 13, 663, 311, 364, 16743, 399, 295, 12590, 7599, 13, 407, 11, 291, 458, 11, 445, 1333, 295, 1723, 291, 50568], "temperature": 0.0, "avg_logprob": -0.09835850870287097, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.03730151429772377}, {"id": 463, "seek": 237746, "start": 2381.54, "end": 2385.94, "text": " the chase. I think that one of the ways of dealing with the bias problem within this as well is we", "tokens": [50568, 264, 15359, 13, 286, 519, 300, 472, 295, 264, 2098, 295, 6260, 365, 264, 12577, 1154, 1951, 341, 382, 731, 307, 321, 50788], "temperature": 0.0, "avg_logprob": -0.09835850870287097, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.03730151429772377}, {"id": 464, "seek": 237746, "start": 2385.94, "end": 2391.06, "text": " got to put everything in. We got to put everything in there, right? The kind of, let's say, we're", "tokens": [50788, 658, 281, 829, 1203, 294, 13, 492, 658, 281, 829, 1203, 294, 456, 11, 558, 30, 440, 733, 295, 11, 718, 311, 584, 11, 321, 434, 51044], "temperature": 0.0, "avg_logprob": -0.09835850870287097, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.03730151429772377}, {"id": 465, "seek": 237746, "start": 2391.06, "end": 2396.1, "text": " sort of mid 2010s sort of ideas like, oh, large models are really bad because they have all this", "tokens": [51044, 1333, 295, 2062, 9657, 82, 1333, 295, 3487, 411, 11, 1954, 11, 2416, 5245, 366, 534, 1578, 570, 436, 362, 439, 341, 51296], "temperature": 0.0, "avg_logprob": -0.09835850870287097, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.03730151429772377}, {"id": 466, "seek": 237746, "start": 2396.1, "end": 2401.2200000000003, "text": " toxic data in it. And what we really need is for me and my friends to make you a clean curated", "tokens": [51296, 12786, 1412, 294, 309, 13, 400, 437, 321, 534, 643, 307, 337, 385, 293, 452, 1855, 281, 652, 291, 257, 2541, 47851, 51552], "temperature": 0.0, "avg_logprob": -0.09835850870287097, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.03730151429772377}, {"id": 467, "seek": 237746, "start": 2401.2200000000003, "end": 2405.94, "text": " model with no bad think in it and everything will be fine. No one really says that anymore,", "tokens": [51552, 2316, 365, 572, 1578, 519, 294, 309, 293, 1203, 486, 312, 2489, 13, 883, 472, 534, 1619, 300, 3602, 11, 51788], "temperature": 0.0, "avg_logprob": -0.09835850870287097, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.03730151429772377}, {"id": 468, "seek": 240594, "start": 2405.94, "end": 2410.1, "text": " but that's what people were really saying at the time. The other thing is, I think it's important", "tokens": [50364, 457, 300, 311, 437, 561, 645, 534, 1566, 412, 264, 565, 13, 440, 661, 551, 307, 11, 286, 519, 309, 311, 1021, 50572], "temperature": 0.0, "avg_logprob": -0.06746529888462376, "compression_ratio": 2.006825938566553, "no_speech_prob": 0.001410054275766015}, {"id": 469, "seek": 240594, "start": 2410.1, "end": 2415.38, "text": " to understand what the toxic data issue is that if you want to have a model that doesn't do something", "tokens": [50572, 281, 1223, 437, 264, 12786, 1412, 2734, 307, 300, 498, 291, 528, 281, 362, 257, 2316, 300, 1177, 380, 360, 746, 50836], "temperature": 0.0, "avg_logprob": -0.06746529888462376, "compression_ratio": 2.006825938566553, "no_speech_prob": 0.001410054275766015}, {"id": 470, "seek": 240594, "start": 2415.38, "end": 2420.34, "text": " that you don't want it to do, like, you know, say bad things or think bad things or do bad things,", "tokens": [50836, 300, 291, 500, 380, 528, 309, 281, 360, 11, 411, 11, 291, 458, 11, 584, 1578, 721, 420, 519, 1578, 721, 420, 360, 1578, 721, 11, 51084], "temperature": 0.0, "avg_logprob": -0.06746529888462376, "compression_ratio": 2.006825938566553, "no_speech_prob": 0.001410054275766015}, {"id": 471, "seek": 240594, "start": 2420.34, "end": 2424.18, "text": " you need to give it examples of what those bad things are for it to actually know what you're", "tokens": [51084, 291, 643, 281, 976, 309, 5110, 295, 437, 729, 1578, 721, 366, 337, 309, 281, 767, 458, 437, 291, 434, 51276], "temperature": 0.0, "avg_logprob": -0.06746529888462376, "compression_ratio": 2.006825938566553, "no_speech_prob": 0.001410054275766015}, {"id": 472, "seek": 240594, "start": 2424.18, "end": 2428.42, "text": " talking about. Because if you take all of those bad things out of the training data, it's much,", "tokens": [51276, 1417, 466, 13, 1436, 498, 291, 747, 439, 295, 729, 1578, 721, 484, 295, 264, 3097, 1412, 11, 309, 311, 709, 11, 51488], "temperature": 0.0, "avg_logprob": -0.06746529888462376, "compression_ratio": 2.006825938566553, "no_speech_prob": 0.001410054275766015}, {"id": 473, "seek": 240594, "start": 2428.42, "end": 2433.54, "text": " much, much, much easier to make the model do those bad things because you haven't told it not to do", "tokens": [51488, 709, 11, 709, 11, 709, 3571, 281, 652, 264, 2316, 360, 729, 1578, 721, 570, 291, 2378, 380, 1907, 309, 406, 281, 360, 51744], "temperature": 0.0, "avg_logprob": -0.06746529888462376, "compression_ratio": 2.006825938566553, "no_speech_prob": 0.001410054275766015}, {"id": 474, "seek": 243354, "start": 2433.54, "end": 2438.34, "text": " those things. That's how you get Tay. And so in order to make the models actually more functional,", "tokens": [50364, 729, 721, 13, 663, 311, 577, 291, 483, 10132, 13, 400, 370, 294, 1668, 281, 652, 264, 5245, 767, 544, 11745, 11, 50604], "temperature": 0.0, "avg_logprob": -0.10361607433998421, "compression_ratio": 1.9218241042345277, "no_speech_prob": 0.0330718457698822}, {"id": 475, "seek": 243354, "start": 2438.34, "end": 2442.66, "text": " you need to give it a lot of examples of things that you that you may not want in this way. I'll", "tokens": [50604, 291, 643, 281, 976, 309, 257, 688, 295, 5110, 295, 721, 300, 291, 300, 291, 815, 406, 528, 294, 341, 636, 13, 286, 603, 50820], "temperature": 0.0, "avg_logprob": -0.10361607433998421, "compression_ratio": 1.9218241042345277, "no_speech_prob": 0.0330718457698822}, {"id": 476, "seek": 243354, "start": 2442.66, "end": 2448.1, "text": " also just sort of end on this last thing is that what is the problem of bias within models is it's", "tokens": [50820, 611, 445, 1333, 295, 917, 322, 341, 1036, 551, 307, 300, 437, 307, 264, 1154, 295, 12577, 1951, 5245, 307, 309, 311, 51092], "temperature": 0.0, "avg_logprob": -0.10361607433998421, "compression_ratio": 1.9218241042345277, "no_speech_prob": 0.0330718457698822}, {"id": 477, "seek": 243354, "start": 2448.1, "end": 2453.3, "text": " actually a really good example of what alignment really means. I'm kind of critical of the idea of", "tokens": [51092, 767, 257, 534, 665, 1365, 295, 437, 18515, 534, 1355, 13, 286, 478, 733, 295, 4924, 295, 264, 1558, 295, 51352], "temperature": 0.0, "avg_logprob": -0.10361607433998421, "compression_ratio": 1.9218241042345277, "no_speech_prob": 0.0330718457698822}, {"id": 478, "seek": 243354, "start": 2453.3, "end": 2457.22, "text": " alignment, not not in the narrow sense of like, when I say open the garage door, I actually wanted", "tokens": [51352, 18515, 11, 406, 406, 294, 264, 9432, 2020, 295, 411, 11, 562, 286, 584, 1269, 264, 14400, 2853, 11, 286, 767, 1415, 51548], "temperature": 0.0, "avg_logprob": -0.10361607433998421, "compression_ratio": 1.9218241042345277, "no_speech_prob": 0.0330718457698822}, {"id": 479, "seek": 243354, "start": 2457.22, "end": 2461.62, "text": " the garage door to open. That's alignment. That's fine. But alignment is like the real purpose of", "tokens": [51548, 264, 14400, 2853, 281, 1269, 13, 663, 311, 18515, 13, 663, 311, 2489, 13, 583, 18515, 307, 411, 264, 957, 4334, 295, 51768], "temperature": 0.0, "avg_logprob": -0.10361607433998421, "compression_ratio": 1.9218241042345277, "no_speech_prob": 0.0330718457698822}, {"id": 480, "seek": 246162, "start": 2461.62, "end": 2466.98, "text": " long term ways of thinking about the role of AI is that it should be as much as AI can be human", "tokens": [50364, 938, 1433, 2098, 295, 1953, 466, 264, 3090, 295, 7318, 307, 300, 309, 820, 312, 382, 709, 382, 7318, 393, 312, 1952, 50632], "temperature": 0.0, "avg_logprob": -0.06867227219698722, "compression_ratio": 1.8549019607843138, "no_speech_prob": 0.013632298447191715}, {"id": 481, "seek": 246162, "start": 2466.98, "end": 2471.94, "text": " like, human centered, a reflection of human cultural norms, a reflection of human values,", "tokens": [50632, 411, 11, 1952, 18988, 11, 257, 12914, 295, 1952, 6988, 24357, 11, 257, 12914, 295, 1952, 4190, 11, 50880], "temperature": 0.0, "avg_logprob": -0.06867227219698722, "compression_ratio": 1.8549019607843138, "no_speech_prob": 0.013632298447191715}, {"id": 482, "seek": 246162, "start": 2471.94, "end": 2476.42, "text": " a reflection of human desires, a reflection of human psychology, the better. I think this is", "tokens": [50880, 257, 12914, 295, 1952, 18005, 11, 257, 12914, 295, 1952, 15105, 11, 264, 1101, 13, 286, 519, 341, 307, 51104], "temperature": 0.0, "avg_logprob": -0.06867227219698722, "compression_ratio": 1.8549019607843138, "no_speech_prob": 0.013632298447191715}, {"id": 483, "seek": 246162, "start": 2476.42, "end": 2481.22, "text": " insane. Even the most cursory look at human history suggests that's the last thing you want to do", "tokens": [51104, 10838, 13, 2754, 264, 881, 13946, 827, 574, 412, 1952, 2503, 13409, 300, 311, 264, 1036, 551, 291, 528, 281, 360, 51344], "temperature": 0.0, "avg_logprob": -0.06867227219698722, "compression_ratio": 1.8549019607843138, "no_speech_prob": 0.013632298447191715}, {"id": 484, "seek": 246162, "start": 2481.22, "end": 2487.38, "text": " to basically supercharge whatever humans are. It's not a misanthropic statement. I'm just saying", "tokens": [51344, 281, 1936, 1687, 13604, 2035, 6255, 366, 13, 467, 311, 406, 257, 3346, 282, 14222, 299, 5629, 13, 286, 478, 445, 1566, 51652], "temperature": 0.0, "avg_logprob": -0.06867227219698722, "compression_ratio": 1.8549019607843138, "no_speech_prob": 0.013632298447191715}, {"id": 485, "seek": 248738, "start": 2487.38, "end": 2494.02, "text": " it's naive as a meta heuristic bias in models, racism in models, sexism in models,", "tokens": [50364, 309, 311, 29052, 382, 257, 19616, 415, 374, 3142, 12577, 294, 5245, 11, 12664, 294, 5245, 11, 3260, 1434, 294, 5245, 11, 50696], "temperature": 0.0, "avg_logprob": -0.13203218442584397, "compression_ratio": 1.8373015873015872, "no_speech_prob": 0.2390093058347702}, {"id": 486, "seek": 248738, "start": 2494.02, "end": 2499.7000000000003, "text": " bomb making in models. These are, this is what alignment looks like. The reason that you have", "tokens": [50696, 7851, 1455, 294, 5245, 13, 1981, 366, 11, 341, 307, 437, 18515, 1542, 411, 13, 440, 1778, 300, 291, 362, 50980], "temperature": 0.0, "avg_logprob": -0.13203218442584397, "compression_ratio": 1.8373015873015872, "no_speech_prob": 0.2390093058347702}, {"id": 487, "seek": 248738, "start": 2499.7000000000003, "end": 2505.2200000000003, "text": " models that are reflecting the history of structural racism in society is because those", "tokens": [50980, 5245, 300, 366, 23543, 264, 2503, 295, 15067, 12664, 294, 4086, 307, 570, 729, 51256], "temperature": 0.0, "avg_logprob": -0.13203218442584397, "compression_ratio": 1.8373015873015872, "no_speech_prob": 0.2390093058347702}, {"id": 488, "seek": 248738, "start": 2505.2200000000003, "end": 2510.98, "text": " models are well aligned, not because those models are not well aligned. That's the important point", "tokens": [51256, 5245, 366, 731, 17962, 11, 406, 570, 729, 5245, 366, 406, 731, 17962, 13, 663, 311, 264, 1021, 935, 51544], "temperature": 0.0, "avg_logprob": -0.13203218442584397, "compression_ratio": 1.8373015873015872, "no_speech_prob": 0.2390093058347702}, {"id": 489, "seek": 248738, "start": 2510.98, "end": 2514.82, "text": " to sort of understand here. The other thing I was going to name with maybe it's just something I've", "tokens": [51544, 281, 1333, 295, 1223, 510, 13, 440, 661, 551, 286, 390, 516, 281, 1315, 365, 1310, 309, 311, 445, 746, 286, 600, 51736], "temperature": 0.0, "avg_logprob": -0.13203218442584397, "compression_ratio": 1.8373015873015872, "no_speech_prob": 0.2390093058347702}, {"id": 490, "seek": 251482, "start": 2514.82, "end": 2518.1000000000004, "text": " been thinking about a lot last couple of days because I had a conversation a couple of days", "tokens": [50364, 668, 1953, 466, 257, 688, 1036, 1916, 295, 1708, 570, 286, 632, 257, 3761, 257, 1916, 295, 1708, 50528], "temperature": 0.0, "avg_logprob": -0.07815892046148126, "compression_ratio": 1.832797427652733, "no_speech_prob": 0.01589415967464447}, {"id": 491, "seek": 251482, "start": 2518.1000000000004, "end": 2522.98, "text": " ago with a quite well known European artist who will remain nameless, someone who's written quite", "tokens": [50528, 2057, 365, 257, 1596, 731, 2570, 6473, 5748, 567, 486, 6222, 8835, 4272, 11, 1580, 567, 311, 3720, 1596, 50772], "temperature": 0.0, "avg_logprob": -0.07815892046148126, "compression_ratio": 1.832797427652733, "no_speech_prob": 0.01589415967464447}, {"id": 492, "seek": 251482, "start": 2522.98, "end": 2530.1800000000003, "text": " a lot on the topic of the role of generative AI and what it really means to be an artist whose", "tokens": [50772, 257, 688, 322, 264, 4829, 295, 264, 3090, 295, 1337, 1166, 7318, 293, 437, 309, 534, 1355, 281, 312, 364, 5748, 6104, 51132], "temperature": 0.0, "avg_logprob": -0.07815892046148126, "compression_ratio": 1.832797427652733, "no_speech_prob": 0.01589415967464447}, {"id": 493, "seek": 251482, "start": 2530.1800000000003, "end": 2534.7400000000002, "text": " work is part of the training data and have their identity reflected in the rest of this as well.", "tokens": [51132, 589, 307, 644, 295, 264, 3097, 1412, 293, 362, 641, 6575, 15502, 294, 264, 1472, 295, 341, 382, 731, 13, 51360], "temperature": 0.0, "avg_logprob": -0.07815892046148126, "compression_ratio": 1.832797427652733, "no_speech_prob": 0.01589415967464447}, {"id": 494, "seek": 251482, "start": 2534.7400000000002, "end": 2538.9, "text": " And has published quite a bit on it and I think has a lot of interesting things to say about it.", "tokens": [51360, 400, 575, 6572, 1596, 257, 857, 322, 309, 293, 286, 519, 575, 257, 688, 295, 1880, 721, 281, 584, 466, 309, 13, 51568], "temperature": 0.0, "avg_logprob": -0.07815892046148126, "compression_ratio": 1.832797427652733, "no_speech_prob": 0.01589415967464447}, {"id": 495, "seek": 251482, "start": 2538.9, "end": 2542.7400000000002, "text": " But one of the things that became clear to me halfway through our conversation is that this", "tokens": [51568, 583, 472, 295, 264, 721, 300, 3062, 1850, 281, 385, 15461, 807, 527, 3761, 307, 300, 341, 51760], "temperature": 0.0, "avg_logprob": -0.07815892046148126, "compression_ratio": 1.832797427652733, "no_speech_prob": 0.01589415967464447}, {"id": 496, "seek": 254274, "start": 2542.74, "end": 2548.2599999999998, "text": " person, they thought that their artwork was actually in the model that like, okay, here's", "tokens": [50364, 954, 11, 436, 1194, 300, 641, 15829, 390, 767, 294, 264, 2316, 300, 411, 11, 1392, 11, 510, 311, 50640], "temperature": 0.0, "avg_logprob": -0.10939519506105234, "compression_ratio": 1.919732441471572, "no_speech_prob": 0.020938973873853683}, {"id": 497, "seek": 254274, "start": 2548.2599999999998, "end": 2553.22, "text": " all my paintings and all the videos I've made and all the rest of the stuff that these are as such as", "tokens": [50640, 439, 452, 14880, 293, 439, 264, 2145, 286, 600, 1027, 293, 439, 264, 1472, 295, 264, 1507, 300, 613, 366, 382, 1270, 382, 50888], "temperature": 0.0, "avg_logprob": -0.10939519506105234, "compression_ratio": 1.919732441471572, "no_speech_prob": 0.020938973873853683}, {"id": 498, "seek": 254274, "start": 2553.22, "end": 2558.2599999999998, "text": " artifacts in the model. And so when someone types in, I want to make a thing that, you know,", "tokens": [50888, 24617, 294, 264, 2316, 13, 400, 370, 562, 1580, 3467, 294, 11, 286, 528, 281, 652, 257, 551, 300, 11, 291, 458, 11, 51140], "temperature": 0.0, "avg_logprob": -0.10939519506105234, "compression_ratio": 1.919732441471572, "no_speech_prob": 0.020938973873853683}, {"id": 499, "seek": 254274, "start": 2558.2599999999998, "end": 2562.74, "text": " looks like this or this or that it almost like a database lookup. It would go look up that item in", "tokens": [51140, 1542, 411, 341, 420, 341, 420, 300, 309, 1920, 411, 257, 8149, 574, 1010, 13, 467, 576, 352, 574, 493, 300, 3174, 294, 51364], "temperature": 0.0, "avg_logprob": -0.10939519506105234, "compression_ratio": 1.919732441471572, "no_speech_prob": 0.020938973873853683}, {"id": 500, "seek": 254274, "start": 2562.74, "end": 2566.8999999999996, "text": " the database and then make something based on that thing. And I tried to explain it like, no,", "tokens": [51364, 264, 8149, 293, 550, 652, 746, 2361, 322, 300, 551, 13, 400, 286, 3031, 281, 2903, 309, 411, 11, 572, 11, 51572], "temperature": 0.0, "avg_logprob": -0.10939519506105234, "compression_ratio": 1.919732441471572, "no_speech_prob": 0.020938973873853683}, {"id": 501, "seek": 254274, "start": 2566.8999999999996, "end": 2571.9399999999996, "text": " it's actually not, it works that the training data and the information in the model are actually", "tokens": [51572, 309, 311, 767, 406, 11, 309, 1985, 300, 264, 3097, 1412, 293, 264, 1589, 294, 264, 2316, 366, 767, 51824], "temperature": 0.0, "avg_logprob": -0.10939519506105234, "compression_ratio": 1.919732441471572, "no_speech_prob": 0.020938973873853683}, {"id": 502, "seek": 257194, "start": 2571.94, "end": 2577.86, "text": " totally different kinds of things. And that your artifact is not sitting there like your profile", "tokens": [50364, 3879, 819, 3685, 295, 721, 13, 400, 300, 428, 34806, 307, 406, 3798, 456, 411, 428, 7964, 50660], "temperature": 0.0, "avg_logprob": -0.1393907226785256, "compression_ratio": 1.9111842105263157, "no_speech_prob": 0.0017542921705171466}, {"id": 503, "seek": 257194, "start": 2577.86, "end": 2582.34, "text": " or your Google profile is not sitting there in the model waiting to be accessed in some sort of way.", "tokens": [50660, 420, 428, 3329, 7964, 307, 406, 3798, 456, 294, 264, 2316, 3806, 281, 312, 34211, 294, 512, 1333, 295, 636, 13, 50884], "temperature": 0.0, "avg_logprob": -0.1393907226785256, "compression_ratio": 1.9111842105263157, "no_speech_prob": 0.0017542921705171466}, {"id": 504, "seek": 257194, "start": 2582.34, "end": 2586.34, "text": " And it kind of is like, it's the way a lot of the discussion of this is going. It's almost as if", "tokens": [50884, 400, 309, 733, 295, 307, 411, 11, 309, 311, 264, 636, 257, 688, 295, 264, 5017, 295, 341, 307, 516, 13, 467, 311, 1920, 382, 498, 51084], "temperature": 0.0, "avg_logprob": -0.1393907226785256, "compression_ratio": 1.9111842105263157, "no_speech_prob": 0.0017542921705171466}, {"id": 505, "seek": 257194, "start": 2586.34, "end": 2590.26, "text": " people presume, and this is, you know, after years of Google, it makes sense that people", "tokens": [51084, 561, 43283, 11, 293, 341, 307, 11, 291, 458, 11, 934, 924, 295, 3329, 11, 309, 1669, 2020, 300, 561, 51280], "temperature": 0.0, "avg_logprob": -0.1393907226785256, "compression_ratio": 1.9111842105263157, "no_speech_prob": 0.0017542921705171466}, {"id": 506, "seek": 257194, "start": 2590.26, "end": 2594.82, "text": " presume that there's like this giant repository of everything that human made that the AI is going", "tokens": [51280, 43283, 300, 456, 311, 411, 341, 7410, 25841, 295, 1203, 300, 1952, 1027, 300, 264, 7318, 307, 516, 51508], "temperature": 0.0, "avg_logprob": -0.1393907226785256, "compression_ratio": 1.9111842105263157, "no_speech_prob": 0.0017542921705171466}, {"id": 507, "seek": 257194, "start": 2594.82, "end": 2600.1, "text": " through and picking from to take examples of. And the point I was making with this is that the way", "tokens": [51508, 807, 293, 8867, 490, 281, 747, 5110, 295, 13, 400, 264, 935, 286, 390, 1455, 365, 341, 307, 300, 264, 636, 51772], "temperature": 0.0, "avg_logprob": -0.1393907226785256, "compression_ratio": 1.9111842105263157, "no_speech_prob": 0.0017542921705171466}, {"id": 508, "seek": 260010, "start": 2600.1, "end": 2604.9, "text": " to think about AI as a form of the collectivization of planetary intelligence, the importance of", "tokens": [50364, 281, 519, 466, 7318, 382, 257, 1254, 295, 264, 2500, 592, 2144, 295, 35788, 7599, 11, 264, 7379, 295, 50604], "temperature": 0.0, "avg_logprob": -0.10546510409464878, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.004068950191140175}, {"id": 509, "seek": 260010, "start": 2604.9, "end": 2610.74, "text": " understanding the way in which this is generating topological models in the forms of differential", "tokens": [50604, 3701, 264, 636, 294, 597, 341, 307, 17746, 1192, 4383, 5245, 294, 264, 6422, 295, 15756, 50896], "temperature": 0.0, "avg_logprob": -0.10546510409464878, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.004068950191140175}, {"id": 510, "seek": 260010, "start": 2610.74, "end": 2616.98, "text": " embeddings in weights. It is something that is so intrinsically collectivized, right? It's so", "tokens": [50896, 12240, 29432, 294, 17443, 13, 467, 307, 746, 300, 307, 370, 28621, 984, 2500, 592, 1602, 11, 558, 30, 467, 311, 370, 51208], "temperature": 0.0, "avg_logprob": -0.10546510409464878, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.004068950191140175}, {"id": 511, "seek": 260010, "start": 2616.98, "end": 2623.86, "text": " intrinsically collectivized that to look at it and say, yeah, but my personal thing that I personally", "tokens": [51208, 28621, 984, 2500, 592, 1602, 300, 281, 574, 412, 309, 293, 584, 11, 1338, 11, 457, 452, 2973, 551, 300, 286, 5665, 51552], "temperature": 0.0, "avg_logprob": -0.10546510409464878, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.004068950191140175}, {"id": 512, "seek": 260010, "start": 2623.86, "end": 2628.98, "text": " made has been taken to make this thing. Like there may be, there's like somewhere deep in here,", "tokens": [51552, 1027, 575, 668, 2726, 281, 652, 341, 551, 13, 1743, 456, 815, 312, 11, 456, 311, 411, 4079, 2452, 294, 510, 11, 51808], "temperature": 0.0, "avg_logprob": -0.10546510409464878, "compression_ratio": 1.8202247191011236, "no_speech_prob": 0.004068950191140175}, {"id": 513, "seek": 262898, "start": 2629.86, "end": 2633.78, "text": " at the fourth decimal point, something is different because you're participating in it.", "tokens": [50408, 412, 264, 6409, 26601, 935, 11, 746, 307, 819, 570, 291, 434, 13950, 294, 309, 13, 50604], "temperature": 0.0, "avg_logprob": -0.07697819440792768, "compression_ratio": 1.9513888888888888, "no_speech_prob": 0.00036826077848672867}, {"id": 514, "seek": 262898, "start": 2633.78, "end": 2638.1, "text": " This is relevant because it's actually, you know, you see yourself in one way or another,", "tokens": [50604, 639, 307, 7340, 570, 309, 311, 767, 11, 291, 458, 11, 291, 536, 1803, 294, 472, 636, 420, 1071, 11, 50820], "temperature": 0.0, "avg_logprob": -0.07697819440792768, "compression_ratio": 1.9513888888888888, "no_speech_prob": 0.00036826077848672867}, {"id": 515, "seek": 262898, "start": 2638.1, "end": 2642.5, "text": " but this presumption of one's own sense of individual intelligence and its relationship", "tokens": [50820, 457, 341, 18028, 1695, 295, 472, 311, 1065, 2020, 295, 2609, 7599, 293, 1080, 2480, 51040], "temperature": 0.0, "avg_logprob": -0.07697819440792768, "compression_ratio": 1.9513888888888888, "no_speech_prob": 0.00036826077848672867}, {"id": 516, "seek": 262898, "start": 2642.5, "end": 2648.02, "text": " to collective intelligence is broken, at least in the West. And therefore our relationship to", "tokens": [51040, 281, 12590, 7599, 307, 5463, 11, 412, 1935, 294, 264, 4055, 13, 400, 4412, 527, 2480, 281, 51316], "temperature": 0.0, "avg_logprob": -0.07697819440792768, "compression_ratio": 1.9513888888888888, "no_speech_prob": 0.00036826077848672867}, {"id": 517, "seek": 262898, "start": 2648.02, "end": 2652.18, "text": " how the artificialization of individual intelligence and the artificialization of collective intelligence", "tokens": [51316, 577, 264, 11677, 2144, 295, 2609, 7599, 293, 264, 11677, 2144, 295, 12590, 7599, 51524], "temperature": 0.0, "avg_logprob": -0.07697819440792768, "compression_ratio": 1.9513888888888888, "no_speech_prob": 0.00036826077848672867}, {"id": 518, "seek": 262898, "start": 2652.18, "end": 2658.1, "text": " would play out, it's not surprising that people would see it in this particular way. And this is", "tokens": [51524, 576, 862, 484, 11, 309, 311, 406, 8830, 300, 561, 576, 536, 309, 294, 341, 1729, 636, 13, 400, 341, 307, 51820], "temperature": 0.0, "avg_logprob": -0.07697819440792768, "compression_ratio": 1.9513888888888888, "no_speech_prob": 0.00036826077848672867}, {"id": 519, "seek": 265810, "start": 2658.1, "end": 2662.74, "text": " a little bit what I mean about provincializing the West, about provincializing the Western", "tokens": [50364, 257, 707, 857, 437, 286, 914, 466, 33293, 3319, 264, 4055, 11, 466, 33293, 3319, 264, 8724, 50596], "temperature": 0.0, "avg_logprob": -0.08923641204833985, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.00023049440642353147}, {"id": 520, "seek": 265810, "start": 2662.74, "end": 2667.94, "text": " theories around AI and how important it is to do that in order to get to the point where", "tokens": [50596, 13667, 926, 7318, 293, 577, 1021, 309, 307, 281, 360, 300, 294, 1668, 281, 483, 281, 264, 935, 689, 50856], "temperature": 0.0, "avg_logprob": -0.08923641204833985, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.00023049440642353147}, {"id": 521, "seek": 265810, "start": 2667.94, "end": 2671.86, "text": " much more planetary discourse and compositional project around AI is even possible.", "tokens": [50856, 709, 544, 35788, 23938, 293, 10199, 2628, 1716, 926, 7318, 307, 754, 1944, 13, 51052], "temperature": 0.0, "avg_logprob": -0.08923641204833985, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.00023049440642353147}, {"id": 522, "seek": 265810, "start": 2673.22, "end": 2676.74, "text": " So maybe going a little further in picking apart the problem of the individual,", "tokens": [51120, 407, 1310, 516, 257, 707, 3052, 294, 8867, 4936, 264, 1154, 295, 264, 2609, 11, 51296], "temperature": 0.0, "avg_logprob": -0.08923641204833985, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.00023049440642353147}, {"id": 523, "seek": 265810, "start": 2678.58, "end": 2682.98, "text": " I'd love to get to kind of the is Benjamin Bratton an anti-humanist type of question.", "tokens": [51388, 286, 1116, 959, 281, 483, 281, 733, 295, 264, 307, 22231, 1603, 1591, 266, 364, 6061, 12, 18796, 468, 2010, 295, 1168, 13, 51608], "temperature": 0.0, "avg_logprob": -0.08923641204833985, "compression_ratio": 1.7023809523809523, "no_speech_prob": 0.00023049440642353147}, {"id": 524, "seek": 268298, "start": 2683.62, "end": 2688.7400000000002, "text": " So in the stack, that user layer that you described, I mean it's pretty intense to", "tokens": [50396, 407, 294, 264, 8630, 11, 300, 4195, 4583, 300, 291, 7619, 11, 286, 914, 309, 311, 1238, 9447, 281, 50652], "temperature": 0.0, "avg_logprob": -0.09190580674580165, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0803099051117897}, {"id": 525, "seek": 268298, "start": 2688.7400000000002, "end": 2694.58, "text": " read as a subject, right? The user, which is the scale that's approaching our individual", "tokens": [50652, 1401, 382, 257, 3983, 11, 558, 30, 440, 4195, 11, 597, 307, 264, 4373, 300, 311, 14908, 527, 2609, 50944], "temperature": 0.0, "avg_logprob": -0.09190580674580165, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0803099051117897}, {"id": 526, "seek": 268298, "start": 2694.58, "end": 2700.9, "text": " scales as humans, this user is in the process of what you call liquefaction. They're being", "tokens": [50944, 17408, 382, 6255, 11, 341, 4195, 307, 294, 264, 1399, 295, 437, 291, 818, 375, 1077, 69, 2894, 13, 814, 434, 885, 51260], "temperature": 0.0, "avg_logprob": -0.09190580674580165, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0803099051117897}, {"id": 527, "seek": 268298, "start": 2700.9, "end": 2705.46, "text": " quantized in terms of data, they're constantly subordinated to all these forces on scales that", "tokens": [51260, 4426, 1602, 294, 2115, 295, 1412, 11, 436, 434, 6460, 1422, 765, 5410, 281, 439, 613, 5874, 322, 17408, 300, 51488], "temperature": 0.0, "avg_logprob": -0.09190580674580165, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0803099051117897}, {"id": 528, "seek": 268298, "start": 2705.46, "end": 2711.62, "text": " are inaccessible to them. And you continue this process of the deprioritization of the individual", "tokens": [51488, 366, 33230, 780, 964, 281, 552, 13, 400, 291, 2354, 341, 1399, 295, 264, 27095, 50017, 2144, 295, 264, 2609, 51796], "temperature": 0.0, "avg_logprob": -0.09190580674580165, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0803099051117897}, {"id": 529, "seek": 271162, "start": 2711.62, "end": 2717.8599999999997, "text": " or the critique of individual agency in wider social and political spaces, as you mentioned", "tokens": [50364, 420, 264, 25673, 295, 2609, 7934, 294, 11842, 2093, 293, 3905, 7673, 11, 382, 291, 2835, 50676], "temperature": 0.0, "avg_logprob": -0.11163225291687766, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.00027798485825769603}, {"id": 530, "seek": 271162, "start": 2717.8599999999997, "end": 2724.2599999999998, "text": " just now, in Revenge of the Real. So I'm wondering, what is actually available to us as agents,", "tokens": [50676, 445, 586, 11, 294, 1300, 46112, 295, 264, 8467, 13, 407, 286, 478, 6359, 11, 437, 307, 767, 2435, 281, 505, 382, 12554, 11, 50996], "temperature": 0.0, "avg_logprob": -0.11163225291687766, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.00027798485825769603}, {"id": 531, "seek": 271162, "start": 2724.9, "end": 2729.62, "text": " especially in the regime of planetary scale computation, these massive inflection points", "tokens": [51028, 2318, 294, 264, 13120, 295, 35788, 4373, 24903, 11, 613, 5994, 1536, 5450, 2793, 51264], "temperature": 0.0, "avg_logprob": -0.11163225291687766, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.00027798485825769603}, {"id": 532, "seek": 271162, "start": 2729.62, "end": 2735.54, "text": " in technology? Are we just idle observers of path dependencies that are flowing through us?", "tokens": [51264, 294, 2899, 30, 2014, 321, 445, 30650, 48090, 295, 3100, 36606, 300, 366, 13974, 807, 505, 30, 51560], "temperature": 0.0, "avg_logprob": -0.11163225291687766, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.00027798485825769603}, {"id": 533, "seek": 273554, "start": 2736.42, "end": 2742.82, "text": " And I guess to probe even further, is the subject or the subject position or subjectivity,", "tokens": [50408, 400, 286, 2041, 281, 22715, 754, 3052, 11, 307, 264, 3983, 420, 264, 3983, 2535, 420, 3983, 4253, 11, 50728], "temperature": 0.0, "avg_logprob": -0.1232850426121762, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.05662408098578453}, {"id": 534, "seek": 273554, "start": 2742.82, "end": 2748.02, "text": " subjective experience, is this a relevant framework anymore or just a piece of legacy", "tokens": [50728, 25972, 1752, 11, 307, 341, 257, 7340, 8388, 3602, 420, 445, 257, 2522, 295, 11711, 50988], "temperature": 0.0, "avg_logprob": -0.1232850426121762, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.05662408098578453}, {"id": 535, "seek": 273554, "start": 2748.02, "end": 2753.62, "text": " 18th century tech? All right, there's a lot of questions in here. I'll do my best. I think I'll", "tokens": [50988, 2443, 392, 4901, 7553, 30, 1057, 558, 11, 456, 311, 257, 688, 295, 1651, 294, 510, 13, 286, 603, 360, 452, 1151, 13, 286, 519, 286, 603, 51268], "temperature": 0.0, "avg_logprob": -0.1232850426121762, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.05662408098578453}, {"id": 536, "seek": 273554, "start": 2753.62, "end": 2759.46, "text": " be comfortable characterizing my work as non-humanist, but not anti-human. I think the", "tokens": [51268, 312, 4619, 2517, 3319, 452, 589, 382, 2107, 12, 18796, 468, 11, 457, 406, 6061, 12, 18796, 13, 286, 519, 264, 51560], "temperature": 0.0, "avg_logprob": -0.1232850426121762, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.05662408098578453}, {"id": 537, "seek": 273554, "start": 2759.46, "end": 2764.58, "text": " distinction between humanism and humans is an important one to make, at the very least, right?", "tokens": [51560, 16844, 1296, 1952, 1434, 293, 6255, 307, 364, 1021, 472, 281, 652, 11, 412, 264, 588, 1935, 11, 558, 30, 51816], "temperature": 0.0, "avg_logprob": -0.1232850426121762, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.05662408098578453}, {"id": 538, "seek": 276458, "start": 2764.58, "end": 2769.94, "text": " I think humanism in its, let's say, now traditional guises, for reasons that we're all", "tokens": [50364, 286, 519, 1952, 1434, 294, 1080, 11, 718, 311, 584, 11, 586, 5164, 695, 3598, 11, 337, 4112, 300, 321, 434, 439, 50632], "temperature": 0.0, "avg_logprob": -0.09257258866962634, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.000185202676220797}, {"id": 539, "seek": 276458, "start": 2769.94, "end": 2774.5, "text": " probably well aware, has a lot of different problems associated with it, including problems", "tokens": [50632, 1391, 731, 3650, 11, 575, 257, 688, 295, 819, 2740, 6615, 365, 309, 11, 3009, 2740, 50860], "temperature": 0.0, "avg_logprob": -0.09257258866962634, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.000185202676220797}, {"id": 540, "seek": 276458, "start": 2774.5, "end": 2780.2599999999998, "text": " that have unfortunately been shuttled along into post-humanism in a lot of different ways,", "tokens": [50860, 300, 362, 7015, 668, 5309, 83, 1493, 2051, 666, 2183, 12, 18796, 1434, 294, 257, 688, 295, 819, 2098, 11, 51148], "temperature": 0.0, "avg_logprob": -0.09257258866962634, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.000185202676220797}, {"id": 541, "seek": 276458, "start": 2780.2599999999998, "end": 2786.2599999999998, "text": " which I find in many respects, in its present guise, is a kind of inadequate sentimentalization", "tokens": [51148, 597, 286, 915, 294, 867, 24126, 11, 294, 1080, 1974, 695, 908, 11, 307, 257, 733, 295, 42107, 42823, 2144, 51448], "temperature": 0.0, "avg_logprob": -0.09257258866962634, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.000185202676220797}, {"id": 542, "seek": 276458, "start": 2786.2599999999998, "end": 2792.18, "text": " of the eclipse of the humanist subject. But it's not anti-human in any sort of sense at all.", "tokens": [51448, 295, 264, 35722, 295, 264, 1952, 468, 3983, 13, 583, 309, 311, 406, 6061, 12, 18796, 294, 604, 1333, 295, 2020, 412, 439, 13, 51744], "temperature": 0.0, "avg_logprob": -0.09257258866962634, "compression_ratio": 1.6900369003690037, "no_speech_prob": 0.000185202676220797}, {"id": 543, "seek": 279218, "start": 2792.8999999999996, "end": 2798.8199999999997, "text": " Humans, if we just really zoom out a little bit and think of humans as this precocious primate", "tokens": [50400, 35809, 11, 498, 321, 445, 534, 8863, 484, 257, 707, 857, 293, 519, 295, 6255, 382, 341, 659, 1291, 4139, 2886, 473, 50696], "temperature": 0.0, "avg_logprob": -0.08444051742553711, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.003592386143282056}, {"id": 544, "seek": 279218, "start": 2798.8199999999997, "end": 2803.94, "text": " that's been around for a few million years and a couple hundred thousand in its present form and", "tokens": [50696, 300, 311, 668, 926, 337, 257, 1326, 2459, 924, 293, 257, 1916, 3262, 4714, 294, 1080, 1974, 1254, 293, 50952], "temperature": 0.0, "avg_logprob": -0.08444051742553711, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.003592386143282056}, {"id": 545, "seek": 279218, "start": 2803.94, "end": 2809.54, "text": " capable of producing amazing feats of symbolic construction and communication over the past", "tokens": [50952, 8189, 295, 10501, 2243, 579, 1720, 295, 25755, 6435, 293, 6101, 670, 264, 1791, 51232], "temperature": 0.0, "avg_logprob": -0.08444051742553711, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.003592386143282056}, {"id": 546, "seek": 279218, "start": 2809.54, "end": 2816.5, "text": " tens of thousands of years and has, for better or worse, largely transformed its host planet in its", "tokens": [51232, 10688, 295, 5383, 295, 924, 293, 575, 11, 337, 1101, 420, 5324, 11, 11611, 16894, 1080, 3975, 5054, 294, 1080, 51580], "temperature": 0.0, "avg_logprob": -0.08444051742553711, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.003592386143282056}, {"id": 547, "seek": 281650, "start": 2816.5, "end": 2822.58, "text": " image, humans are remarkable. But it did all of these things not because it meant to. It wasn't", "tokens": [50364, 3256, 11, 6255, 366, 12802, 13, 583, 309, 630, 439, 295, 613, 721, 406, 570, 309, 4140, 281, 13, 467, 2067, 380, 50668], "temperature": 0.0, "avg_logprob": -0.10386899270509418, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.28081128001213074}, {"id": 548, "seek": 281650, "start": 2822.58, "end": 2827.46, "text": " like that somehow, you know, that homo habilis said, right, got it. First, I make a rock,", "tokens": [50668, 411, 300, 6063, 11, 291, 458, 11, 300, 3655, 78, 36565, 271, 848, 11, 558, 11, 658, 309, 13, 2386, 11, 286, 652, 257, 3727, 11, 50912], "temperature": 0.0, "avg_logprob": -0.10386899270509418, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.28081128001213074}, {"id": 549, "seek": 281650, "start": 2827.46, "end": 2830.58, "text": " you know, then I'm going to do this. And then, you know, Napoleon's going to invade Russia in", "tokens": [50912, 291, 458, 11, 550, 286, 478, 516, 281, 360, 341, 13, 400, 550, 11, 291, 458, 11, 31694, 311, 516, 281, 39171, 6797, 294, 51068], "temperature": 0.0, "avg_logprob": -0.10386899270509418, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.28081128001213074}, {"id": 550, "seek": 281650, "start": 2830.58, "end": 2834.02, "text": " the winter. And then we're going to have American Idol. And then we're, you know,", "tokens": [51068, 264, 6355, 13, 400, 550, 321, 434, 516, 281, 362, 2665, 33266, 13, 400, 550, 321, 434, 11, 291, 458, 11, 51240], "temperature": 0.0, "avg_logprob": -0.10386899270509418, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.28081128001213074}, {"id": 551, "seek": 281650, "start": 2834.02, "end": 2838.74, "text": " it's like, this is not how, this is not how history works, but it's not how evolution works.", "tokens": [51240, 309, 311, 411, 11, 341, 307, 406, 577, 11, 341, 307, 406, 577, 2503, 1985, 11, 457, 309, 311, 406, 577, 9303, 1985, 13, 51476], "temperature": 0.0, "avg_logprob": -0.10386899270509418, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.28081128001213074}, {"id": 552, "seek": 281650, "start": 2838.74, "end": 2843.62, "text": " What we're all concerned about is anthropogenic agency. What was called the Anthropocene is about", "tokens": [51476, 708, 321, 434, 439, 5922, 466, 307, 22727, 25473, 7934, 13, 708, 390, 1219, 264, 12727, 1513, 905, 1450, 307, 466, 51720], "temperature": 0.0, "avg_logprob": -0.10386899270509418, "compression_ratio": 1.8278145695364238, "no_speech_prob": 0.28081128001213074}, {"id": 553, "seek": 284362, "start": 2843.62, "end": 2847.7, "text": " understanding the cumulative effects of anthropogenic agency. But when Darwinian", "tokens": [50364, 3701, 264, 38379, 5065, 295, 22727, 25473, 7934, 13, 583, 562, 30233, 952, 50568], "temperature": 0.0, "avg_logprob": -0.06629424734213918, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.013629738241434097}, {"id": 554, "seek": 284362, "start": 2847.7, "end": 2852.9, "text": " evolution became paradigmatic in the late 19th century, this was a kind of Copernican shift", "tokens": [50568, 9303, 3062, 24709, 2399, 294, 264, 3469, 1294, 392, 4901, 11, 341, 390, 257, 733, 295, 11579, 1248, 8914, 5513, 50828], "temperature": 0.0, "avg_logprob": -0.06629424734213918, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.013629738241434097}, {"id": 555, "seek": 284362, "start": 2852.9, "end": 2859.22, "text": " moment when humans realized like, oh, our own history is actually a history that is floating", "tokens": [50828, 1623, 562, 6255, 5334, 411, 11, 1954, 11, 527, 1065, 2503, 307, 767, 257, 2503, 300, 307, 12607, 51144], "temperature": 0.0, "avg_logprob": -0.06629424734213918, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.013629738241434097}, {"id": 556, "seek": 284362, "start": 2859.22, "end": 2863.7799999999997, "text": " on top of much bigger and deeper histories that are geologic histories, that are biological", "tokens": [51144, 322, 1192, 295, 709, 3801, 293, 7731, 30631, 300, 366, 1519, 36661, 30631, 11, 300, 366, 13910, 51372], "temperature": 0.0, "avg_logprob": -0.06629424734213918, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.013629738241434097}, {"id": 557, "seek": 284362, "start": 2863.7799999999997, "end": 2869.54, "text": " histories, that are evolutionary histories, that are histories of how it is that complex societies", "tokens": [51372, 30631, 11, 300, 366, 27567, 30631, 11, 300, 366, 30631, 295, 577, 309, 307, 300, 3997, 19329, 51660], "temperature": 0.0, "avg_logprob": -0.06629424734213918, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.013629738241434097}, {"id": 558, "seek": 286954, "start": 2869.54, "end": 2875.3, "text": " rise and fall. And that, you know, the amount of mastery and control that we have over this is", "tokens": [50364, 6272, 293, 2100, 13, 400, 300, 11, 291, 458, 11, 264, 2372, 295, 37951, 293, 1969, 300, 321, 362, 670, 341, 307, 50652], "temperature": 0.0, "avg_logprob": -0.12412699583534882, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.3271546959877014}, {"id": 559, "seek": 286954, "start": 2875.3, "end": 2880.34, "text": " relatively limited. It's interesting then, I suppose, in a similar that you get this understanding", "tokens": [50652, 7226, 5567, 13, 467, 311, 1880, 550, 11, 286, 7297, 11, 294, 257, 2531, 300, 291, 483, 341, 3701, 50904], "temperature": 0.0, "avg_logprob": -0.12412699583534882, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.3271546959877014}, {"id": 560, "seek": 286954, "start": 2880.34, "end": 2885.3, "text": " of essentially the ways in which human societies and cultures are, as you say, sort of dependent", "tokens": [50904, 295, 4476, 264, 2098, 294, 597, 1952, 19329, 293, 12951, 366, 11, 382, 291, 584, 11, 1333, 295, 12334, 51152], "temperature": 0.0, "avg_logprob": -0.12412699583534882, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.3271546959877014}, {"id": 561, "seek": 286954, "start": 2885.3, "end": 2890.02, "text": " on forces outside of their control or the result of those forces, right? Evolution would suggest", "tokens": [51152, 322, 5874, 2380, 295, 641, 1969, 420, 264, 1874, 295, 729, 5874, 11, 558, 30, 40800, 576, 3402, 51388], "temperature": 0.0, "avg_logprob": -0.12412699583534882, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.3271546959877014}, {"id": 562, "seek": 286954, "start": 2890.02, "end": 2894.66, "text": " that human culture is just an expression of deeper dynamics and forces in and of itself.", "tokens": [51388, 300, 1952, 3713, 307, 445, 364, 6114, 295, 7731, 15679, 293, 5874, 294, 293, 295, 2564, 13, 51620], "temperature": 0.0, "avg_logprob": -0.12412699583534882, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.3271546959877014}, {"id": 563, "seek": 289466, "start": 2894.66, "end": 2899.46, "text": " At the exact same time, historically, when it also comes to realize that it has transformed", "tokens": [50364, 1711, 264, 1900, 912, 565, 11, 16180, 11, 562, 309, 611, 1487, 281, 4325, 300, 309, 575, 16894, 50604], "temperature": 0.0, "avg_logprob": -0.09550364382632144, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.177599236369133}, {"id": 564, "seek": 289466, "start": 2899.46, "end": 2904.8999999999996, "text": " the entire world in its image in the Anthropocene, both this sense of discovering of the scope of", "tokens": [50604, 264, 2302, 1002, 294, 1080, 3256, 294, 264, 12727, 1513, 905, 1450, 11, 1293, 341, 2020, 295, 24773, 295, 264, 11923, 295, 50876], "temperature": 0.0, "avg_logprob": -0.09550364382632144, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.177599236369133}, {"id": 565, "seek": 289466, "start": 2904.8999999999996, "end": 2910.1, "text": " its agency and discovering of the limitations of its agency, it discovers at the exact same time.", "tokens": [50876, 1080, 7934, 293, 24773, 295, 264, 15705, 295, 1080, 7934, 11, 309, 44522, 412, 264, 1900, 912, 565, 13, 51136], "temperature": 0.0, "avg_logprob": -0.09550364382632144, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.177599236369133}, {"id": 566, "seek": 289466, "start": 2910.1, "end": 2914.74, "text": " But I think the relationship here, I'll put it this way just to sort of not to bury the lead.", "tokens": [51136, 583, 286, 519, 264, 2480, 510, 11, 286, 603, 829, 309, 341, 636, 445, 281, 1333, 295, 406, 281, 28919, 264, 1477, 13, 51368], "temperature": 0.0, "avg_logprob": -0.09550364382632144, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.177599236369133}, {"id": 567, "seek": 289466, "start": 2914.74, "end": 2920.42, "text": " I think the problem that you're identifying here is the way in which in particularly,", "tokens": [51368, 286, 519, 264, 1154, 300, 291, 434, 16696, 510, 307, 264, 636, 294, 597, 294, 4098, 11, 51652], "temperature": 0.0, "avg_logprob": -0.09550364382632144, "compression_ratio": 1.803088803088803, "no_speech_prob": 0.177599236369133}, {"id": 568, "seek": 292042, "start": 2920.42, "end": 2924.9, "text": " I don't want to say in the West in art and design circles, you can slice it however you like,", "tokens": [50364, 286, 500, 380, 528, 281, 584, 294, 264, 4055, 294, 1523, 293, 1715, 13040, 11, 291, 393, 13153, 309, 4461, 291, 411, 11, 50588], "temperature": 0.0, "avg_logprob": -0.07702894378126714, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.00970504805445671}, {"id": 569, "seek": 292042, "start": 2924.9, "end": 2930.1800000000003, "text": " that there's a strong conflation of subjectivity, agency and identity as all basically being the", "tokens": [50588, 300, 456, 311, 257, 2068, 1497, 24278, 295, 3983, 4253, 11, 7934, 293, 6575, 382, 439, 1936, 885, 264, 50852], "temperature": 0.0, "avg_logprob": -0.07702894378126714, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.00970504805445671}, {"id": 570, "seek": 292042, "start": 2930.1800000000003, "end": 2936.82, "text": " same thing. And that the question of agency and how it is that I myself as the protagonist of", "tokens": [50852, 912, 551, 13, 400, 300, 264, 1168, 295, 7934, 293, 577, 309, 307, 300, 286, 2059, 382, 264, 24506, 295, 51184], "temperature": 0.0, "avg_logprob": -0.07702894378126714, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.00970504805445671}, {"id": 571, "seek": 292042, "start": 2936.82, "end": 2942.34, "text": " the world have agency to make change is not the same question of what is your experience of your", "tokens": [51184, 264, 1002, 362, 7934, 281, 652, 1319, 307, 406, 264, 912, 1168, 295, 437, 307, 428, 1752, 295, 428, 51460], "temperature": 0.0, "avg_logprob": -0.07702894378126714, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.00970504805445671}, {"id": 572, "seek": 292042, "start": 2942.34, "end": 2947.14, "text": " own subjectivity. It's not the same question of what is your sense of identity. Your question of", "tokens": [51460, 1065, 3983, 4253, 13, 467, 311, 406, 264, 912, 1168, 295, 437, 307, 428, 2020, 295, 6575, 13, 2260, 1168, 295, 51700], "temperature": 0.0, "avg_logprob": -0.07702894378126714, "compression_ratio": 1.8893280632411067, "no_speech_prob": 0.00970504805445671}, {"id": 573, "seek": 294714, "start": 2947.14, "end": 2951.94, "text": " identity is not necessarily the same question as a sense of agency. Subjectivity, agency,", "tokens": [50364, 6575, 307, 406, 4725, 264, 912, 1168, 382, 257, 2020, 295, 7934, 13, 8511, 1020, 4253, 11, 7934, 11, 50604], "temperature": 0.0, "avg_logprob": -0.10037447955157305, "compression_ratio": 1.7913385826771653, "no_speech_prob": 0.05028841644525528}, {"id": 574, "seek": 294714, "start": 2951.94, "end": 2955.54, "text": " and identity are actually really different kinds of things. And I think it's the conflation of", "tokens": [50604, 293, 6575, 366, 767, 534, 819, 3685, 295, 721, 13, 400, 286, 519, 309, 311, 264, 1497, 24278, 295, 50784], "temperature": 0.0, "avg_logprob": -0.10037447955157305, "compression_ratio": 1.7913385826771653, "no_speech_prob": 0.05028841644525528}, {"id": 575, "seek": 294714, "start": 2955.54, "end": 2960.58, "text": " these that's causing people a lot of headache. I think what you get result is a sense of", "tokens": [50784, 613, 300, 311, 9853, 561, 257, 688, 295, 23520, 13, 286, 519, 437, 291, 483, 1874, 307, 257, 2020, 295, 51036], "temperature": 0.0, "avg_logprob": -0.10037447955157305, "compression_ratio": 1.7913385826771653, "no_speech_prob": 0.05028841644525528}, {"id": 576, "seek": 294714, "start": 2960.58, "end": 2965.46, "text": " diminution of one. I don't have enough agency in the world implies that there needs to be an", "tokens": [51036, 15739, 1448, 295, 472, 13, 286, 500, 380, 362, 1547, 7934, 294, 264, 1002, 18779, 300, 456, 2203, 281, 312, 364, 51280], "temperature": 0.0, "avg_logprob": -0.10037447955157305, "compression_ratio": 1.7913385826771653, "no_speech_prob": 0.05028841644525528}, {"id": 577, "seek": 294714, "start": 2965.46, "end": 2970.5, "text": " inflation of my sense of subjectivity, or even worse, a sense of my experience of my own", "tokens": [51280, 15860, 295, 452, 2020, 295, 3983, 4253, 11, 420, 754, 5324, 11, 257, 2020, 295, 452, 1752, 295, 452, 1065, 51532], "temperature": 0.0, "avg_logprob": -0.10037447955157305, "compression_ratio": 1.7913385826771653, "no_speech_prob": 0.05028841644525528}, {"id": 578, "seek": 297050, "start": 2970.5, "end": 2977.38, "text": " subjectivity. Subjectivity and the aspect of one's own subjecthood becomes not just the path", "tokens": [50364, 3983, 4253, 13, 8511, 1020, 4253, 293, 264, 4171, 295, 472, 311, 1065, 3983, 3809, 3643, 406, 445, 264, 3100, 50708], "temperature": 0.0, "avg_logprob": -0.10027987616402763, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.2504268288612366}, {"id": 579, "seek": 297050, "start": 2977.38, "end": 2982.1, "text": " towards greater agency, it becomes both the form and the content of greater agency. I think this", "tokens": [50708, 3030, 5044, 7934, 11, 309, 3643, 1293, 264, 1254, 293, 264, 2701, 295, 5044, 7934, 13, 286, 519, 341, 50944], "temperature": 0.0, "avg_logprob": -0.10027987616402763, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.2504268288612366}, {"id": 580, "seek": 297050, "start": 2982.1, "end": 2987.3, "text": " is a bit of a close loop way of going about things to be clear. I also think that at least", "tokens": [50944, 307, 257, 857, 295, 257, 1998, 6367, 636, 295, 516, 466, 721, 281, 312, 1850, 13, 286, 611, 519, 300, 412, 1935, 51204], "temperature": 0.0, "avg_logprob": -0.10027987616402763, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.2504268288612366}, {"id": 581, "seek": 297050, "start": 2987.3, "end": 2990.66, "text": " historically it tends to sort of get things backwards. There's a way in which, you know,", "tokens": [51204, 16180, 309, 12258, 281, 1333, 295, 483, 721, 12204, 13, 821, 311, 257, 636, 294, 597, 11, 291, 458, 11, 51372], "temperature": 0.0, "avg_logprob": -0.10027987616402763, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.2504268288612366}, {"id": 582, "seek": 297050, "start": 2990.66, "end": 2994.5, "text": " I think we're all sort of familiar with this tendency in sort of general sense is the idea", "tokens": [51372, 286, 519, 321, 434, 439, 1333, 295, 4963, 365, 341, 18187, 294, 1333, 295, 2674, 2020, 307, 264, 1558, 51564], "temperature": 0.0, "avg_logprob": -0.10027987616402763, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.2504268288612366}, {"id": 583, "seek": 299450, "start": 2994.5, "end": 3001.38, "text": " of how it is that either I myself or we as the people or humans as the anthropogenic agent would", "tokens": [50364, 295, 577, 309, 307, 300, 2139, 286, 2059, 420, 321, 382, 264, 561, 420, 6255, 382, 264, 22727, 25473, 9461, 576, 50708], "temperature": 0.0, "avg_logprob": -0.08880539562391199, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.14395317435264587}, {"id": 584, "seek": 299450, "start": 3001.38, "end": 3008.42, "text": " be able to have more control over our own societies or our own lives over the way that the ecosystems", "tokens": [50708, 312, 1075, 281, 362, 544, 1969, 670, 527, 1065, 19329, 420, 527, 1065, 2909, 670, 264, 636, 300, 264, 32647, 51060], "temperature": 0.0, "avg_logprob": -0.08880539562391199, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.14395317435264587}, {"id": 585, "seek": 299450, "start": 3008.42, "end": 3015.3, "text": " work is if we need to first develop the subjectivity that would allow us to understand what's in front", "tokens": [51060, 589, 307, 498, 321, 643, 281, 700, 1499, 264, 3983, 4253, 300, 576, 2089, 505, 281, 1223, 437, 311, 294, 1868, 51404], "temperature": 0.0, "avg_logprob": -0.08880539562391199, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.14395317435264587}, {"id": 586, "seek": 299450, "start": 3015.3, "end": 3021.06, "text": " of us. And if we can cultivate the subjectivity properly to debate and to draw the fine points", "tokens": [51404, 295, 505, 13, 400, 498, 321, 393, 33341, 264, 3983, 4253, 6108, 281, 7958, 293, 281, 2642, 264, 2489, 2793, 51692], "temperature": 0.0, "avg_logprob": -0.08880539562391199, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.14395317435264587}, {"id": 587, "seek": 302106, "start": 3021.06, "end": 3025.22, "text": " of distinction between what are the proper political subjects and the improper political", "tokens": [50364, 295, 16844, 1296, 437, 366, 264, 2296, 3905, 13066, 293, 264, 40651, 3905, 50572], "temperature": 0.0, "avg_logprob": -0.12147804208703943, "compression_ratio": 2.0815047021943576, "no_speech_prob": 0.22782523930072784}, {"id": 588, "seek": 302106, "start": 3025.22, "end": 3030.42, "text": " subjects, the proper economic. So that once we calibrate subjectivity agency will flow from", "tokens": [50572, 13066, 11, 264, 2296, 4836, 13, 407, 300, 1564, 321, 21583, 4404, 3983, 4253, 7934, 486, 3095, 490, 50832], "temperature": 0.0, "avg_logprob": -0.12147804208703943, "compression_ratio": 2.0815047021943576, "no_speech_prob": 0.22782523930072784}, {"id": 589, "seek": 302106, "start": 3030.42, "end": 3034.82, "text": " there. I think one of the lessons of the Anthropocene is that actually may work the other way around.", "tokens": [50832, 456, 13, 286, 519, 472, 295, 264, 8820, 295, 264, 12727, 1513, 905, 1450, 307, 300, 767, 815, 589, 264, 661, 636, 926, 13, 51052], "temperature": 0.0, "avg_logprob": -0.12147804208703943, "compression_ratio": 2.0815047021943576, "no_speech_prob": 0.22782523930072784}, {"id": 590, "seek": 302106, "start": 3034.82, "end": 3038.34, "text": " One of the things that the Anthropocene is that then you call it whatever you like. I mean,", "tokens": [51052, 1485, 295, 264, 721, 300, 264, 12727, 1513, 905, 1450, 307, 300, 550, 291, 818, 309, 2035, 291, 411, 13, 286, 914, 11, 51228], "temperature": 0.0, "avg_logprob": -0.12147804208703943, "compression_ratio": 2.0815047021943576, "no_speech_prob": 0.22782523930072784}, {"id": 591, "seek": 302106, "start": 3038.34, "end": 3042.1, "text": " I think actually they're not just different words for the same thing. I think capital scene is just", "tokens": [51228, 286, 519, 767, 436, 434, 406, 445, 819, 2283, 337, 264, 912, 551, 13, 286, 519, 4238, 4145, 307, 445, 51416], "temperature": 0.0, "avg_logprob": -0.12147804208703943, "compression_ratio": 2.0815047021943576, "no_speech_prob": 0.22782523930072784}, {"id": 592, "seek": 302106, "start": 3042.1, "end": 3044.98, "text": " a different thing than the Anthropocene, just that they're actually talking about different", "tokens": [51416, 257, 819, 551, 813, 264, 12727, 1513, 905, 1450, 11, 445, 300, 436, 434, 767, 1417, 466, 819, 51560], "temperature": 0.0, "avg_logprob": -0.12147804208703943, "compression_ratio": 2.0815047021943576, "no_speech_prob": 0.22782523930072784}, {"id": 593, "seek": 302106, "start": 3044.98, "end": 3050.42, "text": " kinds of transformations is that whatever slice of humans or whatever actual sort of mega complex", "tokens": [51560, 3685, 295, 34852, 307, 300, 2035, 13153, 295, 6255, 420, 2035, 3539, 1333, 295, 17986, 3997, 51832], "temperature": 0.0, "avg_logprob": -0.12147804208703943, "compression_ratio": 2.0815047021943576, "no_speech_prob": 0.22782523930072784}, {"id": 594, "seek": 305042, "start": 3050.42, "end": 3055.54, "text": " of humans and technologies and microbes and other species that you want to identify as the", "tokens": [50364, 295, 6255, 293, 7943, 293, 35996, 293, 661, 6172, 300, 291, 528, 281, 5876, 382, 264, 50620], "temperature": 0.0, "avg_logprob": -0.10780854736055646, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00016345048788934946}, {"id": 595, "seek": 305042, "start": 3055.54, "end": 3061.94, "text": " Anthropocene complex, it had this agency to transform the world for centuries before it", "tokens": [50620, 12727, 1513, 905, 1450, 3997, 11, 309, 632, 341, 7934, 281, 4088, 264, 1002, 337, 13926, 949, 309, 50940], "temperature": 0.0, "avg_logprob": -0.10780854736055646, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00016345048788934946}, {"id": 596, "seek": 305042, "start": 3061.94, "end": 3067.78, "text": " understood that it was doing that. It had this world changing terraforming agency for centuries,", "tokens": [50940, 7320, 300, 309, 390, 884, 300, 13, 467, 632, 341, 1002, 4473, 26298, 48610, 7934, 337, 13926, 11, 51232], "temperature": 0.0, "avg_logprob": -0.10780854736055646, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00016345048788934946}, {"id": 597, "seek": 305042, "start": 3067.78, "end": 3073.14, "text": " but only in the 1960s and 70s really. And then, you know, to a certain extent with the birth of", "tokens": [51232, 457, 787, 294, 264, 16157, 82, 293, 5285, 82, 534, 13, 400, 550, 11, 291, 458, 11, 281, 257, 1629, 8396, 365, 264, 3965, 295, 51500], "temperature": 0.0, "avg_logprob": -0.10780854736055646, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00016345048788934946}, {"id": 598, "seek": 305042, "start": 3073.14, "end": 3076.82, "text": " climate science, but then really even in the early 2000s, it can occur to us just like, oh,", "tokens": [51500, 5659, 3497, 11, 457, 550, 534, 754, 294, 264, 2440, 8132, 82, 11, 309, 393, 5160, 281, 505, 445, 411, 11, 1954, 11, 51684], "temperature": 0.0, "avg_logprob": -0.10780854736055646, "compression_ratio": 1.7022058823529411, "no_speech_prob": 0.00016345048788934946}, {"id": 599, "seek": 307682, "start": 3077.38, "end": 3084.02, "text": " we have this agency, this terraforming agency that it is not negotiable. It is not something", "tokens": [50392, 321, 362, 341, 7934, 11, 341, 26298, 48610, 7934, 300, 309, 307, 406, 9542, 712, 13, 467, 307, 406, 746, 50724], "temperature": 0.0, "avg_logprob": -0.0802077540644893, "compression_ratio": 1.892116182572614, "no_speech_prob": 0.01281651295721531}, {"id": 600, "seek": 307682, "start": 3084.02, "end": 3090.82, "text": " that it's possible to renounce. And therefore we have to construct a subjectivity that is appropriate", "tokens": [50724, 300, 309, 311, 1944, 281, 8124, 7826, 13, 400, 4412, 321, 362, 281, 7690, 257, 3983, 4253, 300, 307, 6854, 51064], "temperature": 0.0, "avg_logprob": -0.0802077540644893, "compression_ratio": 1.892116182572614, "no_speech_prob": 0.01281651295721531}, {"id": 601, "seek": 307682, "start": 3090.82, "end": 3095.54, "text": " to the scale of this agency, which I think is the right way of going about it. But what this,", "tokens": [51064, 281, 264, 4373, 295, 341, 7934, 11, 597, 286, 519, 307, 264, 558, 636, 295, 516, 466, 309, 13, 583, 437, 341, 11, 51300], "temperature": 0.0, "avg_logprob": -0.0802077540644893, "compression_ratio": 1.892116182572614, "no_speech_prob": 0.01281651295721531}, {"id": 602, "seek": 307682, "start": 3095.54, "end": 3100.5800000000004, "text": " what this implies is that agency precedes subjectivity. The agency precedes the", "tokens": [51300, 437, 341, 18779, 307, 300, 7934, 16969, 279, 3983, 4253, 13, 440, 7934, 16969, 279, 264, 51552], "temperature": 0.0, "avg_logprob": -0.0802077540644893, "compression_ratio": 1.892116182572614, "no_speech_prob": 0.01281651295721531}, {"id": 603, "seek": 307682, "start": 3100.5800000000004, "end": 3105.6200000000003, "text": " subjectivity. The subjectivity is a way of retroactively understanding one's own agency", "tokens": [51552, 3983, 4253, 13, 440, 3983, 4253, 307, 257, 636, 295, 18820, 45679, 3701, 472, 311, 1065, 7934, 51804], "temperature": 0.0, "avg_logprob": -0.0802077540644893, "compression_ratio": 1.892116182572614, "no_speech_prob": 0.01281651295721531}, {"id": 604, "seek": 310562, "start": 3105.62, "end": 3109.8599999999997, "text": " sort of in the world. Obviously it can work the other way around, where people come to rethink", "tokens": [50364, 1333, 295, 294, 264, 1002, 13, 7580, 309, 393, 589, 264, 661, 636, 926, 11, 689, 561, 808, 281, 34595, 50576], "temperature": 0.0, "avg_logprob": -0.1126256057194301, "compression_ratio": 1.8575949367088607, "no_speech_prob": 0.0017541457200422883}, {"id": 605, "seek": 310562, "start": 3109.8599999999997, "end": 3114.2599999999998, "text": " of themselves and their subject positions. And this is another linguistification of the world.", "tokens": [50576, 295, 2969, 293, 641, 3983, 8432, 13, 400, 341, 307, 1071, 21766, 468, 3774, 295, 264, 1002, 13, 50796], "temperature": 0.0, "avg_logprob": -0.1126256057194301, "compression_ratio": 1.8575949367088607, "no_speech_prob": 0.0017541457200422883}, {"id": 606, "seek": 310562, "start": 3114.2599999999998, "end": 3120.58, "text": " Like reality is a big sentence and I'm the first person singular or first person collective subject", "tokens": [50796, 1743, 4103, 307, 257, 955, 8174, 293, 286, 478, 264, 700, 954, 20010, 420, 700, 954, 12590, 3983, 51112], "temperature": 0.0, "avg_logprob": -0.1126256057194301, "compression_ratio": 1.8575949367088607, "no_speech_prob": 0.0017541457200422883}, {"id": 607, "seek": 310562, "start": 3120.58, "end": 3125.2999999999997, "text": " of the sentence. And now what's the verb. But I think we need to be equally attentive to subjectivity", "tokens": [51112, 295, 264, 8174, 13, 400, 586, 437, 311, 264, 9595, 13, 583, 286, 519, 321, 643, 281, 312, 12309, 43661, 281, 3983, 4253, 51348], "temperature": 0.0, "avg_logprob": -0.1126256057194301, "compression_ratio": 1.8575949367088607, "no_speech_prob": 0.0017541457200422883}, {"id": 608, "seek": 310562, "start": 3125.2999999999997, "end": 3129.8599999999997, "text": " as the result of the agency as well. But I think to sort of the gist of your point, I think that", "tokens": [51348, 382, 264, 1874, 295, 264, 7934, 382, 731, 13, 583, 286, 519, 281, 1333, 295, 264, 290, 468, 295, 428, 935, 11, 286, 519, 300, 51576], "temperature": 0.0, "avg_logprob": -0.1126256057194301, "compression_ratio": 1.8575949367088607, "no_speech_prob": 0.0017541457200422883}, {"id": 609, "seek": 310562, "start": 3129.8599999999997, "end": 3134.98, "text": " there at this particular moment, I think part of the ways in which, and I just wrote a piece about", "tokens": [51576, 456, 412, 341, 1729, 1623, 11, 286, 519, 644, 295, 264, 2098, 294, 597, 11, 293, 286, 445, 4114, 257, 2522, 466, 51832], "temperature": 0.0, "avg_logprob": -0.1126256057194301, "compression_ratio": 1.8575949367088607, "no_speech_prob": 0.0017541457200422883}, {"id": 610, "seek": 313498, "start": 3134.98, "end": 3142.58, "text": " this for Tank magazine, which was based on a really, really interesting book called immediacy", "tokens": [50364, 341, 337, 28746, 11332, 11, 597, 390, 2361, 322, 257, 534, 11, 534, 1880, 1446, 1219, 3640, 2551, 50744], "temperature": 0.0, "avg_logprob": -0.10170916853279903, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.0010318381246179342}, {"id": 611, "seek": 313498, "start": 3142.58, "end": 3148.18, "text": " that Anna Kornblot wrote, is that there's an intense focus on the subjective experience of", "tokens": [50744, 300, 12899, 591, 1865, 5199, 310, 4114, 11, 307, 300, 456, 311, 364, 9447, 1879, 322, 264, 25972, 1752, 295, 51024], "temperature": 0.0, "avg_logprob": -0.10170916853279903, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.0010318381246179342}, {"id": 612, "seek": 313498, "start": 3148.18, "end": 3153.14, "text": " subjective experience in and of itself. Again, as the sort of form and content of the way in", "tokens": [51024, 25972, 1752, 294, 293, 295, 2564, 13, 3764, 11, 382, 264, 1333, 295, 1254, 293, 2701, 295, 264, 636, 294, 51272], "temperature": 0.0, "avg_logprob": -0.10170916853279903, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.0010318381246179342}, {"id": 613, "seek": 313498, "start": 3153.14, "end": 3156.98, "text": " which one must calibrate your being in the world that I think at the end of the day is actually", "tokens": [51272, 597, 472, 1633, 21583, 4404, 428, 885, 294, 264, 1002, 300, 286, 519, 412, 264, 917, 295, 264, 786, 307, 767, 51464], "temperature": 0.0, "avg_logprob": -0.10170916853279903, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.0010318381246179342}, {"id": 614, "seek": 313498, "start": 3156.98, "end": 3161.7, "text": " why you have the bad Anthropocene, not the way you get out of it. The reason you have the bad", "tokens": [51464, 983, 291, 362, 264, 1578, 12727, 1513, 905, 1450, 11, 406, 264, 636, 291, 483, 484, 295, 309, 13, 440, 1778, 291, 362, 264, 1578, 51700], "temperature": 0.0, "avg_logprob": -0.10170916853279903, "compression_ratio": 1.749063670411985, "no_speech_prob": 0.0010318381246179342}, {"id": 615, "seek": 316170, "start": 3161.7, "end": 3166.18, "text": " Anthropocene is not because humans rationalize or technologize the world, but rather because", "tokens": [50364, 12727, 1513, 905, 1450, 307, 406, 570, 6255, 15090, 1125, 420, 1537, 1132, 1125, 264, 1002, 11, 457, 2831, 570, 50588], "temperature": 0.0, "avg_logprob": -0.08748483228253888, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.07352793216705322}, {"id": 616, "seek": 316170, "start": 3166.18, "end": 3170.1, "text": " they imagine that the world is basically the background for their own experience of their", "tokens": [50588, 436, 3811, 300, 264, 1002, 307, 1936, 264, 3678, 337, 641, 1065, 1752, 295, 641, 50784], "temperature": 0.0, "avg_logprob": -0.08748483228253888, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.07352793216705322}, {"id": 617, "seek": 316170, "start": 3170.1, "end": 3176.5, "text": " own experience. The true weight, the true weight of that narcissism is something that's probably", "tokens": [50784, 1065, 1752, 13, 440, 2074, 3364, 11, 264, 2074, 3364, 295, 300, 25771, 1434, 307, 746, 300, 311, 1391, 51104], "temperature": 0.0, "avg_logprob": -0.08748483228253888, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.07352793216705322}, {"id": 618, "seek": 316170, "start": 3176.5, "end": 3186.3399999999997, "text": " unaffordable in the long term. A huge thank you to icon and hero Benjamin Bratton for joining us.", "tokens": [51104, 2002, 602, 36793, 294, 264, 938, 1433, 13, 316, 2603, 1309, 291, 281, 6528, 293, 5316, 22231, 1603, 1591, 266, 337, 5549, 505, 13, 51596], "temperature": 0.0, "avg_logprob": -0.08748483228253888, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.07352793216705322}, {"id": 619, "seek": 316170, "start": 3187.3799999999997, "end": 3191.46, "text": " We hope this conversation was as thought-provoking for our listeners as it was for the two of us.", "tokens": [51648, 492, 1454, 341, 3761, 390, 382, 1194, 12, 49911, 5953, 337, 527, 23274, 382, 309, 390, 337, 264, 732, 295, 505, 13, 51852], "temperature": 0.0, "avg_logprob": -0.08748483228253888, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.07352793216705322}, {"id": 620, "seek": 319170, "start": 3191.7, "end": 3198.66, "text": " Damn. On a related note, Roberto and I are giving a symposium for foreign object in March called", "tokens": [50364, 11907, 13, 1282, 257, 4077, 3637, 11, 40354, 293, 286, 366, 2902, 257, 13240, 42161, 337, 5329, 2657, 294, 6129, 1219, 50712], "temperature": 0.0, "avg_logprob": -0.1824546482252038, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0019223480485379696}, {"id": 621, "seek": 319170, "start": 3198.66, "end": 3204.58, "text": " non-player dynamics, agency fetish, and game world as part of their deep object residency agency at", "tokens": [50712, 2107, 12, 19125, 15679, 11, 7934, 15136, 742, 11, 293, 1216, 1002, 382, 644, 295, 641, 2452, 2657, 34014, 7934, 412, 51008], "temperature": 0.0, "avg_logprob": -0.1824546482252038, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0019223480485379696}, {"id": 622, "seek": 319170, "start": 3204.58, "end": 3209.54, "text": " the computational turn led by Sapita Majidi and Razan Agarastani. You can find more information", "tokens": [51008, 264, 28270, 1261, 4684, 538, 49287, 2786, 7048, 12716, 293, 29051, 282, 2725, 289, 525, 3782, 13, 509, 393, 915, 544, 1589, 51256], "temperature": 0.0, "avg_logprob": -0.1824546482252038, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0019223480485379696}, {"id": 623, "seek": 319170, "start": 3209.54, "end": 3215.9399999999996, "text": " about this talk at foreignobjectwithak.com. Subscribe to this podcast for more analysis", "tokens": [51256, 466, 341, 751, 412, 5329, 41070, 11820, 514, 13, 1112, 13, 10611, 281, 341, 7367, 337, 544, 5215, 51576], "temperature": 0.0, "avg_logprob": -0.1824546482252038, "compression_ratio": 1.532258064516129, "no_speech_prob": 0.0019223480485379696}, {"id": 624, "seek": 321594, "start": 3215.94, "end": 3219.78, "text": " at the intersection of algorithm, subjectivity, and the arts.", "tokens": [50364, 412, 264, 15236, 295, 9284, 11, 3983, 4253, 11, 293, 264, 8609, 13, 50556], "temperature": 0.0, "avg_logprob": -0.3296099007129669, "compression_ratio": 0.953125, "no_speech_prob": 0.22982284426689148}], "language": "en"}