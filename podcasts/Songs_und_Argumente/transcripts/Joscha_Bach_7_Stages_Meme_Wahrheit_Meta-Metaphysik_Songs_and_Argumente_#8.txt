So, for me, it was with the Meme that I had this ghostly flash and thought, okay, I notice
that when I look back, I had a certain intellectual development and it's not quite
like it's in the memes, but it's such a thing that I also notice with other people
when they think about how logic works. And I thought, okay, we could just
express it with this meme as if it were a journey. And of course it's not a
journey, but it's more of a branch out into different branches and then you end up in
different areas of the meaning space. But for myself, of course, I can
understand my own path. And this first step is one that I haven't stayed in for a long time.
It's about two and a half thousand years old and goes back to Plato and Aristotle. And these
three laws of logic or laws of thought are derived from the syllogisms, i.e. from the forms
in which linguistic logic worked, from the point of view of Aristotle, who tried to
systematize it. And if you look at it from today's point of view, i.e. with the terms of logic
of Tarski and others, then you have the impression that he was right about 80 percent, which is
somehow much better than most contemporary philosophers, so not so bad. And the three
rules he describes are on the one hand the law of identity and almost identity so that a thing
cannot be itself if you do something about it or take it away. And that every thing
has to be self-identical for all things. This is an action with which you can describe identity.
And the next thing is that he says that a thing cannot be true and false at the same time.
So a statement, a statement, a proposition can either be true or false. And the next thing is,
every statement must be either true or false.
Then we get problems afterwards.
Yes, exactly. And I also had problems with all of these things, because at first it
looks like it would work, but I looked at my own thinking and found that
it was not like that in my own thinking. And one thing that I noticed is, for example,
that it starts with identity. When I try to describe things about their identity,
for example when I write a computer program, then it does not capture how identity really is.
So, for example, when I play a computer game myself, when are two objects in this
computer game identical from the point of view of the player? For example, two trees. I can, for example,
define an object in a computer game in the same way as the tree that I then use several times in the
computer game and put it in there. Is that the same tree or is it another tree if it has different
coordinates? That depends on the context. So I can say that it is the same tree that I
encounter several times or there are different trees that only look the same by chance.
Well, you want a functional description level and on certain selected description levels
it is the same, but it already seems to you that if you keep zooming out in the
description, then at some point it is no longer identical, no matter what you want.
There is also this famous ship of the T-Seus, which is such a philosophical paradox that
philosophers like to dig up. It is about the T-Seus building a ship and
that at some point it sank in the Mediterranean and it was so salty all around that it
remained relatively well preserved. And the archaeologists then pull it out at some point,
it is of course somehow weathered and they decide to restore it and take the
individual boards off the ship and replace them with new boards of the same shape and
color. And gradually the ship is restored and looks the same as before and
next to it is a pile of boards. And that is the same ship as before, just renewed. And now
someone else comes and takes this pile of boards and builds the original ship
together again. And what is the ship of the T-Seus? Intuitively, it is
not that easy to find out in which branch we are now entering. And what we
find out is that this identity is not at all a natural category, but a
cognitive category that we apply under certain conditions. And what these
concepts, what these identities are all together is, from my point of view, that we add a world
line to the thing. So we say that this object exists with a series of instances in time and
we therefore bind the instances of this object into one single object. And what is important
is not that it is identical in nature, but only that they share the identity. So this
property of identity and that means in principle have to have a kind of common
biography. And this attribution of the biography, i.e. an imaginary property,
which objects have together over time or over different observations,
that is the essence of the identity in the way I use it in my head. And this
equality of property, which I have, for example, in a certain logical system,
that these are two objects that are identical when they share all properties, is a much
narrower form of identity that I can submit to the thing, i.e. to my identity concept,
but only under certain conditions. We see that we can define identity in different ways
and from this we get different mathematical concepts in which identity can exist.
The next thing can be true or false. For example, I could say I have a variable
in a computer program and that is a variable that can contain bulges, i.e. a truth value,
binaries in this case, which you can have, for example, a logical true or false. And I can
also define a ternary logic, i.e. which I have true and false and undetermined,
or a probabilistic logic in which I have 100 percent or zero as true and false and the
things in between are then further logic values. And in our own head we realize
that we can use different such logics and convert between them under certain conditions.
So a logic that is binary and only knows true and false and a logic that is true,
false and undetermined. And that with the excluded third is even more complicated,
because this logic works what Gödel has found out under certain conditions no longer.
That is, these original laws of logic do not allow these axioms to be
maintained under all conditions. For example, when I allow infinity,
when I look at many features that lead to contradictions. And that was for Gödel,
who thought that he could maintain the axioms, a catastrophe. But for a computer scientist
this is not such a big problem, because the computer scientist says, okay,
in this programming language I define my logic and my true and false in a very specific way.
And that leads to the fact that I have the opportunity to calculate what I think in
these and all conditions for true and false. There are still essays in which it is done as
these three laws of thinking are absolutely elementary. And then maybe a fourth,
that it is all basic. So, for example, biological organisms have the ability to
compare things and all this is derived from them. And maybe it's very deep-seated,
but from my point of view it doesn't seem deep-seated. I have the impression that it doesn't
lead very far, I can't build a mind out of it.
Logic cannot provide absolute truth, because it is based on unprovable axioms.
Then it's not 2 and 2, 4, but 2 and 2 is 5. So maybe I could take an axiom system
that works like that. And that's the next step you could take in your thinking,
that you discover that mathematics and logic are in some way popular, that you can simply
derive another logic from it if you use other rules. But that doesn't necessarily
lead far in the end, because most of these logics are not very interesting, in the sense that they
don't lead to interesting structures. Namely, not to structures with which I can describe
any reality or interesting system. And the logics, the interesting
structure forms with which I can describe worlds and arithmetic and so on,
there are relatively few of them.
Level 3. Truth depends on its consistent definition in a formal language. It can be
defined in any way we want.
As far as I understand it, that means that the truth is a predicate that we
assign entities in our language. Is that right? What do you think I don't understand then?
What do you think people should understand in order to reach level 3?
There are often discussions at a place where people say define truth. And then you can
refer to Stanford and Cyclopedia on philosophy, or to Wikipedia, or to a number of other websites,
or to certain works by logicians, for example Tarski. And what we find is that they had
a few approaches to logic. In philosophy, for example, there are correspondence theories
that say that things become true by referring to facts in the real world. But then the
question arises, what is actually a fact in the real world? Or there are coherence theories
where you say something is true, when all statements that have a claim to truth are
coherent with each other in a system. That means that there are no constraints.
And usually these systems are not very precisely defined. And the really precise
definitions usually exist in formal logic. And the main traditions use axioms
that often lead to contradictions, for example, if you look at the Goethe proof,
or Tarski's undefinability theorem, which is related to the Goethe set of incompleteness.
I think I'd rather not go into detail. I think it is more important that we first
become aware of what a language can say about the world. And in this tradition
in which we are right now, it is the case that a language cannot say anything about a world
outside of itself. That is different from natural languages, because the natural
languages ​​that we use to communicate with each other say something about a perceptual
world and about a mentally represented world that is not captured in the natural language at all,
but which can show the natural language. And we share enough of this language of thinking
in which we can depict perceptions, so that it is good enough to show. But in the end,
we cannot prove that we are talking about the same thing if we only show, because we do not know
what the other person is showing. And in formal logic you try to exclude that. And the hope
is that what exists in the perceptual world can be depicted in a logical language. And
it is also necessary that I get the whole world into this logical language. And
what Gödel has found out is that the actions he has taken lead to contradictions. That
means I cannot build a machine in mathematics that allows this specification of the language that
Gödel has used to run without this machine flying around your ears,
so that this computer crashes. And it is because the actions that he has used
to run without this machine that the logic is state-free. This means that the truth exists
regardless of the processing process of a program. So you do not have to be determined step by step
with the algorithm, but in total. This state-freedom of mathematics has a very good
property, which is why the mathematicians who want to have it, they allow me to deal with
infinity. If I have to describe everything step by step, then I know how to do it,
and I can decompose and put it together, i.e. conduct constructive mathematics.
But the disadvantage is that I never get to infinity. I can only add step by step,
but something like pi, for example, i.e. a number with an infinite number of places,
I can never completely describe in this way. And that means I can
create a system that runs continuously and, for example, emits a wave. And the wave
is a periodic process, that is, a circle that is pulled apart over time.
Pi has to be known until the end, so that the wave can run completely when the
world is continuous. And that means if I want to describe such a world in my logic,
then I have to deal with infinity and continuity. And that's all I can do
if I have functions that take up an infinite number of bits at once as an argument,
and then perform operations over this infinite number of bits at once, and in the end
also be able to output an infinite number of bits again. If I want to do this step by step,
it doesn't work. And what Gödel found out is that the assumptions that he had about
truth in this language lead to contradictions. And how did he do that?
He first found out that the language should not refer to the outside world,
but it should build everything in itself. And if the language can only refer to things in itself,
then you can't talk about yourself. And that's a problem. How can I
make the language talk about itself? And the solution is that I have to build the language
in itself. So the language has to be so powerful that it can build itself from scratch and that
performance can build itself. In order to do that, in order for a language to be executed,
I need a computer, a system that can, so to speak, go from one step to the next
step. When Gödel did that, that was at the beginning of the 30s,
and he had already started to think about it beforehand, there was no computer yet.
So he looked for one that he could take and he actually discovered one. And those were the
piano axioms. The piano axioms can be derived from all the natural numbers.
And the nice thing is that it's really just a handful of rules with which you can define
how they look. It's basically a kind of number stream that consists of discrete units,
so if one is bigger. And what you need is an operator with which you can practically
take one step forward and backward. And if you use it several times, then you can define the
addition as simply the repeated forward and backward. And the multiplication
can be done by repeated addition. Addition, so to speak, by counting, actually. Yes,
so just counting is basically this teleporting on the number stream. And I can,
by not just taking one step forward and backward, but teleporting. I do that by
defining an operation, a function, that simply consists of the
automating of the forward and backward. And the multiplication is, so to speak,
the automating of the addition, by doing that several times. And the relationship
between the addition and the multiplication is the exponentialization. The cool thing is,
I can define an operation with which I project the two numbers in a room and
perform an addition in the room and then project it back, then with a logarithm,
that's the reverse of the exponentiation, and then it's equivalent. That means,
what it shows is that addition and multiplication are the same if I send them through such a
projection. And this projection is the exponentiation or the logarithm back. And
that means, I can build up the whole math step by step, just by taking these
natural numbers. And the multiplication, so to speak, when the addition is
teleporting, then it's the multiplication of zooming. I can zoom in and zoom out,
and by doing that I can make numbers that are smaller than whole numbers,
rational numbers. And a rational number I can describe as the ratio between
two whole numbers. That's a break. And that means, just a few of the whole numbers
results in any rational number. And Pythagoras hoped, that's it now,
more won't come. But then he found out, and the people in his cult, in his
Mathematica Club back then, that there are still irrational numbers. That means,
those that unfortunately cannot be described as such, for example Pi or the root of two.
And that's a terrible result, because it's the idea that you can lead the whole world from the
integers, from the whole numbers, to nothing. And the legend says that this guy,
who secretly betrayed the world, he shouldn't have betrayed it, he let him kill him,
because it was really a secret that the world shouldn't find out. And from my point of view,
Pythagoras was right. The world wasn't ready. The thing is, that the mathematicians were confused
by that, because they thought there are these irrational numbers. And the computer scientists
later discovered that there are these irrational numbers not as numbers, not as values,
but as functions. That means, I can define such a program that calculates Pi for me,
and this program does that little by little, there are more and more places for me, Pi makes me
more and more precise. But it will never go to the point that it gives me the last place.
And that also means that there can be no computer program from this point of view
that creates a world in which a perfect circle takes place. But the circles that
are described are always rounder, so to speak, but they are never completely round.
And the world in which we are is not necessarily one in which round circles exist,
but it is one in which, in the end, the resolution becomes smaller and smaller,
but never infinitely small, but at some point the world falls apart in quantum effects.
And it never becomes continuous, but it is always continuous, continuous.
And from this point of view, there is no geometry. The geometry is not a real part of
mathematics in the calculation, but something that exists only in the scope of approximation.
That is, so to speak, the mathematics of the numbers that are too big to count them.
And in an area where numbers are too big to count them, that is the world in which we
are in general, where too many molecules are there to observe them individually,
there are sometimes operators that, so to speak, converge at the limit.
And they can be calculated efficiently. And the more the numbers get bigger,
the closer you get to a certain value. And this area is exactly the geometry.
And our own brain discovers a geometry to describe the world,
a certain function with which we can describe continuous surfaces and waves in the ocean,
and things that look like a circle and where we can zoom in at will within the resolution of our brain
and can find simple algebraic expressions with which one can calculate that, so to speak.
And we can also do that in computers. And from this point of view of constructive
mathematics, the world is just like that. So there is no continuous mathematics.
But before the calculation was discovered as a principle and the computer was discovered as a
principle, mathematicians assumed that there was a continuous world.
And what Gödel discovered was that this world, which we have, of course,
foreseen and the languages ​​we have built for it, lead to contradictions.
And that has shaken him very much. And from the point of view of computer scientists,
that is not a problem at all. Now let's go back to what Gödel said.
So we were at level three.
So we were there, the other team was looking for a computer.
The computer he found were the natural numbers.
And the great thing is that he just saw that the natural numbers with which I can
calculate any things. And so I can treat them as if I had a computer.
How can I make logic with it? And his idea was that I can write down my logic with a
formal system, i.e. as symbols, letters and characters.
For example, if I make a sign for and and a sign for or and a sign for a follows from
b and then I can have a sign for variables and then I have such a small alphabet.
And I convert that into numbers.
And ASCII code has not yet been invented, i.e. this American Standard Code of
Information Interchange, which was later solved by Unicode in the computer.
That's why he converted his alphabet into powers with prime numbers.
It was mathematically super clever.
It's called Goedelization and it's one of the main points when you try to understand
the sentence by Goedel, how he exactly converted the letters into numbers.
But that's not important, ASCII works just as well.
With huge numbers, definitely pretty fast.
Yes, but basically it's exactly like the numbers in the computer.
In your computer, there are only numbers in there that are written down with b and n
and you just have an alphabet that you can convert into numbers.
And the cool thing about it is that you can only take these numbers and that you
can develop formulas or functions that calculate your logical operations.
They just say, if these and those numbers are there with which my logical expressions
are written down and I calculate them together, how can I represent arithmetic,
simple things on peony axioms, on the operations in the natural numbers
with which I can calculate how the logic continues, what is the consequence of the logic.
So he built a practical computer out of natural numbers with which he could calculate
logical expressions, which is very clever.
Turing took a different path.
Turing invented the Turing machine for this.
The Turing machine is a tape that is infinitely long and has discrete symbols on it
that can be edited with a typewriter.
And the typewriter has a series of evidence with a small table with which it says,
if I read this symbol, I replace it with the following symbol, change my own condition
and go one field to the right or left.
And with that he can describe any operation.
And this Turing machine is closer to how our current computer works,
because there is an address pointer that points to an address in the memory of the computer
and in there is not a number from a certain alphabet,
for example a byte, 256 different values.
And I can interpret them as letters or numbers and then I can define operations
with these letters and numbers that I connect in a certain way to execute any computer program.
And with any computer program I can then define any logic.
That means they came to it on different paths.
And at the same time with Turing, who was a student of Wittgenstein,
who tried to translate the natural language into such a kind of logic,
there was Alonso Church, who came up with a different formalization
of the lambda calculation.
And the lambda calculation is simply an attempt to define calculations
by searching and substituting on symbols.
And the cool thing is that with this Hütchen calculation,
or with the lambda calculation, or with the searching and substituting calculation,
you can not only do regular expressions practically,
but you can also calculate the Turing machine with it.
And vice versa, you can define the Turing machine that calculates the lambda calculation.
And you can define the Turing machine that calculates the natural numbers.
That means they are all equal.
And the idea that all these calculation systems can do exactly the same thing
is the Church-Turing thesis.
So after naming Alonso Church and Alon Turing,
who had different approaches to calculation and found that they both can do the same thing.
So that we have a practical amount of languages that can do the same thing,
that are equal in their expressive power.
Yes, and Gödel was, so to speak, a pioneer of the whole in his thinking,
who did this with the natural numbers.
And in the end he found something similar.
And he found that I can, in my language,
which calculates itself by translating it into natural languages,
express logical expressions.
And these logical expressions can be self-reliant,
because I can now build the language on myself.
And I can unfortunately have linguistic expressions that define the truth
and then say, I am wrong myself.
And that is ultimately what Tarski did,
he expressed expressions that were self-reliant and said, I am wrong myself.
And that is not compatible with our three basic rules of logic,
because now the thing is not true or false,
but it is something else.
It changes all the time, if you would evaluate it with a computer.
And for an computer scientist this is not a big problem.
The computer scientist simply says,
in my programming language,
you try to write the lambda calculation efficiently as computer programming language,
I can define the truth with true and false.
And then I have a function that calculates it for my expression.
And now I calculate it and determine
as long as I haven't calculated it, it has no value.
And if the thing is self-reliant, it has no defined value,
but it constantly changes.
So no problem, I just defined it like that.
If you don't like it, then define it differently.
And in mathematics, is that a problem, at least in the way we understand it now?
In classical mathematics, exactly.
In pre-constructive mathematics.
But there was a time in mathematics,
before the mathematics of the way came
and the computation was invented as a concept,
already mathematicians like, for example, Brewer,
who said,
what we are doing here is not fine.
It leads to contradictions that may not be cleared up.
And he said,
the problem I see is infinity.
Maybe we shouldn't allow infinity.
And then there was,
that was called intuitionism in mathematics.
And this intuitionism in mathematics
did not go very far.
Most mathematicians found what he said not convincing.
And I think that's because
he didn't express his intuition well enough.
He saw something
and tried to formalize it,
but not so that the other mathematicians
saw everything he meant.
And that is in a certain way
that it fell on our feet
only later became visible.
And where you could guess that
were things like Hilbert's Hotel.
Hilbert's Hotel is a hotel
that has a lot of rooms.
And these rooms are all fully booked.
And now a new guest comes on the way.
And the great thing is that this hotel
can accommodate the guest
because the porter simply
calls all rooms
and tells all guests,
go all one room number to the right.
And because it has an infinite number of rooms,
it's not a problem.
We now have one room free
and can accommodate the new guest.
And that's kind of very sad, isn't it?
What do we do now?
Then the porter says to all guests,
please go to the double room number of yours.
And voila, we now have
an infinite number of rooms free
and can accommodate an infinite number of buses
with new guests.
And the mathematicians,
some of them thought it was good
because it's nice when you
get something out of nothing
and even get a lot.
But other people get stomach pain from it.
Doesn't lead to problems at some point.
And one of the problems
was seen when Kantor
dealt with it.
And then he found that
when we define numbers in such a way
that it is the cardinality,
that is, the number of members of a set,
that is, how many we count,
we define number by number.
We simply say that a number is a number
of things that are in a quantity, for example.
Then we can say that the number of members
that has a quantity is the number.
And there is an empty quantity,
and that corresponds to zero.
And otherwise we get all the numbers,
all the positive numbers,
as the number of members in there.
And because of the previous backward numbers
we also get negative numbers and so on
and can then deduct the rest from it.
And that leads to problems.
If we think about it,
that the amount of all subsets
of a quantity
can also be defined
and the amount of all subsets of a quantity
is greater than the amount of all members
It is clear, right?
So for the quantity of all subsets
for example, if we have four members,
these are all combinations
that these four members can have together.
And that is always more
than there are members.
And that would mean
if we have an infinitely large quantity
and an infinitely many members,
that the quantity of all subsets
must be greater than infinity.
So there are different infinities.
And can we do that?
But what happens
if we do the total quantity?
The quantity of all subsets
also has infinitely many members
and it is a very large kind of infinity
because it also contains the quantity of subsets.
But what about the quantity
of all subsets of the total quantity?
It also has more members
than the total quantity.
But the total quantity
must also contain all of them.
The quantity of all subsets.
And that leads to the unpleasant result
that the quantity of all subsets
does not have as many members
as the quantity of all subsets.
That is, it leads to contradiction
and this contradiction cannot be solved
without further explanation,
at least not in the quantity theory
because it is defined this way.
And for computer scientists
this is not the problem
because they never have the problem
that infinitely many members are in there.
The computer scientists would point out
that this does not work at all
because they can only multiply with two.
And that means there are always only infinitely many
even if they become very large.
It will never go to infinity.
The only thing I can deal with
is unlimited, unboundedness.
And unlimited means
that I will never have a size
but that I only look at an area
where it gets bigger and bigger
and where I can still add something.
And I can find statements
about how unlimited works.
And at some point the numbers will become very large
For example, I can find out
that many corners
that are
and that have very large numbers of corners
are getting closer and closer to a circle.
And that is when I describe the ratio
of circumference and diameter
it always comes closer to a constant
that I can never determine exactly
but can always determine more precisely
the more I add corners, so to speak.
I can do that.
And the computer scientists,
that you mathematicians always wanted to have.
You have never done more than that.
And if you want to describe the world,
that is exactly what you are doing.
And if you claimed that you have always had
infinity and continuity
and you write the outside of your function on it
and they unfold your functions
and look what you have secretly done
underneath the specification,
we notice that you never had infinity
but you have always only cheated.
In your code base of mathematics
you have always only dealt with finite things
and you have always developed tricks
with which you could deal with something similar
as infinity, but there was never infinity
but always only unlimitedness.
Well, and that is possibly important
when we describe reality
because the physicists
have already checked out this code base
from mathematics in the 19th century.
So long before Gödel and Thuring
and Wittgenstein
and have infinity in there.
And that is not a big problem for the mechanics
because the mechanics only works in a nutritional way
and it is also not a problem for thermodynamics
but it is a problem for the foundation physics.
So for the basics of physics.
Because where the lowest level of reality
is to be described,
if there are continuous objects in there
and the continuous objects can not even exist
because no reasonable language can exist
that does not fly around the ears,
then it means that we have to describe
the basics of physics
in a new way.
Yes, and all because we did not
listen to Brauer or Brewer.
Yes, exactly.
Or Pythagoras.
Yes, Pythagoras.
Yes, and Pythagoras did not know yet
how to express it.
And from the current point of view, I would say
that the computer scientists express it correctly
by saying that it must be expressed
in a formal language that is predictable.
That means not only on my particular computer
but at least on theoretically
existing computers.
So on systems that
go from state to state in any way.
Music
Music
Music
Music
Music
Music
So level 3 meant
that the formal language
in which we define the truth must be consistent.
And with Gödel we have now noticed
that at least the classical mathematics
does not have this consistency
because we have statements that are self-contradictory.
Consistency means we have no contradictions,
we obviously have contradictions.
We have a system where we eventually come out
with computation
or with state transitions
within our formal language.
Exactly.
Franz, take level 4.
Because we are currently
moving from level 4 to level 5.
We are now at level 4.
We are now at level 4.
We are now at level 4.
This follows.
Gödel's theory proves
that logic is undecidable
and truth cannot generally be determined
by a mathematical procedure.
We have to build something else
that actually works.
And the thing is that this Gödel's
theory,
his two non-completeness statements,
the philosophers
in my opinion
did not really change the fabric.
And what the philosophers
find out when you look at
the literature and the discussion in philosophy
is, on the whole,
the mathematicians have proven
that mathematics is impotent to describe reality.
Or even the ultimate relativism proof.
We can't find any statements
because everything is contradictory.
That would only be true if the philosophers
went home, which they don't do.
But what the philosophers find out is
that the mathematicians cannot describe the world.
And that gives people
who don't understand mathematics
an advantage,
namely the philosophers.
That means that the philosophers have
possibly techniques that are not accessible
to the mathematicians with their formal languages
and that the mathematicians can use
to capture reality.
And that is a very interesting question.
It leads to the assumption
that someone like Roger Penrose,
similar to the philosopher Lukas
in front of him,
that our mind is able
to perform operations
that go beyond what computers can do.
So for example to discover evidence
and to lead evidence
that computers cannot lead.
But the thing is that the evidence itself,
is a step-by-step
conversion of a statement
into axioms.
That is an algorithm,
that is what Greg Chaitin says.
And of course there are a few evidences
that work a little differently,
but these evidences can be derived from such evidences.
What do you say about the
example of ZFC or ZF,
that at least
Russell's paradox
emerges and I think
you can also avoid
the paradoxes
that you just mentioned
with the amounts of potential
and the cardinalities.
But you also can't get
out of ZF, as far as I know.
And that means
that ZF is also
condemned to failure from the perspective.
They are all formal systems
that work under certain conditions
and do not work under other conditions.
And that means that
in order to bring the whole world into it,
only certain parts fit in.
And when I try to define
numbers myself,
and I am not a mathematician
or a philosopher of mathematics,
then it is quite informal. From my point of view,
the best way to define numbers
is a subsequent labeling scheme.
So it is a procedure
that allows me to
give places a new label,
in such a way that they are ordered.
And that gives me everything
that I want. It is simply an algorithm
with which I can represent
locations,
I can assign
a symbol to them.
And the way we
represent numbers in the simplest way
is that we say
zero, we don't need anything,
one we knock once, two we knock twice,
three we knock three times, four we knock
four times, and so on. And that
becomes very boring and long-winded
very quickly. And that's why
when we as humans
talk about numbers,
we try to compress them mentally.
And the compression algorithm
that we normally use for numbers
is, in our cultural circle,
that we form a tenor logarithm.
That means that we represent the size
of numbers. And that means
that we have tenors, one,
hundreds, thousands, and so on, and so on.
But we could also do it with other powers.
And the practical thing is
that we now have numbers,
we write numbers, describe them,
these regularities of numbers
in a small area, and we learn
a hash function. That's a function
that just represents a mapping,
an image from one number
to another, to make additions
in our head more efficient.
That means that we learn that
if we put two and three together, we get five.
And we learn that by heart.
And if we don't know how to do that,
we just count on our fingers. So we go back
to the knock once, twice, three times.
And by doing that, we can remember how
this hash function worked. And also
we have to learn how to transfer.
That means how to get from one number to the other.
That means we learn in simple algorithms
with which we can transfer in numbers
that we write cleverly and efficiently.
The tenor system is not optimal.
From my point of view,
we should have taken hexadecimal,
the sixteenth system.
Because the nice thing about a sixteenth system
is that it's a two-year potential.
So that you can
multiply and divide.
Another system that is very nice
is the twelfth system.
What is totally cool about the twelfth system
is that it has so many parts. That means
you can describe circles very well with it.
And that's why we still use the twelfth system
to describe the clock and the angle.
You can also describe everything else
well with the twelfth system.
The tenor system is kind of stupid.
That's just because someone says,
I have ten fingers, let's take the tenor system.
It's totally suboptimal.
It's almost as good as the tenor system.
But you can do something much better
with twelfth potential.
There is of course another point.
The twelfth system is so cool
because you can work so well
with binary logic,
with true force 1.0,
with one polarity.
And this one polarity,
this one noticeable difference
to get back to our starting point
is so great as the minimum
from which you can build everything.
It's optimal.
Maybe you shouldn't use a fourth system
or a fifth system as optimal and so on.
And apparently the optimal
is the third system,
the tenor logic.
Why is it optimal?
That was explained to me
by the physicist Ed Fredkin.
And it has something to do with
when I imagine
I'm building a memory chip
or a CPU,
so electronics,
and I try to build up the logic optimally,
then the question is
how does the size of my circuit behave,
how many more circuits do I need
if I want to form more possible states.
And this surface growth
that I need if I want to form more information
has something to do with
how many more digits I need.
For example, in the twelfth system
I can form 256 different values
with 8 digits,
with one byte.
So a three-digit decimal number
or a two-digit hexadecimal number.
The hexadecimal would be the FF
of the 16 digits.
That's what you traditionally
call ABCDEF
for the digits
from 10 to 16.
And in the tenor system
you would take
for example 0 and minus 1 and 1
or 0, 1 and 2
and build up all the numbers from them.
And the cool thing is
that it's more about the optimal growth rate.
And the optimal growth rate
is the number E.
And E is
2.81,
it goes on, I don't know exactly,
embarrassingly.
And 3 is the number
that's next to it.
And Fredkin
was a very wild physicist
who had totally crazy ideas.
But this idea
was checked.
There was a Russian
who built a computer in the Soviet Union
who built terrestrial logic
and they could show that they are more energy efficient
and have less optimal
circuits than the binary system.
But since the world started
with the binary system and already
adjusted to it and so much theory and practice
existed, we stayed with it.
I just understand father-dependency
like the tessitura layout and things like that.
Exactly.
We have to redesign at some point.
But it's also not bad, because in the end
it's equivalent.
And the fourth shows that
we haven't been able to use
this original mathematical logic
since Ergödel.
By the way, the Russellian Antinomy
maybe we can explain it for the listeners.
Do you want to?
The Russellian Antinomy
is either used as
Barbara's paradox, right?
There's the
razor, no, what's his name?
The hairdresser.
The one who razes everyone in his village
except the ones who
shave themselves.
The ones who don't...
We didn't catch that well.
The ones who don't shave themselves.
Exactly.
And then of course the question is
does he shave himself?
That's a contradiction.
Let's go through it so that we can listen
and not think about it.
Yes.
Do you want to do it or what?
Just go close to the microphone.
You don't have to...
So that you can think about it again.
If the razor razes everyone who doesn't shave
himself, it's of course the question who
razes the razor.
And if he doesn't shave himself,
then it means he's one of the instances
that are shaved, because he razes
everyone who doesn't shave himself.
And that means he razes himself, but as soon as
he razes himself, he doesn't shave himself.
It's always stupid when you express it in
words.
Of course, it's a theoretical amount.
The amount of all the amounts that don't
shave themselves.
If they shave themselves, then they
shouldn't shave themselves.
If they don't shave themselves, then they
have to shave themselves, because they
have all the amounts that don't shave
themselves.
Maybe we'll show you some graphs.
But why is that even a problem?
It's a problem because the
way that shaving is done in this
place, and with the language they express
an expression that is valid.
It considers all rules of this
language, and yet it leads
this sentence into contradictions.
It simply leads to the fact that you can't
make a valid statement about the
hairdresser himself and find out if
he's shaving himself or not.
It's not a problem for the computer scientists.
The computer scientists are familiar with the fact
that you can make specifications for
which there is no implementation.
That means that the thing doesn't even exist.
The customer wants it,
he says,
I want this and that,
and you as a computer scientist look at it
and say, I'm sorry, it doesn't work, I can
prove that it doesn't work.
And for the computer scientists it's not a problem at all,
they're used to the fact that the world works that way.
That the language, if you use it
inappropriately, leads to
things that are said in the language
that have nothing to do with the world.
And the only languages that can say
that always agree with the world
are the languages in which
the implementation itself takes place.
That is, there is a difference between
syntax and semantics, classically.
Semantics is the meaning
that the language has, and the syntax
are the rules by which the language
is calculated, that is, in which
the language is written and evaluated.
And originally
people in mathematics
assumed that
the syntax and the semantics
are related to each other, and that
they can be depicted on top of each other,
but that they are fundamentally different.
And in computer science
the semantics of a language is the
causal structure that the language has,
so what the computer program does at the end.
And the computer program always does
something, and maybe it doesn't do
what you want, but that's not the problem
of this language, that's your problem,
that you use the language in a way
that doesn't mean what the language actually means.
And from my point of view
you can interpret existence
in such a way that you say
to exist means to be implemented.
To be implemented means
that there is a system somewhere
that works in such a way
that what is described there happens.
And that is exactly that.
This is a very general term
of implementation. You could, for example, say
is there really money, or is it just
a way of talking about things?
And you could say, is there really money
in the way that money is implemented?
So there are symbol systems
in the world, and there are agreements
and meanings, there are causal structures
that lead to money being there.
And that exists from certain points of view,
and it doesn't exist from other points of view.
It depends on the language that I use
and the causal structures
that exist from the point of view
of this language. And there are certain
moments when the money doesn't exist,
where, for example,
a central bank looks at the thing
and says, it doesn't work anymore, we have to
change the currency.
And at that moment the money stops
to exist for a moment, and
then the money is reinvented,
so to speak, as a kind of software.
And compared to our
poor hairdresser,
the whole thing looks like
that the mathematician has taken the
liberty of using a language
that doesn't describe anything
in the borderline.
Or he has the claim
that it describes something, and
doesn't do anything about it.
Because there obviously can't be this hairdresser.
What the mathematician has done now,
he has just invented a rule
for a kind of game,
and this rule is not solvable, not solvable.
There can't be a game that works like that.
And we can prove that it doesn't work like that.
And that means, don't expect it to work like that.
We can't build a computer program
that works like that.
Very briefly, sorry.
So there is no hairdresser
who doesn't shave themselves.
Yes, there simply can't be one.
There can't be one, and you can't even
really imagine it, by the way.
That's also a bit counterintuitive.
You think, oh, there you imagine something
that you can't implement, but even the
imagination doesn't work. Even that breaks
in when you switch back and forth,
and you can't stabilize it.
That's also important.
If we now move on
to the next stage,
which says
that all consistent notions
of truth are computational,
and the Church-Turing thesis says that
all computational languages are equivalent.
So what that means is
that after Gürtel's theorem has proven
to us that logic is
indecisive and that truth
cannot be defined in general by a mathematical
procedure, it means that
we have simply misdefined truth.
If truth is defined in such a way
that it doesn't have to
rely on hairdressers who don't shave themselves,
then the truth has simply been misdefined.
That means we need
a different theory of truth.
We need a theory of truth that
can be described in such a way
that you can always determine what is true.
That means the truth depends on the
procedure with which you determine it.
And that means, if you claim that
something is true, you have to indicate
with which procedure you come to the truth.
And if this procedure doesn't exist
or you can't indicate it,
then you don't have a theory of truth.
How long did it take you to come to the truth?
That's the question.
Are you able to build a system
that takes endless steps?
It's not just a practical question,
but also a theoretical question.
Is there a hypothetical universe
in which mathematics exists
where you can do that? And it doesn't work.
You get the bounding,
but what you don't get is
the infinity.
You only get the infinity
if you assume it's already there.
It's like a little black box
from which the infinity comes out.
But when you open the box,
you don't see anything in there that makes sense.
In ZF, for example, there's the infinity axiom.
It's introduced by axiom.
Exactly. That's the box you take
and you look at a real universe
as an observer.
Real doesn't necessarily mean physical,
but some hypothetical.
For example, in a computer language,
I build a hypothetical computer.
What would it look like
if I implemented it?
What would the subroutine look like?
That's not possible.
We're still at level 4-5.
We're going over to level 5.
We refer to the mathematical logic
that Götel believed in.
And we say to Götel
that the vibration doesn't come from
the fact that you realized
that mathematics doesn't work,
but that you used the wrong mathematics.
There's a mathematics that works,
and that mathematics is
the constructive mathematics.
The modern name for constructive mathematics
is computation.
We've already talked about infinity.
It's about the fact that
you can't observe infinity.
Every observation is necessarily
a discrete, finite bit vector.
You can't observe infinitely
and then close it off.
It doesn't work.
Empirically, you can't get it in.
You can't construct it.
You can't start to formulate
or build something and get it done.
If it's infinite,
it can't be finished by definition.
If it's infinite, then you get
contradictions like the razor paradox
that we just talked about
and like the problem of holding.
I wanted to talk about holding.
The question is, why is holding
a problem at all?
What Thuring tried to do
was to train
classical mathematics
into computer programs.
But that also means
that I have computer programs
that may have to run infinitely long.
Where I can't determine
the number of steps.
The holding problem says
that there is no algorithm
that can determine for any
unknown algorithm
if it ever comes to an end.
Whether you stop after an infinite number of steps
or stay in an infinite loop.
You can prove
that the holding problem
is due to
reductio ad absurdum,
through negation
of the holding problem.
If you assume
that there is a computer
that solves the holding problem,
then you could build
a loop
that determines
that the algorithm is terminated.
Then the thing runs into an infinite loop.
If I build the thing
on the next step,
to solve the holding problem,
then the assumption that
I can solve the holding problem leads
to the fact that I can never build a computer
that solves the holding problem
for mathematical abstract reasons.
In reality,
the holding problem
does not apply to computer science.
The thing is that
normally my program
should come to an end
after an infinite number of steps.
I know how many there are.
It should be finished today.
Or it should be finished before I die.
That means I can just let the computer run
until it has a result.
If it doesn't, I just stop.
In our own consciousness,
we can only perform
so many operations in our cells
and neurons.
If we don't have a result
after a certain time,
we can't remember what happened before
because we didn't live at that time
or were already dead
or asleep.
We always have to assume
that we can only perform
until we have the result.
The results of the holding problem
are of theoretical interest,
not practical.
The theoretical interest
has to do with the basics of logic
or the basics of mathematics
when I try to translate them into computer science.
The holding problem is that
there are classes of functions
that are allowed in mathematics,
but that are not determinable in computer science.
I can't build a machine
or a theory that is able
to calculate these functions.
The problem is that
before the computer was invented
or this concept was invented,
people thought that
the evaluation of logical expressions
or the mathematics and interpretation
of language worked.
They thought that they could do more
than what any interpretation system could do.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
Let's go back to the beginning.
I went to the Berlin School of Mind and Brain
for some time.
I worked on 손역은 because I was
that didn't fit into the project that took place at the time.
But at the same time, I found out that I was allowed to learn
and organized a seminar on the philosophy of computationalism,
the basics of representation in computationalism.
Among other things, I went through the lambda calculus with the students,
step by step, how it works. It's not that difficult to understand.
The Turing machine, step by step, is also not that difficult to understand,
except for the lowest basics and why they are equivalent.
After that, I asked the students what they think is more powerful.
A majority of the students in the vote said that the lambda calculus is more powerful
because it can make things parallel and the Turing machine only sequentially.
The thing is, however, that time doesn't matter at all,
but I can represent them one by one.
And there are no statements about how fast it runs in the world.
In fact, they are exactly the same because I can represent them
one by one with a very simple procedure.
And this realization that all computational systems can do the same,
that's the Church-Turing thesis.
But that means that I can use any of these systems
to describe the entire reality.
That means I can just take a Turing machine and put it into nothing,
and then the whole universe runs on it.
And that brings us to the next point.
That means, every consistently described universe is equivalent
to some kind of Turing machine,
which is actually a finite but possibly unbounded automaton,
but we can never know what has put a Turing machine into the void.
Maybe we should briefly talk about the term automaton.
The automaton is a kind of calculation to be defined
by discrete state transitions.
And the cellular automaton is very well known.
The cellular automaton is a thing that uses cells next to each other.
They can be, for example, in 1D or 2D.
And then there is a rule that says that the configuration
of the states of the cells goes into the next configurations.
A well-known automaton is, for example, the Game of Life.
It is a very nice automaton that was discovered by John Conway,
and it is two-dimensional.
And that means that it is just a little box thing
where each cell, like on a little box of paper,
has eight neighbors around it.
So three times three is exactly the eight neighbors of the middle cell.
And each cell can have exactly one of two states,
namely living or dead, that is, filled or empty.
We can say in the Game of Life that if a cell has more than three neighbors,
then it dies of infection.
And if there are two or three, then you stay alive.
Because if there are exactly three, you come to life.
If there are less than two, more than three, you die.
Exactly, so actually only two rules and the other implicit is already in there.
And these rules lead to the fact that if you have a random configuration of cells
that they start blubbering and that after a certain time patterns are created there.
And these patterns are always periodic at the end.
And in the boring case, the period is one or zero.
And that means that nothing is there anymore.
But the patterns can also have relatively complex periods.
So very long, complicated patterns are created.
And sometimes it takes a long time for them to go over to such a periodic pattern.
And you can build very interesting structures in this world of the Game of Life,
for example the so-called gliders.
The glider is a kind of particle that plants itself.
That is, it is a pattern that changes its surroundings in such a way
that new cells come to life on its edge and die on the opposite edge of the cell.
And this is how this glider walks through the world one by one.
And the gliders that you see there, they are called gliders.
And the gliders that you have discovered, they move horizontally or vertically.
And from the gliders you can build structures that exchange information.
For example, computers, where the glider is a kind of bit that flies through the area
and then triggers a circuit that generates new gliders,
a glider cannon that periodically shoots new gliders.
And if you aim the glider cannon correctly, then you can shoot broken glider cannons
or shoot again functionally and can practically build transistor logic with it
and then build a new Game of Life on the next level, for example.
If you zoom out, it looks like the Game of Life and re-images itself.
The Game of Life is Turing complete.
And there was a competition between Steven Wolfram and Marvin Minsky
about what is the easiest cellular machine that is Turing complete.
And it is also such a simple rule that can be defined in one-dimensionally automated machines.
And that means that you look at the initial state as the tape of a Turing machine
and the rule that is applied on it is what describes how the evolution is going on.
Then you can convert that into any computer.
Who won? Was it Wolfram and Rohan?
They never really agreed because it depends on what you expect.
With Wolfram it was like this, that one of his employees published the machines that they had found all over the place
before Wolfram did that.
Wolfram was so smart that he sued the employee for copyright infringement.
I think in very rare cases in the history of mathematics,
where a mathematician was sued for copyright for publishing a proof,
which somehow made many people very angry at Wolfram.
When you talk about Turing complete systems or abstract computers,
they are actually only abstract definitions of computations,
so what this system can theoretically do.
And it turns out that such systems, Turing complete systems like the Game of Life
or the abstract computer that Goethe came up with about the piano axioms or a Turing machine,
what is the difference between these systems?
Turing complete systems like the Game of Life or the abstract computer
that Goethe came up with about the piano axioms or a Turing machine,
which is also an abstract thing, or lambda calculus, and the rules of Wolfram.
In the end, they are any pattern generator.
So all patterns that can be generated can be generated from these things.
A pattern is defined as a lot of recognizable differences.
So patterns are discrete, finite structures, but they can be very large.
And now you are at a point where you realize that when you talk about the universe,
you are actually only talking about a large collection of recognizable differences,
so a large collection of patterns.
And then you can say, well, the universe for me is the amount of all my observations,
in my case also the amount of all observations from other people or measuring instruments and so on and so forth.
But in the end, what you mean by the universe, when you refer to the observation,
is a very, very large and yet finally discrete bit vector.
And this can be produced by a Turing complete system.
That is, you are on the level that you simply say, well, from my perspective,
the necessary condition for the amount of all my observations is a Turing complete system,
some pattern generator. And from my perspective, the universe is a random pattern generator.
So that was my step. But I don't know why that's there.
That would be in the field of metaphysics, where cool people like Wolfram, for example,
dare to get out of the field of metaphysics and really want to explain.
Wolfram's idea of physics was, instead of, so to speak,
to reverse-engineer physics, that is, to transfer our observations from the world
into a series of formulas and then simplify these formulas so far
that you have less and less at the end. Why is that?
Standard model is practically half a page full of formulas,
where you need a long time to understand the individual formulas, what they mean,
but it's still half a page code, so to speak.
And Wolfram's idea is, of course, that it's still way too long.
There has to be a simple rule from which everything is derived.
And what do I think of this simple rule?
I could simply number all the cellular machines that exist
and search for something like our universe at the end.
And then I would find out which of these machines we are in.
And the question that is still open, the spoiler,
Wolfram has not been able to find these machines so far.
And the idea is, I think, brilliant to say that we are trying to search for physics from another series,
that we are trying to say all possible laws of the series,
through which the world can be expressed.
This is of course very difficult, because it could be that it is difficult
to simulate a computer program that comes out clearly over the clock,
simply because you have to look at so many elements at once.
And the other question is, where does this Turing machine come from now?
So what has this Turing machine done in the lesson?
Why is there something rather than nothing?
And that is the question that you are confronted with at this level.
And this question has no obvious answer.
By the way, with all theories that are still there,
even if people say they find computation strange,
so far everyone has had the problem that you need any first cause,
the first movement or something, why is there nothing?
Exactly. And Christianity was not so helpful,
because many of these considerations, which played a role in defining a civilization spirit,
so a total god, god is, I think we atheists often don't really understand what god means,
but god is a technical term, and god is something similar to the human self,
the self is a technical term, the self is a model of a person that is created in our brain
about ourselves, and that we use to move and understand ourselves in the world.
And this includes, for example, that we perceive ourselves as human beings,
and that we have a biography, and that we make decisions ourselves,
and have expectations of the future, and have certain identifications.
And all of this is in a self-model and definitely what kind of agent we are,
so this personal, human self.
And this is not identical to our consciousness.
Our consciousness is also a kind of self, but it is a more minimal self,
in which practically an observer perceives himself while observing,
even before he has a language or something else,
and this minimal self is a kind of attention self,
where practically the observer perceives himself as the thing that has attention.
And the personal self is already much more charged up, has much more properties,
it has a first-person perspective and the identification with the way we are in the world.
And besides this personal self, from which exactly one normally exists per human spirit,
then you have such a multiple personality disorder,
there are also selfs that can exist beyond several spirits, and they are called gods.
So gods are clusters of agentic properties that stabilize,
and which, for example via memes or through interactions with each other,
or through archetypes, that is, they are discovered regardless of each other on different spirits,
but because of the same observer, the same world and the same mathematical principles,
they lead to the fact that practically effective agents are created that exist beyond a single spirit.
And political societies are some in which the problem of coordination
over individual self-sufficient agents is solved in such a way that spirits are supported
in their formation beyond the different spirits.
That is, it is not only the case that the individual people in the world interact with each other,
but also selfs that do not define themselves as human beings,
but are there as principles beyond human beings, for example social principles.
For example, there is a god of harvest that leads people to coordinate themselves
so that they experience themselves as an agent who now brings in the harvest,
and they all have to serve the individual spirit, the individual body.
And in monotheism it is the case that all these gods are made illegal, forbidden,
and say, no, there is only one god, and that is the god of our civilization.
Originally for the Hebrews it is the god of the tribe,
and for the Catholics it is the attempt to transfer the Roman Empire,
which was relatively rational, but also partly political,
into a kind of cult, and they branched or forged it from the Hebrew cult,
and complemented it with Jesus, the Middle Eastern gnostic,
who was killed by the Romans at the time because he was too arrogant.
They take him in to define the aesthetics of this new cult,
and then they define a single god.
And we find this single god, each one of us,
by searching for the perfect god,
so after the one who is created by the fact that everyone does what should be done best.
If you think about it, you can deduce from it what this agent looks like.
Is that what is created when everyone does what should be done best,
an agent at all, a control system for future conditions?
Yes, otherwise it would be less perfect.
So it should be something that performs control functions,
and it should be the perfect thing that performs control functions.
And compared to the Hebrew god, who is then, for example,
jealous and angry, and makes a lot of decisions,
from which you ask yourself, is it optimal to use a shark shark rain,
or to kill the newborns, or something like that.
This is expanded in Christianity, so to speak.
Then we think, no, we are not looking for what this tribe thought back then,
what this entity then did, or not,
but we think about what would happen in the ideal case,
what is the perfect system.
And the philosophers of Catholicism, especially Thomas Aquinas,
thought very seriously about how to specify this god.
And he put a number of things into the specifications
that do not belong in there from my point of view.
And I say that now as someone who has no idea about theology
and who only says it from his personal point of view,
without any great demand to get it right.
But the idea that the perfect god,
even the one who created the universe,
is more or less an empirical question for me,
that I can decide a priori.
So to consider whether God is an agent,
if I try to see him as an optimal agent,
which results in everyone who discovers him serving him,
I see that one can derive this a priori,
under certain circumstances, without any conditions.
But the idea that our universe was created by him,
that does not really show up.
From my point of view, it is true that the monotheistic god
was discovered only 6,000 years ago,
and before that there are no traces of it.
And that means that it is in its form a human discovery.
And this human discovery is possibly not an absolute discovery,
but has had certain conditions
that people had to develop certain cultures
that brought them to these ideas.
And this entity may actually only arise
through the interaction of people
who discover this entity and want to serve it.
That means as a civilisation spirit,
not as a creator god who created a physical universe.
So there is a trick,
that you try to bring together the unknown
with an answer that is comfortable,
and the one with this god of civilisation,
with whom you try to discipline and harmonize your population,
and to make your behaviour coherent.
And I think that's how you get into trouble,
if you don't bring this back to an empirical question.
Practically, this also leads to logical contradictions,
because this god is not structureless,
he has properties, so that he can be effective.
He has to be structural.
And that means that he is not free of conditions.
And he is not suitable as a condition for the physical universe.
So if he is a condition for the physical universe,
then the physical universe becomes a kind of simulation
that is created by him, is conceivable.
Maybe our god is a being
that existed in the next higher universe,
and created this universe as a kind of computer simulation.
But that doesn't change the fact,
that this next higher universe
also has to be conceivable in some language.
In the broadest sense, with some mathematical language.
And that's where this god has to fall out.
How do I get this precondition-free universe
where the world is created by itself?
And this question, why is there anything at all,
is a question that I haven't seen a satisfying answer for a long time.
And for me it was the point where I realized,
okay, now it's working.
When I read a text by the philosopher Charles Sanders Peirce,
he wrote about existence and non-existence
in this short chapter.
And he states that you can describe the non-existence
by zero and the existence by one.
And then he says,
you could also describe the non-existence
by the negation of existence,
so by minus one.
So there is one way to exist
and two ways not to exist.
And I read that and thought,
that's nonsense.
You didn't discover that there is one way to exist
and two ways not to exist,
but you found labels
for existence and non-existence.
And there are infinitely many,
so unlimitedly many,
that you can create.
And what you used
and put forward for that
is a system of numbers.
And the piano axiom,
this system of numbers,
is so powerful that you can build
a whole universe out of it,
so it's not conditional.
That means you trick yourself,
you pretend that zero and one
are already there out of nothing.
And from that you talk about existence
and non-existence.
But can we talk about non-existence
out of existence?
Actually not, right?
That means we can't talk about existence
and non-existence
at the lowest level of the universe
because there is nothing there.
That means the universe itself
outside of itself
is not determined
about its existence and non-existence.
It doesn't know if it exists or not.
And that means,
if I assume it's conditional,
that the default,
is not that the universe doesn't exist
or that there is nothing there,
but that you can't know
if it exists or not.
If the universe is possible,
that means there is an existing
and non-existent branch.
And the non-existent branch
doesn't contain any observers
who will complain.
And all the observers who have complaints
are us in the existing branch.
And what we have to explain now
is that after we have
derived from the existence
and the fact that
there is nothing in the universe
that prevents that,
why are certain things there
and other things not there?
Why does the universe
have this structure
and not everything?
And the simplest answer is
that the universe is simply everything.
That means that the universe
is possibly used by all machines.
How does it look exactly?
We can take one more step
before the Turing machine.
The Turing machine
that we normally use,
this tape with the one head reader,
is a way to talk about
the calculation.
And it has an interesting feature,
it has an exact sequence state
for every starting state.
And this sequence state
is deterministic,
that means there is no coincidence,
but when the same starting state is
exactly in the same sequence state.
The difference between deterministic
and non-deterministic functions
from this point of view would be
that a non-deterministic function
that can give a different result every time,
would be a deterministic function
every time under the same conditions
that gives the same result.
And the other feature
besides the determinism of the Turing machine
is that it is linear.
That means that every state
has a different function.
But I could also define it differently.
I could make one that is stochastic,
that happens to go into one of several
whether it is linear or non-linear.
But you need coincidence.
Yes, but the coincidence only means
that I have to define a function
that gives me a different result every time.
And that means that it cannot be
compressed over time,
but it is different every time,
and you can do that,
it just gets very long.
I could also make it branched.
If I make a Turing machine
that does not have one subsequent state,
but has several subsequent states,
then it will have more branches
in every state, so to speak.
And that is, in the mathematical sense,
a non-deterministic Turing machine.
That is one that is not so defined
that I only have one subsequent state,
but is less constrained.
That means it can go into several,
because you can't only decide for one.
It's a bit of an unhappy label,
because non-deterministic sounds
a bit esoteric as if it's a real coincidence,
but you can't define it at all,
observe, formalize.
Yes, we can define the real coincidence
by simply saying that our function
must or can give us a different value every time.
But I don't quite understand that,
because the real coincidence
is the problem that you have to get
something out of nothing.
No, you don't have to.
There is, for example,
if you want to decode things,
the problem is that you can
get the key out
with mathematical operations,
which regularity you have
executed in mathematics,
to connect the key
with the text,
so that you get the result.
But there is a trick with which
you can get the key out,
and that is the one-time-pad.
The one-time-pad uses a key
that is just as long as the text you want to decode.
And you exchange it
so that you only use it once.
That means you meet
with your secret agent at the beginning,
and you both use a key
that you generate in such a way
that no one else can know,
and it's only used once.
And each of you has a copy of this key.
And you calculate this key
together with your text every time.
And everything you used,
you throw away and burn it.
And you always know exactly
where you are on your blog,
where you have this long key on it.
And because this function
is not known to anyone else,
so you interact with the other universe
until you let it interact,
this function, from the point of view
of every observer, is absolutely random.
Okay, yes, of course, from the point of view
of every observer anyway,
who often solve themselves
and think that basically
in the substrate universe
there are undetermined things
and there you don't need anything.
Yes, but that just means
in practice there is a bubble
that is hermetically separated
from the rest of the universe
and is exactly the same format.
There is no difference.
If we formalize this in a language,
it is by chance nothing else
than a source that was
nothing in connection with the universe
and there are values that have not been known to you.
Okay, I'll buy one.
So in this respect this is not a problem.
But the non-deterministic Turing machine
was chosen for another reason,
because it is deterministic.
That is, if I have a machine
that goes into several states at the same time,
that goes in parallel,
there is no coincidence in it,
but it just goes into all these states.
The non-determinism comes about
by the fact that when I am calculated
in the Turing machine,
I do not know in which branch I am.
Copies of me are in all other branches,
but there is no criterion
that tells me in which branch I go in,
because these branches
just all exist in parallel.
They are not somehow privileged from each other.
That is, it is not the case that
one is calculated more than the other
or has any properties that
differ from the others,
which you can check by looking at the others,
because the branches do not see each other
The thing that is a coincidence,
so to speak, only affects you
if you are the observer
who is calculated by the machine.
As a student, I got to know this
non-deterministic Turing machine
as an undergrad,
in basic studies,
in complexity theory,
as a method to measure
how the complexity of algorithms is.
I saw this as unimportant,
because I can emulate this
non-deterministic Turing machine
by simply giving the normal Turing machine
a stack, and every time
I do a branch,
I add the condition to the stack,
and later I go back to this point
and then have to calculate
the branch in the other direction.
This makes it completely equivalent.
The disadvantage is that
this non-deterministic Turing machine
of course needs more memory.
I have to calculate the stack
and need more calculation steps.
And vice versa,
it is also equivalent.
I can calculate the non-deterministic Turing machine
by simply ensuring
that there is only one path
that I look at.
This is a real amount,
and they are
at a certain point
equivalent,
if I take into account that
one needs more memory
and calculation time than the other.
But there are practical differences.
For example, I imagine
that our brain
is not a Turing machine,
but the individual neuron
does not go through
100% determinism in the next state
when its environment is in this and that state,
but it goes through a certain
probability in that state.
This means that if I have
a population of neurons next to each other
that have about the same environment
and have a certain amount of coincidence,
then they behave
similarly to a non-deterministic Turing machine.
That is, they are not so constrained
that they only go down one path,
but they sample a whole state space.
They not only do one function,
but they calculate several variants
of this function at the same time.
The substrate, of course,
has a limited resolution,
that is, the paths interact with each other
and this leads to the fact that
these states can collapse,
the neurons can finally vote
on the result.
But the interesting thing is that our brain
and our reasoning processes,
which our mind runs through,
have to be described in such a way that we do not
go through a sequence of states step by step,
but that we at the same time go through
a space of possible states in parallel.
And this space then votes with each other
and collapses again.
And if we try to go backwards through our thoughts,
we do not know in what states we have been,
because all these paths
have no traces left.
Nobody has noticed in what states we have all been.
And we can reconstruct a path,
that is, if we model our brain
as a stochastic,
non-deterministic Turing machine
with a limited substrate resolution,
we have a slightly different paradigm
than a deterministic Turing machine
that is calculated on a von Neumann computer.
Even if it would still be implementable on it?
Yes, it is still implementable on it, exactly.
This leads to confusion
in some philosophers,
for example the integrated information theory,
which is the consciousness theory
of Tononi and his students.
Philosophers believe in something like this.
Tononi says that
he can imagine
that you can build a neuromorphic computer,
one that behaves similarly,
even if it is built artificially,
like the distributed processes
of the information calculation of our brain,
that it can be conscious.
That it says, I feel, so I have consciousness.
But a von Neumann computer,
the one we have implemented
as a digital computer,
which works sequentially, does not.
And this leads to a problem,
namely that
we can,
through the Turing thesis,
which Tononi does not agree on,
prove that we can
emulate the neuromorphic computer
on the sequential computer.
That means we can build a computer
on the von Neumann computer
that behaves exactly like the neuromorphic computer.
They are completely equivalent.
And that means that it also says,
I feel, so I am.
In any case, it is a lie,
because it cannot feel anything
in the way it is conscious.
And that leads to the problem
that consciousness would be an epiphenomenon.
That is, that our statements,
that we are conscious,
are not caused by consciousness itself,
but we move our mouth mechanically by chance
as if it were caused by consciousness,
but it is not.
Consciousness, so to speak,
quietly on the side and observes
our physical realities in our mind,
which some people say, I have no consciousness,
others say, I have a consciousness,
and it cannot influence it somehow.
And epiphenomenonism is therefore
not a good theory,
because epiphenomenonists are not epiphenomenonists,
so they say, my consciousness
will not be physically effective,
because they have consciousness.
Consciousness does not cause epiphenomenonism.
The mouth movements of epiphenomenonist
philosophers are not caused by his consciousness.
And that is a very stupid theory,
which is why Tononi would not sign.
And that means, however,
that the integrated information theory,
if you have it in such a form,
as Tononi has it,
leads to a completely insolvent contradiction.
If you dissolve it, then only
global workspace theory remains.
But this is not a real theory,
but a framework, actually.
It is more of a framework.
But it does not explain functionally
how it comes to such states.
But that's what it means for me as a computer scientist,
I look at it with a certain indifference
and do not quite understand how you can do that.
For me, this is more of a control group,
a theory that has such an obvious logical contradiction
that a computer scientist in elementary school
recognizes that. How can you represent that?
So we have existence and non-existence at the same time.
And we are now in the second phase of existence.
And the simplest form of this
existing phase is
that we apply all operators
in parallel.
And this means that the universe simply
branches, in every step,
maximum branching,
because there is no selection function
for the operators. That means
that all operators are simply applied.
And an operator is a function
that takes a pattern from noticeable differences
and converts it into another
pattern from noticeable differences.
And all operators are simply
applied, and some of these operators
are very often applied, because they are
simple and often, and others are very rare.
And this leads to an interesting concept
of identity, because
there is no one.
What exists now is, for example, that
the photon would be, in this way,
an operator,
which always takes place
when there is an environment on which this operator
fits. So if, so to speak,
there is a photon-shaped hole in the universe,
then this photon operator is used
and shifts the environment
so that
the same hole is created,
plus a little bit further away.
And this way this photon operator
fits again in the next step.
That is, this is a sequence of
related operators
that can be applied one by one,
which leads to the fact that a pattern
continues in the space,
just like the glider in Game of Life.
With the difference that the space is not
given at all, regardless of the patterns,
but the space first arises by
the fact that I collect certain noticeable
differences in such a way that a
pattern is created that can be described as a space.
That is, the photon is an operator
or a sequence of operator
applications that lead to
that first a regularity is created
in which information can
spread and interact with
other information.
And that just results
by the fact that these operators
are possible.
And again to translate,
the base level reality,
that is, the lowest level of reality,
has nothing that goes ahead of it.
That is practically the definition of base level reality.
Every other reality that goes ahead
is practically a kind of simulation reality
where in the
parents' universe a computer
stands in some form
on which this underlying reality
runs. And the physicalism
is the idea that in our world
there is a mechanical amount of rules,
so not some refined
computer program in which
a programmer can intervene at any time,
but simply
a causally closed
amount of simple rules
from which everything emerges.
And that we are already in this base reality.
And the idea that is here
is that this base level
reality, because it has no priors,
nothing that goes ahead of it,
can be described
as simply everything that is possible
exists in it. And that
we exist in the space of all these
possibilities.
And the universe is a
tautology, so mathematically
tautology is something that exists
without conditions, for example A is equal to A
exists through the axiom of identity
in classical logic.
And that means that nothing new is said,
but it just results
from the assumption of these rules
as if there are no conditions.
Tautology are all these conditions
where nothing new is said.
And it can be described
as a non-deterministic
Turing machine,
so this machine that branches,
in this case in all branches,
and with all
Kolmogorov complexity of zero.
And Kolmogorov complexity
describes how complicated the algorithm
is.
Kolmogorov had the idea
that you can describe for different computer programs
or for different
functions how complicated they are.
By this, how long
is the program that you need
to define this function?
If you, like
Stephen Wolfram, try to find a function
from which our universe emerges,
then this function has
no length of zero, but it is an
automatic machine that needs a certain
number of steps to write it down
and a certain number of bits
to write it down.
And that would be the number of bits
that you need to write down an algorithm,
that would be the Kolmogorov complexity.
There is a problem
with Kolmogorov complexity.
When I write down the different computer languages,
they are different in length, depending on the language.
In practice, the problem is not that big,
because I can often
translate into these languages,
and this translation program is not very long,
so that I have a pretty good idea of what
the shortest is.
There are limits
where it is difficult, but in practice
the problem is not that big.
The Solomonov induction has similar problems.
Exactly.
The Solomonov induction basically describes
what a modeling system does.
For example, our mind,
Ray Solomonov had the idea
to systematize it
through a kind of Kolmogorov complexity.
He says that
what a mind does at the end of the day is that
it tries to predict the present
from the previous observations
through the shortest possible program,
that is, with the shortest possible Kolmogorov complexity.
For all pairs of current and
previous observations.
What I just said informally
is very nice to write down formally.
With this, you have a specification
for a universal modeling system,
which has the disadvantage
that it is so general
that you can't calculate it yet.
That means you have to find something
in practice that somehow
brings the Solomonov induction closer.
But back to the Kolmogorov complexity,
if we are in the base-level reality,
then there is no program
that is below this base-level reality.
That means it has to be in tautology.
That means the Kolmogorov complexity
of our universe should be zero.
That means a program of length zero.
And the easiest program of length zero,
if you do not assume
that length zero is nothing,
but all possibilities,
is simply applied to all operators.
And that means every operator is applied
wherever it fits.
There is always an operator
that fits in our world relatively often,
because there are many photon-shaped holes,
because the photon is relatively simple.
But you and I, we are operators
that are very rare, that are very rarely used,
because we need a relatively complex
environment to exist.
And that means we only appear
very, very rarely in our universe.
And then there are
some operators
that can only appear once,
namely on the one hand the Highlander.
You can only enter one of them,
it is defined like this.
And then the universe itself,
the entirety of all possibilities that exist.
And Stephen Wolfram calls it the Rolyard.
That is the thing that occurs
because you apply all operators
unlimitedly to each other.
Can you repeat the name
in English?
The entangled limit of everything.
The entangled limit of all possible computations.
But say it in English.
The entangled limit of all possible computations.
I don't like it that much,
because there are so many terms in there
that you have to explain first.
For example, what does entangled mean at the point?
And what is meant by that?
That they all hang together on one big thing.
That there is a path
in every direction, in every direction.
And limit means in the case that
it is actually unlimited.
In some places it is not unlimited.
Locally it is often the case
that when I have this Turing machine
and I have a branch,
then I don't always get
unlimited different states,
but sometimes the pattern is created by itself.
That means the branches meet
in a certain way by
creating the same patterns
on different paths.
That does not mean that they physically meet
and collide again,
but that when I try to write down
this calculation or to model from the outside,
that I often don't have so many different states
in which it goes,
and that means, for example,
that when I describe the particle,
then it is so that the individual particle
is often defined in such a way
that there is not only a possibility
in which it can exist, but there are many paths
that it can take, but many paths
then meet again. And where they meet,
there is again a convergence.
And this convergence leads to
that a part of the universe can predict
what another part of the universe will do.
And that is the prerequisite
for the creation of structure.
The structure of the universe is created
by the fact that there is control.
That is, a part of the universe can predict
what another part of the universe can do
and this part of the universe
can adjust to the other
and thus produce the local,
more stable structures.
And in a place where this does not happen,
there is no interaction and therefore no control.
And that means that there is then
no interesting structure,
but these are just random fluctuations.
And what you would expect
is that you find a lowest level
in which there are very, very many
random fluctuations,
which lead to nothing,
which are therefore not predictable
and are created by the fact that you do not know
in which path you end.
All paths are going at the same time,
you are always on a certain path of them
and in your path,
which splits up all the time,
that is, every bit in your universe
changes in a random way all the time,
there are these whole splinters
and most of these splinters
do nothing and sometimes
they swing up to a structure
and this structure gets bigger and bigger
depending on how strong
it leads to control processes.
And on the simplest level,
the control processes that occur
are particles that lead
to something like a space
being created in which
the particles can be assigned a momentum.
The momentum is the direction
from which the particles come
and the space is addressable,
that is, you can adjust to it,
what comes from this direction?
And these structures lead to
that these particles,
something like elemental particles,
form, that is, swirls that arise from these particles
that become stable and these swirls
can then, I can try mathematically
to guide which swirls are technically possible
because I apply such functions
to each other and at some point
such swirls arise from swirls
and these are the atoms
and there is a whole zoo
of such atoms that are stable
under these conditions.
And this zoo of atoms,
the physics has not yet figured out
how they can all guide them.
But the suspicion is that
there is a possibility, ultimately from the theory of numbers,
to guide the periodic table of elements
and to think about which things
are stable under which conditions
for how long and in which spaces,
how high-dimensional they are.
And what stands out, for example, is
that most things are only stable
in the three-dimensional space.
That is, the electromagnetism,
the forces with which
the atoms interact,
are ultimately all in this three-dimensional space.
So the photons and neutrinos
and so on are generations
of the same particle classes.
They all find themselves in
such a confined three-dimensional space
in which we are.
And the strong force and the weak force
are only within the atomic nucleus
and do not affect each other
over such a long distance.
Possibly this is due to the fact that
they form whirlpools in higher spaces
than in three-dimensional spaces.
And mathematically you can show that
rotation, so to speak,
is possible in two dimensions,
and in four dimensions,
and in eight dimensions.
But not more in sixteen dimensions.
They get a little more complicated every time.
And if you think about
what a whirlpool formation actually is,
for example, imagine
that you are in a bathtub
and you move your hand and observe
the whirlpools that form
on the surface of the bathtub,
most of these waves
disappear after a short time
in the background.
And the ones that are stable, so to speak,
are round whirlpools that remain
and that, like particles,
pull along the surface of the bathtub
as long as they either
go into other whirlpools
or disappear by friction,
or by interacting
with other things,
for example your hand
or something like that.
But for themselves they are stable.
This is due to the fact that
they rotate exactly into themselves.
That is, that they have an exact angle
in their momentum,
that the information remains constant
in the volume and does not disappear
from this volume, that is,
flows out into the room.
For example, if I look at rotation
in some discrete vector spaces
or something like that,
is that the mathematical formalization
of these whirlpools?
Yes, so if you think about it,
what is a rotation?
It is an operation for 2D,
a vector operation in a
two-dimensional space.
If we think about it,
what do dimensions actually mean?
We get the first dimension
by taking our whole numbers
in the whirlpooling scheme
as a kind of number stream.
We had found that
processes and sequences are sufficient
to move around on a number stream
when it is ordered.
And then we can use additions
to teleport and zoom with multiplication.
And the same thing works
with two-dimensional numbers.
Two-dimensional numbers are created
by taking our number stream
and folding it, for example.
In the computer you can create
the one-dimensional address space
where every memory cell
in the computer gets an address.
From which I can determine the process
like on a Turing machine
where one cell moves forward and backward.
You can also simply say,
I'm going 100 steps forward
and I get to the next cell of my array.
And this simple operation,
right and left, I do plus or minus 1
and up and down I do plus or minus 100.
This way I have a possibility
to fold my number stream in the computer
into a lattice.
And this lattice
is a simple way to make it two-dimensional.
There are many possibilities.
I can do this as a spiral,
then I have something like polar coordinates.
Or as a Hilbert curve
that fills space.
And there are many tricks
with which you can fill the space
in such a way that you always get
smaller 8-core points
and get such an unboundedness.
And in this two-dimensional number space
that I can calculate all together,
these two-dimensional lattice,
I can define a rotation
as an additional operation.
So first of all I can do two-dimensional addition.
That means I teleport
into two dimensions.
I can also zoom into two dimensions
by multiplying my vector,
my coordinate pair
with a one-dimensional number.
I zoom in and out.
But I can also rotate
by using the second coordinate
by multiplying with the complex number.
Rotating leads to
that I can build a structure
that remains constant
even though I move it
away from the dimension.
In the simplest case I rotate
it by 90 degrees.
And I can also rotate it
in parts of 90 degrees
if I manage to define a function
with which I can make the intervals smaller.
And the function
with which I can keep a structure constant
even though I make the intervals smaller
is a rotation.
A kind of continuous permutation.
And if I want to keep a dynamic structure
constant,
so that the information does not dissipate
under information maintenance conditions,
then it has to be a kind of rotation.
Something where the information
rotates back in itself.
And such rotations,
the easiest ones, are a multiplication
with a complex number, with a two-dimensional number.
And the thing is that
in two dimensions
this works, but in three dimensions
I can only
perform two-dimensional rotations,
not three-dimensional.
If you think about it, if we rotate something in 3D,
then it's always about an axis in a plane.
This is because in 3D
there are no complex numbers,
like an i,
in complex numbers there are no
i and j,
which have this property
that you can define multiplications
that lead to a constant
of the pattern.
This means that all three-dimensional rotations
are two-dimensional.
We only use a subset
of the three dimensions.
There is always a certain exception to this.
For example smoke rings.
A smoke ring is a torus
that rotates around a circle.
And this works
because you can compress the air.
In order for the torus to rotate,
you have to compress the air
on the inside of the smoke ring
while it turns inside and
extends outwards.
This works because the air is elastic.
That's why you can blow
air rings underwater.
Many people do it for fun,
because the air is compressible.
But you can't blow water rings underwater,
because the water is not compressible.
The compressibility of the air
means that the space is crumpled.
It is weakly four-dimensional.
This smoke ring is a rotation
that is only possible in three-dimensional space,
because the three-dimensional space
has a little more than three dimensions
if you can compress it.
In four dimensions,
I can define new rotations.
For example, I can take a ball
and rotate it around itself
so that the inside turns outwards.
This is not possible in three-dimensional space,
because the surface will overlap with itself.
But in four-dimensional space,
I have an additional dimension.
That means I have a rotation
where I rotate 360 degrees,
but it is not as precise as before.
I tried to rotate a circle,
but I have to rotate 360 degrees
another time
so that I get back to my starting point.
So I rotate 720 degrees
so that I get around it.
Interestingly,
in music there are rotations
where you have to rotate 360 degrees
so that you get back to your starting point.
These are spinors.
It is really cool
that you can get into mathematics
and derive from the theory of numbers
what rotations are possible.
The cool thing is that these spinor spaces
are the algebras in which rotations are possible.
That is the Fredkin thing, isn't it?
I don't think Fredkin discovered it.
I don't know who discovered it first.
I can't give a reference,
but a friend of mine
gave me a series of texts.
Unfortunately, I lost the source.
You have to ask who it was.
It is that you can derive
the rotations from the theory of numbers
and create the reference to quantum mechanics.
What I find interesting
philosophically and genealogically
is that with methods
of natural sciences
or computer science
in the end,
you define it as a new
rationalist program of philosophy.
Of course.
And I find that funny
that rationalism
finds the way back
into philosophy
via stem or something like that.
That is a nice way back.
I would express it a little differently.
In my opinion,
we have a number of
philosophical fundamental questions.
The famous four questions of philosophy.
What is going on?
Why does this happen to me?
What should I do?
That is the ethics.
And what can I know?
Epistemology,
ontology,
ethics and metaphysics.
These questions can be put
by computationalism
on a common basis.
Computationalism is nothing
other than the amount of all formal languages.
And the subset is implementable.
And after we have seen
that some of the formal languages
are not implementable,
there are specifications
such as the hairdresser
who cuts the beard of all men
who shave themselves,
which leads to contradictions.
We only cut out the languages
that are implementable.
And these are the computational languages.
And after I realized that all these
languages are also equally powerful,
that I can represent each one
through each other,
if they are so powerful that I can build a computer with them,
I have a system with which I can describe reality.
Every possible language in which
something can be represented,
but also every language in which
structural education is possible.
And that allows me
to unite
the philosophy into a system.
And of course I am not the first
who has done this.
But the interesting thing is that
the longer I do this,
the later the parties get.
And what is fascinating for me
is that I did not find out
anything about it in my own studies.
And I was very depressed
that these things, these building blocks,
were not put together in a way that was not recognizable to me.
And I do not have the claim
to say that this is a good alternative
to what my philosophy teachers have done
in my studies,
but in the end I am just a guy
who has existential questions
and who has the same questions from other people
as I do.
And to form a system in which I can understand
how thinking works
and who we are, and how consciousness
and language and representation
can work, but also how existence
can work. And all of this
all fits together.
In addition to the
split
by the non-deterministic Turing machine,
on the one hand,
because a lot of operators are used
in every condition,
we have a loss of information
all the time.
Because only the things that
are controllable, that are controllable
and stabilize, they stay.
And all other things are just
new to us
or they disappear completely.
Nevertheless, there are
these branches.
And that would mean that we accept
something like a multiversum theory,
unless we assume
that it is only certain branches
that are allowed to stabilize at all,
right?
That does not exclude both. The thing is
that it is often the case that several branches
are allowed to stabilize, but you still
do not know which one you are.
That means that you get a situation
that you have a double split experiment
and you do not know through which split
the photon goes, because it goes
through the whole split, but not
through your path. In your path
it only goes through one.
And you do not know which path it is.
As an observer who is embedded
and coexists with this photon,
you do not know if you are in the path
where the photon goes through the right or left split.
The only thing you can say is that
if you look at millions of photons,
then there will be the following statistical
pattern about the interactions.
And this statistical pattern
you can say in advance. And the photon
is also such a statistical pattern.
That means that this is also a regularity
that results in structures in the universe
beginning to be so predictable
for each other that they
can stabilize.
Okay, and how do I make sure
that I'm always on the right path?
Yes, I think
that's a totally nice greeting
that you can derive from it,
that you say, do you always
want to be in the best path
in the multiverse?
But the thing is that this multiverse
does not, so to speak,
arise from the fact that the physicists
have gone wild and
have connections in the world that
you can not check and that
you can never prove and then
postulate all these additional paths,
but that there is simply no possibility
to determine a priori
that there is only one path.
And that in the basic reality it is probably not
possible that there is only one path,
because there is simply nothing that
there is only one path.
I used this thing
where you use all operators on all operators
to discuss with students
who told me, yes, Wolfram
just wrote an article
about it, he has the same idea.
And then I read it and
realized, yes, that looks like the same idea.
I also tried to understand
myself with him.
In the time when I worked at
Interlabs, I held
a number of panel discussions with
people that I found interesting,
which I then invited to discuss
topics with us that interested us.
And Steven Wolfram kindly
agreed to it.
And I tried to ask him
questions about the boundaries of the role.
But unfortunately
he was not ready at the points
where we did not know the answers
to discuss further.
I think he had to figure it out for himself first.
If everything
that is possible is simply the case,
because if you do not have any
prior, no bid
to specify that
the possible is not there,
then people think very quickly, okay, then
everything I can imagine is going to happen somewhere.
And then we asked ourselves if it was the same as the cover.
Because everything that is possible means
that if you apply all operators
that all structures that arise from it
are the possible structures. But it can be very likely
that you can imagine something incomplete,
I don't know, if the unicorn
flies through or something,
which is still not realized.
And I think that would be the difference to this multiversum,
where then suddenly everything that is imaginable
has to exist, because there is everything,
versus, well, everything that
arises from the repeated application of one operator
on all rules is then there.
But do you know the difference?
Yes, of course, the thing is
the Boltzmann universe, so to speak,
the global state of the universe
goes into another global state
that is not correlated with the previous one.
Yes, it is possible,
but extremely unlikely,
because it is an extremely complex
operator, which is therefore
very rare. And that means
the probability that you are currently
in the Boltzmann state is extremely
low, because normally
structured structures in the
Boltzmann universe do not arise.
And that means that the
probability that you are practically in a sequential
or operator sequence
is much, much greater, because there are much more
possibilities, more paths on which
a universe that looks like ours
arises. That means you can't exactly
know where you are in this big fractal,
that you create by the rules, that you just
generate everything from one, all the time
with all possibilities.
It is the probability that you are
in the thing that is created
by the sequential operator application
of simple operators
is astronomically larger.
So the Boltzmann
brain requires a maximum complex
operator, in which you transfer the whole
universe in one step
into a state that is not correlated.
And again,
I want to break it down a bit
again, but on one example.
In the multiverse, one would think,
there is now also a universe
in which my hand is somehow
just above my head.
And in fact you don't know if
that is possible. You don't know if
through the repeated applications of one operator
somewhere
such a branch ends in the situation
that I raise my arm.
So I don't do it on purpose so that we
can continue to doubt whether it is possible.
But with the normal, what I meant,
multiversum theories, they would say
that everything is there,
what is possible. So do you know a little
what I mean? I know what you mean.
You just answered and maybe I didn't
really understand it, but...
Yes, the thing is that it is not clear
how we should prove
that we come to a universe through the simple
application of operators
that looks so macroscopically similar to us
and would be allowed,
but possibly does not work
microscopically. Exactly, okay.
So it could just be that many imaginable things
are not possible
in the real world. They are
still there. Even if we
think, yes, if we could
imagine it, of course the imagination is not complete
and it is not implemented in the imagination,
then it would still be possible. We had
questions yesterday. Did I
satisfy what you asked?
Yes.
Yes.
That is also a bit Wolfram's idea
that through the application of all operators
still something specific happens
and not somehow,
so everything happens, but that the roulette
still somehow has a structure or something.
Everything is possible, it is something specific
in the end. Yes.
Yes. Exactly, but it is something
not, so to speak, one percent homogeneous
or something like that, which always looks exactly
the same in every direction and so on.
You could somehow think that.
And that means,
but if we now express ourselves in this
classical language of philosophy,
then there is something
that we, well, then there is a difference
between what is possible
and what we can imagine.
You could say,
for example, that somehow God's face
appears in the clouds there or
the unicorn flies through here or something like that.
We can imagine that, but we can
perhaps not imagine
that with the complete
causality catch up to the beginning.
And that's why we don't know
whether it is actually a possibility,
even if it is a possible possibility.
Yes, that means
if we say imaginable,
then we have to make it clear that our
representations are very fluid.
We can imagine
that we are millionaires,
but the problem is to get the causal chain
that leads us to become millionaires
is very complex.
That means,
to trigger the control process
that leads to a representation
being justified,
is the problem.
And the control processes are ultimately
on the lowest level of causality
of the universe and then swing up.
And a control process means,
it's a term from the theory of regulation,
from the humanistic theory,
that you have a system that
stabilizes its own structure,
that can be described as if it were to do so.
And that means that as soon as
it runs out of certain parameters,
a counter-force appears that leads
to push it back into these parameters.
And in the case of the
whirl on the surface of the water,
it is as simple as possible.
Because with the whirl in the surface of the water,
you move your hand at the beginning
and all vectors arise in the water molecules.
And most of these lead
to the fact that the information
that is created by the whirl
is no longer recognizable.
After a short time, the waves scatter
and only a very small part of this information
leads to the stable whirl structure.
The rest
simply dissipates out.
It simply evaporates.
It evaporates into the background.
And only the part that, so to speak,
selects itself for it,
by having the exact right vector
to keep itself in this circle, stays there.
And at the next higher level,
these systems can interact with each other.
And they form stable systems
that you can squish a little.
And they come back
to their starting point.
This ideal whirl, as soon as you squish it,
it dissolves.
But in the whirl
that you see in the surface of the water,
it is already a bit more stable.
For example, the surface of the water has viscosity.
That means the molecules
target the surface of the water
in a certain way, so that it tries
to stay stable, but is also stretchable.
And that means that the thing already has a certain
robustness when you push against it
and cause disturbance,
so that it moves back to its original form
and is elastic.
Or imagine a soap bubble.
A soap bubble is such a thing
where you have a structure
in 3D
that balances several
criteria against each other,
namely the surface tension of the fluid
that forms the border of the soap bubble
and the pressure of the air inside.
And the two of them push against each other
so strongly that a stable structure
exists that can withstand many disturbances.
Or imagine
a water molecule
in which you have
two hydrogen atoms
and a oxygen atom.
And they are bound together
by the actinomatism
that arises in the atom shells
and leads to electron pairs.
You can squish the thing
to a certain extent
and it comes back to its original state.
And the things that are stable
over a wide range of squishings
are the periodic table of elements.
And those that are not, are things
that fall apart relatively quickly.
Or fall apart immediately.
And the rare elements are the ones
that fall apart very quickly
under the conditions in which they exist.
That is, they do not stay stable for long,
but fall apart when there are small disturbances.
But the interesting thing about them
is that the disturbances
are always regulated instantly.
So similar to the thermostat.
The thermostat is a controller
that only measures the current
and only regulates in the current.
And the really interesting control systems
are the ones that regulate the future.
So they are prepared
for a number of possible disturbances
in the future and are able
to intervene on future disturbances
in such a way that they now
lead the appropriate future.
We call them agents.
An agent is a controller
for future conditions.
And then it turns out that the easiest
agents that we can see in nature,
which at least I can see, are cells.
It is a kind of super molecule
that I can keep stable
over many, many types of environmental conditions
so that it can predict the future.
And the cell is a totally fascinating system.
It consists on the one hand
of a self-replicator
and on the other hand of a Turing machine.
The Turing machine has a typewriter,
the reprisome,
and the DNA.
And all of these are small tapes
that spin around additionally.
And then there is a neck entropy extractor,
a system that is able
to become stable against disturbances
by spilling these disturbances
out of the system.
And the structures on which
these disturbances are written,
such as molecules that move quickly
and are warm, are spilt out of the cell
that you collect,
do not have this information that disturbs you.
In order to be stable,
an agent has to be able to forget
the disturbances that are added to him.
Otherwise, the knowledge about these disturbances
accumulates in the information collection.
And in order to achieve this,
you have to, so to speak,
clean yourself all the time.
And if you do something clean,
something else becomes dirty.
And what becomes dirty, you have to push out of yourself.
We all live on Earth in a way
that it encodes disturbances
into particles,
mostly as speed differences
in particles.
And this statistic of speed differences
in particles is called heat.
And it is then radiated
into the deep space.
This is the sink for the whole entropy.
And we fill up these particles
by collecting fresh photons
that are nicely sorted from the sun.
And the sun is, so to speak,
a neck entropy of order
that we convert into disorder.
And we, as organisms,
exist as pumps
from order to disorder.
This allows us to maintain
our structure
and by building more and more structures,
building more and more complexity,
we are able to convert more and more
of this neck entropy
into disorder.
You just presented
a fascinating model
with which one can build
a foundation for the four core questions
of philosophy.
And you also said
that you are not the first
to do this.
But why have so few people
accepted this so far?
In physics, in philosophy,
also in mathematics.
Of course, I could say that many people
in computer science have accepted this.
You just talked about
Paul Bauer,
who has already tried
to lead this struggle
in the academy, so to speak.
And I think,
yes, exactly,
Wildberger does it too.
And I don't know, probably Ed Friedkin
did it. And there are some people
who have tried it.
And the best explanation
that I've heard so far
why they don't accept it
– well, I know a little bit
about mathematics – is because
the proofs are simply so inelegant.
That means,
if I now
do a constructive or
an intuitionist mathematics,
then I just need for some proofs
so much longer.
Before that, I could just say, yes,
assuming that it is not so
contradictory, done,
super fast,
not followed by P,
and that went
on for half a page. And if I
want to do this constructively,
then I might need ten pages
or so for it. That means, I think,
why mathematicians don't accept it
is because they first think,
yes, it works, what we do.
We don't have any problems. Why should we change anything?
That you can't
put it in a computer.
The pure mathematicians are not really interested in that.
And they find it somehow
inelegant. Do you think
that's an argument? Can you understand that?
Do you think,
while doing that,
do you have an idea
if there are people who try it?
I have recently read a little introduction
to constructive analysis.
I think it was from 1967 or so.
And then
the introduction was like, yes, and I'm sure
that in the end we will be kept right.
And not,
it won't take too long.
Now it's 50 years
or even longer, and nothing has
been done.
Do you see a way?
I have the impression that something is already happening.
Just like in philosophy
something is happening. It's just
that the rest doesn't go away.
And what very rarely
happens in science is that a group
of people who have built
a tradition for a long time
looks at themselves
and says, okay,
we're giving it up now, or we're done now.
But in the end
these currents are all self-contained
if there are enough time spaces
over time.
And the other problem is that so and so much
fits into the human mind over the lifetime.
I met
Sam Adams, a computer scientist,
an AI researcher.
I went to a conference.
He worked at IBM for many years
and said he had a very
vague feeling when he went to an AI
conference with me,
because he had the impression that he
had been out of the current
business for 20 years,
in the artificial intelligence
and that he was probably completely
detached. He had continued
building his own things all the time,
but maybe since Minsky and so on
what he read and thought back then
is already so much further
and the new generation
has gone far away from him.
And after we left the conference
and went home, he said, we have to hurry.
I mean, I'm old now, but if we can't do it,
this generation can't do it,
because they know less than we do.
And this thing that you
realize at some point, when I was young,
I had the impression that a lot
happened in cognitive science, that there was
an extremely high frequency with which
you had groundbreaking new insights
that came in that changed everything.
And now I have the impression
that every generation still has to
make all the mistakes of the previous generation
again, because you can
make things so difficult for yourself.
And finally, to make progress,
you as an individual
have to dig yourself down a little bit
from the basics and then come back
to the limit and rebuild something at the limit.
And this process takes so long
that there is not always a shortcut.
And you can't
rely on others
because there are so many schools
that don't have a clear picture
of their basics, or there are individuals
that don't have a clear picture of their basics.
And in computer science and mathematics
it is even easier.
In other sciences it is much more difficult.
So, in short,
the reason why
there is no agreement about it,
not even in mathematics itself,
but also not about the different
branches of philosophy or
of science or of social
flow that you try to recognize,
is the Tower of Babel.
The Tower of Babel is the idea that
we are able to build
the only building
that conquers the entire sky.
And the meaning of Heavens
back then was different
than what we understood later.
Today we translate Heavens
as an imaginary place that exists
in a superstitious way
and in which non-existent
supernatural agents stay.
God and angels and
the souls of the dead
or whatever.
And I think that this is a
very important point.
Also a part of the Tower of Babel.
Where Heaven and Earth
first appear is in Genesis
and it describes
that consciousness
is hanging over the substrate
and I think that happens
in every single one of us.
And the border in the substrate
is between the world model and the idea sphere.
And we realize that
in our mind there is a world model
that is what we perceive as stuff in space
not in the physical sense
but in the sense where we see people
with geometry and colors and ideas
and thoughts that touch things.
This stuff in space is a game engine
that our brain generates.
And this is a very special kind of mental state
mental state
because it is correlated to our perception.
That means we are in resonance
with the world around us
through our nervous system
that is coupled with the world
and we build this world model
and the processes around us are tracked.
This world model is stuff in space.
And in the text of Genesis 1
they call it Earth
or world.
And the world is simply the world model.
Rest extensa
in the Dekarczian sense
is not the physics
but it is a mental state
that describes the world model.
And everything else
that we have in our mind
that is not the world model
these are the heavens, the idea spheres
the space in which we have thoughts
about the world that are temporarily
decoupled and can also be
semantically decoupled from our perception.
And in fact there are these two areas
in our spirit and in every spirit
that we know.
So every human and most complicated
animals have a world model
and a space of ideas
that cannot be directly reflected
on perception in the outer environment
or the proprioception.
And if we now go back
and think about what is heaven
that is simply the idea sphere
the noosphere.
And to bring the idea into this space
is the project of the tower
to Babel in this myth.
And this project failed
because the people
did not manage to find a common language
that remained coherent.
The more people worked on it
the more special the languages were
and the more difficult they were to translate.
And that is why the tower fell apart.
So the whole tower broke apart.
God guided her somehow
or gave her the language
that she could no longer understand.
He somehow found the project dangerous.
How is that?
It can be.
But logically it is the case that
the churches have the impression
that the rationalists are trying
to explain the world and forget God
and can no longer explain
and therefore cannot
underline the world.
And if we take our original definition
of God from earlier
from this afternoon
then it was the small written God
the general gods
are simply themselves
that surround different spirits
and the big written God
is the total God
the greatest, best God
that we can construct.
And if we build a civilization
like ours, then it works by doing
even if no one is watching.
That is, the agent that we build together
the system of agency
that we create together
the system in which we control the future together
in the most harmonious possible way
that is God.
God is created by the fact that we recognize
that it makes sense to seek harmony
that we make our behavior
so coherent with each other
as we make our behavior so coherent
in our own spirit
that our hands do something together
In a way, a society should ideally work
in such a way that the individuals
do not work against each other
but are coherent.
But they are not coherent to do something similar
but coherent according to the best possible
set of structural principles
that can be discovered.
And that is what the churches
try to stabilize.
That is, they try to practically synchronize
the gods with the people
so that at the end a civilization is created
that is harmonious
and that the best possible survival
and the longest possible survival is possible for us.
That is the idea behind it.
And the scientists often do not recognize that.
And possibly the message
why the story of the tower of Babel
is told like that
it is not only told as an observation
but as a normative
there is a message in it that God wants to send to the people
is that if you believe too much
in what you do in your private languages
then at the end there is no coherent system
that breaks together in your civilization.
The interpretation is that
I do not agree with the text.
The thing is that there are computational languages
that did not discover them back then.
They did not discover the theory of the Church
they did not discover that they are all equally powerful.
And today we find that out.
We find out why it is possible
that our own spirit agrees with itself on a language
and how it would be conceivable
that one finds a harmonization
over all possible spirits
who are able to think so far
that they can shape the foundations of their own thinking
in the space of all metaphysics
and find out which metaphysics
can work
build a metametaphysics
from which one can derive all possible world models
which are coherent in themselves
and translate them into human beings.
A short question, how is it with the light?
It glows interesting
but it is not a big problem
because in front of me there are various microphones
that I can use in between.
How does it look on the camera?
We can also do it like this
that we turn the table
with 90 degrees
then the sun comes from the side.
This meme is for me
a strange thing
when you express it here and say
Franz that you want to hang it on the kitchen wall.
For me it was more of a fleeting thing
that you quickly
produce within a few minutes.
But it is a bit like that from the ...
That's why I think it's so good
because these are memes.
It is this fleeting meme-like
but there is so beautiful
idea space in it
and this combination makes it
my favorite meme so far.
The thing is that you ...
The memes work because
they capture something so quickly
but what they capture is sometimes quite complex.
For me the meme is a bit like
the famous Chinese painter
who is commissioned by the king
to paint birds
and he
sits there all the time
and sketches and throws away
and sketches and throws away and so on and so forth
and then the king comes to him and says to him
where is the bird that you should paint for me?
And the artist
sits down, takes a fresh sheet of paper
takes his brush and makes
a complex stroke and has the bird.
And the king is super
disappointed and says
why? You just did it so quickly.
What is the thing?
And he says, no, to make it
I had to become the one who can do it all.
And for me it is also like that.
For me it is the result of
thinking about all the stuff for several decades
and then trying to draw my own path
and to think about
which steps I have come up with.
And of course it is not the only
possible path and probably
not the end of the path because many details are missing.
But it is interesting to
get to know yourself.
And the great thing about this medium Twitter
is that you ...
that there are so many people on it
that you can develop very special thoughts.
And then when you
train your followers accordingly
so that they are able to get along with it
you just have among your
tens of thousands of followers always some
who can get along with your thoughts and who can interface
and can bring you further into the discussion.
And here you also have to shout out
to the expanding brain thing itself
because it really fits on it
so I sometimes still get such jitters
when I only concentrate on the pictures
together with what is there
and then it's always ...
well.
Yes, it was just such a spiritual ...
it fits together so well that I quickly tap it down
and put it on Twitter.
It is also interesting because for me I would say
if I had to think about
what my intellectual
trajectory has most influenced
then it was probably
this nerd Twitter bubble
and for most people
Twitter is not the place
where you think you can find knowledge
because at the same time so many nerdy things happen
but also so beautiful.
Sure, as soon as I get out of my filter bubble
it gets uncomfortable.
Some people think that filter bubble is something totally bad
but filter bubble is the equivalent
of a room in which you
can play a concert, for example.
Another is a box match, another is a kindergarten
and another is a
theater play and they can all
only work because they are separated
and because they can separate
for the duration of the performance.
We wrote a few more comments about this meme
which was also
part of the experience
when the meme was aired on Twitter.
One comment below was
there is no independent truth
then you answered welcome to level 2
and the person answered with a smiley
got me.
Very nice interaction.
At level 1 you think
that logic is absolute
and at level 2 you can see
that the logic depends on
the definition in the action system.
We have not yet come to the point
where we can have any action system
which is not available
so it is only level 2.
What did you answer with level 3?
What is that?
What is the definition in a formal language?
It can be defined in any way we want.
What if the universe contains
non-computable processes?
Computation with general
real-valued variables?
Level 3.
At this point we have not
taken the step to constructive mathematics
and believe that
languages that are not implementable
are suitable to
describe truth in such a way
that you can claim that all these things exist.
Truth
that can work with it
must exist in the form
that is, it must be implementable
that is, it is pre-computationalist.
Exactly, it is the case that
he took the kettlebell
but he has not yet come out
and has understood the consequence.
That is, he is exactly at the point
where he sees that these languages
that he uses to describe reality
are running in boundaries
because they run in self-contradictions
and that means that the reality he builds with
does not end and he does not see the way out of it.
Universe is a computer
but that we can build universal computers
capable of simulating any physical system.
Thoughts?
Level 4.
That he has not yet taken the step
to recognize that
the pre-computational languages
are running in contradictions.
The thing is, when we accept
that nothing can be thought of
that is not computational
and nothing can be observed
then we can not even
refer to this universe
in our thoughts.
Of course, every system
in which we speak about something
can only refer to the system itself.
If we have a representation system
in which we can talk about the universe
and think about it
then we can only talk and think about
universes that can be reproduced
in the representation system.
And to claim that they can exist
does not make sense
because every alternative to the system
leads to contradictions.
That is, the systems that we can think about
and that we can claim
that it is a universe
must be in the class of systems
that can be described in languages
that are not self-contradictory
because the others are not existent.
And also not thinkable in that sense.
Yes, that's what it means.
We can not think about it
in such a way that our language does not break.
And I think that's the big thing
that still confuses people
even further.
That they somehow think, well,
it could be that it is continuous
and they still do not notice
that the idea of continuous
has the same meaning
as a direct contradiction at the end.
Yes, but each of these ideas has confused me
for a long time.
For me it is really a progression
that I have made myself
because I had to find out why it is so and so.
And I did not have the feeling
that someone helped me, but on the contrary.
I tried to get it out
against the institutions
that explained to me that they are the axioms
and I realize, no,
your identity concept is not mine
because it does not work that way
when I try to think.
This is a complicated question.
The thing is that on the lowest level
where we do not need a memory,
so to speak, and the universe
only exists as a condition
in the space of the possible,
the question is, is there
a limitation?
So is the possible
limited by something?
Or if there is boundlessness
on the lowest level of the possibility,
then of course it leads
to an infinity.
It is just that the operators
who work on it
usually cannot lead to structures
that inevitably integrate many elements
from one step to the next.
And that means
that there is a limit
where the question for me
is not clear and I suspect
that the unboundedness
on the lowest level, because we do not
need a memory and no sequential
addition of operations,
theoretically could lead to an infinity.
But it is not
a constructive infinity.
That is, wherever we accept that the universe
is constructive, where we describe things
as transitions through other things
and the world in which
we have, for example, information
and structure formation,
wherever information and structure
formation play a role, it probably
cannot work.
And that is something where I
do not see the clear proof. I have the impression
that I see it, but I cannot
write it down formally. That is why
it is so that I hallucinate it.
But then I have another question, if you
talk about the lowest possible level
where the unboundedness becomes a real infinity,
then we get the same
problems, or that you
simply cannot keep the concept infinity
up at all, because if you look closely,
it is exactly the same.
It is above all the next
problem is that I cannot talk about it,
that is, I cannot think about it. And since I
cannot think about it, it cannot
be claimed in any way
that we are in it.
Because I cannot refer to the thing where
we are supposed to be in it.
I always imagine that when I have
continual ideas about
the universe, or space, or time,
I simply replace it with
x and not x. And then I say
that what I am currently thinking
about has exactly the same
meaning as this. It is simply
another token for contradiction.
And that is it.
But that is something where I say,
as far as I understand it logically,
there is no possibility that we are in
the universe. But it is also the case
that when I go through my one progression,
through these steps, it was often the case
that I am at a point where I say
I cannot logically understand why
there is something, instead of something else.
This question seems, in principle,
unanswerable.
And that confused me, because I thought
how can it be that there are questions that
are unanswered, because you can always find a curve.
And at some point you
realize, oh, there is this
solution. But that also shows
how fragile our own thinking is
and how unreliable it is.
And in the end, unfortunately,
that is something that we have to leave to the machines.
I wanted to read a quote from Einstein
where I wonder
if he describes
similar problems.
I had that in a planning concept, but I don't know
if you read it in such detail.
Einstein on the discreteness of space-time.
But you have correctly
grasped
the drawback
that the continuum brings.
If the molecular view of
matter is the correct
and appropriate one, that is,
if a part of the universe
is to be represented by a finite number
of moving points,
then the continuum
of the present theory contains
too great a manifold of possibilities.
I also believe
that this too great
is responsible for the fact
that our present means of description
miscarry with the quantum theory.
The problem seems to me
how one can formulate statements
about a discontinuum
without calling
upon a continuum
as an aid.
The letter, the continuum,
should be banned from the theory
as a supplementary construction
not justified by the essence
of the problem, which corresponds
to nothing real.
But we still lack
the mathematical structure, unfortunately.
How much have I already
plagued myself in this way?
The idea that one
can assemble the universe
from discrete units,
but the idea that one
derives geometry from discrete
operations, from quantum theory
and so on and so forth,
he probably didn't have any tools for that.
And even now it's problematic.
Wolfram tries to do that,
but it's not trivial.
I don't know how much mathematics
is in this area, but Einstein
has seen this possibility.
He has been confronted with the fact
that there are singularities in space-time.
And singularities
are nothing positive.
That is, that the mathematical description
no longer works, that you
practically get a division by zero
and the value is no longer valid
that you get.
So singularity means
that practically the dimensions
in which a model is described
are not sufficient to
grasp the dynamics of the model.
And he then noticed
that one can push the singularities
in space-time around,
i.e. these places where space-time
breaks down and then practically push them
into the middle of masses.
And that means that in the middle of masses
there is a point where space-time
no longer really works, but maybe
it's not that bad, maybe it has to do with
how space-time is knotted in a certain way
and runs out of the rudder.
But he was confronted with it relatively early
that the possibility exists that
the space-time description that he makes,
i.e. the continuous geometric space-time
is not suitable to describe the absolute
basics of physics.
And the other thing is this confrontation
with the quantization of the universe
that he saw relatively early.
Einstein was a very broad thinker
who did not focus on a certain idea
because he knew
how he made his ideas.
He was not a member, he was not a type
whose theories came about
that he took over from other teachers
that he did not modify easily.
Instead, he is someone who has invented
these ideas all by himself and together with
other people who have also invented ideas
that were discussed about it.
That is, it is a completely different approach
than what scientists experience today
who practically think about subpar themes.
That is, they marry into a school from the beginning
and try to take over the concepts
and try to understand as well as possible,
to make them fit and then add an epsilon.
So since it was clear to him
that he had invented this continuous space-time
to describe the world,
that it was a good formalism,
it is also clear to him
that one can get out of it
and find alternatives.
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
I love you, I love you, I love you, I love you, I love you, I love you, I love you,
I love you, I love you, I love you, I love you, I love you,
I love you, I love you, I love you,
I love you, I love you, I love you, I love you,
I love you, I love you, I love you, I love you,
I love you, I love you, I love you,
I love you, I love you, I love you,
I love you, I love you, I love you,
I love you, I love you, I love you,
I love you, I love you,
I love you, I love you,
You
